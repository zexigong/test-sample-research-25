messages:
- content: You are an AI agent expert in writing unit tests. Your task is to write
    unit tests for the given code files of the repository. Make sure the tests can
    be executed without lint or compile errors.
  role: system
- content: "### Task Information\nBased on the source code, write/rewrite tests to\
    \ cover the source code.\nRepository: seaborn\nTest File Path: seaborn\\test_statistics\\\
    test_statistics.py\nProject Programming Language: Python\nTesting Framework: pytest\n\
    ### Source File Content\n### Source File Content:\n\"\"\"Statistical transformations\
    \ for visualization.\n\nThis module is currently private, but is being written\
    \ to eventually form part\nof the public API.\n\nThe classes should behave roughly\
    \ in the style of scikit-learn.\n\n- All data-independent parameters should be\
    \ passed to the class constructor.\n- Each class should implement a default transformation\
    \ that is exposed through\n  __call__. These are currently written for vector\
    \ arguments, but I think\n  consuming a whole `plot_data` DataFrame and return\
    \ it with transformed\n  variables would make more sense.\n- Some class have data-dependent\
    \ preprocessing that should be cached and used\n  multiple times (think defining\
    \ histogram bins off all data and then counting\n  observations within each bin\
    \ multiple times per data subsets). These currently\n  have unique names, but\
    \ it would be good to have a common name. Not quite\n  `fit`, but something similar.\n\
    - Alternatively, the transform interface could take some information about grouping\n\
    \  variables and do a groupby internally.\n- Some classes should define alternate\
    \ transforms that might make the most sense\n  with a different function. For\
    \ example, KDE usually evaluates the distribution\n  on a regular grid, but it\
    \ would be useful for it to transform at the actual\n  datapoints. Then again,\
    \ this could be controlled by a parameter at  the time of\n  class instantiation.\n\
    \n\"\"\"\nfrom numbers import Number\nfrom statistics import NormalDist\nimport\
    \ numpy as np\nimport pandas as pd\ntry:\n    from scipy.stats import gaussian_kde\n\
    \    _no_scipy = False\nexcept ImportError:\n    from .external.kde import gaussian_kde\n\
    \    _no_scipy = True\n\nfrom .algorithms import bootstrap\nfrom .utils import\
    \ _check_argument\n\n\nclass KDE:\n    \"\"\"Univariate and bivariate kernel density\
    \ estimator.\"\"\"\n    def __init__(\n        self, *,\n        bw_method=None,\n\
    \        bw_adjust=1,\n        gridsize=200,\n        cut=3,\n        clip=None,\n\
    \        cumulative=False,\n    ):\n        \"\"\"Initialize the estimator with\
    \ its parameters.\n\n        Parameters\n        ----------\n        bw_method\
    \ : string, scalar, or callable, optional\n            Method for determining\
    \ the smoothing bandwidth to use; passed to\n            :class:`scipy.stats.gaussian_kde`.\n\
    \        bw_adjust : number, optional\n            Factor that multiplicatively\
    \ scales the value chosen using\n            ``bw_method``. Increasing will make\
    \ the curve smoother. See Notes.\n        gridsize : int, optional\n         \
    \   Number of points on each dimension of the evaluation grid.\n        cut :\
    \ number, optional\n            Factor, multiplied by the smoothing bandwidth,\
    \ that determines how\n            far the evaluation grid extends past the extreme\
    \ datapoints. When\n            set to 0, truncate the curve at the data limits.\n\
    \        clip : pair of numbers or None, or a pair of such pairs\n           \
    \ Do not evaluate the density outside of these limits.\n        cumulative : bool,\
    \ optional\n            If True, estimate a cumulative distribution function.\
    \ Requires scipy.\n\n        \"\"\"\n        if clip is None:\n            clip\
    \ = None, None\n\n        self.bw_method = bw_method\n        self.bw_adjust =\
    \ bw_adjust\n        self.gridsize = gridsize\n        self.cut = cut\n      \
    \  self.clip = clip\n        self.cumulative = cumulative\n\n        if cumulative\
    \ and _no_scipy:\n            raise RuntimeError(\"Cumulative KDE evaluation requires\
    \ scipy\")\n\n        self.support = None\n\n    def _define_support_grid(self,\
    \ x, bw, cut, clip, gridsize):\n        \"\"\"Create the grid of evaluation points\
    \ depending for vector x.\"\"\"\n        clip_lo = -np.inf if clip[0] is None\
    \ else clip[0]\n        clip_hi = +np.inf if clip[1] is None else clip[1]\n  \
    \      gridmin = max(x.min() - bw * cut, clip_lo)\n        gridmax = min(x.max()\
    \ + bw * cut, clip_hi)\n        return np.linspace(gridmin, gridmax, gridsize)\n\
    \n    def _define_support_univariate(self, x, weights):\n        \"\"\"Create\
    \ a 1D grid of evaluation points.\"\"\"\n        kde = self._fit(x, weights)\n\
    \        bw = np.sqrt(kde.covariance.squeeze())\n        grid = self._define_support_grid(\n\
    \            x, bw, self.cut, self.clip, self.gridsize\n        )\n        return\
    \ grid\n\n    def _define_support_bivariate(self, x1, x2, weights):\n        \"\
    \"\"Create a 2D grid of evaluation points.\"\"\"\n        clip = self.clip\n \
    \       if clip[0] is None or np.isscalar(clip[0]):\n            clip = (clip,\
    \ clip)\n\n        kde = self._fit([x1, x2], weights)\n        bw = np.sqrt(np.diag(kde.covariance).squeeze())\n\
    \n        grid1 = self._define_support_grid(\n            x1, bw[0], self.cut,\
    \ clip[0], self.gridsize\n        )\n        grid2 = self._define_support_grid(\n\
    \            x2, bw[1], self.cut, clip[1], self.gridsize\n        )\n\n      \
    \  return grid1, grid2\n\n    def define_support(self, x1, x2=None, weights=None,\
    \ cache=True):\n        \"\"\"Create the evaluation grid for a given data set.\"\
    \"\"\n        if x2 is None:\n            support = self._define_support_univariate(x1,\
    \ weights)\n        else:\n            support = self._define_support_bivariate(x1,\
    \ x2, weights)\n\n        if cache:\n            self.support = support\n\n  \
    \      return support\n\n    def _fit(self, fit_data, weights=None):\n       \
    \ \"\"\"Fit the scipy kde while adding bw_adjust logic and version check.\"\"\"\
    \n        fit_kws = {\"bw_method\": self.bw_method}\n        if weights is not\
    \ None:\n            fit_kws[\"weights\"] = weights\n\n        kde = gaussian_kde(fit_data,\
    \ **fit_kws)\n        kde.set_bandwidth(kde.factor * self.bw_adjust)\n\n     \
    \   return kde\n\n    def _eval_univariate(self, x, weights=None):\n        \"\
    \"\"Fit and evaluate a univariate on univariate data.\"\"\"\n        support =\
    \ self.support\n        if support is None:\n            support = self.define_support(x,\
    \ cache=False)\n\n        kde = self._fit(x, weights)\n\n        if self.cumulative:\n\
    \            s_0 = support[0]\n            density = np.array([\n            \
    \    kde.integrate_box_1d(s_0, s_i) for s_i in support\n            ])\n     \
    \   else:\n            density = kde(support)\n\n        return density, support\n\
    \n    def _eval_bivariate(self, x1, x2, weights=None):\n        \"\"\"Fit and\
    \ evaluate a univariate on bivariate data.\"\"\"\n        support = self.support\n\
    \        if support is None:\n            support = self.define_support(x1, x2,\
    \ cache=False)\n\n        kde = self._fit([x1, x2], weights)\n\n        if self.cumulative:\n\
    \n            grid1, grid2 = support\n            density = np.zeros((grid1.size,\
    \ grid2.size))\n            p0 = grid1.min(), grid2.min()\n            for i,\
    \ xi in enumerate(grid1):\n                for j, xj in enumerate(grid2):\n  \
    \                  density[i, j] = kde.integrate_box(p0, (xi, xj))\n\n       \
    \ else:\n\n            xx1, xx2 = np.meshgrid(*support)\n            density =\
    \ kde([xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\n\n        return density,\
    \ support\n\n    def __call__(self, x1, x2=None, weights=None):\n        \"\"\"\
    Fit and evaluate on univariate or bivariate data.\"\"\"\n        if x2 is None:\n\
    \            return self._eval_univariate(x1, weights)\n        else:\n      \
    \      return self._eval_bivariate(x1, x2, weights)\n\n\n# Note: we no longer\
    \ use this for univariate histograms in histplot,\n# preferring _stats.Hist. We'll\
    \ deprecate this once we have a bivariate Stat class.\nclass Histogram:\n    \"\
    \"\"Univariate and bivariate histogram estimator.\"\"\"\n    def __init__(\n \
    \       self,\n        stat=\"count\",\n        bins=\"auto\",\n        binwidth=None,\n\
    \        binrange=None,\n        discrete=False,\n        cumulative=False,\n\
    \    ):\n        \"\"\"Initialize the estimator with its parameters.\n\n     \
    \   Parameters\n        ----------\n        stat : str\n            Aggregate\
    \ statistic to compute in each bin.\n\n            - `count`: show the number\
    \ of observations in each bin\n            - `frequency`: show the number of observations\
    \ divided by the bin width\n            - `probability` or `proportion`: normalize\
    \ such that bar heights sum to 1\n            - `percent`: normalize such that\
    \ bar heights sum to 100\n            - `density`: normalize such that the total\
    \ area of the histogram equals 1\n\n        bins : str, number, vector, or a pair\
    \ of such values\n            Generic bin parameter that can be the name of a\
    \ reference rule,\n            the number of bins, or the breaks of the bins.\n\
    \            Passed to :func:`numpy.histogram_bin_edges`.\n        binwidth :\
    \ number or pair of numbers\n            Width of each bin, overrides ``bins``\
    \ but can be used with\n            ``binrange``.\n        binrange : pair of\
    \ numbers or a pair of pairs\n            Lowest and highest value for bin edges;\
    \ can be used either\n            with ``bins`` or ``binwidth``. Defaults to data\
    \ extremes.\n        discrete : bool or pair of bools\n            If True, set\
    \ ``binwidth`` and ``binrange`` such that bin\n            edges cover integer\
    \ values in the dataset.\n        cumulative : bool\n            If True, return\
    \ the cumulative statistic.\n\n        \"\"\"\n        stat_choices = [\n    \
    \        \"count\", \"frequency\", \"density\", \"probability\", \"proportion\"\
    , \"percent\",\n        ]\n        _check_argument(\"stat\", stat_choices, stat)\n\
    \n        self.stat = stat\n        self.bins = bins\n        self.binwidth =\
    \ binwidth\n        self.binrange = binrange\n        self.discrete = discrete\n\
    \        self.cumulative = cumulative\n\n        self.bin_kws = None\n\n    def\
    \ _define_bin_edges(self, x, weights, bins, binwidth, binrange, discrete):\n \
    \       \"\"\"Inner function that takes bin parameters as arguments.\"\"\"\n \
    \       if binrange is None:\n            start, stop = x.min(), x.max()\n   \
    \     else:\n            start, stop = binrange\n\n        if discrete:\n    \
    \        bin_edges = np.arange(start - .5, stop + 1.5)\n        elif binwidth\
    \ is not None:\n            step = binwidth\n            bin_edges = np.arange(start,\
    \ stop + step, step)\n            # Handle roundoff error (maybe there is a less\
    \ clumsy way?)\n            if bin_edges.max() < stop or len(bin_edges) < 2:\n\
    \                bin_edges = np.append(bin_edges, bin_edges.max() + step)\n  \
    \      else:\n            bin_edges = np.histogram_bin_edges(\n              \
    \  x, bins, binrange, weights,\n            )\n        return bin_edges\n\n  \
    \  def define_bin_params(self, x1, x2=None, weights=None, cache=True):\n     \
    \   \"\"\"Given data, return numpy.histogram parameters to define bins.\"\"\"\n\
    \        if x2 is None:\n\n            bin_edges = self._define_bin_edges(\n \
    \               x1, weights, self.bins, self.binwidth, self.binrange, self.discrete,\n\
    \            )\n\n            if isinstance(self.bins, (str, Number)):\n     \
    \           n_bins = len(bin_edges) - 1\n                bin_range = bin_edges.min(),\
    \ bin_edges.max()\n                bin_kws = dict(bins=n_bins, range=bin_range)\n\
    \            else:\n                bin_kws = dict(bins=bin_edges)\n\n       \
    \ else:\n\n            bin_edges = []\n            for i, x in enumerate([x1,\
    \ x2]):\n\n                # Resolve out whether bin parameters are shared\n \
    \               # or specific to each variable\n\n                bins = self.bins\n\
    \                if not bins or isinstance(bins, (str, Number)):\n           \
    \         pass\n                elif isinstance(bins[i], str):\n             \
    \       bins = bins[i]\n                elif len(bins) == 2:\n               \
    \     bins = bins[i]\n\n                binwidth = self.binwidth\n           \
    \     if binwidth is None:\n                    pass\n                elif not\
    \ isinstance(binwidth, Number):\n                    binwidth = binwidth[i]\n\n\
    \                binrange = self.binrange\n                if binrange is None:\n\
    \                    pass\n                elif not isinstance(binrange[0], Number):\n\
    \                    binrange = binrange[i]\n\n                discrete = self.discrete\n\
    \                if not isinstance(discrete, bool):\n                    discrete\
    \ = discrete[i]\n\n                # Define the bins for this variable\n\n   \
    \             bin_edges.append(self._define_bin_edges(\n                    x,\
    \ weights, bins, binwidth, binrange, discrete,\n                ))\n\n       \
    \     bin_kws = dict(bins=tuple(bin_edges))\n\n        if cache:\n           \
    \ self.bin_kws = bin_kws\n\n        return bin_kws\n\n    def _eval_bivariate(self,\
    \ x1, x2, weights):\n        \"\"\"Inner function for histogram of two variables.\"\
    \"\"\n        bin_kws = self.bin_kws\n        if bin_kws is None:\n          \
    \  bin_kws = self.define_bin_params(x1, x2, cache=False)\n\n        density =\
    \ self.stat == \"density\"\n\n        hist, *bin_edges = np.histogram2d(\n   \
    \         x1, x2, **bin_kws, weights=weights, density=density\n        )\n\n \
    \       area = np.outer(\n            np.diff(bin_edges[0]),\n            np.diff(bin_edges[1]),\n\
    \        )\n\n        if self.stat == \"probability\" or self.stat == \"proportion\"\
    :\n            hist = hist.astype(float) / hist.sum()\n        elif self.stat\
    \ == \"percent\":\n            hist = hist.astype(float) / hist.sum() * 100\n\
    \        elif self.stat == \"frequency\":\n            hist = hist.astype(float)\
    \ / area\n\n        if self.cumulative:\n            if self.stat in [\"density\"\
    , \"frequency\"]:\n                hist = (hist * area).cumsum(axis=0).cumsum(axis=1)\n\
    \            else:\n                hist = hist.cumsum(axis=0).cumsum(axis=1)\n\
    \n        return hist, bin_edges\n\n    def _eval_univariate(self, x, weights):\n\
    \        \"\"\"Inner function for histogram of one variable.\"\"\"\n        bin_kws\
    \ = self.bin_kws\n        if bin_kws is None:\n            bin_kws = self.define_bin_params(x,\
    \ weights=weights, cache=False)\n\n        density = self.stat == \"density\"\n\
    \        hist, bin_edges = np.histogram(\n            x, **bin_kws, weights=weights,\
    \ density=density,\n        )\n\n        if self.stat == \"probability\" or self.stat\
    \ == \"proportion\":\n            hist = hist.astype(float) / hist.sum()\n   \
    \     elif self.stat == \"percent\":\n            hist = hist.astype(float) /\
    \ hist.sum() * 100\n        elif self.stat == \"frequency\":\n            hist\
    \ = hist.astype(float) / np.diff(bin_edges)\n\n        if self.cumulative:\n \
    \           if self.stat in [\"density\", \"frequency\"]:\n                hist\
    \ = (hist * np.diff(bin_edges)).cumsum()\n            else:\n                hist\
    \ = hist.cumsum()\n\n        return hist, bin_edges\n\n    def __call__(self,\
    \ x1, x2=None, weights=None):\n        \"\"\"Count the occurrences in each bin,\
    \ maybe normalize.\"\"\"\n        if x2 is None:\n            return self._eval_univariate(x1,\
    \ weights)\n        else:\n            return self._eval_bivariate(x1, x2, weights)\n\
    \n\nclass ECDF:\n    \"\"\"Univariate empirical cumulative distribution estimator.\"\
    \"\"\n    def __init__(self, stat=\"proportion\", complementary=False):\n    \
    \    \"\"\"Initialize the class with its parameters\n\n        Parameters\n  \
    \      ----------\n        stat : {{\"proportion\", \"percent\", \"count\"}}\n\
    \            Distribution statistic to compute.\n        complementary : bool\n\
    \            If True, use the complementary CDF (1 - CDF)\n\n        \"\"\"\n\
    \        _check_argument(\"stat\", [\"count\", \"percent\", \"proportion\"], stat)\n\
    \        self.stat = stat\n        self.complementary = complementary\n\n    def\
    \ _eval_bivariate(self, x1, x2, weights):\n        \"\"\"Inner function for ECDF\
    \ of two variables.\"\"\"\n        raise NotImplementedError(\"Bivariate ECDF\
    \ is not implemented\")\n\n    def _eval_univariate(self, x, weights):\n     \
    \   \"\"\"Inner function for ECDF of one variable.\"\"\"\n        sorter = x.argsort()\n\
    \        x = x[sorter]\n        weights = weights[sorter]\n        y = weights.cumsum()\n\
    \n        if self.stat in [\"percent\", \"proportion\"]:\n            y = y /\
    \ y.max()\n        if self.stat == \"percent\":\n            y = y * 100\n\n \
    \       x = np.r_[-np.inf, x]\n        y = np.r_[0, y]\n\n        if self.complementary:\n\
    \            y = y.max() - y\n\n        return y, x\n\n    def __call__(self,\
    \ x1, x2=None, weights=None):\n        \"\"\"Return proportion or count of observations\
    \ below each sorted datapoint.\"\"\"\n        x1 = np.asarray(x1)\n        if\
    \ weights is None:\n            weights = np.ones_like(x1)\n        else:\n  \
    \          weights = np.asarray(weights)\n\n        if x2 is None:\n         \
    \   return self._eval_univariate(x1, weights)\n        else:\n            return\
    \ self._eval_bivariate(x1, x2, weights)\n\n\nclass EstimateAggregator:\n\n   \
    \ def __init__(self, estimator, errorbar=None, **boot_kws):\n        \"\"\"\n\
    \        Data aggregator that produces an estimate and error bar interval.\n\n\
    \        Parameters\n        ----------\n        estimator : callable or string\n\
    \            Function (or method name) that maps a vector to a scalar.\n     \
    \   errorbar : string, (string, number) tuple, or callable\n            Name of\
    \ errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n   \
    \         with a method name and a level parameter, or a function that maps from\
    \ a\n            vector to a (min, max) interval, or None to hide errorbar. See\
    \ the\n            :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\n\
    \        boot_kws\n            Additional keywords are passed to bootstrap when\
    \ error_method is \"ci\".\n\n        \"\"\"\n        self.estimator = estimator\n\
    \n        method, level = _validate_errorbar_arg(errorbar)\n        self.error_method\
    \ = method\n        self.error_level = level\n\n        self.boot_kws = boot_kws\n\
    \n    def __call__(self, data, var):\n        \"\"\"Aggregate over `var` column\
    \ of `data` with estimate and error interval.\"\"\"\n        vals = data[var]\n\
    \        if callable(self.estimator):\n            # You would think we could\
    \ pass to vals.agg, and yet:\n            # https://github.com/mwaskom/seaborn/issues/2943\n\
    \            estimate = self.estimator(vals)\n        else:\n            estimate\
    \ = vals.agg(self.estimator)\n\n        # Options that produce no error bars\n\
    \        if self.error_method is None:\n            err_min = err_max = np.nan\n\
    \        elif len(data) <= 1:\n            err_min = err_max = np.nan\n\n    \
    \    # Generic errorbars from user-supplied function\n        elif callable(self.error_method):\n\
    \            err_min, err_max = self.error_method(vals)\n\n        # Parametric\
    \ options\n        elif self.error_method == \"sd\":\n            half_interval\
    \ = vals.std() * self.error_level\n            err_min, err_max = estimate - half_interval,\
    \ estimate + half_interval\n        elif self.error_method == \"se\":\n      \
    \      half_interval = vals.sem() * self.error_level\n            err_min, err_max\
    \ = estimate - half_interval, estimate + half_interval\n\n        # Nonparametric\
    \ options\n        elif self.error_method == \"pi\":\n            err_min, err_max\
    \ = _percentile_interval(vals, self.error_level)\n        elif self.error_method\
    \ == \"ci\":\n            units = data.get(\"units\", None)\n            boots\
    \ = bootstrap(vals, units=units, func=self.estimator, **self.boot_kws)\n     \
    \       err_min, err_max = _percentile_interval(boots, self.error_level)\n\n \
    \       return pd.Series({var: estimate, f\"{var}min\": err_min, f\"{var}max\"\
    : err_max})\n\n\nclass WeightedAggregator:\n\n    def __init__(self, estimator,\
    \ errorbar=None, **boot_kws):\n        \"\"\"\n        Data aggregator that produces\
    \ a weighted estimate and error bar interval.\n\n        Parameters\n        ----------\n\
    \        estimator : string\n            Function (or method name) that maps a\
    \ vector to a scalar. Currently\n            supports only \"mean\".\n       \
    \ errorbar : string or (string, number) tuple\n            Name of errorbar method\
    \ or a tuple with a method name and a level parameter.\n            Currently\
    \ the only supported method is \"ci\".\n        boot_kws\n            Additional\
    \ keywords are passed to bootstrap when error_method is \"ci\".\n\n        \"\"\
    \"\n        if estimator != \"mean\":\n            # Note that, while other weighted\
    \ estimators may make sense (e.g. median),\n            # I'm not aware of an\
    \ implementation in our dependencies. We can add one\n            # in seaborn\
    \ later, if there is sufficient interest. For now, limit to mean.\n          \
    \  raise ValueError(f\"Weighted estimator must be 'mean', not {estimator!r}.\"\
    )\n        self.estimator = estimator\n\n        method, level = _validate_errorbar_arg(errorbar)\n\
    \        if method is not None and method != \"ci\":\n            # As with the\
    \ estimator, weighted 'sd' or 'pi' error bars may make sense.\n            # But\
    \ we'll keep things simple for now and limit to (bootstrap) CI.\n            raise\
    \ ValueError(f\"Error bar method must be 'ci', not {method!r}.\")\n        self.error_method\
    \ = method\n        self.error_level = level\n\n        self.boot_kws = boot_kws\n\
    \n    def __call__(self, data, var):\n        \"\"\"Aggregate over `var` column\
    \ of `data` with estimate and error interval.\"\"\"\n        vals = data[var]\n\
    \        weights = data[\"weight\"]\n\n        estimate = np.average(vals, weights=weights)\n\
    \n        if self.error_method == \"ci\" and len(data) > 1:\n\n            def\
    \ error_func(x, w):\n                return np.average(x, weights=w)\n\n     \
    \       boots = bootstrap(vals, weights, func=error_func, **self.boot_kws)\n \
    \           err_min, err_max = _percentile_interval(boots, self.error_level)\n\
    \n        else:\n            err_min = err_max = np.nan\n\n        return pd.Series({var:\
    \ estimate, f\"{var}min\": err_min, f\"{var}max\": err_max})\n\n\nclass LetterValues:\n\
    \n    def __init__(self, k_depth, outlier_prop, trust_alpha):\n        \"\"\"\n\
    \        Compute percentiles of a distribution using various tail stopping rules.\n\
    \n        Parameters\n        ----------\n        k_depth: \"tukey\", \"proportion\"\
    , \"trustworthy\", or \"full\"\n            Stopping rule for choosing tail percentiled\
    \ to show:\n\n            - tukey: Show a similar number of outliers as in a conventional\
    \ boxplot.\n            - proportion: Show approximately `outlier_prop` outliers.\n\
    \            - trust_alpha: Use `trust_alpha` level for most extreme tail percentile.\n\
    \n        outlier_prop: float\n            Parameter for `k_depth=\"proportion\"\
    ` setting the expected outlier rate.\n        trust_alpha: float\n           \
    \ Parameter for `k_depth=\"trustworthy\"` setting the confidence threshold.\n\n\
    \        Notes\n        -----\n        Based on the proposal in this paper:\n\
    \        https://vita.had.co.nz/papers/letter-value-plot.pdf\n\n        \"\"\"\
    \n        k_options = [\"tukey\", \"proportion\", \"trustworthy\", \"full\"]\n\
    \        if isinstance(k_depth, str):\n            _check_argument(\"k_depth\"\
    , k_options, k_depth)\n        elif not isinstance(k_depth, int):\n          \
    \  err = (\n                \"The `k_depth` parameter must be either an integer\
    \ or string \"\n                f\"(one of {k_options}), not {k_depth!r}.\"\n\
    \            )\n            raise TypeError(err)\n\n        self.k_depth = k_depth\n\
    \        self.outlier_prop = outlier_prop\n        self.trust_alpha = trust_alpha\n\
    \n    def _compute_k(self, n):\n\n        # Select the depth, i.e. number of boxes\
    \ to draw, based on the method\n        if self.k_depth == \"full\":\n       \
    \     # extend boxes to 100% of the data\n            k = int(np.log2(n)) + 1\n\
    \        elif self.k_depth == \"tukey\":\n            # This results with 5-8\
    \ points in each tail\n            k = int(np.log2(n)) - 3\n        elif self.k_depth\
    \ == \"proportion\":\n            k = int(np.log2(n)) - int(np.log2(n * self.outlier_prop))\
    \ + 1\n        elif self.k_depth == \"trustworthy\":\n            normal_quantile_func\
    \ = np.vectorize(NormalDist().inv_cdf)\n            point_conf = 2 * normal_quantile_func(1\
    \ - self.trust_alpha / 2) ** 2\n            k = int(np.log2(n / point_conf)) +\
    \ 1\n        else:\n            # Allow having k directly specified as input\n\
    \            k = int(self.k_depth)\n\n        return max(k, 1)\n\n    def __call__(self,\
    \ x):\n        \"\"\"Evaluate the letter values.\"\"\"\n        k = self._compute_k(len(x))\n\
    \        exp = np.arange(k + 1, 1, -1), np.arange(2, k + 2)\n        levels =\
    \ k + 1 - np.concatenate([exp[0], exp[1][1:]])\n        percentiles = 100 * np.concatenate([0.5\
    \ ** exp[0], 1 - 0.5 ** exp[1]])\n        if self.k_depth == \"full\":\n     \
    \       percentiles[0] = 0\n            percentiles[-1] = 100\n        values\
    \ = np.percentile(x, percentiles)\n        fliers = np.asarray(x[(x < values.min())\
    \ | (x > values.max())])\n        median = np.percentile(x, 50)\n\n        return\
    \ {\n            \"k\": k,\n            \"levels\": levels,\n            \"percs\"\
    : percentiles,\n            \"values\": values,\n            \"fliers\": fliers,\n\
    \            \"median\": median,\n        }\n\n\ndef _percentile_interval(data,\
    \ width):\n    \"\"\"Return a percentile interval from data of a given width.\"\
    \"\"\n    edge = (100 - width) / 2\n    percentiles = edge, 100 - edge\n    return\
    \ np.nanpercentile(data, percentiles)\n\n\ndef _validate_errorbar_arg(arg):\n\
    \    \"\"\"Check type and value of errorbar argument and assign default level.\"\
    \"\"\n    DEFAULT_LEVELS = {\n        \"ci\": 95,\n        \"pi\": 95,\n     \
    \   \"se\": 1,\n        \"sd\": 1,\n    }\n\n    usage = \"`errorbar` must be\
    \ a callable, string, or (string, number) tuple\"\n\n    if arg is None:\n   \
    \     return None, None\n    elif callable(arg):\n        return arg, None\n \
    \   elif isinstance(arg, str):\n        method = arg\n        level = DEFAULT_LEVELS.get(method,\
    \ None)\n    else:\n        try:\n            method, level = arg\n        except\
    \ (ValueError, TypeError) as err:\n            raise err.__class__(usage) from\
    \ err\n\n    _check_argument(\"errorbar\", list(DEFAULT_LEVELS), method)\n   \
    \ if level is not None and not isinstance(level, Number):\n        raise TypeError(usage)\n\
    \n    return method, level\n\n### Source File Dependency Files Content\n### Dependency\
    \ File: algorithms.py\n\"\"\"Algorithms to support fitting routines in seaborn\
    \ plotting functions.\"\"\"\nimport numpy as np\nimport warnings\n\n\ndef bootstrap(*args,\
    \ **kwargs):\n    \"\"\"Resample one or more arrays with replacement and store\
    \ aggregate values.\n\n    Positional arguments are a sequence of arrays to bootstrap\
    \ along the first\n    axis and pass to a summary function.\n\n    Keyword arguments:\n\
    \        n_boot : int, default=10000\n            Number of iterations\n     \
    \   axis : int, default=None\n            Will pass axis to ``func`` as a keyword\
    \ argument.\n        units : array, default=None\n            Array of sampling\
    \ unit IDs. When used the bootstrap resamples units\n            and then observations\
    \ within units instead of individual\n            datapoints.\n        func :\
    \ string or callable, default=\"mean\"\n            Function to call on the args\
    \ that are passed in. If string, uses as\n            name of function in the\
    \ numpy namespace. If nans are present in the\n            data, will try to use\
    \ nan-aware version of named function.\n        seed : Generator | SeedSequence\
    \ | RandomState | int | None\n            Seed for the random number generator;\
    \ useful if you want\n            reproducible resamples.\n\n    Returns\n   \
    \ -------\n    boot_dist: array\n        array of bootstrapped statistic values\n\
    \n    \"\"\"\n    # Ensure list of arrays are same length\n    if len(np.unique(list(map(len,\
    \ args)))) > 1:\n        raise ValueError(\"All input arrays must have the same\
    \ length\")\n    n = len(args[0])\n\n    # Default keyword arguments\n    n_boot\
    \ = kwargs.get(\"n_boot\", 10000)\n    func = kwargs.get(\"func\", \"mean\")\n\
    \    axis = kwargs.get(\"axis\", None)\n    units = kwargs.get(\"units\", None)\n\
    \    random_seed = kwargs.get(\"random_seed\", None)\n    if random_seed is not\
    \ None:\n        msg = \"`random_seed` has been renamed to `seed` and will be\
    \ removed\"\n        warnings.warn(msg)\n    seed = kwargs.get(\"seed\", random_seed)\n\
    \    if axis is None:\n        func_kwargs = dict()\n    else:\n        func_kwargs\
    \ = dict(axis=axis)\n\n    # Initialize the resampler\n    if isinstance(seed,\
    \ np.random.RandomState):\n        rng = seed\n    else:\n        rng = np.random.default_rng(seed)\n\
    \n    # Coerce to arrays\n    args = list(map(np.asarray, args))\n    if units\
    \ is not None:\n        units = np.asarray(units)\n\n    if isinstance(func, str):\n\
    \n        # Allow named numpy functions\n        f = getattr(np, func)\n\n   \
    \     # Try to use nan-aware version of function if necessary\n        missing_data\
    \ = np.isnan(np.sum(np.column_stack(args)))\n\n        if missing_data and not\
    \ func.startswith(\"nan\"):\n            nanf = getattr(np, f\"nan{func}\", None)\n\
    \            if nanf is None:\n                msg = f\"Data contain nans but\
    \ no nan-aware version of `{func}` found\"\n                warnings.warn(msg,\
    \ UserWarning)\n            else:\n                f = nanf\n\n    else:\n   \
    \     f = func\n\n    # Handle numpy changes\n    try:\n        integers = rng.integers\n\
    \    except AttributeError:\n        integers = rng.randint\n\n    # Do the bootstrap\n\
    \    if units is not None:\n        return _structured_bootstrap(args, n_boot,\
    \ units, f,\n                                     func_kwargs, integers)\n\n \
    \   boot_dist = []\n    for i in range(int(n_boot)):\n        resampler = integers(0,\
    \ n, n, dtype=np.intp)  # intp is indexing dtype\n        sample = [a.take(resampler,\
    \ axis=0) for a in args]\n        boot_dist.append(f(*sample, **func_kwargs))\n\
    \    return np.array(boot_dist)\n\n\ndef _structured_bootstrap(args, n_boot, units,\
    \ func, func_kwargs, integers):\n    \"\"\"Resample units instead of datapoints.\"\
    \"\"\n    unique_units = np.unique(units)\n    n_units = len(unique_units)\n\n\
    \    args = [[a[units == unit] for unit in unique_units] for a in args]\n\n  \
    \  boot_dist = []\n    for i in range(int(n_boot)):\n        resampler = integers(0,\
    \ n_units, n_units, dtype=np.intp)\n        sample = [[a[i] for i in resampler]\
    \ for a in args]\n        lengths = map(len, sample[0])\n        resampler = [integers(0,\
    \ n, n, dtype=np.intp) for n in lengths]\n        sample = [[c.take(r, axis=0)\
    \ for c, r in zip(a, resampler)] for a in sample]\n        sample = list(map(np.concatenate,\
    \ sample))\n        boot_dist.append(func(*sample, **func_kwargs))\n    return\
    \ np.array(boot_dist)\n\n\n### Dependency File: utils.py\n\"\"\"Utility functions,\
    \ mostly for internal use.\"\"\"\nimport os\nimport inspect\nimport warnings\n\
    import colorsys\nfrom contextlib import contextmanager\nfrom urllib.request import\
    \ urlopen, urlretrieve\nfrom types import ModuleType\n\nimport numpy as np\nimport\
    \ pandas as pd\nimport matplotlib as mpl\nfrom matplotlib.colors import to_rgb\n\
    import matplotlib.pyplot as plt\nfrom matplotlib.cbook import normalize_kwargs\n\
    \nfrom seaborn._core.typing import deprecated\nfrom seaborn.external.version import\
    \ Version\nfrom seaborn.external.appdirs import user_cache_dir\n\n__all__ = [\"\
    desaturate\", \"saturate\", \"set_hls_values\", \"move_legend\",\n           \"\
    despine\", \"get_dataset_names\", \"get_data_home\", \"load_dataset\"]\n\nDATASET_SOURCE\
    \ = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master\"\nDATASET_NAMES_URL\
    \ = f\"{DATASET_SOURCE}/dataset_names.txt\"\n\n\ndef ci_to_errsize(cis, heights):\n\
    \    \"\"\"Convert intervals to error arguments relative to plot heights.\n\n\
    \    Parameters\n    ----------\n    cis : 2 x n sequence\n        sequence of\
    \ confidence interval limits\n    heights : n sequence\n        sequence of plot\
    \ heights\n\n    Returns\n    -------\n    errsize : 2 x n array\n        sequence\
    \ of error size relative to height values in correct\n        format as argument\
    \ for plt.bar\n\n    \"\"\"\n    cis = np.atleast_2d(cis).reshape(2, -1)\n   \
    \ heights = np.atleast_1d(heights)\n    errsize = []\n    for i, (low, high) in\
    \ enumerate(np.transpose(cis)):\n        h = heights[i]\n        elow = h - low\n\
    \        ehigh = high - h\n        errsize.append([elow, ehigh])\n\n    errsize\
    \ = np.asarray(errsize).T\n    return errsize\n\n\ndef _draw_figure(fig):\n  \
    \  \"\"\"Force draw of a matplotlib figure, accounting for back-compat.\"\"\"\n\
    \    # See https://github.com/matplotlib/matplotlib/issues/19197 for context\n\
    \    fig.canvas.draw()\n    if fig.stale:\n        try:\n            fig.draw(fig.canvas.get_renderer())\n\
    \        except AttributeError:\n            pass\n\n\ndef _default_color(method,\
    \ hue, color, kws, saturation=1):\n    \"\"\"If needed, get a default color by\
    \ using the matplotlib property cycle.\"\"\"\n\n    if hue is not None:\n    \
    \    # This warning is probably user-friendly, but it's currently triggered\n\
    \        # in a FacetGrid context and I don't want to mess with that logic right\
    \ now\n        #  if color is not None:\n        #      msg = \"`color` is ignored\
    \ when `hue` is assigned.\"\n        #      warnings.warn(msg)\n        return\
    \ None\n\n    kws = kws.copy()\n    kws.pop(\"label\", None)\n\n    if color is\
    \ not None:\n        if saturation < 1:\n            color = desaturate(color,\
    \ saturation)\n        return color\n\n    elif method.__name__ == \"plot\":\n\
    \n        color = normalize_kwargs(kws, mpl.lines.Line2D).get(\"color\")\n   \
    \     scout, = method([], [], scalex=False, scaley=False, color=color)\n     \
    \   color = scout.get_color()\n        scout.remove()\n\n    elif method.__name__\
    \ == \"scatter\":\n\n        # Matplotlib will raise if the size of x/y don't\
    \ match s/c,\n        # and the latter might be in the kws dict\n        scout_size\
    \ = max(\n            np.atleast_1d(kws.get(key, [])).shape[0]\n            for\
    \ key in [\"s\", \"c\", \"fc\", \"facecolor\", \"facecolors\"]\n        )\n  \
    \      scout_x = scout_y = np.full(scout_size, np.nan)\n\n        scout = method(scout_x,\
    \ scout_y, **kws)\n        facecolors = scout.get_facecolors()\n\n        if not\
    \ len(facecolors):\n            # Handle bug in matplotlib <= 3.2 (I think)\n\
    \            # This will limit the ability to use non color= kwargs to specify\n\
    \            # a color in versions of matplotlib with the bug, but trying to\n\
    \            # work out what the user wanted by re-implementing the broken logic\n\
    \            # of inspecting the kwargs is probably too brittle.\n           \
    \ single_color = False\n        else:\n            single_color = np.unique(facecolors,\
    \ axis=0).shape[0] == 1\n\n        # Allow the user to specify an array of colors\
    \ through various kwargs\n        if \"c\" not in kws and single_color:\n    \
    \        color = to_rgb(facecolors[0])\n\n        scout.remove()\n\n    elif method.__name__\
    \ == \"bar\":\n\n        # bar() needs masked, not empty data, to generate a patch\n\
    \        scout, = method([np.nan], [np.nan], **kws)\n        color = to_rgb(scout.get_facecolor())\n\
    \        scout.remove()\n        # Axes.bar adds both a patch and a container\n\
    \        method.__self__.containers.pop(-1)\n\n    elif method.__name__ == \"\
    fill_between\":\n\n        kws = normalize_kwargs(kws, mpl.collections.PolyCollection)\n\
    \        scout = method([], [], **kws)\n        facecolor = scout.get_facecolor()\n\
    \        color = to_rgb(facecolor[0])\n        scout.remove()\n\n    if saturation\
    \ < 1:\n        color = desaturate(color, saturation)\n\n    return color\n\n\n\
    def desaturate(color, prop):\n    \"\"\"Decrease the saturation channel of a color\
    \ by some percent.\n\n    Parameters\n    ----------\n    color : matplotlib color\n\
    \        hex, rgb-tuple, or html color name\n    prop : float\n        saturation\
    \ channel of color will be multiplied by this value\n\n    Returns\n    -------\n\
    \    new_color : rgb tuple\n        desaturated color code in RGB tuple representation\n\
    \n    \"\"\"\n    # Check inputs\n    if not 0 <= prop <= 1:\n        raise ValueError(\"\
    prop must be between 0 and 1\")\n\n    # Get rgb tuple rep\n    rgb = to_rgb(color)\n\
    \n    # Short circuit to avoid floating point issues\n    if prop == 1:\n    \
    \    return rgb\n\n    # Convert to hls\n    h, l, s = colorsys.rgb_to_hls(*rgb)\n\
    \n    # Desaturate the saturation channel\n    s *= prop\n\n    # Convert back\
    \ to rgb\n    new_color = colorsys.hls_to_rgb(h, l, s)\n\n    return new_color\n\
    \n\ndef saturate(color):\n    \"\"\"Return a fully saturated color with the same\
    \ hue.\n\n    Parameters\n    ----------\n    color : matplotlib color\n     \
    \   hex, rgb-tuple, or html color name\n\n    Returns\n    -------\n    new_color\
    \ : rgb tuple\n        saturated color code in RGB tuple representation\n\n  \
    \  \"\"\"\n    return set_hls_values(color, s=1)\n\n\ndef set_hls_values(color,\
    \ h=None, l=None, s=None):  # noqa\n    \"\"\"Independently manipulate the h,\
    \ l, or s channels of a color.\n\n    Parameters\n    ----------\n    color :\
    \ matplotlib color\n        hex, rgb-tuple, or html color name\n    h, l, s :\
    \ floats between 0 and 1, or None\n        new values for each channel in hls\
    \ space\n\n    Returns\n    -------\n    new_color : rgb tuple\n        new color\
    \ code in RGB tuple representation\n\n    \"\"\"\n    # Get an RGB tuple representation\n\
    \    rgb = to_rgb(color)\n    vals = list(colorsys.rgb_to_hls(*rgb))\n    for\
    \ i, val in enumerate([h, l, s]):\n        if val is not None:\n            vals[i]\
    \ = val\n\n    rgb = colorsys.hls_to_rgb(*vals)\n    return rgb\n\n\ndef axlabel(xlabel,\
    \ ylabel, **kwargs):\n    \"\"\"Grab current axis and label it.\n\n    DEPRECATED:\
    \ will be removed in a future version.\n\n    \"\"\"\n    msg = \"This function\
    \ is deprecated and will be removed in a future version\"\n    warnings.warn(msg,\
    \ FutureWarning)\n    ax = plt.gca()\n    ax.set_xlabel(xlabel, **kwargs)\n  \
    \  ax.set_ylabel(ylabel, **kwargs)\n\n\ndef remove_na(vector):\n    \"\"\"Helper\
    \ method for removing null values from data vectors.\n\n    Parameters\n    ----------\n\
    \    vector : vector object\n        Must implement boolean masking with [] subscript\
    \ syntax.\n\n    Returns\n    -------\n    clean_clean : same type as ``vector``\n\
    \        Vector of data with null values removed. May be a copy or a view.\n\n\
    \    \"\"\"\n    return vector[pd.notnull(vector)]\n\n\ndef get_color_cycle():\n\
    \    \"\"\"Return the list of colors in the current matplotlib color cycle\n\n\
    \    Parameters\n    ----------\n    None\n\n    Returns\n    -------\n    colors\
    \ : list\n        List of matplotlib colors in the current cycle, or dark gray\
    \ if\n        the current color cycle is empty.\n    \"\"\"\n    cycler = mpl.rcParams['axes.prop_cycle']\n\
    \    return cycler.by_key()['color'] if 'color' in cycler.keys else [\".15\"]\n\
    \n\ndef despine(fig=None, ax=None, top=True, right=True, left=False,\n       \
    \     bottom=False, offset=None, trim=False):\n    \"\"\"Remove the top and right\
    \ spines from plot(s).\n\n    fig : matplotlib figure, optional\n        Figure\
    \ to despine all axes of, defaults to the current figure.\n    ax : matplotlib\
    \ axes, optional\n        Specific axes object to despine. Ignored if fig is provided.\n\
    \    top, right, left, bottom : boolean, optional\n        If True, remove that\
    \ spine.\n    offset : int or dict, optional\n        Absolute distance, in points,\
    \ spines should be moved away\n        from the axes (negative values move spines\
    \ inward). A single value\n        applies to all spines; a dict can be used to\
    \ set offset values per\n        side.\n    trim : bool, optional\n        If\
    \ True, limit spines to the smallest and largest major tick\n        on each non-despined\
    \ axis.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    # Get references\
    \ to the axes we want\n    if fig is None and ax is None:\n        axes = plt.gcf().axes\n\
    \    elif fig is not None:\n        axes = fig.axes\n    elif ax is not None:\n\
    \        axes = [ax]\n\n    for ax_i in axes:\n        for side in [\"top\", \"\
    right\", \"left\", \"bottom\"]:\n            # Toggle the spine objects\n    \
    \        is_visible = not locals()[side]\n            ax_i.spines[side].set_visible(is_visible)\n\
    \            if offset is not None and is_visible:\n                try:\n   \
    \                 val = offset.get(side, 0)\n                except AttributeError:\n\
    \                    val = offset\n                ax_i.spines[side].set_position(('outward',\
    \ val))\n\n        # Potentially move the ticks\n        if left and not right:\n\
    \            maj_on = any(\n                t.tick1line.get_visible()\n      \
    \          for t in ax_i.yaxis.majorTicks\n            )\n            min_on =\
    \ any(\n                t.tick1line.get_visible()\n                for t in ax_i.yaxis.minorTicks\n\
    \            )\n            ax_i.yaxis.set_ticks_position(\"right\")\n       \
    \     for t in ax_i.yaxis.majorTicks:\n                t.tick2line.set_visible(maj_on)\n\
    \            for t in ax_i.yaxis.minorTicks:\n                t.tick2line.set_visible(min_on)\n\
    \n        if bottom and not top:\n            maj_on = any(\n                t.tick1line.get_visible()\n\
    \                for t in ax_i.xaxis.majorTicks\n            )\n            min_on\
    \ = any(\n                t.tick1line.get_visible()\n                for t in\
    \ ax_i.xaxis.minorTicks\n            )\n            ax_i.xaxis.set_ticks_position(\"\
    top\")\n            for t in ax_i.xaxis.majorTicks:\n                t.tick2line.set_visible(maj_on)\n\
    \            for t in ax_i.xaxis.minorTicks:\n                t.tick2line.set_visible(min_on)\n\
    \n        if trim:\n            # clip off the parts of the spines that extend\
    \ past major ticks\n            xticks = np.asarray(ax_i.get_xticks())\n     \
    \       if xticks.size:\n                firsttick = np.compress(xticks >= min(ax_i.get_xlim()),\n\
    \                                        xticks)[0]\n                lasttick\
    \ = np.compress(xticks <= max(ax_i.get_xlim()),\n                            \
    \           xticks)[-1]\n                ax_i.spines['bottom'].set_bounds(firsttick,\
    \ lasttick)\n                ax_i.spines['top'].set_bounds(firsttick, lasttick)\n\
    \                newticks = xticks.compress(xticks <= lasttick)\n            \
    \    newticks = newticks.compress(newticks >= firsttick)\n                ax_i.set_xticks(newticks)\n\
    \n            yticks = np.asarray(ax_i.get_yticks())\n            if yticks.size:\n\
    \                firsttick = np.compress(yticks >= min(ax_i.get_ylim()),\n   \
    \                                     yticks)[0]\n                lasttick = np.compress(yticks\
    \ <= max(ax_i.get_ylim()),\n                                       yticks)[-1]\n\
    \                ax_i.spines['left'].set_bounds(firsttick, lasttick)\n       \
    \         ax_i.spines['right'].set_bounds(firsttick, lasttick)\n             \
    \   newticks = yticks.compress(yticks <= lasttick)\n                newticks =\
    \ newticks.compress(newticks >= firsttick)\n                ax_i.set_yticks(newticks)\n\
    \n\ndef move_legend(obj, loc, **kwargs):\n    \"\"\"\n    Recreate a plot's legend\
    \ at a new location.\n\n    The name is a slight misnomer. Matplotlib legends\
    \ do not expose public\n    control over their position parameters. So this function\
    \ creates a new legend,\n    copying over the data from the original object, which\
    \ is then removed.\n\n    Parameters\n    ----------\n    obj : the object with\
    \ the plot\n        This argument can be either a seaborn or matplotlib object:\n\
    \n        - :class:`seaborn.FacetGrid` or :class:`seaborn.PairGrid`\n        -\
    \ :class:`matplotlib.axes.Axes` or :class:`matplotlib.figure.Figure`\n\n    loc\
    \ : str or int\n        Location argument, as in :meth:`matplotlib.axes.Axes.legend`.\n\
    \n    kwargs\n        Other keyword arguments are passed to :meth:`matplotlib.axes.Axes.legend`.\n\
    \n    Examples\n    --------\n\n    .. include:: ../docstrings/move_legend.rst\n\
    \n    \"\"\"\n    # This is a somewhat hackish solution that will hopefully be\
    \ obviated by\n    # upstream improvements to matplotlib legends that make them\
    \ easier to\n    # modify after creation.\n\n    from seaborn.axisgrid import\
    \ Grid  # Avoid circular import\n\n    # Locate the legend object and a method\
    \ to recreate the legend\n    if isinstance(obj, Grid):\n        old_legend =\
    \ obj.legend\n        legend_func = obj.figure.legend\n    elif isinstance(obj,\
    \ mpl.axes.Axes):\n        old_legend = obj.legend_\n        legend_func = obj.legend\n\
    \    elif isinstance(obj, mpl.figure.Figure):\n        if obj.legends:\n     \
    \       old_legend = obj.legends[-1]\n        else:\n            old_legend =\
    \ None\n        legend_func = obj.legend\n    else:\n        err = \"`obj` must\
    \ be a seaborn Grid or matplotlib Axes or Figure instance.\"\n        raise TypeError(err)\n\
    \n    if old_legend is None:\n        err = f\"{obj} has no legend attached.\"\
    \n        raise ValueError(err)\n\n    # Extract the components of the legend\
    \ we need to reuse\n    # Import here to avoid a circular import\n    from seaborn._compat\
    \ import get_legend_handles\n    handles = get_legend_handles(old_legend)\n  \
    \  labels = [t.get_text() for t in old_legend.get_texts()]\n\n    # Handle the\
    \ case where the user is trying to override the labels\n    if (new_labels :=\
    \ kwargs.pop(\"labels\", None)) is not None:\n        if len(new_labels) != len(labels):\n\
    \            err = \"Length of new labels does not match existing legend.\"\n\
    \            raise ValueError(err)\n        labels = new_labels\n\n    # Extract\
    \ legend properties that can be passed to the recreation method\n    # (Vexingly,\
    \ these don't all round-trip)\n    legend_kws = inspect.signature(mpl.legend.Legend).parameters\n\
    \    props = {k: v for k, v in old_legend.properties().items() if k in legend_kws}\n\
    \n    # Delegate default bbox_to_anchor rules to matplotlib\n    props.pop(\"\
    bbox_to_anchor\")\n\n    # Try to propagate the existing title and font properties;\
    \ respect new ones too\n    title = props.pop(\"title\")\n    if \"title\" in\
    \ kwargs:\n        title.set_text(kwargs.pop(\"title\"))\n    title_kwargs = {k:\
    \ v for k, v in kwargs.items() if k.startswith(\"title_\")}\n    for key, val\
    \ in title_kwargs.items():\n        title.set(**{key[6:]: val})\n        kwargs.pop(key)\n\
    \n    # Try to respect the frame visibility\n    kwargs.setdefault(\"frameon\"\
    , old_legend.legendPatch.get_visible())\n\n    # Remove the old legend and create\
    \ the new one\n    props.update(kwargs)\n    old_legend.remove()\n    new_legend\
    \ = legend_func(handles, labels, loc=loc, **props)\n    new_legend.set_title(title.get_text(),\
    \ title.get_fontproperties())\n\n    # Let the Grid object continue to track the\
    \ correct legend object\n    if isinstance(obj, Grid):\n        obj._legend =\
    \ new_legend\n\n\ndef _kde_support(data, bw, gridsize, cut, clip):\n    \"\"\"\
    Establish support for a kernel density estimate.\"\"\"\n    support_min = max(data.min()\
    \ - bw * cut, clip[0])\n    support_max = min(data.max() + bw * cut, clip[1])\n\
    \    support = np.linspace(support_min, support_max, gridsize)\n\n    return support\n\
    \n\ndef ci(a, which=95, axis=None):\n    \"\"\"Return a percentile range from\
    \ an array of values.\"\"\"\n    p = 50 - which / 2, 50 + which / 2\n    return\
    \ np.nanpercentile(a, p, axis)\n\n\ndef get_dataset_names():\n    \"\"\"Report\
    \ available example datasets, useful for reporting issues.\n\n    Requires an\
    \ internet connection.\n\n    \"\"\"\n    with urlopen(DATASET_NAMES_URL) as resp:\n\
    \        txt = resp.read()\n\n    dataset_names = [name.strip() for name in txt.decode().split(\"\
    \\n\")]\n    return list(filter(None, dataset_names))\n\n\ndef get_data_home(data_home=None):\n\
    \    \"\"\"Return a path to the cache directory for example datasets.\n\n    This\
    \ directory is used by :func:`load_dataset`.\n\n    If the ``data_home`` argument\
    \ is not provided, it will use a directory\n    specified by the `SEABORN_DATA`\
    \ environment variable (if it exists)\n    or otherwise default to an OS-appropriate\
    \ user cache location.\n\n    \"\"\"\n    if data_home is None:\n        data_home\
    \ = os.environ.get(\"SEABORN_DATA\", user_cache_dir(\"seaborn\"))\n    data_home\
    \ = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n   \
    \     os.makedirs(data_home)\n    return data_home\n\n\ndef load_dataset(name,\
    \ cache=True, data_home=None, **kws):\n    \"\"\"Load an example dataset from\
    \ the online repository (requires internet).\n\n    This function provides quick\
    \ access to a small number of example datasets\n    that are useful for documenting\
    \ seaborn or generating reproducible examples\n    for bug reports. It is not\
    \ necessary for normal usage.\n\n    Note that some of the datasets have a small\
    \ amount of preprocessing applied\n    to define a proper ordering for categorical\
    \ variables.\n\n    Use :func:`get_dataset_names` to see a list of available datasets.\n\
    \n    Parameters\n    ----------\n    name : str\n        Name of the dataset\
    \ (``{name}.csv`` on\n        https://github.com/mwaskom/seaborn-data).\n    cache\
    \ : boolean, optional\n        If True, try to load from the local cache first,\
    \ and save to the cache\n        if a download is required.\n    data_home : string,\
    \ optional\n        The directory in which to cache data; see :func:`get_data_home`.\n\
    \    kws : keys and values, optional\n        Additional keyword arguments are\
    \ passed to passed through to\n        :func:`pandas.read_csv`.\n\n    Returns\n\
    \    -------\n    df : :class:`pandas.DataFrame`\n        Tabular data, possibly\
    \ with some preprocessing applied.\n\n    \"\"\"\n    # A common beginner mistake\
    \ is to assume that one's personal data needs\n    # to be passed through this\
    \ function to be usable with seaborn.\n    # Let's provide a more helpful error\
    \ than you would otherwise get.\n    if isinstance(name, pd.DataFrame):\n    \
    \    err = (\n            \"This function accepts only strings (the name of an\
    \ example dataset). \"\n            \"You passed a pandas DataFrame. If you have\
    \ your own dataset, \"\n            \"it is not necessary to use this function\
    \ before plotting.\"\n        )\n        raise TypeError(err)\n\n    url = f\"\
    {DATASET_SOURCE}/{name}.csv\"\n\n    if cache:\n        cache_path = os.path.join(get_data_home(data_home),\
    \ os.path.basename(url))\n        if not os.path.exists(cache_path):\n       \
    \     if name not in get_dataset_names():\n                raise ValueError(f\"\
    '{name}' is not one of the example datasets.\")\n            urlretrieve(url,\
    \ cache_path)\n        full_path = cache_path\n    else:\n        full_path =\
    \ url\n\n    df = pd.read_csv(full_path, **kws)\n\n    if df.iloc[-1].isnull().all():\n\
    \        df = df.iloc[:-1]\n\n    # Set some columns as a categorical type with\
    \ ordered levels\n\n    if name == \"tips\":\n        df[\"day\"] = pd.Categorical(df[\"\
    day\"], [\"Thur\", \"Fri\", \"Sat\", \"Sun\"])\n        df[\"sex\"] = pd.Categorical(df[\"\
    sex\"], [\"Male\", \"Female\"])\n        df[\"time\"] = pd.Categorical(df[\"time\"\
    ], [\"Lunch\", \"Dinner\"])\n        df[\"smoker\"] = pd.Categorical(df[\"smoker\"\
    ], [\"Yes\", \"No\"])\n\n    elif name == \"flights\":\n        months = df[\"\
    month\"].str[:3]\n        df[\"month\"] = pd.Categorical(months, months.unique())\n\
    \n    elif name == \"exercise\":\n        df[\"time\"] = pd.Categorical(df[\"\
    time\"], [\"1 min\", \"15 min\", \"30 min\"])\n        df[\"kind\"] = pd.Categorical(df[\"\
    kind\"], [\"rest\", \"walking\", \"running\"])\n        df[\"diet\"] = pd.Categorical(df[\"\
    diet\"], [\"no fat\", \"low fat\"])\n\n    elif name == \"titanic\":\n       \
    \ df[\"class\"] = pd.Categorical(df[\"class\"], [\"First\", \"Second\", \"Third\"\
    ])\n        df[\"deck\"] = pd.Categorical(df[\"deck\"], list(\"ABCDEFG\"))\n\n\
    \    elif name == \"penguins\":\n        df[\"sex\"] = df[\"sex\"].str.title()\n\
    \n    elif name == \"diamonds\":\n        df[\"color\"] = pd.Categorical(\n  \
    \          df[\"color\"], [\"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"],\n\
    \        )\n        df[\"clarity\"] = pd.Categorical(\n            df[\"clarity\"\
    ], [\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\"],\n\
    \        )\n        df[\"cut\"] = pd.Categorical(\n            df[\"cut\"], [\"\
    Ideal\", \"Premium\", \"Very Good\", \"Good\", \"Fair\"],\n        )\n\n    elif\
    \ name == \"taxis\":\n        df[\"pickup\"] = pd.to_datetime(df[\"pickup\"])\n\
    \        df[\"dropoff\"] = pd.to_datetime(df[\"dropoff\"])\n\n    elif name ==\
    \ \"seaice\":\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\n    elif\
    \ name == \"dowjones\":\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\
    \n    return df\n\n\ndef axis_ticklabels_overlap(labels):\n    \"\"\"Return a\
    \ boolean for whether the list of ticklabels have overlaps.\n\n    Parameters\n\
    \    ----------\n    labels : list of matplotlib ticklabels\n\n    Returns\n \
    \   -------\n    overlap : boolean\n        True if any of the labels overlap.\n\
    \n    \"\"\"\n    if not labels:\n        return False\n    try:\n        bboxes\
    \ = [l.get_window_extent() for l in labels]\n        overlaps = [b.count_overlaps(bboxes)\
    \ for b in bboxes]\n        return max(overlaps) > 1\n    except RuntimeError:\n\
    \        # Issue on macos backend raises an error in the above code\n        return\
    \ False\n\n\ndef axes_ticklabels_overlap(ax):\n    \"\"\"Return booleans for whether\
    \ the x and y ticklabels on an Axes overlap.\n\n    Parameters\n    ----------\n\
    \    ax : matplotlib Axes\n\n    Returns\n    -------\n    x_overlap, y_overlap\
    \ : booleans\n        True when the labels on that axis overlap.\n\n    \"\"\"\
    \n    return (axis_ticklabels_overlap(ax.get_xticklabels()),\n            axis_ticklabels_overlap(ax.get_yticklabels()))\n\
    \n\ndef locator_to_legend_entries(locator, limits, dtype):\n    \"\"\"Return levels\
    \ and formatted levels for brief numeric legends.\"\"\"\n    raw_levels = locator.tick_values(*limits).astype(dtype)\n\
    \n    # The locator can return ticks outside the limits, clip them here\n    raw_levels\
    \ = [l for l in raw_levels if l >= limits[0] and l <= limits[1]]\n\n    class\
    \ dummy_axis:\n        def get_view_interval(self):\n            return limits\n\
    \n    if isinstance(locator, mpl.ticker.LogLocator):\n        formatter = mpl.ticker.LogFormatter()\n\
    \    else:\n        formatter = mpl.ticker.ScalarFormatter()\n        # Avoid\
    \ having an offset/scientific notation which we don't currently\n        # have\
    \ any way of representing in the legend\n        formatter.set_useOffset(False)\n\
    \        formatter.set_scientific(False)\n    formatter.axis = dummy_axis()\n\n\
    \    formatted_levels = formatter.format_ticks(raw_levels)\n\n    return raw_levels,\
    \ formatted_levels\n\n\ndef relative_luminance(color):\n    \"\"\"Calculate the\
    \ relative luminance of a color according to W3C standards\n\n    Parameters\n\
    \    ----------\n    color : matplotlib color or sequence of matplotlib colors\n\
    \        Hex code, rgb-tuple, or html color name.\n\n    Returns\n    -------\n\
    \    luminance : float(s) between 0 and 1\n\n    \"\"\"\n    rgb = mpl.colors.colorConverter.to_rgba_array(color)[:,\
    \ :3]\n    rgb = np.where(rgb <= .03928, rgb / 12.92, ((rgb + .055) / 1.055) **\
    \ 2.4)\n    lum = rgb.dot([.2126, .7152, .0722])\n    try:\n        return lum.item()\n\
    \    except ValueError:\n        return lum\n\n\ndef to_utf8(obj):\n    \"\"\"\
    Return a string representing a Python object.\n\n    Strings (i.e. type ``str``)\
    \ are returned unchanged.\n\n    Byte strings (i.e. type ``bytes``) are returned\
    \ as UTF-8-decoded strings.\n\n    For other objects, the method ``__str__()``\
    \ is called, and the result is\n    returned as a string.\n\n    Parameters\n\
    \    ----------\n    obj : object\n        Any Python object\n\n    Returns\n\
    \    -------\n    s : str\n        UTF-8-decoded string representation of ``obj``\n\
    \n    \"\"\"\n    if isinstance(obj, str):\n        return obj\n    try:\n   \
    \     return obj.decode(encoding=\"utf-8\")\n    except AttributeError:  # obj\
    \ is not bytes-like\n        return str(obj)\n\n\ndef _check_argument(param, options,\
    \ value, prefix=False):\n    \"\"\"Raise if value for param is not in options.\"\
    \"\"\n    if prefix and value is not None:\n        failure = not any(value.startswith(p)\
    \ for p in options if isinstance(p, str))\n    else:\n        failure = value\
    \ not in options\n    if failure:\n        raise ValueError(\n            f\"\
    The value for `{param}` must be one of {options}, \"\n            f\"but {repr(value)}\
    \ was passed.\"\n        )\n    return value\n\n\ndef _assign_default_kwargs(kws,\
    \ call_func, source_func):\n    \"\"\"Assign default kwargs for call_func using\
    \ values from source_func.\"\"\"\n    # This exists so that axes-level functions\
    \ and figure-level functions can\n    # both call a Plotter method while having\
    \ the default kwargs be defined in\n    # the signature of the axes-level function.\n\
    \    # An alternative would be to have a decorator on the method that sets its\n\
    \    # defaults based on those defined in the axes-level function.\n    # Then\
    \ the figure-level function would not need to worry about defaults.\n    # I am\
    \ not sure which is better.\n    needed = inspect.signature(call_func).parameters\n\
    \    defaults = inspect.signature(source_func).parameters\n\n    for param in\
    \ needed:\n        if param in defaults and param not in kws:\n            kws[param]\
    \ = defaults[param].default\n\n    return kws\n\n\ndef adjust_legend_subtitles(legend):\n\
    \    \"\"\"\n    Make invisible-handle \"subtitles\" entries look more like titles.\n\
    \n    Note: This function is not part of the public API and may be changed or\
    \ removed.\n\n    \"\"\"\n    # Legend title not in rcParams until 3.0\n    font_size\
    \ = plt.rcParams.get(\"legend.title_fontsize\", None)\n    hpackers = legend.findobj(mpl.offsetbox.VPacker)[0].get_children()\n\
    \    for hpack in hpackers:\n        draw_area, text_area = hpack.get_children()\n\
    \        handles = draw_area.get_children()\n        if not all(artist.get_visible()\
    \ for artist in handles):\n            draw_area.set_width(0)\n            for\
    \ text in text_area.get_children():\n                if font_size is not None:\n\
    \                    text.set_size(font_size)\n\n\ndef _deprecate_ci(errorbar,\
    \ ci):\n    \"\"\"\n    Warn on usage of ci= and convert to appropriate errorbar=\
    \ arg.\n\n    ci was deprecated when errorbar was added in 0.12. It should not\
    \ be removed\n    completely for some time, but it can be moved out of function\
    \ definitions\n    (and extracted from kwargs) after one cycle.\n\n    \"\"\"\n\
    \    if ci is not deprecated and ci != \"deprecated\":\n        if ci is None:\n\
    \            errorbar = None\n        elif ci == \"sd\":\n            errorbar\
    \ = \"sd\"\n        else:\n            errorbar = (\"ci\", ci)\n        msg =\
    \ (\n            \"\\n\\nThe `ci` parameter is deprecated. \"\n            f\"\
    Use `errorbar={repr(errorbar)}` for the same effect.\\n\"\n        )\n       \
    \ warnings.warn(msg, FutureWarning, stacklevel=3)\n\n    return errorbar\n\n\n\
    def _get_transform_functions(ax, axis):\n    \"\"\"Return the forward and inverse\
    \ transforms for a given axis.\"\"\"\n    axis_obj = getattr(ax, f\"{axis}axis\"\
    )\n    transform = axis_obj.get_transform()\n    return transform.transform, transform.inverted().transform\n\
    \n\n@contextmanager\ndef _disable_autolayout():\n    \"\"\"Context manager for\
    \ preventing rc-controlled auto-layout behavior.\"\"\"\n    # This is a workaround\
    \ for an issue in matplotlib, for details see\n    # https://github.com/mwaskom/seaborn/issues/2914\n\
    \    # The only affect of this rcParam is to set the default value for\n    #\
    \ layout= in plt.figure, so we could just do that instead.\n    # But then we\
    \ would need to own the complexity of the transition\n    # from tight_layout=True\
    \ -> layout=\"tight\". This seems easier,\n    # but can be removed when (if)\
    \ that is simpler on the matplotlib side,\n    # or if the layout algorithms are\
    \ improved to handle figure legends.\n    orig_val = mpl.rcParams[\"figure.autolayout\"\
    ]\n    try:\n        mpl.rcParams[\"figure.autolayout\"] = False\n        yield\n\
    \    finally:\n        mpl.rcParams[\"figure.autolayout\"] = orig_val\n\n\ndef\
    \ _version_predates(lib: ModuleType, version: str) -> bool:\n    \"\"\"Helper\
    \ function for checking version compatibility.\"\"\"\n    return Version(lib.__version__)\
    \ < Version(version)\n\n\ndef _scatter_legend_artist(**kws):\n\n    kws = normalize_kwargs(kws,\
    \ mpl.collections.PathCollection)\n\n    edgecolor = kws.pop(\"edgecolor\", None)\n\
    \    rc = mpl.rcParams\n    line_kws = {\n        \"linestyle\": \"\",\n     \
    \   \"marker\": kws.pop(\"marker\", \"o\"),\n        \"markersize\": np.sqrt(kws.pop(\"\
    s\", rc[\"lines.markersize\"] ** 2)),\n        \"markerfacecolor\": kws.pop(\"\
    facecolor\", kws.get(\"color\")),\n        \"markeredgewidth\": kws.pop(\"linewidth\"\
    , 0),\n        **kws,\n    }\n\n    if edgecolor is not None:\n        if edgecolor\
    \ == \"face\":\n            line_kws[\"markeredgecolor\"] = line_kws[\"markerfacecolor\"\
    ]\n        else:\n            line_kws[\"markeredgecolor\"] = edgecolor\n\n  \
    \  return mpl.lines.Line2D([], [], **line_kws)\n\n\ndef _get_patch_legend_artist(fill):\n\
    \n    def legend_artist(**kws):\n\n        color = kws.pop(\"color\", None)\n\
    \        if color is not None:\n            if fill:\n                kws[\"facecolor\"\
    ] = color\n            else:\n                kws[\"edgecolor\"] = color\n   \
    \             kws[\"facecolor\"] = \"none\"\n\n        return mpl.patches.Rectangle((0,\
    \ 0), 0, 0, **kws)\n\n    return legend_artist\n\nOutput the complete test file,\
    \ code only, no explanations.\n### Time\nCurrent time: 2025-03-17 01:27:09\n"
  role: user
