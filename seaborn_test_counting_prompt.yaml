messages:
- content: You are an AI agent expert in writing unit tests. Your task is to write
    unit tests for the given code files of the repository. Make sure the tests can
    be executed without lint or compile errors.
  role: system
- content: "### Task Information\nBased on the source code, write/rewrite tests to\
    \ cover the source code.\nRepository: seaborn\nTest File Path: seaborn\\test_counting\\\
    test_counting.py\nProject Programming Language: Python\nTesting Framework: pytest\n\
    ### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\
    from dataclasses import dataclass\nfrom typing import ClassVar\n\nimport numpy\
    \ as np\nimport pandas as pd\nfrom pandas import DataFrame\n\nfrom seaborn._core.groupby\
    \ import GroupBy\nfrom seaborn._core.scales import Scale\nfrom seaborn._stats.base\
    \ import Stat\n\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from\
    \ numpy.typing import ArrayLike\n\n\n@dataclass\nclass Count(Stat):\n    \"\"\"\
    \n    Count distinct observations within groups.\n\n    See Also\n    --------\n\
    \    Hist : A more fully-featured transform including binning and/or normalization.\n\
    \n    Examples\n    --------\n    .. include:: ../docstrings/objects.Count.rst\n\
    \n    \"\"\"\n    group_by_orient: ClassVar[bool] = True\n\n    def __call__(\n\
    \        self, data: DataFrame, groupby: GroupBy, orient: str, scales: dict[str,\
    \ Scale],\n    ) -> DataFrame:\n\n        var = {\"x\": \"y\", \"y\": \"x\"}[orient]\n\
    \        res = (\n            groupby\n            .agg(data.assign(**{var: data[orient]}),\
    \ {var: len})\n            .dropna(subset=[\"x\", \"y\"])\n            .reset_index(drop=True)\n\
    \        )\n        return res\n\n\n@dataclass\nclass Hist(Stat):\n    \"\"\"\n\
    \    Bin observations, count them, and optionally normalize or cumulate.\n\n \
    \   Parameters\n    ----------\n    stat : str\n        Aggregate statistic to\
    \ compute in each bin:\n\n        - `count`: the number of observations\n    \
    \    - `density`: normalize so that the total area of the histogram equals 1\n\
    \        - `percent`: normalize so that bar heights sum to 100\n        - `probability`\
    \ or `proportion`: normalize so that bar heights sum to 1\n        - `frequency`:\
    \ divide the number of observations by the bin width\n\n    bins : str, int, or\
    \ ArrayLike\n        Generic parameter that can be the name of a reference rule,\
    \ the number\n        of bins, or the bin breaks. Passed to :func:`numpy.histogram_bin_edges`.\n\
    \    binwidth : float\n        Width of each bin; overrides `bins` but can be\
    \ used with `binrange`.\n        Note that if `binwidth` does not evenly divide\
    \ the bin range, the actual\n        bin width used will be only approximately\
    \ equal to the parameter value.\n    binrange : (min, max)\n        Lowest and\
    \ highest value for bin edges; can be used with either\n        `bins` (when a\
    \ number) or `binwidth`. Defaults to data extremes.\n    common_norm : bool or\
    \ list of variables\n        When not `False`, the normalization is applied across\
    \ groups. Use\n        `True` to normalize across all groups, or pass variable\
    \ name(s) that\n        define normalization groups.\n    common_bins : bool or\
    \ list of variables\n        When not `False`, the same bins are used for all\
    \ groups. Use `True` to\n        share bins across all groups, or pass variable\
    \ name(s) to share within.\n    cumulative : bool\n        If True, cumulate the\
    \ bin values.\n    discrete : bool\n        If True, set `binwidth` and `binrange`\
    \ so that bins have unit width and\n        are centered on integer values\n\n\
    \    Notes\n    -----\n    The choice of bins for computing and plotting a histogram\
    \ can exert\n    substantial influence on the insights that one is able to draw\
    \ from the\n    visualization. If the bins are too large, they may erase important\
    \ features.\n    On the other hand, bins that are too small may be dominated by\
    \ random\n    variability, obscuring the shape of the true underlying distribution.\
    \ The\n    default bin size is determined using a reference rule that depends\
    \ on the\n    sample size and variance. This works well in many cases, (i.e.,\
    \ with\n    \"well-behaved\" data) but it fails in others. It is always a good\
    \ to try\n    different bin sizes to be sure that you are not missing something\
    \ important.\n    This function allows you to specify bins in several different\
    \ ways, such as\n    by setting the total number of bins to use, the width of\
    \ each bin, or the\n    specific locations where the bins should break.\n\n  \
    \  Examples\n    --------\n    .. include:: ../docstrings/objects.Hist.rst\n\n\
    \    \"\"\"\n    stat: str = \"count\"\n    bins: str | int | ArrayLike = \"auto\"\
    \n    binwidth: float | None = None\n    binrange: tuple[float, float] | None\
    \ = None\n    common_norm: bool | list[str] = True\n    common_bins: bool | list[str]\
    \ = True\n    cumulative: bool = False\n    discrete: bool = False\n\n    def\
    \ __post_init__(self):\n\n        stat_options = [\n            \"count\", \"\
    density\", \"percent\", \"probability\", \"proportion\", \"frequency\"\n     \
    \   ]\n        self._check_param_one_of(\"stat\", stat_options)\n\n    def _define_bin_edges(self,\
    \ vals, weight, bins, binwidth, binrange, discrete):\n        \"\"\"Inner function\
    \ that takes bin parameters as arguments.\"\"\"\n        vals = vals.replace(-np.inf,\
    \ np.nan).replace(np.inf, np.nan).dropna()\n\n        if binrange is None:\n \
    \           start, stop = vals.min(), vals.max()\n        else:\n            start,\
    \ stop = binrange\n\n        if discrete:\n            bin_edges = np.arange(start\
    \ - .5, stop + 1.5)\n        else:\n            if binwidth is not None:\n   \
    \             bins = int(round((stop - start) / binwidth))\n            bin_edges\
    \ = np.histogram_bin_edges(vals, bins, binrange, weight)\n\n        # TODO warning\
    \ or cap on too many bins?\n\n        return bin_edges\n\n    def _define_bin_params(self,\
    \ data, orient, scale_type):\n        \"\"\"Given data, return numpy.histogram\
    \ parameters to define bins.\"\"\"\n        vals = data[orient]\n        weights\
    \ = data.get(\"weight\", None)\n\n        # TODO We'll want this for ordinal /\
    \ discrete scales too\n        # (Do we need discrete as a parameter or just infer\
    \ from scale?)\n        discrete = self.discrete or scale_type == \"nominal\"\n\
    \n        bin_edges = self._define_bin_edges(\n            vals, weights, self.bins,\
    \ self.binwidth, self.binrange, discrete,\n        )\n\n        if isinstance(self.bins,\
    \ (str, int)):\n            n_bins = len(bin_edges) - 1\n            bin_range\
    \ = bin_edges.min(), bin_edges.max()\n            bin_kws = dict(bins=n_bins,\
    \ range=bin_range)\n        else:\n            bin_kws = dict(bins=bin_edges)\n\
    \n        return bin_kws\n\n    def _get_bins_and_eval(self, data, orient, groupby,\
    \ scale_type):\n\n        bin_kws = self._define_bin_params(data, orient, scale_type)\n\
    \        return groupby.apply(data, self._eval, orient, bin_kws)\n\n    def _eval(self,\
    \ data, orient, bin_kws):\n\n        vals = data[orient]\n        weights = data.get(\"\
    weight\", None)\n\n        density = self.stat == \"density\"\n        hist, edges\
    \ = np.histogram(vals, **bin_kws, weights=weights, density=density)\n\n      \
    \  width = np.diff(edges)\n        center = edges[:-1] + width / 2\n\n       \
    \ return pd.DataFrame({orient: center, \"count\": hist, \"space\": width})\n\n\
    \    def _normalize(self, data):\n\n        hist = data[\"count\"]\n        if\
    \ self.stat == \"probability\" or self.stat == \"proportion\":\n            hist\
    \ = hist.astype(float) / hist.sum()\n        elif self.stat == \"percent\":\n\
    \            hist = hist.astype(float) / hist.sum() * 100\n        elif self.stat\
    \ == \"frequency\":\n            hist = hist.astype(float) / data[\"space\"]\n\
    \n        if self.cumulative:\n            if self.stat in [\"density\", \"frequency\"\
    ]:\n                hist = (hist * data[\"space\"]).cumsum()\n            else:\n\
    \                hist = hist.cumsum()\n\n        return data.assign(**{self.stat:\
    \ hist})\n\n    def __call__(\n        self, data: DataFrame, groupby: GroupBy,\
    \ orient: str, scales: dict[str, Scale],\n    ) -> DataFrame:\n\n        scale_type\
    \ = scales[orient].__class__.__name__.lower()\n        grouping_vars = [str(v)\
    \ for v in data if v in groupby.order]\n        if not grouping_vars or self.common_bins\
    \ is True:\n            bin_kws = self._define_bin_params(data, orient, scale_type)\n\
    \            data = groupby.apply(data, self._eval, orient, bin_kws)\n       \
    \ else:\n            if self.common_bins is False:\n                bin_groupby\
    \ = GroupBy(grouping_vars)\n            else:\n                bin_groupby = GroupBy(self.common_bins)\n\
    \                self._check_grouping_vars(\"common_bins\", grouping_vars)\n\n\
    \            data = bin_groupby.apply(\n                data, self._get_bins_and_eval,\
    \ orient, groupby, scale_type,\n            )\n\n        if not grouping_vars\
    \ or self.common_norm is True:\n            data = self._normalize(data)\n   \
    \     else:\n            if self.common_norm is False:\n                norm_groupby\
    \ = GroupBy(grouping_vars)\n            else:\n                norm_groupby =\
    \ GroupBy(self.common_norm)\n                self._check_grouping_vars(\"common_norm\"\
    , grouping_vars)\n            data = norm_groupby.apply(data, self._normalize)\n\
    \n        other = {\"x\": \"y\", \"y\": \"x\"}[orient]\n        return data.assign(**{other:\
    \ data[self.stat]})\n\n### Source File Dependency Files Content\n### Dependency\
    \ File: base.py\n\"\"\"Base module for statistical transformations.\"\"\"\nfrom\
    \ __future__ import annotations\nfrom collections.abc import Iterable\nfrom dataclasses\
    \ import dataclass\nfrom typing import ClassVar, Any\nimport warnings\n\nfrom\
    \ typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from pandas import DataFrame\n\
    \    from seaborn._core.groupby import GroupBy\n    from seaborn._core.scales\
    \ import Scale\n\n\n@dataclass\nclass Stat:\n    \"\"\"Base class for objects\
    \ that apply statistical transformations.\"\"\"\n\n    # The class supports a\
    \ partial-function application pattern. The object is\n    # initialized with\
    \ desired parameters and the result is a callable that\n    # accepts and returns\
    \ dataframes.\n\n    # The statistical transformation logic should not add any\
    \ state to the instance\n    # beyond what is defined with the initialization\
    \ parameters.\n\n    # Subclasses can declare whether the orient dimension should\
    \ be used in grouping\n    # TODO consider whether this should be a parameter.\
    \ Motivating example:\n    # use the same KDE class violin plots and univariate\
    \ density estimation.\n    # In the former case, we would expect separate densities\
    \ for each unique\n    # value on the orient axis, but we would not in the latter\
    \ case.\n    group_by_orient: ClassVar[bool] = False\n\n    def _check_param_one_of(self,\
    \ param: str, options: Iterable[Any]) -> None:\n        \"\"\"Raise when parameter\
    \ value is not one of a specified set.\"\"\"\n        value = getattr(self, param)\n\
    \        if value not in options:\n            *most, last = options\n       \
    \     option_str = \", \".join(f\"{x!r}\" for x in most[:-1]) + f\" or {last!r}\"\
    \n            err = \" \".join([\n                f\"The `{param}` parameter for\
    \ `{self.__class__.__name__}` must be\",\n                f\"one of {option_str};\
    \ not {value!r}.\",\n            ])\n            raise ValueError(err)\n\n   \
    \ def _check_grouping_vars(\n        self, param: str, data_vars: list[str], stacklevel:\
    \ int = 2,\n    ) -> None:\n        \"\"\"Warn if vars are named in parameter\
    \ without being present in the data.\"\"\"\n        param_vars = getattr(self,\
    \ param)\n        undefined = set(param_vars) - set(data_vars)\n        if undefined:\n\
    \            param = f\"{self.__class__.__name__}.{param}\"\n            names\
    \ = \", \".join(f\"{x!r}\" for x in undefined)\n            msg = f\"Undefined\
    \ variable(s) passed for {param}: {names}.\"\n            warnings.warn(msg, stacklevel=stacklevel)\n\
    \n    def __call__(\n        self,\n        data: DataFrame,\n        groupby:\
    \ GroupBy,\n        orient: str,\n        scales: dict[str, Scale],\n    ) ->\
    \ DataFrame:\n        \"\"\"Apply statistical transform to data subgroups and\
    \ return combined result.\"\"\"\n        return data\n\n\n### Dependency File:\
    \ groupby.py\n\"\"\"Simplified split-apply-combine paradigm on dataframes for\
    \ internal use.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import\
    \ cast, Iterable\n\nimport pandas as pd\n\nfrom seaborn._core.rules import categorical_order\n\
    \nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from typing import\
    \ Callable\n    from pandas import DataFrame, MultiIndex, Index\n\n\nclass GroupBy:\n\
    \    \"\"\"\n    Interface for Pandas GroupBy operations allowing specified group\
    \ order.\n\n    Writing our own class to do this has a few advantages:\n    -\
    \ It constrains the interface between Plot and Stat/Move objects\n    - It allows\
    \ control over the row order of the GroupBy result, which is\n      important\
    \ when using in the context of some Move operations (dodge, stack, ...)\n    -\
    \ It simplifies some complexities regarding the return type and Index contents\n\
    \      one encounters with Pandas, especially for DataFrame -> DataFrame applies\n\
    \    - It increases future flexibility regarding alternate DataFrame libraries\n\
    \n    \"\"\"\n    def __init__(self, order: list[str] | dict[str, list | None]):\n\
    \        \"\"\"\n        Initialize the GroupBy from grouping variables and optional\
    \ level orders.\n\n        Parameters\n        ----------\n        order\n   \
    \         List of variable names or dict mapping names to desired level orders.\n\
    \            Level order values can be None to use default ordering rules. The\n\
    \            variables can include names that are not expected to appear in the\n\
    \            data; these will be dropped before the groups are defined.\n\n  \
    \      \"\"\"\n        if not order:\n            raise ValueError(\"GroupBy requires\
    \ at least one grouping variable\")\n\n        if isinstance(order, list):\n \
    \           order = {k: None for k in order}\n        self.order = order\n\n \
    \   def _get_groups(\n        self, data: DataFrame\n    ) -> tuple[str | list[str],\
    \ Index | MultiIndex]:\n        \"\"\"Return index with Cartesian product of ordered\
    \ grouping variable levels.\"\"\"\n        levels = {}\n        for var, order\
    \ in self.order.items():\n            if var in data:\n                if order\
    \ is None:\n                    order = categorical_order(data[var])\n       \
    \         levels[var] = order\n\n        grouper: str | list[str]\n        groups:\
    \ Index | MultiIndex\n        if not levels:\n            grouper = []\n     \
    \       groups = pd.Index([])\n        elif len(levels) > 1:\n            grouper\
    \ = list(levels)\n            groups = pd.MultiIndex.from_product(levels.values(),\
    \ names=grouper)\n        else:\n            grouper, = list(levels)\n       \
    \     groups = pd.Index(levels[grouper], name=grouper)\n        return grouper,\
    \ groups\n\n    def _reorder_columns(self, res, data):\n        \"\"\"Reorder\
    \ result columns to match original order with new columns appended.\"\"\"\n  \
    \      cols = [c for c in data if c in res]\n        cols += [c for c in res if\
    \ c not in data]\n        return res.reindex(columns=pd.Index(cols))\n\n    def\
    \ agg(self, data: DataFrame, *args, **kwargs) -> DataFrame:\n        \"\"\"\n\
    \        Reduce each group to a single row in the output.\n\n        The output\
    \ will have a row for each unique combination of the grouping\n        variable\
    \ levels with null values for the aggregated variable(s) where\n        those\
    \ combinations do not appear in the dataset.\n\n        \"\"\"\n        grouper,\
    \ groups = self._get_groups(data)\n\n        if not grouper:\n            # We\
    \ will need to see whether there are valid usecases that end up here\n       \
    \     raise ValueError(\"No grouping variables are present in dataframe\")\n\n\
    \        res = (\n            data\n            .groupby(grouper, sort=False,\
    \ observed=False)\n            .agg(*args, **kwargs)\n            .reindex(groups)\n\
    \            .reset_index()\n            .pipe(self._reorder_columns, data)\n\
    \        )\n\n        return res\n\n    def apply(\n        self, data: DataFrame,\
    \ func: Callable[..., DataFrame],\n        *args, **kwargs,\n    ) -> DataFrame:\n\
    \        \"\"\"Apply a DataFrame -> DataFrame mapping to each group.\"\"\"\n \
    \       grouper, groups = self._get_groups(data)\n\n        if not grouper:\n\
    \            return self._reorder_columns(func(data, *args, **kwargs), data)\n\
    \n        parts = {}\n        for key, part_df in data.groupby(grouper, sort=False,\
    \ observed=False):\n            parts[key] = func(part_df, *args, **kwargs)\n\
    \        stack = []\n        for key in groups:\n            if key in parts:\n\
    \                if isinstance(grouper, list):\n                    # Implies\
    \ that we had a MultiIndex so key is iterable\n                    group_ids =\
    \ dict(zip(grouper, cast(Iterable, key)))\n                else:\n           \
    \         group_ids = {grouper: key}\n                stack.append(parts[key].assign(**group_ids))\n\
    \n        res = pd.concat(stack, ignore_index=True)\n        return self._reorder_columns(res,\
    \ data)\n\n\n### Dependency File: scales.py\nfrom __future__ import annotations\n\
    import re\nfrom copy import copy\nfrom collections.abc import Sequence\nfrom dataclasses\
    \ import dataclass\nfrom functools import partial\nfrom typing import Any, Callable,\
    \ Tuple, Optional, ClassVar\n\nimport numpy as np\nimport matplotlib as mpl\n\
    from matplotlib.ticker import (\n    Locator,\n    Formatter,\n    AutoLocator,\n\
    \    AutoMinorLocator,\n    FixedLocator,\n    LinearLocator,\n    LogLocator,\n\
    \    SymmetricalLogLocator,\n    MaxNLocator,\n    MultipleLocator,\n    EngFormatter,\n\
    \    FuncFormatter,\n    LogFormatterSciNotation,\n    ScalarFormatter,\n    StrMethodFormatter,\n\
    )\nfrom matplotlib.dates import (\n    AutoDateLocator,\n    AutoDateFormatter,\n\
    \    ConciseDateFormatter,\n)\nfrom matplotlib.axis import Axis\nfrom matplotlib.scale\
    \ import ScaleBase\nfrom pandas import Series\n\nfrom seaborn._core.rules import\
    \ categorical_order\nfrom seaborn._core.typing import Default, default\n\nfrom\
    \ typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from seaborn._core.plot\
    \ import Plot\n    from seaborn._core.properties import Property\n    from numpy.typing\
    \ import ArrayLike, NDArray\n\n    TransFuncs = Tuple[\n        Callable[[ArrayLike],\
    \ ArrayLike], Callable[[ArrayLike], ArrayLike]\n    ]\n\n    # TODO Reverting\
    \ typing to Any as it was proving too complicated to\n    # work out the right\
    \ way to communicate the types to mypy. Revisit!\n    Pipeline = Sequence[Optional[Callable[[Any],\
    \ Any]]]\n\n\nclass Scale:\n    \"\"\"Base class for objects that map data values\
    \ to visual properties.\"\"\"\n\n    values: tuple | str | list | dict | None\n\
    \n    _priority: ClassVar[int]\n    _pipeline: Pipeline\n    _matplotlib_scale:\
    \ ScaleBase\n    _spacer: staticmethod\n    _legend: tuple[list[Any], list[str]]\
    \ | None\n\n    def __post_init__(self):\n\n        self._tick_params = None\n\
    \        self._label_params = None\n        self._legend = None\n\n    def tick(self):\n\
    \        raise NotImplementedError()\n\n    def label(self):\n        raise NotImplementedError()\n\
    \n    def _get_locators(self):\n        raise NotImplementedError()\n\n    def\
    \ _get_formatter(self, locator: Locator | None = None):\n        raise NotImplementedError()\n\
    \n    def _get_scale(self, name: str, forward: Callable, inverse: Callable):\n\
    \n        major_locator, minor_locator = self._get_locators(**self._tick_params)\n\
    \        major_formatter = self._get_formatter(major_locator, **self._label_params)\n\
    \n        class InternalScale(mpl.scale.FuncScale):\n            def set_default_locators_and_formatters(self,\
    \ axis):\n                axis.set_major_locator(major_locator)\n            \
    \    if minor_locator is not None:\n                    axis.set_minor_locator(minor_locator)\n\
    \                axis.set_major_formatter(major_formatter)\n\n        return InternalScale(name,\
    \ (forward, inverse))\n\n    def _spacing(self, x: Series) -> float:\n       \
    \ space = self._spacer(x)\n        if np.isnan(space):\n            # This happens\
    \ when there is no variance in the orient coordinate data\n            # Not exactly\
    \ clear what the right default is, but 1 seems reasonable?\n            return\
    \ 1\n        return space\n\n    def _setup(\n        self, data: Series, prop:\
    \ Property, axis: Axis | None = None,\n    ) -> Scale:\n        raise NotImplementedError()\n\
    \n    def _finalize(self, p: Plot, axis: Axis) -> None:\n        \"\"\"Perform\
    \ scale-specific axis tweaks after adding artists.\"\"\"\n        pass\n\n   \
    \ def __call__(self, data: Series) -> ArrayLike:\n\n        trans_data: Series\
    \ | NDArray | list\n\n        # TODO sometimes we need to handle scalars (e.g.\
    \ for Line)\n        # but what is the best way to do that?\n        scalar_data\
    \ = np.isscalar(data)\n        if scalar_data:\n            trans_data = np.array([data])\n\
    \        else:\n            trans_data = data\n\n        for func in self._pipeline:\n\
    \            if func is not None:\n                trans_data = func(trans_data)\n\
    \n        if scalar_data:\n            return trans_data[0]\n        else:\n \
    \           return trans_data\n\n    @staticmethod\n    def _identity():\n\n \
    \       class Identity(Scale):\n            _pipeline = []\n            _spacer\
    \ = None\n            _legend = None\n            _matplotlib_scale = None\n\n\
    \        return Identity()\n\n\n@dataclass\nclass Boolean(Scale):\n    \"\"\"\n\
    \    A scale with a discrete domain of True and False values.\n\n    The behavior\
    \ is similar to the :class:`Nominal` scale, but property\n    mappings and legends\
    \ will use a [True, False] ordering rather than\n    a sort using numeric rules.\
    \ Coordinate variables accomplish this by\n    inverting axis limits so as to\
    \ maintain underlying numeric positioning.\n    Input data are cast to boolean\
    \ values, respecting missing data.\n\n    \"\"\"\n    values: tuple | list | dict\
    \ | None = None\n\n    _priority: ClassVar[int] = 3\n\n    def _setup(\n     \
    \   self, data: Series, prop: Property, axis: Axis | None = None,\n    ) -> Scale:\n\
    \n        new = copy(self)\n        if new._tick_params is None:\n           \
    \ new = new.tick()\n        if new._label_params is None:\n            new = new.label()\n\
    \n        def na_safe_cast(x):\n            # TODO this doesn't actually need\
    \ to be a closure\n            if np.isscalar(x):\n                return float(bool(x))\n\
    \            else:\n                if hasattr(x, \"notna\"):\n              \
    \      # Handle pd.NA; np<>pd interop with NA is tricky\n                    use\
    \ = x.notna().to_numpy()\n                else:\n                    use = np.isfinite(x)\n\
    \                out = np.full(len(x), np.nan, dtype=float)\n                out[use]\
    \ = x[use].astype(bool).astype(float)\n                return out\n\n        new._pipeline\
    \ = [na_safe_cast, prop.get_mapping(new, data)]\n        new._spacer = _default_spacer\n\
    \        if prop.legend:\n            new._legend = [True, False], [\"True\",\
    \ \"False\"]\n\n        forward, inverse = _make_identity_transforms()\n     \
    \   mpl_scale = new._get_scale(str(data.name), forward, inverse)\n\n        axis\
    \ = PseudoAxis(mpl_scale) if axis is None else axis\n        mpl_scale.set_default_locators_and_formatters(axis)\n\
    \        new._matplotlib_scale = mpl_scale\n\n        return new\n\n    def _finalize(self,\
    \ p: Plot, axis: Axis) -> None:\n\n        # We want values to appear in a True,\
    \ False order but also want\n        # True/False to be drawn at 1/0 positions\
    \ respectively to avoid nasty\n        # surprises if additional artists are added\
    \ through the matplotlib API.\n        # We accomplish this using axis inversion\
    \ akin to what we do in Nominal.\n\n        ax = axis.axes\n        name = axis.axis_name\n\
    \        axis.grid(False, which=\"both\")\n        if name not in p._limits:\n\
    \            nticks = len(axis.get_major_ticks())\n            lo, hi = -.5, nticks\
    \ - .5\n            if name == \"x\":\n                lo, hi = hi, lo\n     \
    \       set_lim = getattr(ax, f\"set_{name}lim\")\n            set_lim(lo, hi,\
    \ auto=None)\n\n    def tick(self, locator: Locator | None = None):\n        new\
    \ = copy(self)\n        new._tick_params = {\"locator\": locator}\n        return\
    \ new\n\n    def label(self, formatter: Formatter | None = None):\n        new\
    \ = copy(self)\n        new._label_params = {\"formatter\": formatter}\n     \
    \   return new\n\n    def _get_locators(self, locator):\n        if locator is\
    \ not None:\n            return locator\n        return FixedLocator([0, 1]),\
    \ None\n\n    def _get_formatter(self, locator, formatter):\n        if formatter\
    \ is not None:\n            return formatter\n        return FuncFormatter(lambda\
    \ x, _: str(bool(x)))\n\n\n@dataclass\nclass Nominal(Scale):\n    \"\"\"\n   \
    \ A categorical scale without relative importance / magnitude.\n    \"\"\"\n \
    \   # Categorical (convert to strings), un-sortable\n\n    values: tuple | str\
    \ | list | dict | None = None\n    order: list | None = None\n\n    _priority:\
    \ ClassVar[int] = 4\n\n    def _setup(\n        self, data: Series, prop: Property,\
    \ axis: Axis | None = None,\n    ) -> Scale:\n\n        new = copy(self)\n   \
    \     if new._tick_params is None:\n            new = new.tick()\n        if new._label_params\
    \ is None:\n            new = new.label()\n\n        # TODO flexibility over format()\
    \ which isn't great for numbers / dates\n        stringify = np.vectorize(format,\
    \ otypes=[\"object\"])\n\n        units_seed = categorical_order(data, new.order)\n\
    \n        # TODO move to Nominal._get_scale?\n        # TODO this needs some more\
    \ complicated rethinking about how to pass\n        # a unit dictionary down to\
    \ these methods, along with how much we want\n        # to invest in their API.\
    \ What is it useful for tick() to do here?\n        # (Ordinal may be different\
    \ if we draw that contrast).\n        # Any customization we do to allow, e.g.,\
    \ label wrapping will probably\n        # require defining our own Formatter subclass.\n\
    \        # We could also potentially implement auto-wrapping in an Axis subclass\n\
    \        # (see Axis.draw ... it already is computing the bboxes).\n        #\
    \ major_locator, minor_locator = new._get_locators(**new._tick_params)\n     \
    \   # major_formatter = new._get_formatter(major_locator, **new._label_params)\n\
    \n        class CatScale(mpl.scale.LinearScale):\n            def set_default_locators_and_formatters(self,\
    \ axis):\n                ...\n                # axis.set_major_locator(major_locator)\n\
    \                # if minor_locator is not None:\n                #     axis.set_minor_locator(minor_locator)\n\
    \                # axis.set_major_formatter(major_formatter)\n\n        mpl_scale\
    \ = CatScale(data.name)\n        if axis is None:\n            axis = PseudoAxis(mpl_scale)\n\
    \n            # TODO Currently just used in non-Coordinate contexts, but should\n\
    \            # we use this to (A) set the padding we want for categorial plots\n\
    \            # and (B) allow the values parameter for a Coordinate to set xlim/ylim\n\
    \            axis.set_view_interval(0, len(units_seed) - 1)\n\n        new._matplotlib_scale\
    \ = mpl_scale\n\n        # TODO array cast necessary to handle float/int mixture,\
    \ which we need\n        # to solve in a more systematic way probably\n      \
    \  # (i.e. if we have [1, 2.5], do we want [1.0, 2.5]? Unclear)\n        axis.update_units(stringify(np.array(units_seed)))\n\
    \n        # TODO define this more centrally\n        def convert_units(x):\n \
    \           # TODO only do this with explicit order?\n            # (But also\
    \ category dtype?)\n            # TODO isin fails when units_seed mixes numbers\
    \ and strings (numpy error?)\n            # but np.isin also does not seem any\
    \ faster? (Maybe not broadcasting in C)\n            # keep = x.isin(units_seed)\n\
    \            keep = np.array([x_ in units_seed for x_ in x], bool)\n         \
    \   out = np.full(len(x), np.nan)\n            out[keep] = axis.convert_units(stringify(x[keep]))\n\
    \            return out\n\n        new._pipeline = [convert_units, prop.get_mapping(new,\
    \ data)]\n        new._spacer = _default_spacer\n\n        if prop.legend:\n \
    \           new._legend = units_seed, list(stringify(units_seed))\n\n        return\
    \ new\n\n    def _finalize(self, p: Plot, axis: Axis) -> None:\n\n        ax =\
    \ axis.axes\n        name = axis.axis_name\n        axis.grid(False, which=\"\
    both\")\n        if name not in p._limits:\n            nticks = len(axis.get_major_ticks())\n\
    \            lo, hi = -.5, nticks - .5\n            if name == \"y\":\n      \
    \          lo, hi = hi, lo\n            set_lim = getattr(ax, f\"set_{name}lim\"\
    )\n            set_lim(lo, hi, auto=None)\n\n    def tick(self, locator: Locator\
    \ | None = None) -> Nominal:\n        \"\"\"\n        Configure the selection\
    \ of ticks for the scale's axis or legend.\n\n        .. note::\n            This\
    \ API is under construction and will be enhanced over time.\n            At the\
    \ moment, it is probably not very useful.\n\n        Parameters\n        ----------\n\
    \        locator : :class:`matplotlib.ticker.Locator` subclass\n            Pre-configured\
    \ matplotlib locator; other parameters will not be used.\n\n        Returns\n\
    \        -------\n        Copy of self with new tick configuration.\n\n      \
    \  \"\"\"\n        new = copy(self)\n        new._tick_params = {\"locator\":\
    \ locator}\n        return new\n\n    def label(self, formatter: Formatter | None\
    \ = None) -> Nominal:\n        \"\"\"\n        Configure the selection of labels\
    \ for the scale's axis or legend.\n\n        .. note::\n            This API is\
    \ under construction and will be enhanced over time.\n            At the moment,\
    \ it is probably not very useful.\n\n        Parameters\n        ----------\n\
    \        formatter : :class:`matplotlib.ticker.Formatter` subclass\n         \
    \   Pre-configured matplotlib formatter; other parameters will not be used.\n\n\
    \        Returns\n        -------\n        scale\n            Copy of self with\
    \ new tick configuration.\n\n        \"\"\"\n        new = copy(self)\n      \
    \  new._label_params = {\"formatter\": formatter}\n        return new\n\n    def\
    \ _get_locators(self, locator):\n\n        if locator is not None:\n         \
    \   return locator, None\n\n        locator = mpl.category.StrCategoryLocator({})\n\
    \n        return locator, None\n\n    def _get_formatter(self, locator, formatter):\n\
    \n        if formatter is not None:\n            return formatter\n\n        formatter\
    \ = mpl.category.StrCategoryFormatter({})\n\n        return formatter\n\n\n@dataclass\n\
    class Ordinal(Scale):\n    # Categorical (convert to strings), sortable, can skip\
    \ ticklabels\n    ...\n\n\n@dataclass\nclass Discrete(Scale):\n    # Numeric,\
    \ integral, can skip ticks/ticklabels\n    ...\n\n\n@dataclass\nclass ContinuousBase(Scale):\n\
    \n    values: tuple | str | None = None\n    norm: tuple | None = None\n\n   \
    \ def _setup(\n        self, data: Series, prop: Property, axis: Axis | None =\
    \ None,\n    ) -> Scale:\n\n        new = copy(self)\n        if new._tick_params\
    \ is None:\n            new = new.tick()\n        if new._label_params is None:\n\
    \            new = new.label()\n\n        forward, inverse = new._get_transform()\n\
    \n        mpl_scale = new._get_scale(str(data.name), forward, inverse)\n\n   \
    \     if axis is None:\n            axis = PseudoAxis(mpl_scale)\n           \
    \ axis.update_units(data)\n\n        mpl_scale.set_default_locators_and_formatters(axis)\n\
    \        new._matplotlib_scale = mpl_scale\n\n        normalize: Optional[Callable[[ArrayLike],\
    \ ArrayLike]]\n        if prop.normed:\n            if new.norm is None:\n   \
    \             vmin, vmax = data.min(), data.max()\n            else:\n       \
    \         vmin, vmax = new.norm\n            vmin, vmax = map(float, axis.convert_units((vmin,\
    \ vmax)))\n            a = forward(vmin)\n            b = forward(vmax) - forward(vmin)\n\
    \n            def normalize(x):\n                return (x - a) / b\n\n      \
    \  else:\n            normalize = vmin = vmax = None\n\n        new._pipeline\
    \ = [\n            axis.convert_units,\n            forward,\n            normalize,\n\
    \            prop.get_mapping(new, data)\n        ]\n\n        def spacer(x):\n\
    \            x = x.dropna().unique()\n            if len(x) < 2:\n           \
    \     return np.nan\n            return np.min(np.diff(np.sort(x)))\n        new._spacer\
    \ = spacer\n\n        # TODO How to allow disabling of legend for all uses of\
    \ property?\n        # Could add a Scale parameter, or perhaps Scale.suppress()?\n\
    \        # Are there other useful parameters that would be in Scale.legend()\n\
    \        # besides allowing Scale.legend(False)?\n        if prop.legend:\n  \
    \          axis.set_view_interval(vmin, vmax)\n            locs = axis.major.locator()\n\
    \            locs = locs[(vmin <= locs) & (locs <= vmax)]\n            # Avoid\
    \ having an offset / scientific notation in a legend\n            # as we don't\
    \ represent that anywhere so it ends up incorrect.\n            # This could become\
    \ an option (e.g. Continuous.label(offset=True))\n            # in which case\
    \ we would need to figure out how to show it.\n            if hasattr(axis.major.formatter,\
    \ \"set_useOffset\"):\n                axis.major.formatter.set_useOffset(False)\n\
    \            if hasattr(axis.major.formatter, \"set_scientific\"):\n         \
    \       axis.major.formatter.set_scientific(False)\n            labels = axis.major.formatter.format_ticks(locs)\n\
    \            new._legend = list(locs), list(labels)\n\n        return new\n\n\
    \    def _get_transform(self):\n\n        arg = self.trans\n\n        def get_param(method,\
    \ default):\n            if arg == method:\n                return default\n \
    \           return float(arg[len(method):])\n\n        if arg is None:\n     \
    \       return _make_identity_transforms()\n        elif isinstance(arg, tuple):\n\
    \            return arg\n        elif isinstance(arg, str):\n            if arg\
    \ == \"ln\":\n                return _make_log_transforms()\n            elif\
    \ arg == \"logit\":\n                base = get_param(\"logit\", 10)\n       \
    \         return _make_logit_transforms(base)\n            elif arg.startswith(\"\
    log\"):\n                base = get_param(\"log\", 10)\n                return\
    \ _make_log_transforms(base)\n            elif arg.startswith(\"symlog\"):\n \
    \               c = get_param(\"symlog\", 1)\n                return _make_symlog_transforms(c)\n\
    \            elif arg.startswith(\"pow\"):\n                exp = get_param(\"\
    pow\", 2)\n                return _make_power_transforms(exp)\n            elif\
    \ arg == \"sqrt\":\n                return _make_sqrt_transforms()\n         \
    \   else:\n                raise ValueError(f\"Unknown value provided for trans:\
    \ {arg!r}\")\n\n\n@dataclass\nclass Continuous(ContinuousBase):\n    \"\"\"\n\
    \    A numeric scale supporting norms and functional transforms.\n    \"\"\"\n\
    \    values: tuple | str | None = None\n    trans: str | TransFuncs | None = None\n\
    \n    # TODO Add this to deal with outliers?\n    # outside: Literal[\"keep\"\
    , \"drop\", \"clip\"] = \"keep\"\n\n    _priority: ClassVar[int] = 1\n\n    def\
    \ tick(\n        self,\n        locator: Locator | None = None, *,\n        at:\
    \ Sequence[float] | None = None,\n        upto: int | None = None,\n        count:\
    \ int | None = None,\n        every: float | None = None,\n        between: tuple[float,\
    \ float] | None = None,\n        minor: int | None = None,\n    ) -> Continuous:\n\
    \        \"\"\"\n        Configure the selection of ticks for the scale's axis\
    \ or legend.\n\n        Parameters\n        ----------\n        locator : :class:`matplotlib.ticker.Locator`\
    \ subclass\n            Pre-configured matplotlib locator; other parameters will\
    \ not be used.\n        at : sequence of floats\n            Place ticks at these\
    \ specific locations (in data units).\n        upto : int\n            Choose\
    \ \"nice\" locations for ticks, but do not exceed this number.\n        count\
    \ : int\n            Choose exactly this number of ticks, bounded by `between`\
    \ or axis limits.\n        every : float\n            Choose locations at this\
    \ interval of separation (in data units).\n        between : pair of floats\n\
    \            Bound upper / lower ticks when using `every` or `count`.\n      \
    \  minor : int\n            Number of unlabeled ticks to draw between labeled\
    \ \"major\" ticks.\n\n        Returns\n        -------\n        scale\n      \
    \      Copy of self with new tick configuration.\n\n        \"\"\"\n        #\
    \ Input checks\n        if locator is not None and not isinstance(locator, Locator):\n\
    \            raise TypeError(\n                f\"Tick locator must be an instance\
    \ of {Locator!r}, \"\n                f\"not {type(locator)!r}.\"\n          \
    \  )\n        log_base, symlog_thresh = self._parse_for_log_params(self.trans)\n\
    \        if log_base or symlog_thresh:\n            if count is not None and between\
    \ is None:\n                raise RuntimeError(\"`count` requires `between` with\
    \ log transform.\")\n            if every is not None:\n                raise\
    \ RuntimeError(\"`every` not supported with log transform.\")\n\n        new =\
    \ copy(self)\n        new._tick_params = {\n            \"locator\": locator,\n\
    \            \"at\": at,\n            \"upto\": upto,\n            \"count\":\
    \ count,\n            \"every\": every,\n            \"between\": between,\n \
    \           \"minor\": minor,\n        }\n        return new\n\n    def label(\n\
    \        self,\n        formatter: Formatter | None = None, *,\n        like:\
    \ str | Callable | None = None,\n        base: int | None | Default = default,\n\
    \        unit: str | None = None,\n    ) -> Continuous:\n        \"\"\"\n    \
    \    Configure the appearance of tick labels for the scale's axis or legend.\n\
    \n        Parameters\n        ----------\n        formatter : :class:`matplotlib.ticker.Formatter`\
    \ subclass\n            Pre-configured formatter to use; other parameters will\
    \ be ignored.\n        like : str or callable\n            Either a format pattern\
    \ (e.g., `\".2f\"`), a format string with fields named\n            `x` and/or\
    \ `pos` (e.g., `\"${x:.2f}\"`), or a callable with a signature like\n        \
    \    `f(x: float, pos: int) -> str`. In the latter variants, `x` is passed as\
    \ the\n            tick value and `pos` is passed as the tick index.\n       \
    \ base : number\n            Use log formatter (with scientific notation) having\
    \ this value as the base.\n            Set to `None` to override the default formatter\
    \ with a log transform.\n        unit : str or (str, str) tuple\n            Use\
    \  SI prefixes with these units (e.g., with `unit=\"g\"`, a tick value\n     \
    \       of 5000 will appear as `5 kg`). When a tuple, the first element gives\
    \ the\n            separator between the number and unit.\n\n        Returns\n\
    \        -------\n        scale\n            Copy of self with new label configuration.\n\
    \n        \"\"\"\n        # Input checks\n        if formatter is not None and\
    \ not isinstance(formatter, Formatter):\n            raise TypeError(\n      \
    \          f\"Label formatter must be an instance of {Formatter!r}, \"\n     \
    \           f\"not {type(formatter)!r}\"\n            )\n        if like is not\
    \ None and not (isinstance(like, str) or callable(like)):\n            msg = f\"\
    `like` must be a string or callable, not {type(like).__name__}.\"\n          \
    \  raise TypeError(msg)\n\n        new = copy(self)\n        new._label_params\
    \ = {\n            \"formatter\": formatter,\n            \"like\": like,\n  \
    \          \"base\": base,\n            \"unit\": unit,\n        }\n        return\
    \ new\n\n    def _parse_for_log_params(\n        self, trans: str | TransFuncs\
    \ | None\n    ) -> tuple[float | None, float | None]:\n\n        log_base = symlog_thresh\
    \ = None\n        if isinstance(trans, str):\n            m = re.match(r\"^log(\\\
    d*)\", trans)\n            if m is not None:\n                log_base = float(m[1]\
    \ or 10)\n            m = re.match(r\"symlog(\\d*)\", trans)\n            if m\
    \ is not None:\n                symlog_thresh = float(m[1] or 1)\n        return\
    \ log_base, symlog_thresh\n\n    def _get_locators(self, locator, at, upto, count,\
    \ every, between, minor):\n\n        log_base, symlog_thresh = self._parse_for_log_params(self.trans)\n\
    \n        if locator is not None:\n            major_locator = locator\n\n   \
    \     elif upto is not None:\n            if log_base:\n                major_locator\
    \ = LogLocator(base=log_base, numticks=upto)\n            else:\n            \
    \    major_locator = MaxNLocator(upto, steps=[1, 1.5, 2, 2.5, 3, 5, 10])\n\n \
    \       elif count is not None:\n            if between is None:\n           \
    \     # This is rarely useful (unless you are setting limits)\n              \
    \  major_locator = LinearLocator(count)\n            else:\n                if\
    \ log_base or symlog_thresh:\n                    forward, inverse = self._get_transform()\n\
    \                    lo, hi = forward(between)\n                    ticks = inverse(np.linspace(lo,\
    \ hi, num=count))\n                else:\n                    ticks = np.linspace(*between,\
    \ num=count)\n                major_locator = FixedLocator(ticks)\n\n        elif\
    \ every is not None:\n            if between is None:\n                major_locator\
    \ = MultipleLocator(every)\n            else:\n                lo, hi = between\n\
    \                ticks = np.arange(lo, hi + every, every)\n                major_locator\
    \ = FixedLocator(ticks)\n\n        elif at is not None:\n            major_locator\
    \ = FixedLocator(at)\n\n        else:\n            if log_base:\n            \
    \    major_locator = LogLocator(log_base)\n            elif symlog_thresh:\n \
    \               major_locator = SymmetricalLogLocator(linthresh=symlog_thresh,\
    \ base=10)\n            else:\n                major_locator = AutoLocator()\n\
    \n        if minor is None:\n            minor_locator = LogLocator(log_base,\
    \ subs=None) if log_base else None\n        else:\n            if log_base:\n\
    \                subs = np.linspace(0, log_base, minor + 2)[1:-1]\n          \
    \      minor_locator = LogLocator(log_base, subs=subs)\n            else:\n  \
    \              minor_locator = AutoMinorLocator(minor + 1)\n\n        return major_locator,\
    \ minor_locator\n\n    def _get_formatter(self, locator, formatter, like, base,\
    \ unit):\n\n        log_base, symlog_thresh = self._parse_for_log_params(self.trans)\n\
    \        if base is default:\n            if symlog_thresh:\n                log_base\
    \ = 10\n            base = log_base\n\n        if formatter is not None:\n   \
    \         return formatter\n\n        if like is not None:\n            if isinstance(like,\
    \ str):\n                if \"{x\" in like or \"{pos\" in like:\n            \
    \        fmt = like\n                else:\n                    fmt = f\"{{x:{like}}}\"\
    \n                formatter = StrMethodFormatter(fmt)\n            else:\n   \
    \             formatter = FuncFormatter(like)\n\n        elif base is not None:\n\
    \            # We could add other log options if necessary\n            formatter\
    \ = LogFormatterSciNotation(base)\n\n        elif unit is not None:\n        \
    \    if isinstance(unit, tuple):\n                sep, unit = unit\n         \
    \   elif not unit:\n                sep = \"\"\n            else:\n          \
    \      sep = \" \"\n            formatter = EngFormatter(unit, sep=sep)\n\n  \
    \      else:\n            formatter = ScalarFormatter()\n\n        return formatter\n\
    \n\n@dataclass\nclass Temporal(ContinuousBase):\n    \"\"\"\n    A scale for date/time\
    \ data.\n    \"\"\"\n    # TODO date: bool?\n    # For when we only care about\
    \ the time component, would affect\n    # default formatter and norm conversion.\
    \ Should also happen in\n    # Property.default_scale. The alternative was having\
    \ distinct\n    # Calendric / Temporal scales, but that feels a bit fussy, and\
    \ it\n    # would get in the way of using first-letter shorthands because\n  \
    \  # Calendric and Continuous would collide. Still, we haven't implemented\n \
    \   # those yet, and having a clear distinction betewen date(time) / time\n  \
    \  # may be more useful.\n\n    trans = None\n\n    _priority: ClassVar[int] =\
    \ 2\n\n    def tick(\n        self, locator: Locator | None = None, *,\n     \
    \   upto: int | None = None,\n    ) -> Temporal:\n        \"\"\"\n        Configure\
    \ the selection of ticks for the scale's axis or legend.\n\n        .. note::\n\
    \            This API is under construction and will be enhanced over time.\n\n\
    \        Parameters\n        ----------\n        locator : :class:`matplotlib.ticker.Locator`\
    \ subclass\n            Pre-configured matplotlib locator; other parameters will\
    \ not be used.\n        upto : int\n            Choose \"nice\" locations for\
    \ ticks, but do not exceed this number.\n\n        Returns\n        -------\n\
    \        scale\n            Copy of self with new tick configuration.\n\n    \
    \    \"\"\"\n        if locator is not None and not isinstance(locator, Locator):\n\
    \            err = (\n                f\"Tick locator must be an instance of {Locator!r},\
    \ \"\n                f\"not {type(locator)!r}.\"\n            )\n           \
    \ raise TypeError(err)\n\n        new = copy(self)\n        new._tick_params =\
    \ {\"locator\": locator, \"upto\": upto}\n        return new\n\n    def label(\n\
    \        self,\n        formatter: Formatter | None = None, *,\n        concise:\
    \ bool = False,\n    ) -> Temporal:\n        \"\"\"\n        Configure the appearance\
    \ of tick labels for the scale's axis or legend.\n\n        .. note::\n      \
    \      This API is under construction and will be enhanced over time.\n\n    \
    \    Parameters\n        ----------\n        formatter : :class:`matplotlib.ticker.Formatter`\
    \ subclass\n            Pre-configured formatter to use; other parameters will\
    \ be ignored.\n        concise : bool\n            If True, use :class:`matplotlib.dates.ConciseDateFormatter`\
    \ to make\n            the tick labels as compact as possible.\n\n        Returns\n\
    \        -------\n        scale\n            Copy of self with new label configuration.\n\
    \n        \"\"\"\n        new = copy(self)\n        new._label_params = {\"formatter\"\
    : formatter, \"concise\": concise}\n        return new\n\n    def _get_locators(self,\
    \ locator, upto):\n\n        if locator is not None:\n            major_locator\
    \ = locator\n        elif upto is not None:\n            major_locator = AutoDateLocator(minticks=2,\
    \ maxticks=upto)\n\n        else:\n            major_locator = AutoDateLocator(minticks=2,\
    \ maxticks=6)\n        minor_locator = None\n\n        return major_locator, minor_locator\n\
    \n    def _get_formatter(self, locator, formatter, concise):\n\n        if formatter\
    \ is not None:\n            return formatter\n\n        if concise:\n        \
    \    # TODO ideally we would have concise coordinate ticks,\n            # but\
    \ full semantic ticks. Is that possible?\n            formatter = ConciseDateFormatter(locator)\n\
    \        else:\n            formatter = AutoDateFormatter(locator)\n\n       \
    \ return formatter\n\n\n# -----------------------------------------------------------------------------------\
    \ #\n\n\n# TODO Have this separate from Temporal or have Temporal(date=True) or\
    \ similar?\n# class Calendric(Scale):\n\n# TODO Needed? Or handle this at layer\
    \ (in stat or as param, eg binning=)\n# class Binned(Scale):\n\n# TODO any need\
    \ for color-specific scales?\n# class Sequential(Continuous):\n# class Diverging(Continuous):\n\
    # class Qualitative(Nominal):\n\n\n# -----------------------------------------------------------------------------------\
    \ #\n\n\nclass PseudoAxis:\n    \"\"\"\n    Internal class implementing minimal\
    \ interface equivalent to matplotlib Axis.\n\n    Coordinate variables are typically\
    \ scaled by attaching the Axis object from\n    the figure where the plot will\
    \ end up. Matplotlib has no similar concept of\n    and axis for the other mappable\
    \ variables (color, etc.), but to simplify the\n    code, this object acts like\
    \ an Axis and can be used to scale other variables.\n\n    \"\"\"\n    axis_name\
    \ = \"\"  # Matplotlib requirement but not actually used\n\n    def __init__(self,\
    \ scale):\n\n        self.converter = None\n        self.units = None\n      \
    \  self.scale = scale\n        self.major = mpl.axis.Ticker()\n        self.minor\
    \ = mpl.axis.Ticker()\n\n        # It appears that this needs to be initialized\
    \ this way on matplotlib 3.1,\n        # but not later versions. It is unclear\
    \ whether there are any issues with it.\n        self._data_interval = None, None\n\
    \n        scale.set_default_locators_and_formatters(self)\n        # self.set_default_intervals()\
    \  Is this ever needed?\n\n    def set_view_interval(self, vmin, vmax):\n    \
    \    self._view_interval = vmin, vmax\n\n    def get_view_interval(self):\n  \
    \      return self._view_interval\n\n    # TODO do we want to distinguish view/data\
    \ intervals? e.g. for a legend\n    # we probably want to represent the full range\
    \ of the data values, but\n    # still norm the colormap. If so, we'll need to\
    \ track data range separately\n    # from the norm, which we currently don't do.\n\
    \n    def set_data_interval(self, vmin, vmax):\n        self._data_interval =\
    \ vmin, vmax\n\n    def get_data_interval(self):\n        return self._data_interval\n\
    \n    def get_tick_space(self):\n        # TODO how to do this in a configurable\
    \ / auto way?\n        # Would be cool to have legend density adapt to figure\
    \ size, etc.\n        return 5\n\n    def set_major_locator(self, locator):\n\
    \        self.major.locator = locator\n        locator.set_axis(self)\n\n    def\
    \ set_major_formatter(self, formatter):\n        self.major.formatter = formatter\n\
    \        formatter.set_axis(self)\n\n    def set_minor_locator(self, locator):\n\
    \        self.minor.locator = locator\n        locator.set_axis(self)\n\n    def\
    \ set_minor_formatter(self, formatter):\n        self.minor.formatter = formatter\n\
    \        formatter.set_axis(self)\n\n    def set_units(self, units):\n       \
    \ self.units = units\n\n    def update_units(self, x):\n        \"\"\"Pass units\
    \ to the internal converter, potentially updating its mapping.\"\"\"\n       \
    \ self.converter = mpl.units.registry.get_converter(x)\n        if self.converter\
    \ is not None:\n            self.converter.default_units(x, self)\n\n        \
    \    info = self.converter.axisinfo(self.units, self)\n\n            if info is\
    \ None:\n                return\n            if info.majloc is not None:\n   \
    \             self.set_major_locator(info.majloc)\n            if info.majfmt\
    \ is not None:\n                self.set_major_formatter(info.majfmt)\n\n    \
    \        # This is in matplotlib method; do we need this?\n            # self.set_default_intervals()\n\
    \n    def convert_units(self, x):\n        \"\"\"Return a numeric representation\
    \ of the input data.\"\"\"\n        if np.issubdtype(np.asarray(x).dtype, np.number):\n\
    \            return x\n        elif self.converter is None:\n            return\
    \ x\n        return self.converter.convert(x, self.units, self)\n\n    def get_scale(self):\n\
    \        # Note that matplotlib actually returns a string here!\n        # (e.g.,\
    \ with a log scale, axis.get_scale() returns \"log\")\n        # Currently we\
    \ just hit it with minor ticks where it checks for\n        # scale == \"log\"\
    . I'm not sure how you'd actually use log-scale\n        # minor \"ticks\" in\
    \ a legend context, so this is fine....\n        return self.scale\n\n    def\
    \ get_majorticklocs(self):\n        return self.major.locator()\n\n\n# ------------------------------------------------------------------------------------\
    \ #\n# Transform function creation\n\n\ndef _make_identity_transforms() -> TransFuncs:\n\
    \n    def identity(x):\n        return x\n\n    return identity, identity\n\n\n\
    def _make_logit_transforms(base: float | None = None) -> TransFuncs:\n\n    log,\
    \ exp = _make_log_transforms(base)\n\n    def logit(x):\n        with np.errstate(invalid=\"\
    ignore\", divide=\"ignore\"):\n            return log(x) - log(1 - x)\n\n    def\
    \ expit(x):\n        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n\
    \            return exp(x) / (1 + exp(x))\n\n    return logit, expit\n\n\ndef\
    \ _make_log_transforms(base: float | None = None) -> TransFuncs:\n\n    fs: TransFuncs\n\
    \    if base is None:\n        fs = np.log, np.exp\n    elif base == 2:\n    \
    \    fs = np.log2, partial(np.power, 2)\n    elif base == 10:\n        fs = np.log10,\
    \ partial(np.power, 10)\n    else:\n        def forward(x):\n            return\
    \ np.log(x) / np.log(base)\n        fs = forward, partial(np.power, base)\n\n\
    \    def log(x: ArrayLike) -> ArrayLike:\n        with np.errstate(invalid=\"\
    ignore\", divide=\"ignore\"):\n            return fs[0](x)\n\n    def exp(x: ArrayLike)\
    \ -> ArrayLike:\n        with np.errstate(invalid=\"ignore\", divide=\"ignore\"\
    ):\n            return fs[1](x)\n\n    return log, exp\n\n\ndef _make_symlog_transforms(c:\
    \ float = 1, base: float = 10) -> TransFuncs:\n\n    # From https://iopscience.iop.org/article/10.1088/0957-0233/24/2/027001\n\
    \n    # Note: currently not using base because we only get\n    # one parameter\
    \ from the string, and are using c (this is consistent with d3)\n\n    log, exp\
    \ = _make_log_transforms(base)\n\n    def symlog(x):\n        with np.errstate(invalid=\"\
    ignore\", divide=\"ignore\"):\n            return np.sign(x) * log(1 + np.abs(np.divide(x,\
    \ c)))\n\n    def symexp(x):\n        with np.errstate(invalid=\"ignore\", divide=\"\
    ignore\"):\n            return np.sign(x) * c * (exp(np.abs(x)) - 1)\n\n    return\
    \ symlog, symexp\n\n\ndef _make_sqrt_transforms() -> TransFuncs:\n\n    def sqrt(x):\n\
    \        return np.sign(x) * np.sqrt(np.abs(x))\n\n    def square(x):\n      \
    \  return np.sign(x) * np.square(x)\n\n    return sqrt, square\n\n\ndef _make_power_transforms(exp:\
    \ float) -> TransFuncs:\n\n    def forward(x):\n        return np.sign(x) * np.power(np.abs(x),\
    \ exp)\n\n    def inverse(x):\n        return np.sign(x) * np.power(np.abs(x),\
    \ 1 / exp)\n\n    return forward, inverse\n\n\ndef _default_spacer(x: Series)\
    \ -> float:\n    return 1\n\nOutput the complete test file, code only, no explanations.\n\
    ### Time\nCurrent time: 2025-03-17 01:26:09\n"
  role: user
