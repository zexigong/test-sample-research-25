messages:
- content: You are an AI agent expert in writing unit tests. Your task is to write
    unit tests for the given code files of the repository. Make sure the tests can
    be executed without lint or compile errors.
  role: system
- content: "### Task Information\nBased on the source code, write/rewrite tests to\
    \ cover the source code.\nRepository: seaborn\nTest File Path: seaborn\\test_density\\\
    test_density.py\nProject Programming Language: Python\nTesting Framework: pytest\n\
    ### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\
    from dataclasses import dataclass\nfrom typing import Any, Callable\n\nimport\
    \ numpy as np\nfrom numpy import ndarray\nimport pandas as pd\nfrom pandas import\
    \ DataFrame\ntry:\n    from scipy.stats import gaussian_kde\n    _no_scipy = False\n\
    except ImportError:\n    from seaborn.external.kde import gaussian_kde\n    _no_scipy\
    \ = True\n\nfrom seaborn._core.groupby import GroupBy\nfrom seaborn._core.scales\
    \ import Scale\nfrom seaborn._stats.base import Stat\n\n\n@dataclass\nclass KDE(Stat):\n\
    \    \"\"\"\n    Compute a univariate kernel density estimate.\n\n    Parameters\n\
    \    ----------\n    bw_adjust : float\n        Factor that multiplicatively scales\
    \ the value chosen using\n        `bw_method`. Increasing will make the curve\
    \ smoother. See Notes.\n    bw_method : string, scalar, or callable\n        Method\
    \ for determining the smoothing bandwidth to use. Passed directly\n        to\
    \ :class:`scipy.stats.gaussian_kde`; see there for options.\n    common_norm :\
    \ bool or list of variables\n        If `True`, normalize so that the areas of\
    \ all curves sums to 1.\n        If `False`, normalize each curve independently.\
    \ If a list, defines\n        variable(s) to group by and normalize within.\n\
    \    common_grid : bool or list of variables\n        If `True`, all curves will\
    \ share the same evaluation grid.\n        If `False`, each evaluation grid is\
    \ independent. If a list, defines\n        variable(s) to group by and share a\
    \ grid within.\n    gridsize : int or None\n        Number of points in the evaluation\
    \ grid. If None, the density is\n        evaluated at the original datapoints.\n\
    \    cut : float\n        Factor, multiplied by the kernel bandwidth, that determines\
    \ how far\n        the evaluation grid extends past the extreme datapoints. When\
    \ set to 0,\n        the curve is truncated at the data limits.\n    cumulative\
    \ : bool\n        If True, estimate a cumulative distribution function. Requires\
    \ scipy.\n\n    Notes\n    -----\n    The *bandwidth*, or standard deviation of\
    \ the smoothing kernel, is an\n    important parameter. Much like histogram bin\
    \ width, using the wrong\n    bandwidth can produce a distorted representation.\
    \ Over-smoothing can erase\n    true features, while under-smoothing can create\
    \ false ones. The default\n    uses a rule-of-thumb that works best for distributions\
    \ that are roughly\n    bell-shaped. It is a good idea to check the default by\
    \ varying `bw_adjust`.\n\n    Because the smoothing is performed with a Gaussian\
    \ kernel, the estimated\n    density curve can extend to values that may not make\
    \ sense. For example, the\n    curve may be drawn over negative values when data\
    \ that are naturally\n    positive. The `cut` parameter can be used to control\
    \ the evaluation range,\n    but datasets that have many observations close to\
    \ a natural boundary may be\n    better served by a different method.\n\n    Similar\
    \ distortions may arise when a dataset is naturally discrete or \"spiky\"\n  \
    \  (containing many repeated observations of the same value). KDEs will always\n\
    \    produce a smooth curve, which could be misleading.\n\n    The units on the\
    \ density axis are a common source of confusion. While kernel\n    density estimation\
    \ produces a probability distribution, the height of the curve\n    at each point\
    \ gives a density, not a probability. A probability can be obtained\n    only\
    \ by integrating the density across a range. The curve is normalized so\n    that\
    \ the integral over all possible values is 1, meaning that the scale of\n    the\
    \ density axis depends on the data values.\n\n    If scipy is installed, its cython-accelerated\
    \ implementation will be used.\n\n    Examples\n    --------\n    .. include::\
    \ ../docstrings/objects.KDE.rst\n\n    \"\"\"\n    bw_adjust: float = 1\n    bw_method:\
    \ str | float | Callable[[gaussian_kde], float] = \"scott\"\n    common_norm:\
    \ bool | list[str] = True\n    common_grid: bool | list[str] = True\n    gridsize:\
    \ int | None = 200\n    cut: float = 3\n    cumulative: bool = False\n\n    def\
    \ __post_init__(self):\n\n        if self.cumulative and _no_scipy:\n        \
    \    raise RuntimeError(\"Cumulative KDE evaluation requires scipy\")\n\n    def\
    \ _check_var_list_or_boolean(self, param: str, grouping_vars: Any) -> None:\n\
    \        \"\"\"Do input checks on grouping parameters.\"\"\"\n        value =\
    \ getattr(self, param)\n        if not (\n            isinstance(value, bool)\n\
    \            or (isinstance(value, list) and all(isinstance(v, str) for v in value))\n\
    \        ):\n            param_name = f\"{self.__class__.__name__}.{param}\"\n\
    \            raise TypeError(f\"{param_name} must be a boolean or list of strings.\"\
    )\n        self._check_grouping_vars(param, grouping_vars, stacklevel=3)\n\n \
    \   def _fit(self, data: DataFrame, orient: str) -> gaussian_kde:\n        \"\"\
    \"Fit and return a KDE object.\"\"\"\n        # TODO need to handle singular data\n\
    \n        fit_kws: dict[str, Any] = {\"bw_method\": self.bw_method}\n        if\
    \ \"weight\" in data:\n            fit_kws[\"weights\"] = data[\"weight\"]\n \
    \       kde = gaussian_kde(data[orient], **fit_kws)\n        kde.set_bandwidth(kde.factor\
    \ * self.bw_adjust)\n\n        return kde\n\n    def _get_support(self, data:\
    \ DataFrame, orient: str) -> ndarray:\n        \"\"\"Define the grid that the\
    \ KDE will be evaluated on.\"\"\"\n        if self.gridsize is None:\n       \
    \     return data[orient].to_numpy()\n\n        kde = self._fit(data, orient)\n\
    \        bw = np.sqrt(kde.covariance.squeeze())\n        gridmin = data[orient].min()\
    \ - bw * self.cut\n        gridmax = data[orient].max() + bw * self.cut\n    \
    \    return np.linspace(gridmin, gridmax, self.gridsize)\n\n    def _fit_and_evaluate(\n\
    \        self, data: DataFrame, orient: str, support: ndarray\n    ) -> DataFrame:\n\
    \        \"\"\"Transform single group by fitting a KDE and evaluating on a support\
    \ grid.\"\"\"\n        empty = pd.DataFrame(columns=[orient, \"weight\", \"density\"\
    ], dtype=float)\n        if len(data) < 2:\n            return empty\n       \
    \ try:\n            kde = self._fit(data, orient)\n        except np.linalg.LinAlgError:\n\
    \            return empty\n\n        if self.cumulative:\n            s_0 = support[0]\n\
    \            density = np.array([kde.integrate_box_1d(s_0, s_i) for s_i in support])\n\
    \        else:\n            density = kde(support)\n\n        weight = data[\"\
    weight\"].sum()\n        return pd.DataFrame({orient: support, \"weight\": weight,\
    \ \"density\": density})\n\n    def _transform(\n        self, data: DataFrame,\
    \ orient: str, grouping_vars: list[str]\n    ) -> DataFrame:\n        \"\"\"Transform\
    \ multiple groups by fitting KDEs and evaluating.\"\"\"\n        empty = pd.DataFrame(columns=[*data.columns,\
    \ \"density\"], dtype=float)\n        if len(data) < 2:\n            return empty\n\
    \        try:\n            support = self._get_support(data, orient)\n       \
    \ except np.linalg.LinAlgError:\n            return empty\n\n        grouping_vars\
    \ = [x for x in grouping_vars if data[x].nunique() > 1]\n        if not grouping_vars:\n\
    \            return self._fit_and_evaluate(data, orient, support)\n        groupby\
    \ = GroupBy(grouping_vars)\n        return groupby.apply(data, self._fit_and_evaluate,\
    \ orient, support)\n\n    def __call__(\n        self, data: DataFrame, groupby:\
    \ GroupBy, orient: str, scales: dict[str, Scale],\n    ) -> DataFrame:\n\n   \
    \     if \"weight\" not in data:\n            data = data.assign(weight=1)\n \
    \       data = data.dropna(subset=[orient, \"weight\"])\n\n        # Transform\
    \ each group separately\n        grouping_vars = [str(v) for v in data if v in\
    \ groupby.order]\n        if not grouping_vars or self.common_grid is True:\n\
    \            res = self._transform(data, orient, grouping_vars)\n        else:\n\
    \            if self.common_grid is False:\n                grid_vars = grouping_vars\n\
    \            else:\n                self._check_var_list_or_boolean(\"common_grid\"\
    , grouping_vars)\n                grid_vars = [v for v in self.common_grid if\
    \ v in grouping_vars]\n\n            res = (\n                GroupBy(grid_vars)\n\
    \                .apply(data, self._transform, orient, grouping_vars)\n      \
    \      )\n\n        # Normalize, potentially within groups\n        if not grouping_vars\
    \ or self.common_norm is True:\n            res = res.assign(group_weight=data[\"\
    weight\"].sum())\n        else:\n            if self.common_norm is False:\n \
    \               norm_vars = grouping_vars\n            else:\n               \
    \ self._check_var_list_or_boolean(\"common_norm\", grouping_vars)\n          \
    \      norm_vars = [v for v in self.common_norm if v in grouping_vars]\n\n   \
    \         res = res.join(\n                data.groupby(norm_vars)[\"weight\"\
    ].sum().rename(\"group_weight\"),\n                on=norm_vars,\n           \
    \ )\n\n        res[\"density\"] *= res.eval(\"weight / group_weight\")\n     \
    \   value = {\"x\": \"y\", \"y\": \"x\"}[orient]\n        res[value] = res[\"\
    density\"]\n        return res.drop([\"weight\", \"group_weight\"], axis=1)\n\n\
    ### Source File Dependency Files Content\n### Dependency File: base.py\n\"\"\"\
    Base module for statistical transformations.\"\"\"\nfrom __future__ import annotations\n\
    from collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom\
    \ typing import ClassVar, Any\nimport warnings\n\nfrom typing import TYPE_CHECKING\n\
    if TYPE_CHECKING:\n    from pandas import DataFrame\n    from seaborn._core.groupby\
    \ import GroupBy\n    from seaborn._core.scales import Scale\n\n\n@dataclass\n\
    class Stat:\n    \"\"\"Base class for objects that apply statistical transformations.\"\
    \"\"\n\n    # The class supports a partial-function application pattern. The object\
    \ is\n    # initialized with desired parameters and the result is a callable that\n\
    \    # accepts and returns dataframes.\n\n    # The statistical transformation\
    \ logic should not add any state to the instance\n    # beyond what is defined\
    \ with the initialization parameters.\n\n    # Subclasses can declare whether\
    \ the orient dimension should be used in grouping\n    # TODO consider whether\
    \ this should be a parameter. Motivating example:\n    # use the same KDE class\
    \ violin plots and univariate density estimation.\n    # In the former case, we\
    \ would expect separate densities for each unique\n    # value on the orient axis,\
    \ but we would not in the latter case.\n    group_by_orient: ClassVar[bool] =\
    \ False\n\n    def _check_param_one_of(self, param: str, options: Iterable[Any])\
    \ -> None:\n        \"\"\"Raise when parameter value is not one of a specified\
    \ set.\"\"\"\n        value = getattr(self, param)\n        if value not in options:\n\
    \            *most, last = options\n            option_str = \", \".join(f\"{x!r}\"\
    \ for x in most[:-1]) + f\" or {last!r}\"\n            err = \" \".join([\n  \
    \              f\"The `{param}` parameter for `{self.__class__.__name__}` must\
    \ be\",\n                f\"one of {option_str}; not {value!r}.\",\n         \
    \   ])\n            raise ValueError(err)\n\n    def _check_grouping_vars(\n \
    \       self, param: str, data_vars: list[str], stacklevel: int = 2,\n    ) ->\
    \ None:\n        \"\"\"Warn if vars are named in parameter without being present\
    \ in the data.\"\"\"\n        param_vars = getattr(self, param)\n        undefined\
    \ = set(param_vars) - set(data_vars)\n        if undefined:\n            param\
    \ = f\"{self.__class__.__name__}.{param}\"\n            names = \", \".join(f\"\
    {x!r}\" for x in undefined)\n            msg = f\"Undefined variable(s) passed\
    \ for {param}: {names}.\"\n            warnings.warn(msg, stacklevel=stacklevel)\n\
    \n    def __call__(\n        self,\n        data: DataFrame,\n        groupby:\
    \ GroupBy,\n        orient: str,\n        scales: dict[str, Scale],\n    ) ->\
    \ DataFrame:\n        \"\"\"Apply statistical transform to data subgroups and\
    \ return combined result.\"\"\"\n        return data\n\n\n### Dependency File:\
    \ groupby.py\n\"\"\"Simplified split-apply-combine paradigm on dataframes for\
    \ internal use.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import\
    \ cast, Iterable\n\nimport pandas as pd\n\nfrom seaborn._core.rules import categorical_order\n\
    \nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from typing import\
    \ Callable\n    from pandas import DataFrame, MultiIndex, Index\n\n\nclass GroupBy:\n\
    \    \"\"\"\n    Interface for Pandas GroupBy operations allowing specified group\
    \ order.\n\n    Writing our own class to do this has a few advantages:\n    -\
    \ It constrains the interface between Plot and Stat/Move objects\n    - It allows\
    \ control over the row order of the GroupBy result, which is\n      important\
    \ when using in the context of some Move operations (dodge, stack, ...)\n    -\
    \ It simplifies some complexities regarding the return type and Index contents\n\
    \      one encounters with Pandas, especially for DataFrame -> DataFrame applies\n\
    \    - It increases future flexibility regarding alternate DataFrame libraries\n\
    \n    \"\"\"\n    def __init__(self, order: list[str] | dict[str, list | None]):\n\
    \        \"\"\"\n        Initialize the GroupBy from grouping variables and optional\
    \ level orders.\n\n        Parameters\n        ----------\n        order\n   \
    \         List of variable names or dict mapping names to desired level orders.\n\
    \            Level order values can be None to use default ordering rules. The\n\
    \            variables can include names that are not expected to appear in the\n\
    \            data; these will be dropped before the groups are defined.\n\n  \
    \      \"\"\"\n        if not order:\n            raise ValueError(\"GroupBy requires\
    \ at least one grouping variable\")\n\n        if isinstance(order, list):\n \
    \           order = {k: None for k in order}\n        self.order = order\n\n \
    \   def _get_groups(\n        self, data: DataFrame\n    ) -> tuple[str | list[str],\
    \ Index | MultiIndex]:\n        \"\"\"Return index with Cartesian product of ordered\
    \ grouping variable levels.\"\"\"\n        levels = {}\n        for var, order\
    \ in self.order.items():\n            if var in data:\n                if order\
    \ is None:\n                    order = categorical_order(data[var])\n       \
    \         levels[var] = order\n\n        grouper: str | list[str]\n        groups:\
    \ Index | MultiIndex\n        if not levels:\n            grouper = []\n     \
    \       groups = pd.Index([])\n        elif len(levels) > 1:\n            grouper\
    \ = list(levels)\n            groups = pd.MultiIndex.from_product(levels.values(),\
    \ names=grouper)\n        else:\n            grouper, = list(levels)\n       \
    \     groups = pd.Index(levels[grouper], name=grouper)\n        return grouper,\
    \ groups\n\n    def _reorder_columns(self, res, data):\n        \"\"\"Reorder\
    \ result columns to match original order with new columns appended.\"\"\"\n  \
    \      cols = [c for c in data if c in res]\n        cols += [c for c in res if\
    \ c not in data]\n        return res.reindex(columns=pd.Index(cols))\n\n    def\
    \ agg(self, data: DataFrame, *args, **kwargs) -> DataFrame:\n        \"\"\"\n\
    \        Reduce each group to a single row in the output.\n\n        The output\
    \ will have a row for each unique combination of the grouping\n        variable\
    \ levels with null values for the aggregated variable(s) where\n        those\
    \ combinations do not appear in the dataset.\n\n        \"\"\"\n        grouper,\
    \ groups = self._get_groups(data)\n\n        if not grouper:\n            # We\
    \ will need to see whether there are valid usecases that end up here\n       \
    \     raise ValueError(\"No grouping variables are present in dataframe\")\n\n\
    \        res = (\n            data\n            .groupby(grouper, sort=False,\
    \ observed=False)\n            .agg(*args, **kwargs)\n            .reindex(groups)\n\
    \            .reset_index()\n            .pipe(self._reorder_columns, data)\n\
    \        )\n\n        return res\n\n    def apply(\n        self, data: DataFrame,\
    \ func: Callable[..., DataFrame],\n        *args, **kwargs,\n    ) -> DataFrame:\n\
    \        \"\"\"Apply a DataFrame -> DataFrame mapping to each group.\"\"\"\n \
    \       grouper, groups = self._get_groups(data)\n\n        if not grouper:\n\
    \            return self._reorder_columns(func(data, *args, **kwargs), data)\n\
    \n        parts = {}\n        for key, part_df in data.groupby(grouper, sort=False,\
    \ observed=False):\n            parts[key] = func(part_df, *args, **kwargs)\n\
    \        stack = []\n        for key in groups:\n            if key in parts:\n\
    \                if isinstance(grouper, list):\n                    # Implies\
    \ that we had a MultiIndex so key is iterable\n                    group_ids =\
    \ dict(zip(grouper, cast(Iterable, key)))\n                else:\n           \
    \         group_ids = {grouper: key}\n                stack.append(parts[key].assign(**group_ids))\n\
    \n        res = pd.concat(stack, ignore_index=True)\n        return self._reorder_columns(res,\
    \ data)\n\n\n### Dependency File: scales.py\nfrom __future__ import annotations\n\
    import re\nfrom copy import copy\nfrom collections.abc import Sequence\nfrom dataclasses\
    \ import dataclass\nfrom functools import partial\nfrom typing import Any, Callable,\
    \ Tuple, Optional, ClassVar\n\nimport numpy as np\nimport matplotlib as mpl\n\
    from matplotlib.ticker import (\n    Locator,\n    Formatter,\n    AutoLocator,\n\
    \    AutoMinorLocator,\n    FixedLocator,\n    LinearLocator,\n    LogLocator,\n\
    \    SymmetricalLogLocator,\n    MaxNLocator,\n    MultipleLocator,\n    EngFormatter,\n\
    \    FuncFormatter,\n    LogFormatterSciNotation,\n    ScalarFormatter,\n    StrMethodFormatter,\n\
    )\nfrom matplotlib.dates import (\n    AutoDateLocator,\n    AutoDateFormatter,\n\
    \    ConciseDateFormatter,\n)\nfrom matplotlib.axis import Axis\nfrom matplotlib.scale\
    \ import ScaleBase\nfrom pandas import Series\n\nfrom seaborn._core.rules import\
    \ categorical_order\nfrom seaborn._core.typing import Default, default\n\nfrom\
    \ typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from seaborn._core.plot\
    \ import Plot\n    from seaborn._core.properties import Property\n    from numpy.typing\
    \ import ArrayLike, NDArray\n\n    TransFuncs = Tuple[\n        Callable[[ArrayLike],\
    \ ArrayLike], Callable[[ArrayLike], ArrayLike]\n    ]\n\n    # TODO Reverting\
    \ typing to Any as it was proving too complicated to\n    # work out the right\
    \ way to communicate the types to mypy. Revisit!\n    Pipeline = Sequence[Optional[Callable[[Any],\
    \ Any]]]\n\n\nclass Scale:\n    \"\"\"Base class for objects that map data values\
    \ to visual properties.\"\"\"\n\n    values: tuple | str | list | dict | None\n\
    \n    _priority: ClassVar[int]\n    _pipeline: Pipeline\n    _matplotlib_scale:\
    \ ScaleBase\n    _spacer: staticmethod\n    _legend: tuple[list[Any], list[str]]\
    \ | None\n\n    def __post_init__(self):\n\n        self._tick_params = None\n\
    \        self._label_params = None\n        self._legend = None\n\n    def tick(self):\n\
    \        raise NotImplementedError()\n\n    def label(self):\n        raise NotImplementedError()\n\
    \n    def _get_locators(self):\n        raise NotImplementedError()\n\n    def\
    \ _get_formatter(self, locator: Locator | None = None):\n        raise NotImplementedError()\n\
    \n    def _get_scale(self, name: str, forward: Callable, inverse: Callable):\n\
    \n        major_locator, minor_locator = self._get_locators(**self._tick_params)\n\
    \        major_formatter = self._get_formatter(major_locator, **self._label_params)\n\
    \n        class InternalScale(mpl.scale.FuncScale):\n            def set_default_locators_and_formatters(self,\
    \ axis):\n                axis.set_major_locator(major_locator)\n            \
    \    if minor_locator is not None:\n                    axis.set_minor_locator(minor_locator)\n\
    \                axis.set_major_formatter(major_formatter)\n\n        return InternalScale(name,\
    \ (forward, inverse))\n\n    def _spacing(self, x: Series) -> float:\n       \
    \ space = self._spacer(x)\n        if np.isnan(space):\n            # This happens\
    \ when there is no variance in the orient coordinate data\n            # Not exactly\
    \ clear what the right default is, but 1 seems reasonable?\n            return\
    \ 1\n        return space\n\n    def _setup(\n        self, data: Series, prop:\
    \ Property, axis: Axis | None = None,\n    ) -> Scale:\n        raise NotImplementedError()\n\
    \n    def _finalize(self, p: Plot, axis: Axis) -> None:\n        \"\"\"Perform\
    \ scale-specific axis tweaks after adding artists.\"\"\"\n        pass\n\n   \
    \ def __call__(self, data: Series) -> ArrayLike:\n\n        trans_data: Series\
    \ | NDArray | list\n\n        # TODO sometimes we need to handle scalars (e.g.\
    \ for Line)\n        # but what is the best way to do that?\n        scalar_data\
    \ = np.isscalar(data)\n        if scalar_data:\n            trans_data = np.array([data])\n\
    \        else:\n            trans_data = data\n\n        for func in self._pipeline:\n\
    \            if func is not None:\n                trans_data = func(trans_data)\n\
    \n        if scalar_data:\n            return trans_data[0]\n        else:\n \
    \           return trans_data\n\n    @staticmethod\n    def _identity():\n\n \
    \       class Identity(Scale):\n            _pipeline = []\n            _spacer\
    \ = None\n            _legend = None\n            _matplotlib_scale = None\n\n\
    \        return Identity()\n\n\n@dataclass\nclass Boolean(Scale):\n    \"\"\"\n\
    \    A scale with a discrete domain of True and False values.\n\n    The behavior\
    \ is similar to the :class:`Nominal` scale, but property\n    mappings and legends\
    \ will use a [True, False] ordering rather than\n    a sort using numeric rules.\
    \ Coordinate variables accomplish this by\n    inverting axis limits so as to\
    \ maintain underlying numeric positioning.\n    Input data are cast to boolean\
    \ values, respecting missing data.\n\n    \"\"\"\n    values: tuple | list | dict\
    \ | None = None\n\n    _priority: ClassVar[int] = 3\n\n    def _setup(\n     \
    \   self, data: Series, prop: Property, axis: Axis | None = None,\n    ) -> Scale:\n\
    \n        new = copy(self)\n        if new._tick_params is None:\n           \
    \ new = new.tick()\n        if new._label_params is None:\n            new = new.label()\n\
    \n        def na_safe_cast(x):\n            # TODO this doesn't actually need\
    \ to be a closure\n            if np.isscalar(x):\n                return float(bool(x))\n\
    \            else:\n                if hasattr(x, \"notna\"):\n              \
    \      # Handle pd.NA; np<>pd interop with NA is tricky\n                    use\
    \ = x.notna().to_numpy()\n                else:\n                    use = np.isfinite(x)\n\
    \                out = np.full(len(x), np.nan, dtype=float)\n                out[use]\
    \ = x[use].astype(bool).astype(float)\n                return out\n\n        new._pipeline\
    \ = [na_safe_cast, prop.get_mapping(new, data)]\n        new._spacer = _default_spacer\n\
    \        if prop.legend:\n            new._legend = [True, False], [\"True\",\
    \ \"False\"]\n\n        forward, inverse = _make_identity_transforms()\n     \
    \   mpl_scale = new._get_scale(str(data.name), forward, inverse)\n\n        axis\
    \ = PseudoAxis(mpl_scale) if axis is None else axis\n        mpl_scale.set_default_locators_and_formatters(axis)\n\
    \        new._matplotlib_scale = mpl_scale\n\n        return new\n\n    def _finalize(self,\
    \ p: Plot, axis: Axis) -> None:\n\n        # We want values to appear in a True,\
    \ False order but also want\n        # True/False to be drawn at 1/0 positions\
    \ respectively to avoid nasty\n        # surprises if additional artists are added\
    \ through the matplotlib API.\n        # We accomplish this using axis inversion\
    \ akin to what we do in Nominal.\n\n        ax = axis.axes\n        name = axis.axis_name\n\
    \        axis.grid(False, which=\"both\")\n        if name not in p._limits:\n\
    \            nticks = len(axis.get_major_ticks())\n            lo, hi = -.5, nticks\
    \ - .5\n            if name == \"x\":\n                lo, hi = hi, lo\n     \
    \       set_lim = getattr(ax, f\"set_{name}lim\")\n            set_lim(lo, hi,\
    \ auto=None)\n\n    def tick(self, locator: Locator | None = None):\n        new\
    \ = copy(self)\n        new._tick_params = {\"locator\": locator}\n        return\
    \ new\n\n    def label(self, formatter: Formatter | None = None):\n        new\
    \ = copy(self)\n        new._label_params = {\"formatter\": formatter}\n     \
    \   return new\n\n    def _get_locators(self, locator):\n        if locator is\
    \ not None:\n            return locator\n        return FixedLocator([0, 1]),\
    \ None\n\n    def _get_formatter(self, locator, formatter):\n        if formatter\
    \ is not None:\n            return formatter\n        return FuncFormatter(lambda\
    \ x, _: str(bool(x)))\n\n\n@dataclass\nclass Nominal(Scale):\n    \"\"\"\n   \
    \ A categorical scale without relative importance / magnitude.\n    \"\"\"\n \
    \   # Categorical (convert to strings), un-sortable\n\n    values: tuple | str\
    \ | list | dict | None = None\n    order: list | None = None\n\n    _priority:\
    \ ClassVar[int] = 4\n\n    def _setup(\n        self, data: Series, prop: Property,\
    \ axis: Axis | None = None,\n    ) -> Scale:\n\n        new = copy(self)\n   \
    \     if new._tick_params is None:\n            new = new.tick()\n        if new._label_params\
    \ is None:\n            new = new.label()\n\n        # TODO flexibility over format()\
    \ which isn't great for numbers / dates\n        stringify = np.vectorize(format,\
    \ otypes=[\"object\"])\n\n        units_seed = categorical_order(data, new.order)\n\
    \n        # TODO move to Nominal._get_scale?\n        # TODO this needs some more\
    \ complicated rethinking about how to pass\n        # a unit dictionary down to\
    \ these methods, along with how much we want\n        # to invest in their API.\
    \ What is it useful for tick() to do here?\n        # (Ordinal may be different\
    \ if we draw that contrast).\n        # Any customization we do to allow, e.g.,\
    \ label wrapping will probably\n        # require defining our own Formatter subclass.\n\
    \        # We could also potentially implement auto-wrapping in an Axis subclass\n\
    \        # (see Axis.draw ... it already is computing the bboxes).\n        #\
    \ major_locator, minor_locator = new._get_locators(**new._tick_params)\n     \
    \   # major_formatter = new._get_formatter(major_locator, **new._label_params)\n\
    \n        class CatScale(mpl.scale.LinearScale):\n            def set_default_locators_and_formatters(self,\
    \ axis):\n                ...\n                # axis.set_major_locator(major_locator)\n\
    \                # if minor_locator is not None:\n                #     axis.set_minor_locator(minor_locator)\n\
    \                # axis.set_major_formatter(major_formatter)\n\n        mpl_scale\
    \ = CatScale(data.name)\n        if axis is None:\n            axis = PseudoAxis(mpl_scale)\n\
    \n            # TODO Currently just used in non-Coordinate contexts, but should\n\
    \            # we use this to (A) set the padding we want for categorial plots\n\
    \            # and (B) allow the values parameter for a Coordinate to set xlim/ylim\n\
    \            axis.set_view_interval(0, len(units_seed) - 1)\n\n        new._matplotlib_scale\
    \ = mpl_scale\n\n        # TODO array cast necessary to handle float/int mixture,\
    \ which we need\n        # to solve in a more systematic way probably\n      \
    \  # (i.e. if we have [1, 2.5], do we want [1.0, 2.5]? Unclear)\n        axis.update_units(stringify(np.array(units_seed)))\n\
    \n        # TODO define this more centrally\n        def convert_units(x):\n \
    \           # TODO only do this with explicit order?\n            # (But also\
    \ category dtype?)\n            # TODO isin fails when units_seed mixes numbers\
    \ and strings (numpy error?)\n            # but np.isin also does not seem any\
    \ faster? (Maybe not broadcasting in C)\n            # keep = x.isin(units_seed)\n\
    \            keep = np.array([x_ in units_seed for x_ in x], bool)\n         \
    \   out = np.full(len(x), np.nan)\n            out[keep] = axis.convert_units(stringify(x[keep]))\n\
    \            return out\n\n        new._pipeline = [convert_units, prop.get_mapping(new,\
    \ data)]\n        new._spacer = _default_spacer\n\n        if prop.legend:\n \
    \           new._legend = units_seed, list(stringify(units_seed))\n\n        return\
    \ new\n\n    def _finalize(self, p: Plot, axis: Axis) -> None:\n\n        ax =\
    \ axis.axes\n        name = axis.axis_name\n        axis.grid(False, which=\"\
    both\")\n        if name not in p._limits:\n            nticks = len(axis.get_major_ticks())\n\
    \            lo, hi = -.5, nticks - .5\n            if name == \"y\":\n      \
    \          lo, hi = hi, lo\n            set_lim = getattr(ax, f\"set_{name}lim\"\
    )\n            set_lim(lo, hi, auto=None)\n\n    def tick(self, locator: Locator\
    \ | None = None) -> Nominal:\n        \"\"\"\n        Configure the selection\
    \ of ticks for the scale's axis or legend.\n\n        .. note::\n            This\
    \ API is under construction and will be enhanced over time.\n            At the\
    \ moment, it is probably not very useful.\n\n        Parameters\n        ----------\n\
    \        locator : :class:`matplotlib.ticker.Locator` subclass\n            Pre-configured\
    \ matplotlib locator; other parameters will not be used.\n\n        Returns\n\
    \        -------\n        Copy of self with new tick configuration.\n\n      \
    \  \"\"\"\n        new = copy(self)\n        new._tick_params = {\"locator\":\
    \ locator}\n        return new\n\n    def label(self, formatter: Formatter | None\
    \ = None) -> Nominal:\n        \"\"\"\n        Configure the selection of labels\
    \ for the scale's axis or legend.\n\n        .. note::\n            This API is\
    \ under construction and will be enhanced over time.\n            At the moment,\
    \ it is probably not very useful.\n\n        Parameters\n        ----------\n\
    \        formatter : :class:`matplotlib.ticker.Formatter` subclass\n         \
    \   Pre-configured matplotlib formatter; other parameters will not be used.\n\n\
    \        Returns\n        -------\n        scale\n            Copy of self with\
    \ new tick configuration.\n\n        \"\"\"\n        new = copy(self)\n      \
    \  new._label_params = {\"formatter\": formatter}\n        return new\n\n    def\
    \ _get_locators(self, locator):\n\n        if locator is not None:\n         \
    \   return locator, None\n\n        locator = mpl.category.StrCategoryLocator({})\n\
    \n        return locator, None\n\n    def _get_formatter(self, locator, formatter):\n\
    \n        if formatter is not None:\n            return formatter\n\n        formatter\
    \ = mpl.category.StrCategoryFormatter({})\n\n        return formatter\n\n\n@dataclass\n\
    class Ordinal(Scale):\n    # Categorical (convert to strings), sortable, can skip\
    \ ticklabels\n    ...\n\n\n@dataclass\nclass Discrete(Scale):\n    # Numeric,\
    \ integral, can skip ticks/ticklabels\n    ...\n\n\n@dataclass\nclass ContinuousBase(Scale):\n\
    \n    values: tuple | str | None = None\n    norm: tuple | None = None\n\n   \
    \ def _setup(\n        self, data: Series, prop: Property, axis: Axis | None =\
    \ None,\n    ) -> Scale:\n\n        new = copy(self)\n        if new._tick_params\
    \ is None:\n            new = new.tick()\n        if new._label_params is None:\n\
    \            new = new.label()\n\n        forward, inverse = new._get_transform()\n\
    \n        mpl_scale = new._get_scale(str(data.name), forward, inverse)\n\n   \
    \     if axis is None:\n            axis = PseudoAxis(mpl_scale)\n           \
    \ axis.update_units(data)\n\n        mpl_scale.set_default_locators_and_formatters(axis)\n\
    \        new._matplotlib_scale = mpl_scale\n\n        normalize: Optional[Callable[[ArrayLike],\
    \ ArrayLike]]\n        if prop.normed:\n            if new.norm is None:\n   \
    \             vmin, vmax = data.min(), data.max()\n            else:\n       \
    \         vmin, vmax = new.norm\n            vmin, vmax = map(float, axis.convert_units((vmin,\
    \ vmax)))\n            a = forward(vmin)\n            b = forward(vmax) - forward(vmin)\n\
    \n            def normalize(x):\n                return (x - a) / b\n\n      \
    \  else:\n            normalize = vmin = vmax = None\n\n        new._pipeline\
    \ = [\n            axis.convert_units,\n            forward,\n            normalize,\n\
    \            prop.get_mapping(new, data)\n        ]\n\n        def spacer(x):\n\
    \            x = x.dropna().unique()\n            if len(x) < 2:\n           \
    \     return np.nan\n            return np.min(np.diff(np.sort(x)))\n        new._spacer\
    \ = spacer\n\n        # TODO How to allow disabling of legend for all uses of\
    \ property?\n        # Could add a Scale parameter, or perhaps Scale.suppress()?\n\
    \        # Are there other useful parameters that would be in Scale.legend()\n\
    \        # besides allowing Scale.legend(False)?\n        if prop.legend:\n  \
    \          axis.set_view_interval(vmin, vmax)\n            locs = axis.major.locator()\n\
    \            locs = locs[(vmin <= locs) & (locs <= vmax)]\n            # Avoid\
    \ having an offset / scientific notation in a legend\n            # as we don't\
    \ represent that anywhere so it ends up incorrect.\n            # This could become\
    \ an option (e.g. Continuous.label(offset=True))\n            # in which case\
    \ we would need to figure out how to show it.\n            if hasattr(axis.major.formatter,\
    \ \"set_useOffset\"):\n                axis.major.formatter.set_useOffset(False)\n\
    \            if hasattr(axis.major.formatter, \"set_scientific\"):\n         \
    \       axis.major.formatter.set_scientific(False)\n            labels = axis.major.formatter.format_ticks(locs)\n\
    \            new._legend = list(locs), list(labels)\n\n        return new\n\n\
    \    def _get_transform(self):\n\n        arg = self.trans\n\n        def get_param(method,\
    \ default):\n            if arg == method:\n                return default\n \
    \           return float(arg[len(method):])\n\n        if arg is None:\n     \
    \       return _make_identity_transforms()\n        elif isinstance(arg, tuple):\n\
    \            return arg\n        elif isinstance(arg, str):\n            if arg\
    \ == \"ln\":\n                return _make_log_transforms()\n            elif\
    \ arg == \"logit\":\n                base = get_param(\"logit\", 10)\n       \
    \         return _make_logit_transforms(base)\n            elif arg.startswith(\"\
    log\"):\n                base = get_param(\"log\", 10)\n                return\
    \ _make_log_transforms(base)\n            elif arg.startswith(\"symlog\"):\n \
    \               c = get_param(\"symlog\", 1)\n                return _make_symlog_transforms(c)\n\
    \            elif arg.startswith(\"pow\"):\n                exp = get_param(\"\
    pow\", 2)\n                return _make_power_transforms(exp)\n            elif\
    \ arg == \"sqrt\":\n                return _make_sqrt_transforms()\n         \
    \   else:\n                raise ValueError(f\"Unknown value provided for trans:\
    \ {arg!r}\")\n\n\n@dataclass\nclass Continuous(ContinuousBase):\n    \"\"\"\n\
    \    A numeric scale supporting norms and functional transforms.\n    \"\"\"\n\
    \    values: tuple | str | None = None\n    trans: str | TransFuncs | None = None\n\
    \n    # TODO Add this to deal with outliers?\n    # outside: Literal[\"keep\"\
    , \"drop\", \"clip\"] = \"keep\"\n\n    _priority: ClassVar[int] = 1\n\n    def\
    \ tick(\n        self,\n        locator: Locator | None = None, *,\n        at:\
    \ Sequence[float] | None = None,\n        upto: int | None = None,\n        count:\
    \ int | None = None,\n        every: float | None = None,\n        between: tuple[float,\
    \ float] | None = None,\n        minor: int | None = None,\n    ) -> Continuous:\n\
    \        \"\"\"\n        Configure the selection of ticks for the scale's axis\
    \ or legend.\n\n        Parameters\n        ----------\n        locator : :class:`matplotlib.ticker.Locator`\
    \ subclass\n            Pre-configured matplotlib locator; other parameters will\
    \ not be used.\n        at : sequence of floats\n            Place ticks at these\
    \ specific locations (in data units).\n        upto : int\n            Choose\
    \ \"nice\" locations for ticks, but do not exceed this number.\n        count\
    \ : int\n            Choose exactly this number of ticks, bounded by `between`\
    \ or axis limits.\n        every : float\n            Choose locations at this\
    \ interval of separation (in data units).\n        between : pair of floats\n\
    \            Bound upper / lower ticks when using `every` or `count`.\n      \
    \  minor : int\n            Number of unlabeled ticks to draw between labeled\
    \ \"major\" ticks.\n\n        Returns\n        -------\n        scale\n      \
    \      Copy of self with new tick configuration.\n\n        \"\"\"\n        #\
    \ Input checks\n        if locator is not None and not isinstance(locator, Locator):\n\
    \            raise TypeError(\n                f\"Tick locator must be an instance\
    \ of {Locator!r}, \"\n                f\"not {type(locator)!r}.\"\n          \
    \  )\n        log_base, symlog_thresh = self._parse_for_log_params(self.trans)\n\
    \        if log_base or symlog_thresh:\n            if count is not None and between\
    \ is None:\n                raise RuntimeError(\"`count` requires `between` with\
    \ log transform.\")\n            if every is not None:\n                raise\
    \ RuntimeError(\"`every` not supported with log transform.\")\n\n        new =\
    \ copy(self)\n        new._tick_params = {\n            \"locator\": locator,\n\
    \            \"at\": at,\n            \"upto\": upto,\n            \"count\":\
    \ count,\n            \"every\": every,\n            \"between\": between,\n \
    \           \"minor\": minor,\n        }\n        return new\n\n    def label(\n\
    \        self,\n        formatter: Formatter | None = None, *,\n        like:\
    \ str | Callable | None = None,\n        base: int | None | Default = default,\n\
    \        unit: str | None = None,\n    ) -> Continuous:\n        \"\"\"\n    \
    \    Configure the appearance of tick labels for the scale's axis or legend.\n\
    \n        Parameters\n        ----------\n        formatter : :class:`matplotlib.ticker.Formatter`\
    \ subclass\n            Pre-configured formatter to use; other parameters will\
    \ be ignored.\n        like : str or callable\n            Either a format pattern\
    \ (e.g., `\".2f\"`), a format string with fields named\n            `x` and/or\
    \ `pos` (e.g., `\"${x:.2f}\"`), or a callable with a signature like\n        \
    \    `f(x: float, pos: int) -> str`. In the latter variants, `x` is passed as\
    \ the\n            tick value and `pos` is passed as the tick index.\n       \
    \ base : number\n            Use log formatter (with scientific notation) having\
    \ this value as the base.\n            Set to `None` to override the default formatter\
    \ with a log transform.\n        unit : str or (str, str) tuple\n            Use\
    \  SI prefixes with these units (e.g., with `unit=\"g\"`, a tick value\n     \
    \       of 5000 will appear as `5 kg`). When a tuple, the first element gives\
    \ the\n            separator between the number and unit.\n\n        Returns\n\
    \        -------\n        scale\n            Copy of self with new label configuration.\n\
    \n        \"\"\"\n        # Input checks\n        if formatter is not None and\
    \ not isinstance(formatter, Formatter):\n            raise TypeError(\n      \
    \          f\"Label formatter must be an instance of {Formatter!r}, \"\n     \
    \           f\"not {type(formatter)!r}\"\n            )\n        if like is not\
    \ None and not (isinstance(like, str) or callable(like)):\n            msg = f\"\
    `like` must be a string or callable, not {type(like).__name__}.\"\n          \
    \  raise TypeError(msg)\n\n        new = copy(self)\n        new._label_params\
    \ = {\n            \"formatter\": formatter,\n            \"like\": like,\n  \
    \          \"base\": base,\n            \"unit\": unit,\n        }\n        return\
    \ new\n\n    def _parse_for_log_params(\n        self, trans: str | TransFuncs\
    \ | None\n    ) -> tuple[float | None, float | None]:\n\n        log_base = symlog_thresh\
    \ = None\n        if isinstance(trans, str):\n            m = re.match(r\"^log(\\\
    d*)\", trans)\n            if m is not None:\n                log_base = float(m[1]\
    \ or 10)\n            m = re.match(r\"symlog(\\d*)\", trans)\n            if m\
    \ is not None:\n                symlog_thresh = float(m[1] or 1)\n        return\
    \ log_base, symlog_thresh\n\n    def _get_locators(self, locator, at, upto, count,\
    \ every, between, minor):\n\n        log_base, symlog_thresh = self._parse_for_log_params(self.trans)\n\
    \n        if locator is not None:\n            major_locator = locator\n\n   \
    \     elif upto is not None:\n            if log_base:\n                major_locator\
    \ = LogLocator(base=log_base, numticks=upto)\n            else:\n            \
    \    major_locator = MaxNLocator(upto, steps=[1, 1.5, 2, 2.5, 3, 5, 10])\n\n \
    \       elif count is not None:\n            if between is None:\n           \
    \     # This is rarely useful (unless you are setting limits)\n              \
    \  major_locator = LinearLocator(count)\n            else:\n                if\
    \ log_base or symlog_thresh:\n                    forward, inverse = self._get_transform()\n\
    \                    lo, hi = forward(between)\n                    ticks = inverse(np.linspace(lo,\
    \ hi, num=count))\n                else:\n                    ticks = np.linspace(*between,\
    \ num=count)\n                major_locator = FixedLocator(ticks)\n\n        elif\
    \ every is not None:\n            if between is None:\n                major_locator\
    \ = MultipleLocator(every)\n            else:\n                lo, hi = between\n\
    \                ticks = np.arange(lo, hi + every, every)\n                major_locator\
    \ = FixedLocator(ticks)\n\n        elif at is not None:\n            major_locator\
    \ = FixedLocator(at)\n\n        else:\n            if log_base:\n            \
    \    major_locator = LogLocator(log_base)\n            elif symlog_thresh:\n \
    \               major_locator = SymmetricalLogLocator(linthresh=symlog_thresh,\
    \ base=10)\n            else:\n                major_locator = AutoLocator()\n\
    \n        if minor is None:\n            minor_locator = LogLocator(log_base,\
    \ subs=None) if log_base else None\n        else:\n            if log_base:\n\
    \                subs = np.linspace(0, log_base, minor + 2)[1:-1]\n          \
    \      minor_locator = LogLocator(log_base, subs=subs)\n            else:\n  \
    \              minor_locator = AutoMinorLocator(minor + 1)\n\n        return major_locator,\
    \ minor_locator\n\n    def _get_formatter(self, locator, formatter, like, base,\
    \ unit):\n\n        log_base, symlog_thresh = self._parse_for_log_params(self.trans)\n\
    \        if base is default:\n            if symlog_thresh:\n                log_base\
    \ = 10\n            base = log_base\n\n        if formatter is not None:\n   \
    \         return formatter\n\n        if like is not None:\n            if isinstance(like,\
    \ str):\n                if \"{x\" in like or \"{pos\" in like:\n            \
    \        fmt = like\n                else:\n                    fmt = f\"{{x:{like}}}\"\
    \n                formatter = StrMethodFormatter(fmt)\n            else:\n   \
    \             formatter = FuncFormatter(like)\n\n        elif base is not None:\n\
    \            # We could add other log options if necessary\n            formatter\
    \ = LogFormatterSciNotation(base)\n\n        elif unit is not None:\n        \
    \    if isinstance(unit, tuple):\n                sep, unit = unit\n         \
    \   elif not unit:\n                sep = \"\"\n            else:\n          \
    \      sep = \" \"\n            formatter = EngFormatter(unit, sep=sep)\n\n  \
    \      else:\n            formatter = ScalarFormatter()\n\n        return formatter\n\
    \n\n@dataclass\nclass Temporal(ContinuousBase):\n    \"\"\"\n    A scale for date/time\
    \ data.\n    \"\"\"\n    # TODO date: bool?\n    # For when we only care about\
    \ the time component, would affect\n    # default formatter and norm conversion.\
    \ Should also happen in\n    # Property.default_scale. The alternative was having\
    \ distinct\n    # Calendric / Temporal scales, but that feels a bit fussy, and\
    \ it\n    # would get in the way of using first-letter shorthands because\n  \
    \  # Calendric and Continuous would collide. Still, we haven't implemented\n \
    \   # those yet, and having a clear distinction betewen date(time) / time\n  \
    \  # may be more useful.\n\n    trans = None\n\n    _priority: ClassVar[int] =\
    \ 2\n\n    def tick(\n        self, locator: Locator | None = None, *,\n     \
    \   upto: int | None = None,\n    ) -> Temporal:\n        \"\"\"\n        Configure\
    \ the selection of ticks for the scale's axis or legend.\n\n        .. note::\n\
    \            This API is under construction and will be enhanced over time.\n\n\
    \        Parameters\n        ----------\n        locator : :class:`matplotlib.ticker.Locator`\
    \ subclass\n            Pre-configured matplotlib locator; other parameters will\
    \ not be used.\n        upto : int\n            Choose \"nice\" locations for\
    \ ticks, but do not exceed this number.\n\n        Returns\n        -------\n\
    \        scale\n            Copy of self with new tick configuration.\n\n    \
    \    \"\"\"\n        if locator is not None and not isinstance(locator, Locator):\n\
    \            err = (\n                f\"Tick locator must be an instance of {Locator!r},\
    \ \"\n                f\"not {type(locator)!r}.\"\n            )\n           \
    \ raise TypeError(err)\n\n        new = copy(self)\n        new._tick_params =\
    \ {\"locator\": locator, \"upto\": upto}\n        return new\n\n    def label(\n\
    \        self,\n        formatter: Formatter | None = None, *,\n        concise:\
    \ bool = False,\n    ) -> Temporal:\n        \"\"\"\n        Configure the appearance\
    \ of tick labels for the scale's axis or legend.\n\n        .. note::\n      \
    \      This API is under construction and will be enhanced over time.\n\n    \
    \    Parameters\n        ----------\n        formatter : :class:`matplotlib.ticker.Formatter`\
    \ subclass\n            Pre-configured formatter to use; other parameters will\
    \ be ignored.\n        concise : bool\n            If True, use :class:`matplotlib.dates.ConciseDateFormatter`\
    \ to make\n            the tick labels as compact as possible.\n\n        Returns\n\
    \        -------\n        scale\n            Copy of self with new label configuration.\n\
    \n        \"\"\"\n        new = copy(self)\n        new._label_params = {\"formatter\"\
    : formatter, \"concise\": concise}\n        return new\n\n    def _get_locators(self,\
    \ locator, upto):\n\n        if locator is not None:\n            major_locator\
    \ = locator\n        elif upto is not None:\n            major_locator = AutoDateLocator(minticks=2,\
    \ maxticks=upto)\n\n        else:\n            major_locator = AutoDateLocator(minticks=2,\
    \ maxticks=6)\n        minor_locator = None\n\n        return major_locator, minor_locator\n\
    \n    def _get_formatter(self, locator, formatter, concise):\n\n        if formatter\
    \ is not None:\n            return formatter\n\n        if concise:\n        \
    \    # TODO ideally we would have concise coordinate ticks,\n            # but\
    \ full semantic ticks. Is that possible?\n            formatter = ConciseDateFormatter(locator)\n\
    \        else:\n            formatter = AutoDateFormatter(locator)\n\n       \
    \ return formatter\n\n\n# -----------------------------------------------------------------------------------\
    \ #\n\n\n# TODO Have this separate from Temporal or have Temporal(date=True) or\
    \ similar?\n# class Calendric(Scale):\n\n# TODO Needed? Or handle this at layer\
    \ (in stat or as param, eg binning=)\n# class Binned(Scale):\n\n# TODO any need\
    \ for color-specific scales?\n# class Sequential(Continuous):\n# class Diverging(Continuous):\n\
    # class Qualitative(Nominal):\n\n\n# -----------------------------------------------------------------------------------\
    \ #\n\n\nclass PseudoAxis:\n    \"\"\"\n    Internal class implementing minimal\
    \ interface equivalent to matplotlib Axis.\n\n    Coordinate variables are typically\
    \ scaled by attaching the Axis object from\n    the figure where the plot will\
    \ end up. Matplotlib has no similar concept of\n    and axis for the other mappable\
    \ variables (color, etc.), but to simplify the\n    code, this object acts like\
    \ an Axis and can be used to scale other variables.\n\n    \"\"\"\n    axis_name\
    \ = \"\"  # Matplotlib requirement but not actually used\n\n    def __init__(self,\
    \ scale):\n\n        self.converter = None\n        self.units = None\n      \
    \  self.scale = scale\n        self.major = mpl.axis.Ticker()\n        self.minor\
    \ = mpl.axis.Ticker()\n\n        # It appears that this needs to be initialized\
    \ this way on matplotlib 3.1,\n        # but not later versions. It is unclear\
    \ whether there are any issues with it.\n        self._data_interval = None, None\n\
    \n        scale.set_default_locators_and_formatters(self)\n        # self.set_default_intervals()\
    \  Is this ever needed?\n\n    def set_view_interval(self, vmin, vmax):\n    \
    \    self._view_interval = vmin, vmax\n\n    def get_view_interval(self):\n  \
    \      return self._view_interval\n\n    # TODO do we want to distinguish view/data\
    \ intervals? e.g. for a legend\n    # we probably want to represent the full range\
    \ of the data values, but\n    # still norm the colormap. If so, we'll need to\
    \ track data range separately\n    # from the norm, which we currently don't do.\n\
    \n    def set_data_interval(self, vmin, vmax):\n        self._data_interval =\
    \ vmin, vmax\n\n    def get_data_interval(self):\n        return self._data_interval\n\
    \n    def get_tick_space(self):\n        # TODO how to do this in a configurable\
    \ / auto way?\n        # Would be cool to have legend density adapt to figure\
    \ size, etc.\n        return 5\n\n    def set_major_locator(self, locator):\n\
    \        self.major.locator = locator\n        locator.set_axis(self)\n\n    def\
    \ set_major_formatter(self, formatter):\n        self.major.formatter = formatter\n\
    \        formatter.set_axis(self)\n\n    def set_minor_locator(self, locator):\n\
    \        self.minor.locator = locator\n        locator.set_axis(self)\n\n    def\
    \ set_minor_formatter(self, formatter):\n        self.minor.formatter = formatter\n\
    \        formatter.set_axis(self)\n\n    def set_units(self, units):\n       \
    \ self.units = units\n\n    def update_units(self, x):\n        \"\"\"Pass units\
    \ to the internal converter, potentially updating its mapping.\"\"\"\n       \
    \ self.converter = mpl.units.registry.get_converter(x)\n        if self.converter\
    \ is not None:\n            self.converter.default_units(x, self)\n\n        \
    \    info = self.converter.axisinfo(self.units, self)\n\n            if info is\
    \ None:\n                return\n            if info.majloc is not None:\n   \
    \             self.set_major_locator(info.majloc)\n            if info.majfmt\
    \ is not None:\n                self.set_major_formatter(info.majfmt)\n\n    \
    \        # This is in matplotlib method; do we need this?\n            # self.set_default_intervals()\n\
    \n    def convert_units(self, x):\n        \"\"\"Return a numeric representation\
    \ of the input data.\"\"\"\n        if np.issubdtype(np.asarray(x).dtype, np.number):\n\
    \            return x\n        elif self.converter is None:\n            return\
    \ x\n        return self.converter.convert(x, self.units, self)\n\n    def get_scale(self):\n\
    \        # Note that matplotlib actually returns a string here!\n        # (e.g.,\
    \ with a log scale, axis.get_scale() returns \"log\")\n        # Currently we\
    \ just hit it with minor ticks where it checks for\n        # scale == \"log\"\
    . I'm not sure how you'd actually use log-scale\n        # minor \"ticks\" in\
    \ a legend context, so this is fine....\n        return self.scale\n\n    def\
    \ get_majorticklocs(self):\n        return self.major.locator()\n\n\n# ------------------------------------------------------------------------------------\
    \ #\n# Transform function creation\n\n\ndef _make_identity_transforms() -> TransFuncs:\n\
    \n    def identity(x):\n        return x\n\n    return identity, identity\n\n\n\
    def _make_logit_transforms(base: float | None = None) -> TransFuncs:\n\n    log,\
    \ exp = _make_log_transforms(base)\n\n    def logit(x):\n        with np.errstate(invalid=\"\
    ignore\", divide=\"ignore\"):\n            return log(x) - log(1 - x)\n\n    def\
    \ expit(x):\n        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n\
    \            return exp(x) / (1 + exp(x))\n\n    return logit, expit\n\n\ndef\
    \ _make_log_transforms(base: float | None = None) -> TransFuncs:\n\n    fs: TransFuncs\n\
    \    if base is None:\n        fs = np.log, np.exp\n    elif base == 2:\n    \
    \    fs = np.log2, partial(np.power, 2)\n    elif base == 10:\n        fs = np.log10,\
    \ partial(np.power, 10)\n    else:\n        def forward(x):\n            return\
    \ np.log(x) / np.log(base)\n        fs = forward, partial(np.power, base)\n\n\
    \    def log(x: ArrayLike) -> ArrayLike:\n        with np.errstate(invalid=\"\
    ignore\", divide=\"ignore\"):\n            return fs[0](x)\n\n    def exp(x: ArrayLike)\
    \ -> ArrayLike:\n        with np.errstate(invalid=\"ignore\", divide=\"ignore\"\
    ):\n            return fs[1](x)\n\n    return log, exp\n\n\ndef _make_symlog_transforms(c:\
    \ float = 1, base: float = 10) -> TransFuncs:\n\n    # From https://iopscience.iop.org/article/10.1088/0957-0233/24/2/027001\n\
    \n    # Note: currently not using base because we only get\n    # one parameter\
    \ from the string, and are using c (this is consistent with d3)\n\n    log, exp\
    \ = _make_log_transforms(base)\n\n    def symlog(x):\n        with np.errstate(invalid=\"\
    ignore\", divide=\"ignore\"):\n            return np.sign(x) * log(1 + np.abs(np.divide(x,\
    \ c)))\n\n    def symexp(x):\n        with np.errstate(invalid=\"ignore\", divide=\"\
    ignore\"):\n            return np.sign(x) * c * (exp(np.abs(x)) - 1)\n\n    return\
    \ symlog, symexp\n\n\ndef _make_sqrt_transforms() -> TransFuncs:\n\n    def sqrt(x):\n\
    \        return np.sign(x) * np.sqrt(np.abs(x))\n\n    def square(x):\n      \
    \  return np.sign(x) * np.square(x)\n\n    return sqrt, square\n\n\ndef _make_power_transforms(exp:\
    \ float) -> TransFuncs:\n\n    def forward(x):\n        return np.sign(x) * np.power(np.abs(x),\
    \ exp)\n\n    def inverse(x):\n        return np.sign(x) * np.power(np.abs(x),\
    \ 1 / exp)\n\n    return forward, inverse\n\n\ndef _default_spacer(x: Series)\
    \ -> float:\n    return 1\n\nOutput the complete test file, code only, no explanations.\n\
    ### Time\nCurrent time: 2025-03-14 17:17:32\n"
  role: user
