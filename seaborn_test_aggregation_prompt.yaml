messages:
- content: You are an AI agent expert in writing unit tests. Your task is to write
    unit tests for the given code files of the repository. Make sure the tests can
    be executed without lint or compile errors.
  role: system
- content: "### Task Information\nBased on the source code, write/rewrite tests to\
    \ cover the source code.\nRepository: seaborn\nTest File Path: seaborn\\test_aggregation\\\
    test_aggregation.py\nProject Programming Language: Python\nTesting Framework:\
    \ pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import\
    \ annotations\nfrom dataclasses import dataclass\nfrom typing import ClassVar,\
    \ Callable\n\nimport pandas as pd\nfrom pandas import DataFrame\n\nfrom seaborn._core.scales\
    \ import Scale\nfrom seaborn._core.groupby import GroupBy\nfrom seaborn._stats.base\
    \ import Stat\nfrom seaborn._statistics import (\n    EstimateAggregator,\n  \
    \  WeightedAggregator,\n)\nfrom seaborn._core.typing import Vector\n\n\n@dataclass\n\
    class Agg(Stat):\n    \"\"\"\n    Aggregate data along the value axis using given\
    \ method.\n\n    Parameters\n    ----------\n    func : str or callable\n    \
    \    Name of a :class:`pandas.Series` method or a vector -> scalar function.\n\
    \n    See Also\n    --------\n    objects.Est : Aggregation with error bars.\n\
    \n    Examples\n    --------\n    .. include:: ../docstrings/objects.Agg.rst\n\
    \n    \"\"\"\n    func: str | Callable[[Vector], float] = \"mean\"\n\n    group_by_orient:\
    \ ClassVar[bool] = True\n\n    def __call__(\n        self, data: DataFrame, groupby:\
    \ GroupBy, orient: str, scales: dict[str, Scale],\n    ) -> DataFrame:\n\n   \
    \     var = {\"x\": \"y\", \"y\": \"x\"}.get(orient)\n        res = (\n      \
    \      groupby\n            .agg(data, {var: self.func})\n            .dropna(subset=[var])\n\
    \            .reset_index(drop=True)\n        )\n        return res\n\n\n@dataclass\n\
    class Est(Stat):\n    \"\"\"\n    Calculate a point estimate and error bar interval.\n\
    \n    For more information about the various `errorbar` choices, see the\n   \
    \ :doc:`errorbar tutorial </tutorial/error_bars>`.\n\n    Additional variables:\n\
    \n    - **weight**: When passed to a layer that uses this stat, a weighted estimate\n\
    \      will be computed. Note that use of weights currently limits the choice\
    \ of\n      function and error bar method  to `\"mean\"` and `\"ci\"`, respectively.\n\
    \n    Parameters\n    ----------\n    func : str or callable\n        Name of\
    \ a :class:`numpy.ndarray` method or a vector -> scalar function.\n    errorbar\
    \ : str, (str, float) tuple, or callable\n        Name of errorbar method (one\
    \ of \"ci\", \"pi\", \"se\" or \"sd\"), or a tuple\n        with a method name\
    \ ane a level parameter, or a function that maps from a\n        vector to a (min,\
    \ max) interval.\n    n_boot : int\n       Number of bootstrap samples to draw\
    \ for \"ci\" errorbars.\n    seed : int\n        Seed for the PRNG used to draw\
    \ bootstrap samples.\n\n    Examples\n    --------\n    .. include:: ../docstrings/objects.Est.rst\n\
    \n    \"\"\"\n    func: str | Callable[[Vector], float] = \"mean\"\n    errorbar:\
    \ str | tuple[str, float] = (\"ci\", 95)\n    n_boot: int = 1000\n    seed: int\
    \ | None = None\n\n    group_by_orient: ClassVar[bool] = True\n\n    def _process(\n\
    \        self, data: DataFrame, var: str, estimator: EstimateAggregator\n    )\
    \ -> DataFrame:\n        # Needed because GroupBy.apply assumes func is DataFrame\
    \ -> DataFrame\n        # which we could probably make more general to allow Series\
    \ return\n        res = estimator(data, var)\n        return pd.DataFrame([res])\n\
    \n    def __call__(\n        self, data: DataFrame, groupby: GroupBy, orient:\
    \ str, scales: dict[str, Scale],\n    ) -> DataFrame:\n\n        boot_kws = {\"\
    n_boot\": self.n_boot, \"seed\": self.seed}\n        if \"weight\" in data:\n\
    \            engine = WeightedAggregator(self.func, self.errorbar, **boot_kws)\n\
    \        else:\n            engine = EstimateAggregator(self.func, self.errorbar,\
    \ **boot_kws)\n\n        var = {\"x\": \"y\", \"y\": \"x\"}[orient]\n        res\
    \ = (\n            groupby\n            .apply(data, self._process, var, engine)\n\
    \            .dropna(subset=[var])\n            .reset_index(drop=True)\n    \
    \    )\n\n        res = res.fillna({f\"{var}min\": res[var], f\"{var}max\": res[var]})\n\
    \n        return res\n\n\n@dataclass\nclass Rolling(Stat):\n    ...\n\n    def\
    \ __call__(self, data, groupby, orient, scales):\n        ...\n\n### Source File\
    \ Dependency Files Content\n### Dependency File: base.py\n\"\"\"Base module for\
    \ statistical transformations.\"\"\"\nfrom __future__ import annotations\nfrom\
    \ collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom typing\
    \ import ClassVar, Any\nimport warnings\n\nfrom typing import TYPE_CHECKING\n\
    if TYPE_CHECKING:\n    from pandas import DataFrame\n    from seaborn._core.groupby\
    \ import GroupBy\n    from seaborn._core.scales import Scale\n\n\n@dataclass\n\
    class Stat:\n    \"\"\"Base class for objects that apply statistical transformations.\"\
    \"\"\n\n    # The class supports a partial-function application pattern. The object\
    \ is\n    # initialized with desired parameters and the result is a callable that\n\
    \    # accepts and returns dataframes.\n\n    # The statistical transformation\
    \ logic should not add any state to the instance\n    # beyond what is defined\
    \ with the initialization parameters.\n\n    # Subclasses can declare whether\
    \ the orient dimension should be used in grouping\n    # TODO consider whether\
    \ this should be a parameter. Motivating example:\n    # use the same KDE class\
    \ violin plots and univariate density estimation.\n    # In the former case, we\
    \ would expect separate densities for each unique\n    # value on the orient axis,\
    \ but we would not in the latter case.\n    group_by_orient: ClassVar[bool] =\
    \ False\n\n    def _check_param_one_of(self, param: str, options: Iterable[Any])\
    \ -> None:\n        \"\"\"Raise when parameter value is not one of a specified\
    \ set.\"\"\"\n        value = getattr(self, param)\n        if value not in options:\n\
    \            *most, last = options\n            option_str = \", \".join(f\"{x!r}\"\
    \ for x in most[:-1]) + f\" or {last!r}\"\n            err = \" \".join([\n  \
    \              f\"The `{param}` parameter for `{self.__class__.__name__}` must\
    \ be\",\n                f\"one of {option_str}; not {value!r}.\",\n         \
    \   ])\n            raise ValueError(err)\n\n    def _check_grouping_vars(\n \
    \       self, param: str, data_vars: list[str], stacklevel: int = 2,\n    ) ->\
    \ None:\n        \"\"\"Warn if vars are named in parameter without being present\
    \ in the data.\"\"\"\n        param_vars = getattr(self, param)\n        undefined\
    \ = set(param_vars) - set(data_vars)\n        if undefined:\n            param\
    \ = f\"{self.__class__.__name__}.{param}\"\n            names = \", \".join(f\"\
    {x!r}\" for x in undefined)\n            msg = f\"Undefined variable(s) passed\
    \ for {param}: {names}.\"\n            warnings.warn(msg, stacklevel=stacklevel)\n\
    \n    def __call__(\n        self,\n        data: DataFrame,\n        groupby:\
    \ GroupBy,\n        orient: str,\n        scales: dict[str, Scale],\n    ) ->\
    \ DataFrame:\n        \"\"\"Apply statistical transform to data subgroups and\
    \ return combined result.\"\"\"\n        return data\n\n\n### Dependency File:\
    \ groupby.py\n\"\"\"Simplified split-apply-combine paradigm on dataframes for\
    \ internal use.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import\
    \ cast, Iterable\n\nimport pandas as pd\n\nfrom seaborn._core.rules import categorical_order\n\
    \nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from typing import\
    \ Callable\n    from pandas import DataFrame, MultiIndex, Index\n\n\nclass GroupBy:\n\
    \    \"\"\"\n    Interface for Pandas GroupBy operations allowing specified group\
    \ order.\n\n    Writing our own class to do this has a few advantages:\n    -\
    \ It constrains the interface between Plot and Stat/Move objects\n    - It allows\
    \ control over the row order of the GroupBy result, which is\n      important\
    \ when using in the context of some Move operations (dodge, stack, ...)\n    -\
    \ It simplifies some complexities regarding the return type and Index contents\n\
    \      one encounters with Pandas, especially for DataFrame -> DataFrame applies\n\
    \    - It increases future flexibility regarding alternate DataFrame libraries\n\
    \n    \"\"\"\n    def __init__(self, order: list[str] | dict[str, list | None]):\n\
    \        \"\"\"\n        Initialize the GroupBy from grouping variables and optional\
    \ level orders.\n\n        Parameters\n        ----------\n        order\n   \
    \         List of variable names or dict mapping names to desired level orders.\n\
    \            Level order values can be None to use default ordering rules. The\n\
    \            variables can include names that are not expected to appear in the\n\
    \            data; these will be dropped before the groups are defined.\n\n  \
    \      \"\"\"\n        if not order:\n            raise ValueError(\"GroupBy requires\
    \ at least one grouping variable\")\n\n        if isinstance(order, list):\n \
    \           order = {k: None for k in order}\n        self.order = order\n\n \
    \   def _get_groups(\n        self, data: DataFrame\n    ) -> tuple[str | list[str],\
    \ Index | MultiIndex]:\n        \"\"\"Return index with Cartesian product of ordered\
    \ grouping variable levels.\"\"\"\n        levels = {}\n        for var, order\
    \ in self.order.items():\n            if var in data:\n                if order\
    \ is None:\n                    order = categorical_order(data[var])\n       \
    \         levels[var] = order\n\n        grouper: str | list[str]\n        groups:\
    \ Index | MultiIndex\n        if not levels:\n            grouper = []\n     \
    \       groups = pd.Index([])\n        elif len(levels) > 1:\n            grouper\
    \ = list(levels)\n            groups = pd.MultiIndex.from_product(levels.values(),\
    \ names=grouper)\n        else:\n            grouper, = list(levels)\n       \
    \     groups = pd.Index(levels[grouper], name=grouper)\n        return grouper,\
    \ groups\n\n    def _reorder_columns(self, res, data):\n        \"\"\"Reorder\
    \ result columns to match original order with new columns appended.\"\"\"\n  \
    \      cols = [c for c in data if c in res]\n        cols += [c for c in res if\
    \ c not in data]\n        return res.reindex(columns=pd.Index(cols))\n\n    def\
    \ agg(self, data: DataFrame, *args, **kwargs) -> DataFrame:\n        \"\"\"\n\
    \        Reduce each group to a single row in the output.\n\n        The output\
    \ will have a row for each unique combination of the grouping\n        variable\
    \ levels with null values for the aggregated variable(s) where\n        those\
    \ combinations do not appear in the dataset.\n\n        \"\"\"\n        grouper,\
    \ groups = self._get_groups(data)\n\n        if not grouper:\n            # We\
    \ will need to see whether there are valid usecases that end up here\n       \
    \     raise ValueError(\"No grouping variables are present in dataframe\")\n\n\
    \        res = (\n            data\n            .groupby(grouper, sort=False,\
    \ observed=False)\n            .agg(*args, **kwargs)\n            .reindex(groups)\n\
    \            .reset_index()\n            .pipe(self._reorder_columns, data)\n\
    \        )\n\n        return res\n\n    def apply(\n        self, data: DataFrame,\
    \ func: Callable[..., DataFrame],\n        *args, **kwargs,\n    ) -> DataFrame:\n\
    \        \"\"\"Apply a DataFrame -> DataFrame mapping to each group.\"\"\"\n \
    \       grouper, groups = self._get_groups(data)\n\n        if not grouper:\n\
    \            return self._reorder_columns(func(data, *args, **kwargs), data)\n\
    \n        parts = {}\n        for key, part_df in data.groupby(grouper, sort=False,\
    \ observed=False):\n            parts[key] = func(part_df, *args, **kwargs)\n\
    \        stack = []\n        for key in groups:\n            if key in parts:\n\
    \                if isinstance(grouper, list):\n                    # Implies\
    \ that we had a MultiIndex so key is iterable\n                    group_ids =\
    \ dict(zip(grouper, cast(Iterable, key)))\n                else:\n           \
    \         group_ids = {grouper: key}\n                stack.append(parts[key].assign(**group_ids))\n\
    \n        res = pd.concat(stack, ignore_index=True)\n        return self._reorder_columns(res,\
    \ data)\n\n\n### Dependency File: scales.py\nfrom __future__ import annotations\n\
    import re\nfrom copy import copy\nfrom collections.abc import Sequence\nfrom dataclasses\
    \ import dataclass\nfrom functools import partial\nfrom typing import Any, Callable,\
    \ Tuple, Optional, ClassVar\n\nimport numpy as np\nimport matplotlib as mpl\n\
    from matplotlib.ticker import (\n    Locator,\n    Formatter,\n    AutoLocator,\n\
    \    AutoMinorLocator,\n    FixedLocator,\n    LinearLocator,\n    LogLocator,\n\
    \    SymmetricalLogLocator,\n    MaxNLocator,\n    MultipleLocator,\n    EngFormatter,\n\
    \    FuncFormatter,\n    LogFormatterSciNotation,\n    ScalarFormatter,\n    StrMethodFormatter,\n\
    )\nfrom matplotlib.dates import (\n    AutoDateLocator,\n    AutoDateFormatter,\n\
    \    ConciseDateFormatter,\n)\nfrom matplotlib.axis import Axis\nfrom matplotlib.scale\
    \ import ScaleBase\nfrom pandas import Series\n\nfrom seaborn._core.rules import\
    \ categorical_order\nfrom seaborn._core.typing import Default, default\n\nfrom\
    \ typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from seaborn._core.plot\
    \ import Plot\n    from seaborn._core.properties import Property\n    from numpy.typing\
    \ import ArrayLike, NDArray\n\n    TransFuncs = Tuple[\n        Callable[[ArrayLike],\
    \ ArrayLike], Callable[[ArrayLike], ArrayLike]\n    ]\n\n    # TODO Reverting\
    \ typing to Any as it was proving too complicated to\n    # work out the right\
    \ way to communicate the types to mypy. Revisit!\n    Pipeline = Sequence[Optional[Callable[[Any],\
    \ Any]]]\n\n\nclass Scale:\n    \"\"\"Base class for objects that map data values\
    \ to visual properties.\"\"\"\n\n    values: tuple | str | list | dict | None\n\
    \n    _priority: ClassVar[int]\n    _pipeline: Pipeline\n    _matplotlib_scale:\
    \ ScaleBase\n    _spacer: staticmethod\n    _legend: tuple[list[Any], list[str]]\
    \ | None\n\n    def __post_init__(self):\n\n        self._tick_params = None\n\
    \        self._label_params = None\n        self._legend = None\n\n    def tick(self):\n\
    \        raise NotImplementedError()\n\n    def label(self):\n        raise NotImplementedError()\n\
    \n    def _get_locators(self):\n        raise NotImplementedError()\n\n    def\
    \ _get_formatter(self, locator: Locator | None = None):\n        raise NotImplementedError()\n\
    \n    def _get_scale(self, name: str, forward: Callable, inverse: Callable):\n\
    \n        major_locator, minor_locator = self._get_locators(**self._tick_params)\n\
    \        major_formatter = self._get_formatter(major_locator, **self._label_params)\n\
    \n        class InternalScale(mpl.scale.FuncScale):\n            def set_default_locators_and_formatters(self,\
    \ axis):\n                axis.set_major_locator(major_locator)\n            \
    \    if minor_locator is not None:\n                    axis.set_minor_locator(minor_locator)\n\
    \                axis.set_major_formatter(major_formatter)\n\n        return InternalScale(name,\
    \ (forward, inverse))\n\n    def _spacing(self, x: Series) -> float:\n       \
    \ space = self._spacer(x)\n        if np.isnan(space):\n            # This happens\
    \ when there is no variance in the orient coordinate data\n            # Not exactly\
    \ clear what the right default is, but 1 seems reasonable?\n            return\
    \ 1\n        return space\n\n    def _setup(\n        self, data: Series, prop:\
    \ Property, axis: Axis | None = None,\n    ) -> Scale:\n        raise NotImplementedError()\n\
    \n    def _finalize(self, p: Plot, axis: Axis) -> None:\n        \"\"\"Perform\
    \ scale-specific axis tweaks after adding artists.\"\"\"\n        pass\n\n   \
    \ def __call__(self, data: Series) -> ArrayLike:\n\n        trans_data: Series\
    \ | NDArray | list\n\n        # TODO sometimes we need to handle scalars (e.g.\
    \ for Line)\n        # but what is the best way to do that?\n        scalar_data\
    \ = np.isscalar(data)\n        if scalar_data:\n            trans_data = np.array([data])\n\
    \        else:\n            trans_data = data\n\n        for func in self._pipeline:\n\
    \            if func is not None:\n                trans_data = func(trans_data)\n\
    \n        if scalar_data:\n            return trans_data[0]\n        else:\n \
    \           return trans_data\n\n    @staticmethod\n    def _identity():\n\n \
    \       class Identity(Scale):\n            _pipeline = []\n            _spacer\
    \ = None\n            _legend = None\n            _matplotlib_scale = None\n\n\
    \        return Identity()\n\n\n@dataclass\nclass Boolean(Scale):\n    \"\"\"\n\
    \    A scale with a discrete domain of True and False values.\n\n    The behavior\
    \ is similar to the :class:`Nominal` scale, but property\n    mappings and legends\
    \ will use a [True, False] ordering rather than\n    a sort using numeric rules.\
    \ Coordinate variables accomplish this by\n    inverting axis limits so as to\
    \ maintain underlying numeric positioning.\n    Input data are cast to boolean\
    \ values, respecting missing data.\n\n    \"\"\"\n    values: tuple | list | dict\
    \ | None = None\n\n    _priority: ClassVar[int] = 3\n\n    def _setup(\n     \
    \   self, data: Series, prop: Property, axis: Axis | None = None,\n    ) -> Scale:\n\
    \n        new = copy(self)\n        if new._tick_params is None:\n           \
    \ new = new.tick()\n        if new._label_params is None:\n            new = new.label()\n\
    \n        def na_safe_cast(x):\n            # TODO this doesn't actually need\
    \ to be a closure\n            if np.isscalar(x):\n                return float(bool(x))\n\
    \            else:\n                if hasattr(x, \"notna\"):\n              \
    \      # Handle pd.NA; np<>pd interop with NA is tricky\n                    use\
    \ = x.notna().to_numpy()\n                else:\n                    use = np.isfinite(x)\n\
    \                out = np.full(len(x), np.nan, dtype=float)\n                out[use]\
    \ = x[use].astype(bool).astype(float)\n                return out\n\n        new._pipeline\
    \ = [na_safe_cast, prop.get_mapping(new, data)]\n        new._spacer = _default_spacer\n\
    \        if prop.legend:\n            new._legend = [True, False], [\"True\",\
    \ \"False\"]\n\n        forward, inverse = _make_identity_transforms()\n     \
    \   mpl_scale = new._get_scale(str(data.name), forward, inverse)\n\n        axis\
    \ = PseudoAxis(mpl_scale) if axis is None else axis\n        mpl_scale.set_default_locators_and_formatters(axis)\n\
    \        new._matplotlib_scale = mpl_scale\n\n        return new\n\n    def _finalize(self,\
    \ p: Plot, axis: Axis) -> None:\n\n        # We want values to appear in a True,\
    \ False order but also want\n        # True/False to be drawn at 1/0 positions\
    \ respectively to avoid nasty\n        # surprises if additional artists are added\
    \ through the matplotlib API.\n        # We accomplish this using axis inversion\
    \ akin to what we do in Nominal.\n\n        ax = axis.axes\n        name = axis.axis_name\n\
    \        axis.grid(False, which=\"both\")\n        if name not in p._limits:\n\
    \            nticks = len(axis.get_major_ticks())\n            lo, hi = -.5, nticks\
    \ - .5\n            if name == \"x\":\n                lo, hi = hi, lo\n     \
    \       set_lim = getattr(ax, f\"set_{name}lim\")\n            set_lim(lo, hi,\
    \ auto=None)\n\n    def tick(self, locator: Locator | None = None):\n        new\
    \ = copy(self)\n        new._tick_params = {\"locator\": locator}\n        return\
    \ new\n\n    def label(self, formatter: Formatter | None = None):\n        new\
    \ = copy(self)\n        new._label_params = {\"formatter\": formatter}\n     \
    \   return new\n\n    def _get_locators(self, locator):\n        if locator is\
    \ not None:\n            return locator\n        return FixedLocator([0, 1]),\
    \ None\n\n    def _get_formatter(self, locator, formatter):\n        if formatter\
    \ is not None:\n            return formatter\n        return FuncFormatter(lambda\
    \ x, _: str(bool(x)))\n\n\n@dataclass\nclass Nominal(Scale):\n    \"\"\"\n   \
    \ A categorical scale without relative importance / magnitude.\n    \"\"\"\n \
    \   # Categorical (convert to strings), un-sortable\n\n    values: tuple | str\
    \ | list | dict | None = None\n    order: list | None = None\n\n    _priority:\
    \ ClassVar[int] = 4\n\n    def _setup(\n        self, data: Series, prop: Property,\
    \ axis: Axis | None = None,\n    ) -> Scale:\n\n        new = copy(self)\n   \
    \     if new._tick_params is None:\n            new = new.tick()\n        if new._label_params\
    \ is None:\n            new = new.label()\n\n        # TODO flexibility over format()\
    \ which isn't great for numbers / dates\n        stringify = np.vectorize(format,\
    \ otypes=[\"object\"])\n\n        units_seed = categorical_order(data, new.order)\n\
    \n        # TODO move to Nominal._get_scale?\n        # TODO this needs some more\
    \ complicated rethinking about how to pass\n        # a unit dictionary down to\
    \ these methods, along with how much we want\n        # to invest in their API.\
    \ What is it useful for tick() to do here?\n        # (Ordinal may be different\
    \ if we draw that contrast).\n        # Any customization we do to allow, e.g.,\
    \ label wrapping will probably\n        # require defining our own Formatter subclass.\n\
    \        # We could also potentially implement auto-wrapping in an Axis subclass\n\
    \        # (see Axis.draw ... it already is computing the bboxes).\n        #\
    \ major_locator, minor_locator = new._get_locators(**new._tick_params)\n     \
    \   # major_formatter = new._get_formatter(major_locator, **new._label_params)\n\
    \n        class CatScale(mpl.scale.LinearScale):\n            def set_default_locators_and_formatters(self,\
    \ axis):\n                ...\n                # axis.set_major_locator(major_locator)\n\
    \                # if minor_locator is not None:\n                #     axis.set_minor_locator(minor_locator)\n\
    \                # axis.set_major_formatter(major_formatter)\n\n        mpl_scale\
    \ = CatScale(data.name)\n        if axis is None:\n            axis = PseudoAxis(mpl_scale)\n\
    \n            # TODO Currently just used in non-Coordinate contexts, but should\n\
    \            # we use this to (A) set the padding we want for categorial plots\n\
    \            # and (B) allow the values parameter for a Coordinate to set xlim/ylim\n\
    \            axis.set_view_interval(0, len(units_seed) - 1)\n\n        new._matplotlib_scale\
    \ = mpl_scale\n\n        # TODO array cast necessary to handle float/int mixture,\
    \ which we need\n        # to solve in a more systematic way probably\n      \
    \  # (i.e. if we have [1, 2.5], do we want [1.0, 2.5]? Unclear)\n        axis.update_units(stringify(np.array(units_seed)))\n\
    \n        # TODO define this more centrally\n        def convert_units(x):\n \
    \           # TODO only do this with explicit order?\n            # (But also\
    \ category dtype?)\n            # TODO isin fails when units_seed mixes numbers\
    \ and strings (numpy error?)\n            # but np.isin also does not seem any\
    \ faster? (Maybe not broadcasting in C)\n            # keep = x.isin(units_seed)\n\
    \            keep = np.array([x_ in units_seed for x_ in x], bool)\n         \
    \   out = np.full(len(x), np.nan)\n            out[keep] = axis.convert_units(stringify(x[keep]))\n\
    \            return out\n\n        new._pipeline = [convert_units, prop.get_mapping(new,\
    \ data)]\n        new._spacer = _default_spacer\n\n        if prop.legend:\n \
    \           new._legend = units_seed, list(stringify(units_seed))\n\n        return\
    \ new\n\n    def _finalize(self, p: Plot, axis: Axis) -> None:\n\n        ax =\
    \ axis.axes\n        name = axis.axis_name\n        axis.grid(False, which=\"\
    both\")\n        if name not in p._limits:\n            nticks = len(axis.get_major_ticks())\n\
    \            lo, hi = -.5, nticks - .5\n            if name == \"y\":\n      \
    \          lo, hi = hi, lo\n            set_lim = getattr(ax, f\"set_{name}lim\"\
    )\n            set_lim(lo, hi, auto=None)\n\n    def tick(self, locator: Locator\
    \ | None = None) -> Nominal:\n        \"\"\"\n        Configure the selection\
    \ of ticks for the scale's axis or legend.\n\n        .. note::\n            This\
    \ API is under construction and will be enhanced over time.\n            At the\
    \ moment, it is probably not very useful.\n\n        Parameters\n        ----------\n\
    \        locator : :class:`matplotlib.ticker.Locator` subclass\n            Pre-configured\
    \ matplotlib locator; other parameters will not be used.\n\n        Returns\n\
    \        -------\n        Copy of self with new tick configuration.\n\n      \
    \  \"\"\"\n        new = copy(self)\n        new._tick_params = {\"locator\":\
    \ locator}\n        return new\n\n    def label(self, formatter: Formatter | None\
    \ = None) -> Nominal:\n        \"\"\"\n        Configure the selection of labels\
    \ for the scale's axis or legend.\n\n        .. note::\n            This API is\
    \ under construction and will be enhanced over time.\n            At the moment,\
    \ it is probably not very useful.\n\n        Parameters\n        ----------\n\
    \        formatter : :class:`matplotlib.ticker.Formatter` subclass\n         \
    \   Pre-configured matplotlib formatter; other parameters will not be used.\n\n\
    \        Returns\n        -------\n        scale\n            Copy of self with\
    \ new tick configuration.\n\n        \"\"\"\n        new = copy(self)\n      \
    \  new._label_params = {\"formatter\": formatter}\n        return new\n\n    def\
    \ _get_locators(self, locator):\n\n        if locator is not None:\n         \
    \   return locator, None\n\n        locator = mpl.category.StrCategoryLocator({})\n\
    \n        return locator, None\n\n    def _get_formatter(self, locator, formatter):\n\
    \n        if formatter is not None:\n            return formatter\n\n        formatter\
    \ = mpl.category.StrCategoryFormatter({})\n\n        return formatter\n\n\n@dataclass\n\
    class Ordinal(Scale):\n    # Categorical (convert to strings), sortable, can skip\
    \ ticklabels\n    ...\n\n\n@dataclass\nclass Discrete(Scale):\n    # Numeric,\
    \ integral, can skip ticks/ticklabels\n    ...\n\n\n@dataclass\nclass ContinuousBase(Scale):\n\
    \n    values: tuple | str | None = None\n    norm: tuple | None = None\n\n   \
    \ def _setup(\n        self, data: Series, prop: Property, axis: Axis | None =\
    \ None,\n    ) -> Scale:\n\n        new = copy(self)\n        if new._tick_params\
    \ is None:\n            new = new.tick()\n        if new._label_params is None:\n\
    \            new = new.label()\n\n        forward, inverse = new._get_transform()\n\
    \n        mpl_scale = new._get_scale(str(data.name), forward, inverse)\n\n   \
    \     if axis is None:\n            axis = PseudoAxis(mpl_scale)\n           \
    \ axis.update_units(data)\n\n        mpl_scale.set_default_locators_and_formatters(axis)\n\
    \        new._matplotlib_scale = mpl_scale\n\n        normalize: Optional[Callable[[ArrayLike],\
    \ ArrayLike]]\n        if prop.normed:\n            if new.norm is None:\n   \
    \             vmin, vmax = data.min(), data.max()\n            else:\n       \
    \         vmin, vmax = new.norm\n            vmin, vmax = map(float, axis.convert_units((vmin,\
    \ vmax)))\n            a = forward(vmin)\n            b = forward(vmax) - forward(vmin)\n\
    \n            def normalize(x):\n                return (x - a) / b\n\n      \
    \  else:\n            normalize = vmin = vmax = None\n\n        new._pipeline\
    \ = [\n            axis.convert_units,\n            forward,\n            normalize,\n\
    \            prop.get_mapping(new, data)\n        ]\n\n        def spacer(x):\n\
    \            x = x.dropna().unique()\n            if len(x) < 2:\n           \
    \     return np.nan\n            return np.min(np.diff(np.sort(x)))\n        new._spacer\
    \ = spacer\n\n        # TODO How to allow disabling of legend for all uses of\
    \ property?\n        # Could add a Scale parameter, or perhaps Scale.suppress()?\n\
    \        # Are there other useful parameters that would be in Scale.legend()\n\
    \        # besides allowing Scale.legend(False)?\n        if prop.legend:\n  \
    \          axis.set_view_interval(vmin, vmax)\n            locs = axis.major.locator()\n\
    \            locs = locs[(vmin <= locs) & (locs <= vmax)]\n            # Avoid\
    \ having an offset / scientific notation in a legend\n            # as we don't\
    \ represent that anywhere so it ends up incorrect.\n            # This could become\
    \ an option (e.g. Continuous.label(offset=True))\n            # in which case\
    \ we would need to figure out how to show it.\n            if hasattr(axis.major.formatter,\
    \ \"set_useOffset\"):\n                axis.major.formatter.set_useOffset(False)\n\
    \            if hasattr(axis.major.formatter, \"set_scientific\"):\n         \
    \       axis.major.formatter.set_scientific(False)\n            labels = axis.major.formatter.format_ticks(locs)\n\
    \            new._legend = list(locs), list(labels)\n\n        return new\n\n\
    \    def _get_transform(self):\n\n        arg = self.trans\n\n        def get_param(method,\
    \ default):\n            if arg == method:\n                return default\n \
    \           return float(arg[len(method):])\n\n        if arg is None:\n     \
    \       return _make_identity_transforms()\n        elif isinstance(arg, tuple):\n\
    \            return arg\n        elif isinstance(arg, str):\n            if arg\
    \ == \"ln\":\n                return _make_log_transforms()\n            elif\
    \ arg == \"logit\":\n                base = get_param(\"logit\", 10)\n       \
    \         return _make_logit_transforms(base)\n            elif arg.startswith(\"\
    log\"):\n                base = get_param(\"log\", 10)\n                return\
    \ _make_log_transforms(base)\n            elif arg.startswith(\"symlog\"):\n \
    \               c = get_param(\"symlog\", 1)\n                return _make_symlog_transforms(c)\n\
    \            elif arg.startswith(\"pow\"):\n                exp = get_param(\"\
    pow\", 2)\n                return _make_power_transforms(exp)\n            elif\
    \ arg == \"sqrt\":\n                return _make_sqrt_transforms()\n         \
    \   else:\n                raise ValueError(f\"Unknown value provided for trans:\
    \ {arg!r}\")\n\n\n@dataclass\nclass Continuous(ContinuousBase):\n    \"\"\"\n\
    \    A numeric scale supporting norms and functional transforms.\n    \"\"\"\n\
    \    values: tuple | str | None = None\n    trans: str | TransFuncs | None = None\n\
    \n    # TODO Add this to deal with outliers?\n    # outside: Literal[\"keep\"\
    , \"drop\", \"clip\"] = \"keep\"\n\n    _priority: ClassVar[int] = 1\n\n    def\
    \ tick(\n        self,\n        locator: Locator | None = None, *,\n        at:\
    \ Sequence[float] | None = None,\n        upto: int | None = None,\n        count:\
    \ int | None = None,\n        every: float | None = None,\n        between: tuple[float,\
    \ float] | None = None,\n        minor: int | None = None,\n    ) -> Continuous:\n\
    \        \"\"\"\n        Configure the selection of ticks for the scale's axis\
    \ or legend.\n\n        Parameters\n        ----------\n        locator : :class:`matplotlib.ticker.Locator`\
    \ subclass\n            Pre-configured matplotlib locator; other parameters will\
    \ not be used.\n        at : sequence of floats\n            Place ticks at these\
    \ specific locations (in data units).\n        upto : int\n            Choose\
    \ \"nice\" locations for ticks, but do not exceed this number.\n        count\
    \ : int\n            Choose exactly this number of ticks, bounded by `between`\
    \ or axis limits.\n        every : float\n            Choose locations at this\
    \ interval of separation (in data units).\n        between : pair of floats\n\
    \            Bound upper / lower ticks when using `every` or `count`.\n      \
    \  minor : int\n            Number of unlabeled ticks to draw between labeled\
    \ \"major\" ticks.\n\n        Returns\n        -------\n        scale\n      \
    \      Copy of self with new tick configuration.\n\n        \"\"\"\n        #\
    \ Input checks\n        if locator is not None and not isinstance(locator, Locator):\n\
    \            raise TypeError(\n                f\"Tick locator must be an instance\
    \ of {Locator!r}, \"\n                f\"not {type(locator)!r}.\"\n          \
    \  )\n        log_base, symlog_thresh = self._parse_for_log_params(self.trans)\n\
    \        if log_base or symlog_thresh:\n            if count is not None and between\
    \ is None:\n                raise RuntimeError(\"`count` requires `between` with\
    \ log transform.\")\n            if every is not None:\n                raise\
    \ RuntimeError(\"`every` not supported with log transform.\")\n\n        new =\
    \ copy(self)\n        new._tick_params = {\n            \"locator\": locator,\n\
    \            \"at\": at,\n            \"upto\": upto,\n            \"count\":\
    \ count,\n            \"every\": every,\n            \"between\": between,\n \
    \           \"minor\": minor,\n        }\n        return new\n\n    def label(\n\
    \        self,\n        formatter: Formatter | None = None, *,\n        like:\
    \ str | Callable | None = None,\n        base: int | None | Default = default,\n\
    \        unit: str | None = None,\n    ) -> Continuous:\n        \"\"\"\n    \
    \    Configure the appearance of tick labels for the scale's axis or legend.\n\
    \n        Parameters\n        ----------\n        formatter : :class:`matplotlib.ticker.Formatter`\
    \ subclass\n            Pre-configured formatter to use; other parameters will\
    \ be ignored.\n        like : str or callable\n            Either a format pattern\
    \ (e.g., `\".2f\"`), a format string with fields named\n            `x` and/or\
    \ `pos` (e.g., `\"${x:.2f}\"`), or a callable with a signature like\n        \
    \    `f(x: float, pos: int) -> str`. In the latter variants, `x` is passed as\
    \ the\n            tick value and `pos` is passed as the tick index.\n       \
    \ base : number\n            Use log formatter (with scientific notation) having\
    \ this value as the base.\n            Set to `None` to override the default formatter\
    \ with a log transform.\n        unit : str or (str, str) tuple\n            Use\
    \  SI prefixes with these units (e.g., with `unit=\"g\"`, a tick value\n     \
    \       of 5000 will appear as `5 kg`). When a tuple, the first element gives\
    \ the\n            separator between the number and unit.\n\n        Returns\n\
    \        -------\n        scale\n            Copy of self with new label configuration.\n\
    \n        \"\"\"\n        # Input checks\n        if formatter is not None and\
    \ not isinstance(formatter, Formatter):\n            raise TypeError(\n      \
    \          f\"Label formatter must be an instance of {Formatter!r}, \"\n     \
    \           f\"not {type(formatter)!r}\"\n            )\n        if like is not\
    \ None and not (isinstance(like, str) or callable(like)):\n            msg = f\"\
    `like` must be a string or callable, not {type(like).__name__}.\"\n          \
    \  raise TypeError(msg)\n\n        new = copy(self)\n        new._label_params\
    \ = {\n            \"formatter\": formatter,\n            \"like\": like,\n  \
    \          \"base\": base,\n            \"unit\": unit,\n        }\n        return\
    \ new\n\n    def _parse_for_log_params(\n        self, trans: str | TransFuncs\
    \ | None\n    ) -> tuple[float | None, float | None]:\n\n        log_base = symlog_thresh\
    \ = None\n        if isinstance(trans, str):\n            m = re.match(r\"^log(\\\
    d*)\", trans)\n            if m is not None:\n                log_base = float(m[1]\
    \ or 10)\n            m = re.match(r\"symlog(\\d*)\", trans)\n            if m\
    \ is not None:\n                symlog_thresh = float(m[1] or 1)\n        return\
    \ log_base, symlog_thresh\n\n    def _get_locators(self, locator, at, upto, count,\
    \ every, between, minor):\n\n        log_base, symlog_thresh = self._parse_for_log_params(self.trans)\n\
    \n        if locator is not None:\n            major_locator = locator\n\n   \
    \     elif upto is not None:\n            if log_base:\n                major_locator\
    \ = LogLocator(base=log_base, numticks=upto)\n            else:\n            \
    \    major_locator = MaxNLocator(upto, steps=[1, 1.5, 2, 2.5, 3, 5, 10])\n\n \
    \       elif count is not None:\n            if between is None:\n           \
    \     # This is rarely useful (unless you are setting limits)\n              \
    \  major_locator = LinearLocator(count)\n            else:\n                if\
    \ log_base or symlog_thresh:\n                    forward, inverse = self._get_transform()\n\
    \                    lo, hi = forward(between)\n                    ticks = inverse(np.linspace(lo,\
    \ hi, num=count))\n                else:\n                    ticks = np.linspace(*between,\
    \ num=count)\n                major_locator = FixedLocator(ticks)\n\n        elif\
    \ every is not None:\n            if between is None:\n                major_locator\
    \ = MultipleLocator(every)\n            else:\n                lo, hi = between\n\
    \                ticks = np.arange(lo, hi + every, every)\n                major_locator\
    \ = FixedLocator(ticks)\n\n        elif at is not None:\n            major_locator\
    \ = FixedLocator(at)\n\n        else:\n            if log_base:\n            \
    \    major_locator = LogLocator(log_base)\n            elif symlog_thresh:\n \
    \               major_locator = SymmetricalLogLocator(linthresh=symlog_thresh,\
    \ base=10)\n            else:\n                major_locator = AutoLocator()\n\
    \n        if minor is None:\n            minor_locator = LogLocator(log_base,\
    \ subs=None) if log_base else None\n        else:\n            if log_base:\n\
    \                subs = np.linspace(0, log_base, minor + 2)[1:-1]\n          \
    \      minor_locator = LogLocator(log_base, subs=subs)\n            else:\n  \
    \              minor_locator = AutoMinorLocator(minor + 1)\n\n        return major_locator,\
    \ minor_locator\n\n    def _get_formatter(self, locator, formatter, like, base,\
    \ unit):\n\n        log_base, symlog_thresh = self._parse_for_log_params(self.trans)\n\
    \        if base is default:\n            if symlog_thresh:\n                log_base\
    \ = 10\n            base = log_base\n\n        if formatter is not None:\n   \
    \         return formatter\n\n        if like is not None:\n            if isinstance(like,\
    \ str):\n                if \"{x\" in like or \"{pos\" in like:\n            \
    \        fmt = like\n                else:\n                    fmt = f\"{{x:{like}}}\"\
    \n                formatter = StrMethodFormatter(fmt)\n            else:\n   \
    \             formatter = FuncFormatter(like)\n\n        elif base is not None:\n\
    \            # We could add other log options if necessary\n            formatter\
    \ = LogFormatterSciNotation(base)\n\n        elif unit is not None:\n        \
    \    if isinstance(unit, tuple):\n                sep, unit = unit\n         \
    \   elif not unit:\n                sep = \"\"\n            else:\n          \
    \      sep = \" \"\n            formatter = EngFormatter(unit, sep=sep)\n\n  \
    \      else:\n            formatter = ScalarFormatter()\n\n        return formatter\n\
    \n\n@dataclass\nclass Temporal(ContinuousBase):\n    \"\"\"\n    A scale for date/time\
    \ data.\n    \"\"\"\n    # TODO date: bool?\n    # For when we only care about\
    \ the time component, would affect\n    # default formatter and norm conversion.\
    \ Should also happen in\n    # Property.default_scale. The alternative was having\
    \ distinct\n    # Calendric / Temporal scales, but that feels a bit fussy, and\
    \ it\n    # would get in the way of using first-letter shorthands because\n  \
    \  # Calendric and Continuous would collide. Still, we haven't implemented\n \
    \   # those yet, and having a clear distinction betewen date(time) / time\n  \
    \  # may be more useful.\n\n    trans = None\n\n    _priority: ClassVar[int] =\
    \ 2\n\n    def tick(\n        self, locator: Locator | None = None, *,\n     \
    \   upto: int | None = None,\n    ) -> Temporal:\n        \"\"\"\n        Configure\
    \ the selection of ticks for the scale's axis or legend.\n\n        .. note::\n\
    \            This API is under construction and will be enhanced over time.\n\n\
    \        Parameters\n        ----------\n        locator : :class:`matplotlib.ticker.Locator`\
    \ subclass\n            Pre-configured matplotlib locator; other parameters will\
    \ not be used.\n        upto : int\n            Choose \"nice\" locations for\
    \ ticks, but do not exceed this number.\n\n        Returns\n        -------\n\
    \        scale\n            Copy of self with new tick configuration.\n\n    \
    \    \"\"\"\n        if locator is not None and not isinstance(locator, Locator):\n\
    \            err = (\n                f\"Tick locator must be an instance of {Locator!r},\
    \ \"\n                f\"not {type(locator)!r}.\"\n            )\n           \
    \ raise TypeError(err)\n\n        new = copy(self)\n        new._tick_params =\
    \ {\"locator\": locator, \"upto\": upto}\n        return new\n\n    def label(\n\
    \        self,\n        formatter: Formatter | None = None, *,\n        concise:\
    \ bool = False,\n    ) -> Temporal:\n        \"\"\"\n        Configure the appearance\
    \ of tick labels for the scale's axis or legend.\n\n        .. note::\n      \
    \      This API is under construction and will be enhanced over time.\n\n    \
    \    Parameters\n        ----------\n        formatter : :class:`matplotlib.ticker.Formatter`\
    \ subclass\n            Pre-configured formatter to use; other parameters will\
    \ be ignored.\n        concise : bool\n            If True, use :class:`matplotlib.dates.ConciseDateFormatter`\
    \ to make\n            the tick labels as compact as possible.\n\n        Returns\n\
    \        -------\n        scale\n            Copy of self with new label configuration.\n\
    \n        \"\"\"\n        new = copy(self)\n        new._label_params = {\"formatter\"\
    : formatter, \"concise\": concise}\n        return new\n\n    def _get_locators(self,\
    \ locator, upto):\n\n        if locator is not None:\n            major_locator\
    \ = locator\n        elif upto is not None:\n            major_locator = AutoDateLocator(minticks=2,\
    \ maxticks=upto)\n\n        else:\n            major_locator = AutoDateLocator(minticks=2,\
    \ maxticks=6)\n        minor_locator = None\n\n        return major_locator, minor_locator\n\
    \n    def _get_formatter(self, locator, formatter, concise):\n\n        if formatter\
    \ is not None:\n            return formatter\n\n        if concise:\n        \
    \    # TODO ideally we would have concise coordinate ticks,\n            # but\
    \ full semantic ticks. Is that possible?\n            formatter = ConciseDateFormatter(locator)\n\
    \        else:\n            formatter = AutoDateFormatter(locator)\n\n       \
    \ return formatter\n\n\n# -----------------------------------------------------------------------------------\
    \ #\n\n\n# TODO Have this separate from Temporal or have Temporal(date=True) or\
    \ similar?\n# class Calendric(Scale):\n\n# TODO Needed? Or handle this at layer\
    \ (in stat or as param, eg binning=)\n# class Binned(Scale):\n\n# TODO any need\
    \ for color-specific scales?\n# class Sequential(Continuous):\n# class Diverging(Continuous):\n\
    # class Qualitative(Nominal):\n\n\n# -----------------------------------------------------------------------------------\
    \ #\n\n\nclass PseudoAxis:\n    \"\"\"\n    Internal class implementing minimal\
    \ interface equivalent to matplotlib Axis.\n\n    Coordinate variables are typically\
    \ scaled by attaching the Axis object from\n    the figure where the plot will\
    \ end up. Matplotlib has no similar concept of\n    and axis for the other mappable\
    \ variables (color, etc.), but to simplify the\n    code, this object acts like\
    \ an Axis and can be used to scale other variables.\n\n    \"\"\"\n    axis_name\
    \ = \"\"  # Matplotlib requirement but not actually used\n\n    def __init__(self,\
    \ scale):\n\n        self.converter = None\n        self.units = None\n      \
    \  self.scale = scale\n        self.major = mpl.axis.Ticker()\n        self.minor\
    \ = mpl.axis.Ticker()\n\n        # It appears that this needs to be initialized\
    \ this way on matplotlib 3.1,\n        # but not later versions. It is unclear\
    \ whether there are any issues with it.\n        self._data_interval = None, None\n\
    \n        scale.set_default_locators_and_formatters(self)\n        # self.set_default_intervals()\
    \  Is this ever needed?\n\n    def set_view_interval(self, vmin, vmax):\n    \
    \    self._view_interval = vmin, vmax\n\n    def get_view_interval(self):\n  \
    \      return self._view_interval\n\n    # TODO do we want to distinguish view/data\
    \ intervals? e.g. for a legend\n    # we probably want to represent the full range\
    \ of the data values, but\n    # still norm the colormap. If so, we'll need to\
    \ track data range separately\n    # from the norm, which we currently don't do.\n\
    \n    def set_data_interval(self, vmin, vmax):\n        self._data_interval =\
    \ vmin, vmax\n\n    def get_data_interval(self):\n        return self._data_interval\n\
    \n    def get_tick_space(self):\n        # TODO how to do this in a configurable\
    \ / auto way?\n        # Would be cool to have legend density adapt to figure\
    \ size, etc.\n        return 5\n\n    def set_major_locator(self, locator):\n\
    \        self.major.locator = locator\n        locator.set_axis(self)\n\n    def\
    \ set_major_formatter(self, formatter):\n        self.major.formatter = formatter\n\
    \        formatter.set_axis(self)\n\n    def set_minor_locator(self, locator):\n\
    \        self.minor.locator = locator\n        locator.set_axis(self)\n\n    def\
    \ set_minor_formatter(self, formatter):\n        self.minor.formatter = formatter\n\
    \        formatter.set_axis(self)\n\n    def set_units(self, units):\n       \
    \ self.units = units\n\n    def update_units(self, x):\n        \"\"\"Pass units\
    \ to the internal converter, potentially updating its mapping.\"\"\"\n       \
    \ self.converter = mpl.units.registry.get_converter(x)\n        if self.converter\
    \ is not None:\n            self.converter.default_units(x, self)\n\n        \
    \    info = self.converter.axisinfo(self.units, self)\n\n            if info is\
    \ None:\n                return\n            if info.majloc is not None:\n   \
    \             self.set_major_locator(info.majloc)\n            if info.majfmt\
    \ is not None:\n                self.set_major_formatter(info.majfmt)\n\n    \
    \        # This is in matplotlib method; do we need this?\n            # self.set_default_intervals()\n\
    \n    def convert_units(self, x):\n        \"\"\"Return a numeric representation\
    \ of the input data.\"\"\"\n        if np.issubdtype(np.asarray(x).dtype, np.number):\n\
    \            return x\n        elif self.converter is None:\n            return\
    \ x\n        return self.converter.convert(x, self.units, self)\n\n    def get_scale(self):\n\
    \        # Note that matplotlib actually returns a string here!\n        # (e.g.,\
    \ with a log scale, axis.get_scale() returns \"log\")\n        # Currently we\
    \ just hit it with minor ticks where it checks for\n        # scale == \"log\"\
    . I'm not sure how you'd actually use log-scale\n        # minor \"ticks\" in\
    \ a legend context, so this is fine....\n        return self.scale\n\n    def\
    \ get_majorticklocs(self):\n        return self.major.locator()\n\n\n# ------------------------------------------------------------------------------------\
    \ #\n# Transform function creation\n\n\ndef _make_identity_transforms() -> TransFuncs:\n\
    \n    def identity(x):\n        return x\n\n    return identity, identity\n\n\n\
    def _make_logit_transforms(base: float | None = None) -> TransFuncs:\n\n    log,\
    \ exp = _make_log_transforms(base)\n\n    def logit(x):\n        with np.errstate(invalid=\"\
    ignore\", divide=\"ignore\"):\n            return log(x) - log(1 - x)\n\n    def\
    \ expit(x):\n        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n\
    \            return exp(x) / (1 + exp(x))\n\n    return logit, expit\n\n\ndef\
    \ _make_log_transforms(base: float | None = None) -> TransFuncs:\n\n    fs: TransFuncs\n\
    \    if base is None:\n        fs = np.log, np.exp\n    elif base == 2:\n    \
    \    fs = np.log2, partial(np.power, 2)\n    elif base == 10:\n        fs = np.log10,\
    \ partial(np.power, 10)\n    else:\n        def forward(x):\n            return\
    \ np.log(x) / np.log(base)\n        fs = forward, partial(np.power, base)\n\n\
    \    def log(x: ArrayLike) -> ArrayLike:\n        with np.errstate(invalid=\"\
    ignore\", divide=\"ignore\"):\n            return fs[0](x)\n\n    def exp(x: ArrayLike)\
    \ -> ArrayLike:\n        with np.errstate(invalid=\"ignore\", divide=\"ignore\"\
    ):\n            return fs[1](x)\n\n    return log, exp\n\n\ndef _make_symlog_transforms(c:\
    \ float = 1, base: float = 10) -> TransFuncs:\n\n    # From https://iopscience.iop.org/article/10.1088/0957-0233/24/2/027001\n\
    \n    # Note: currently not using base because we only get\n    # one parameter\
    \ from the string, and are using c (this is consistent with d3)\n\n    log, exp\
    \ = _make_log_transforms(base)\n\n    def symlog(x):\n        with np.errstate(invalid=\"\
    ignore\", divide=\"ignore\"):\n            return np.sign(x) * log(1 + np.abs(np.divide(x,\
    \ c)))\n\n    def symexp(x):\n        with np.errstate(invalid=\"ignore\", divide=\"\
    ignore\"):\n            return np.sign(x) * c * (exp(np.abs(x)) - 1)\n\n    return\
    \ symlog, symexp\n\n\ndef _make_sqrt_transforms() -> TransFuncs:\n\n    def sqrt(x):\n\
    \        return np.sign(x) * np.sqrt(np.abs(x))\n\n    def square(x):\n      \
    \  return np.sign(x) * np.square(x)\n\n    return sqrt, square\n\n\ndef _make_power_transforms(exp:\
    \ float) -> TransFuncs:\n\n    def forward(x):\n        return np.sign(x) * np.power(np.abs(x),\
    \ exp)\n\n    def inverse(x):\n        return np.sign(x) * np.power(np.abs(x),\
    \ 1 / exp)\n\n    return forward, inverse\n\n\ndef _default_spacer(x: Series)\
    \ -> float:\n    return 1\n\n\n### Dependency File: typing.py\nfrom __future__\
    \ import annotations\n\nfrom collections.abc import Iterable, Mapping\nfrom datetime\
    \ import date, datetime, timedelta\nfrom typing import Any, Optional, Union, Tuple,\
    \ List, Dict\n\nfrom numpy import ndarray  # TODO use ArrayLike?\nfrom pandas\
    \ import Series, Index, Timestamp, Timedelta\nfrom matplotlib.colors import Colormap,\
    \ Normalize\n\n\nColumnName = Union[\n    str, bytes, date, datetime, timedelta,\
    \ bool, complex, Timestamp, Timedelta\n]\nVector = Union[Series, Index, ndarray]\n\
    \nVariableSpec = Union[ColumnName, Vector, None]\nVariableSpecList = Union[List[VariableSpec],\
    \ Index, None]\n\n# A DataSource can be an object implementing __dataframe__,\
    \ or a Mapping\n# (and is optional in all contexts where it is used).\n# I don't\
    \ think there's an abc for \"has __dataframe__\", so we type as object\n# but\
    \ keep the (slightly odd) Union alias for better user-facing annotations.\nDataSource\
    \ = Union[object, Mapping, None]\n\nOrderSpec = Union[Iterable, None]  # TODO\
    \ technically str is iterable\nNormSpec = Union[Tuple[Optional[float], Optional[float]],\
    \ Normalize, None]\n\n# TODO for discrete mappings, it would be ideal to use a\
    \ parameterized type\n# as the dict values / list entries should be of specific\
    \ type(s) for each method\nPaletteSpec = Union[str, list, dict, Colormap, None]\n\
    DiscreteValueSpec = Union[dict, list, None]\nContinuousValueSpec = Union[\n  \
    \  Tuple[float, float], List[float], Dict[Any, float], None,\n]\n\n\nclass Default:\n\
    \    def __repr__(self):\n        return \"<default>\"\n\n\nclass Deprecated:\n\
    \    def __repr__(self):\n        return \"<deprecated>\"\n\n\ndefault = Default()\n\
    deprecated = Deprecated()\n\n\n### Dependency File: _statistics.py\n\"\"\"Statistical\
    \ transformations for visualization.\n\nThis module is currently private, but\
    \ is being written to eventually form part\nof the public API.\n\nThe classes\
    \ should behave roughly in the style of scikit-learn.\n\n- All data-independent\
    \ parameters should be passed to the class constructor.\n- Each class should implement\
    \ a default transformation that is exposed through\n  __call__. These are currently\
    \ written for vector arguments, but I think\n  consuming a whole `plot_data` DataFrame\
    \ and return it with transformed\n  variables would make more sense.\n- Some class\
    \ have data-dependent preprocessing that should be cached and used\n  multiple\
    \ times (think defining histogram bins off all data and then counting\n  observations\
    \ within each bin multiple times per data subsets). These currently\n  have unique\
    \ names, but it would be good to have a common name. Not quite\n  `fit`, but something\
    \ similar.\n- Alternatively, the transform interface could take some information\
    \ about grouping\n  variables and do a groupby internally.\n- Some classes should\
    \ define alternate transforms that might make the most sense\n  with a different\
    \ function. For example, KDE usually evaluates the distribution\n  on a regular\
    \ grid, but it would be useful for it to transform at the actual\n  datapoints.\
    \ Then again, this could be controlled by a parameter at  the time of\n  class\
    \ instantiation.\n\n\"\"\"\nfrom numbers import Number\nfrom statistics import\
    \ NormalDist\nimport numpy as np\nimport pandas as pd\ntry:\n    from scipy.stats\
    \ import gaussian_kde\n    _no_scipy = False\nexcept ImportError:\n    from .external.kde\
    \ import gaussian_kde\n    _no_scipy = True\n\nfrom .algorithms import bootstrap\n\
    from .utils import _check_argument\n\n\nclass KDE:\n    \"\"\"Univariate and bivariate\
    \ kernel density estimator.\"\"\"\n    def __init__(\n        self, *,\n     \
    \   bw_method=None,\n        bw_adjust=1,\n        gridsize=200,\n        cut=3,\n\
    \        clip=None,\n        cumulative=False,\n    ):\n        \"\"\"Initialize\
    \ the estimator with its parameters.\n\n        Parameters\n        ----------\n\
    \        bw_method : string, scalar, or callable, optional\n            Method\
    \ for determining the smoothing bandwidth to use; passed to\n            :class:`scipy.stats.gaussian_kde`.\n\
    \        bw_adjust : number, optional\n            Factor that multiplicatively\
    \ scales the value chosen using\n            ``bw_method``. Increasing will make\
    \ the curve smoother. See Notes.\n        gridsize : int, optional\n         \
    \   Number of points on each dimension of the evaluation grid.\n        cut :\
    \ number, optional\n            Factor, multiplied by the smoothing bandwidth,\
    \ that determines how\n            far the evaluation grid extends past the extreme\
    \ datapoints. When\n            set to 0, truncate the curve at the data limits.\n\
    \        clip : pair of numbers or None, or a pair of such pairs\n           \
    \ Do not evaluate the density outside of these limits.\n        cumulative : bool,\
    \ optional\n            If True, estimate a cumulative distribution function.\
    \ Requires scipy.\n\n        \"\"\"\n        if clip is None:\n            clip\
    \ = None, None\n\n        self.bw_method = bw_method\n        self.bw_adjust =\
    \ bw_adjust\n        self.gridsize = gridsize\n        self.cut = cut\n      \
    \  self.clip = clip\n        self.cumulative = cumulative\n\n        if cumulative\
    \ and _no_scipy:\n            raise RuntimeError(\"Cumulative KDE evaluation requires\
    \ scipy\")\n\n        self.support = None\n\n    def _define_support_grid(self,\
    \ x, bw, cut, clip, gridsize):\n        \"\"\"Create the grid of evaluation points\
    \ depending for vector x.\"\"\"\n        clip_lo = -np.inf if clip[0] is None\
    \ else clip[0]\n        clip_hi = +np.inf if clip[1] is None else clip[1]\n  \
    \      gridmin = max(x.min() - bw * cut, clip_lo)\n        gridmax = min(x.max()\
    \ + bw * cut, clip_hi)\n        return np.linspace(gridmin, gridmax, gridsize)\n\
    \n    def _define_support_univariate(self, x, weights):\n        \"\"\"Create\
    \ a 1D grid of evaluation points.\"\"\"\n        kde = self._fit(x, weights)\n\
    \        bw = np.sqrt(kde.covariance.squeeze())\n        grid = self._define_support_grid(\n\
    \            x, bw, self.cut, self.clip, self.gridsize\n        )\n        return\
    \ grid\n\n    def _define_support_bivariate(self, x1, x2, weights):\n        \"\
    \"\"Create a 2D grid of evaluation points.\"\"\"\n        clip = self.clip\n \
    \       if clip[0] is None or np.isscalar(clip[0]):\n            clip = (clip,\
    \ clip)\n\n        kde = self._fit([x1, x2], weights)\n        bw = np.sqrt(np.diag(kde.covariance).squeeze())\n\
    \n        grid1 = self._define_support_grid(\n            x1, bw[0], self.cut,\
    \ clip[0], self.gridsize\n        )\n        grid2 = self._define_support_grid(\n\
    \            x2, bw[1], self.cut, clip[1], self.gridsize\n        )\n\n      \
    \  return grid1, grid2\n\n    def define_support(self, x1, x2=None, weights=None,\
    \ cache=True):\n        \"\"\"Create the evaluation grid for a given data set.\"\
    \"\"\n        if x2 is None:\n            support = self._define_support_univariate(x1,\
    \ weights)\n        else:\n            support = self._define_support_bivariate(x1,\
    \ x2, weights)\n\n        if cache:\n            self.support = support\n\n  \
    \      return support\n\n    def _fit(self, fit_data, weights=None):\n       \
    \ \"\"\"Fit the scipy kde while adding bw_adjust logic and version check.\"\"\"\
    \n        fit_kws = {\"bw_method\": self.bw_method}\n        if weights is not\
    \ None:\n            fit_kws[\"weights\"] = weights\n\n        kde = gaussian_kde(fit_data,\
    \ **fit_kws)\n        kde.set_bandwidth(kde.factor * self.bw_adjust)\n\n     \
    \   return kde\n\n    def _eval_univariate(self, x, weights=None):\n        \"\
    \"\"Fit and evaluate a univariate on univariate data.\"\"\"\n        support =\
    \ self.support\n        if support is None:\n            support = self.define_support(x,\
    \ cache=False)\n\n        kde = self._fit(x, weights)\n\n        if self.cumulative:\n\
    \            s_0 = support[0]\n            density = np.array([\n            \
    \    kde.integrate_box_1d(s_0, s_i) for s_i in support\n            ])\n     \
    \   else:\n            density = kde(support)\n\n        return density, support\n\
    \n    def _eval_bivariate(self, x1, x2, weights=None):\n        \"\"\"Fit and\
    \ evaluate a univariate on bivariate data.\"\"\"\n        support = self.support\n\
    \        if support is None:\n            support = self.define_support(x1, x2,\
    \ cache=False)\n\n        kde = self._fit([x1, x2], weights)\n\n        if self.cumulative:\n\
    \n            grid1, grid2 = support\n            density = np.zeros((grid1.size,\
    \ grid2.size))\n            p0 = grid1.min(), grid2.min()\n            for i,\
    \ xi in enumerate(grid1):\n                for j, xj in enumerate(grid2):\n  \
    \                  density[i, j] = kde.integrate_box(p0, (xi, xj))\n\n       \
    \ else:\n\n            xx1, xx2 = np.meshgrid(*support)\n            density =\
    \ kde([xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\n\n        return density,\
    \ support\n\n    def __call__(self, x1, x2=None, weights=None):\n        \"\"\"\
    Fit and evaluate on univariate or bivariate data.\"\"\"\n        if x2 is None:\n\
    \            return self._eval_univariate(x1, weights)\n        else:\n      \
    \      return self._eval_bivariate(x1, x2, weights)\n\n\n# Note: we no longer\
    \ use this for univariate histograms in histplot,\n# preferring _stats.Hist. We'll\
    \ deprecate this once we have a bivariate Stat class.\nclass Histogram:\n    \"\
    \"\"Univariate and bivariate histogram estimator.\"\"\"\n    def __init__(\n \
    \       self,\n        stat=\"count\",\n        bins=\"auto\",\n        binwidth=None,\n\
    \        binrange=None,\n        discrete=False,\n        cumulative=False,\n\
    \    ):\n        \"\"\"Initialize the estimator with its parameters.\n\n     \
    \   Parameters\n        ----------\n        stat : str\n            Aggregate\
    \ statistic to compute in each bin.\n\n            - `count`: show the number\
    \ of observations in each bin\n            - `frequency`: show the number of observations\
    \ divided by the bin width\n            - `probability` or `proportion`: normalize\
    \ such that bar heights sum to 1\n            - `percent`: normalize such that\
    \ bar heights sum to 100\n            - `density`: normalize such that the total\
    \ area of the histogram equals 1\n\n        bins : str, number, vector, or a pair\
    \ of such values\n            Generic bin parameter that can be the name of a\
    \ reference rule,\n            the number of bins, or the breaks of the bins.\n\
    \            Passed to :func:`numpy.histogram_bin_edges`.\n        binwidth :\
    \ number or pair of numbers\n            Width of each bin, overrides ``bins``\
    \ but can be used with\n            ``binrange``.\n        binrange : pair of\
    \ numbers or a pair of pairs\n            Lowest and highest value for bin edges;\
    \ can be used either\n            with ``bins`` or ``binwidth``. Defaults to data\
    \ extremes.\n        discrete : bool or pair of bools\n            If True, set\
    \ ``binwidth`` and ``binrange`` such that bin\n            edges cover integer\
    \ values in the dataset.\n        cumulative : bool\n            If True, return\
    \ the cumulative statistic.\n\n        \"\"\"\n        stat_choices = [\n    \
    \        \"count\", \"frequency\", \"density\", \"probability\", \"proportion\"\
    , \"percent\",\n        ]\n        _check_argument(\"stat\", stat_choices, stat)\n\
    \n        self.stat = stat\n        self.bins = bins\n        self.binwidth =\
    \ binwidth\n        self.binrange = binrange\n        self.discrete = discrete\n\
    \        self.cumulative = cumulative\n\n        self.bin_kws = None\n\n    def\
    \ _define_bin_edges(self, x, weights, bins, binwidth, binrange, discrete):\n \
    \       \"\"\"Inner function that takes bin parameters as arguments.\"\"\"\n \
    \       if binrange is None:\n            start, stop = x.min(), x.max()\n   \
    \     else:\n            start, stop = binrange\n\n        if discrete:\n    \
    \        bin_edges = np.arange(start - .5, stop + 1.5)\n        elif binwidth\
    \ is not None:\n            step = binwidth\n            bin_edges = np.arange(start,\
    \ stop + step, step)\n            # Handle roundoff error (maybe there is a less\
    \ clumsy way?)\n            if bin_edges.max() < stop or len(bin_edges) < 2:\n\
    \                bin_edges = np.append(bin_edges, bin_edges.max() + step)\n  \
    \      else:\n            bin_edges = np.histogram_bin_edges(\n              \
    \  x, bins, binrange, weights,\n            )\n        return bin_edges\n\n  \
    \  def define_bin_params(self, x1, x2=None, weights=None, cache=True):\n     \
    \   \"\"\"Given data, return numpy.histogram parameters to define bins.\"\"\"\n\
    \        if x2 is None:\n\n            bin_edges = self._define_bin_edges(\n \
    \               x1, weights, self.bins, self.binwidth, self.binrange, self.discrete,\n\
    \            )\n\n            if isinstance(self.bins, (str, Number)):\n     \
    \           n_bins = len(bin_edges) - 1\n                bin_range = bin_edges.min(),\
    \ bin_edges.max()\n                bin_kws = dict(bins=n_bins, range=bin_range)\n\
    \            else:\n                bin_kws = dict(bins=bin_edges)\n\n       \
    \ else:\n\n            bin_edges = []\n            for i, x in enumerate([x1,\
    \ x2]):\n\n                # Resolve out whether bin parameters are shared\n \
    \               # or specific to each variable\n\n                bins = self.bins\n\
    \                if not bins or isinstance(bins, (str, Number)):\n           \
    \         pass\n                elif isinstance(bins[i], str):\n             \
    \       bins = bins[i]\n                elif len(bins) == 2:\n               \
    \     bins = bins[i]\n\n                binwidth = self.binwidth\n           \
    \     if binwidth is None:\n                    pass\n                elif not\
    \ isinstance(binwidth, Number):\n                    binwidth = binwidth[i]\n\n\
    \                binrange = self.binrange\n                if binrange is None:\n\
    \                    pass\n                elif not isinstance(binrange[0], Number):\n\
    \                    binrange = binrange[i]\n\n                discrete = self.discrete\n\
    \                if not isinstance(discrete, bool):\n                    discrete\
    \ = discrete[i]\n\n                # Define the bins for this variable\n\n   \
    \             bin_edges.append(self._define_bin_edges(\n                    x,\
    \ weights, bins, binwidth, binrange, discrete,\n                ))\n\n       \
    \     bin_kws = dict(bins=tuple(bin_edges))\n\n        if cache:\n           \
    \ self.bin_kws = bin_kws\n\n        return bin_kws\n\n    def _eval_bivariate(self,\
    \ x1, x2, weights):\n        \"\"\"Inner function for histogram of two variables.\"\
    \"\"\n        bin_kws = self.bin_kws\n        if bin_kws is None:\n          \
    \  bin_kws = self.define_bin_params(x1, x2, cache=False)\n\n        density =\
    \ self.stat == \"density\"\n\n        hist, *bin_edges = np.histogram2d(\n   \
    \         x1, x2, **bin_kws, weights=weights, density=density\n        )\n\n \
    \       area = np.outer(\n            np.diff(bin_edges[0]),\n            np.diff(bin_edges[1]),\n\
    \        )\n\n        if self.stat == \"probability\" or self.stat == \"proportion\"\
    :\n            hist = hist.astype(float) / hist.sum()\n        elif self.stat\
    \ == \"percent\":\n            hist = hist.astype(float) / hist.sum() * 100\n\
    \        elif self.stat == \"frequency\":\n            hist = hist.astype(float)\
    \ / area\n\n        if self.cumulative:\n            if self.stat in [\"density\"\
    , \"frequency\"]:\n                hist = (hist * area).cumsum(axis=0).cumsum(axis=1)\n\
    \            else:\n                hist = hist.cumsum(axis=0).cumsum(axis=1)\n\
    \n        return hist, bin_edges\n\n    def _eval_univariate(self, x, weights):\n\
    \        \"\"\"Inner function for histogram of one variable.\"\"\"\n        bin_kws\
    \ = self.bin_kws\n        if bin_kws is None:\n            bin_kws = self.define_bin_params(x,\
    \ weights=weights, cache=False)\n\n        density = self.stat == \"density\"\n\
    \        hist, bin_edges = np.histogram(\n            x, **bin_kws, weights=weights,\
    \ density=density,\n        )\n\n        if self.stat == \"probability\" or self.stat\
    \ == \"proportion\":\n            hist = hist.astype(float) / hist.sum()\n   \
    \     elif self.stat == \"percent\":\n            hist = hist.astype(float) /\
    \ hist.sum() * 100\n        elif self.stat == \"frequency\":\n            hist\
    \ = hist.astype(float) / np.diff(bin_edges)\n\n        if self.cumulative:\n \
    \           if self.stat in [\"density\", \"frequency\"]:\n                hist\
    \ = (hist * np.diff(bin_edges)).cumsum()\n            else:\n                hist\
    \ = hist.cumsum()\n\n        return hist, bin_edges\n\n    def __call__(self,\
    \ x1, x2=None, weights=None):\n        \"\"\"Count the occurrences in each bin,\
    \ maybe normalize.\"\"\"\n        if x2 is None:\n            return self._eval_univariate(x1,\
    \ weights)\n        else:\n            return self._eval_bivariate(x1, x2, weights)\n\
    \n\nclass ECDF:\n    \"\"\"Univariate empirical cumulative distribution estimator.\"\
    \"\"\n    def __init__(self, stat=\"proportion\", complementary=False):\n    \
    \    \"\"\"Initialize the class with its parameters\n\n        Parameters\n  \
    \      ----------\n        stat : {{\"proportion\", \"percent\", \"count\"}}\n\
    \            Distribution statistic to compute.\n        complementary : bool\n\
    \            If True, use the complementary CDF (1 - CDF)\n\n        \"\"\"\n\
    \        _check_argument(\"stat\", [\"count\", \"percent\", \"proportion\"], stat)\n\
    \        self.stat = stat\n        self.complementary = complementary\n\n    def\
    \ _eval_bivariate(self, x1, x2, weights):\n        \"\"\"Inner function for ECDF\
    \ of two variables.\"\"\"\n        raise NotImplementedError(\"Bivariate ECDF\
    \ is not implemented\")\n\n    def _eval_univariate(self, x, weights):\n     \
    \   \"\"\"Inner function for ECDF of one variable.\"\"\"\n        sorter = x.argsort()\n\
    \        x = x[sorter]\n        weights = weights[sorter]\n        y = weights.cumsum()\n\
    \n        if self.stat in [\"percent\", \"proportion\"]:\n            y = y /\
    \ y.max()\n        if self.stat == \"percent\":\n            y = y * 100\n\n \
    \       x = np.r_[-np.inf, x]\n        y = np.r_[0, y]\n\n        if self.complementary:\n\
    \            y = y.max() - y\n\n        return y, x\n\n    def __call__(self,\
    \ x1, x2=None, weights=None):\n        \"\"\"Return proportion or count of observations\
    \ below each sorted datapoint.\"\"\"\n        x1 = np.asarray(x1)\n        if\
    \ weights is None:\n            weights = np.ones_like(x1)\n        else:\n  \
    \          weights = np.asarray(weights)\n\n        if x2 is None:\n         \
    \   return self._eval_univariate(x1, weights)\n        else:\n            return\
    \ self._eval_bivariate(x1, x2, weights)\n\n\nclass EstimateAggregator:\n\n   \
    \ def __init__(self, estimator, errorbar=None, **boot_kws):\n        \"\"\"\n\
    \        Data aggregator that produces an estimate and error bar interval.\n\n\
    \        Parameters\n        ----------\n        estimator : callable or string\n\
    \            Function (or method name) that maps a vector to a scalar.\n     \
    \   errorbar : string, (string, number) tuple, or callable\n            Name of\
    \ errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n   \
    \         with a method name and a level parameter, or a function that maps from\
    \ a\n            vector to a (min, max) interval, or None to hide errorbar. See\
    \ the\n            :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\n\
    \        boot_kws\n            Additional keywords are passed to bootstrap when\
    \ error_method is \"ci\".\n\n        \"\"\"\n        self.estimator = estimator\n\
    \n        method, level = _validate_errorbar_arg(errorbar)\n        self.error_method\
    \ = method\n        self.error_level = level\n\n        self.boot_kws = boot_kws\n\
    \n    def __call__(self, data, var):\n        \"\"\"Aggregate over `var` column\
    \ of `data` with estimate and error interval.\"\"\"\n        vals = data[var]\n\
    \        if callable(self.estimator):\n            # You would think we could\
    \ pass to vals.agg, and yet:\n            # https://github.com/mwaskom/seaborn/issues/2943\n\
    \            estimate = self.estimator(vals)\n        else:\n            estimate\
    \ = vals.agg(self.estimator)\n\n        # Options that produce no error bars\n\
    \        if self.error_method is None:\n            err_min = err_max = np.nan\n\
    \        elif len(data) <= 1:\n            err_min = err_max = np.nan\n\n    \
    \    # Generic errorbars from user-supplied function\n        elif callable(self.error_method):\n\
    \            err_min, err_max = self.error_method(vals)\n\n        # Parametric\
    \ options\n        elif self.error_method == \"sd\":\n            half_interval\
    \ = vals.std() * self.error_level\n            err_min, err_max = estimate - half_interval,\
    \ estimate + half_interval\n        elif self.error_method == \"se\":\n      \
    \      half_interval = vals.sem() * self.error_level\n            err_min, err_max\
    \ = estimate - half_interval, estimate + half_interval\n\n        # Nonparametric\
    \ options\n        elif self.error_method == \"pi\":\n            err_min, err_max\
    \ = _percentile_interval(vals, self.error_level)\n        elif self.error_method\
    \ == \"ci\":\n            units = data.get(\"units\", None)\n            boots\
    \ = bootstrap(vals, units=units, func=self.estimator, **self.boot_kws)\n     \
    \       err_min, err_max = _percentile_interval(boots, self.error_level)\n\n \
    \       return pd.Series({var: estimate, f\"{var}min\": err_min, f\"{var}max\"\
    : err_max})\n\n\nclass WeightedAggregator:\n\n    def __init__(self, estimator,\
    \ errorbar=None, **boot_kws):\n        \"\"\"\n        Data aggregator that produces\
    \ a weighted estimate and error bar interval.\n\n        Parameters\n        ----------\n\
    \        estimator : string\n            Function (or method name) that maps a\
    \ vector to a scalar. Currently\n            supports only \"mean\".\n       \
    \ errorbar : string or (string, number) tuple\n            Name of errorbar method\
    \ or a tuple with a method name and a level parameter.\n            Currently\
    \ the only supported method is \"ci\".\n        boot_kws\n            Additional\
    \ keywords are passed to bootstrap when error_method is \"ci\".\n\n        \"\"\
    \"\n        if estimator != \"mean\":\n            # Note that, while other weighted\
    \ estimators may make sense (e.g. median),\n            # I'm not aware of an\
    \ implementation in our dependencies. We can add one\n            # in seaborn\
    \ later, if there is sufficient interest. For now, limit to mean.\n          \
    \  raise ValueError(f\"Weighted estimator must be 'mean', not {estimator!r}.\"\
    )\n        self.estimator = estimator\n\n        method, level = _validate_errorbar_arg(errorbar)\n\
    \        if method is not None and method != \"ci\":\n            # As with the\
    \ estimator, weighted 'sd' or 'pi' error bars may make sense.\n            # But\
    \ we'll keep things simple for now and limit to (bootstrap) CI.\n            raise\
    \ ValueError(f\"Error bar method must be 'ci', not {method!r}.\")\n        self.error_method\
    \ = method\n        self.error_level = level\n\n        self.boot_kws = boot_kws\n\
    \n    def __call__(self, data, var):\n        \"\"\"Aggregate over `var` column\
    \ of `data` with estimate and error interval.\"\"\"\n        vals = data[var]\n\
    \        weights = data[\"weight\"]\n\n        estimate = np.average(vals, weights=weights)\n\
    \n        if self.error_method == \"ci\" and len(data) > 1:\n\n            def\
    \ error_func(x, w):\n                return np.average(x, weights=w)\n\n     \
    \       boots = bootstrap(vals, weights, func=error_func, **self.boot_kws)\n \
    \           err_min, err_max = _percentile_interval(boots, self.error_level)\n\
    \n        else:\n            err_min = err_max = np.nan\n\n        return pd.Series({var:\
    \ estimate, f\"{var}min\": err_min, f\"{var}max\": err_max})\n\n\nclass LetterValues:\n\
    \n    def __init__(self, k_depth, outlier_prop, trust_alpha):\n        \"\"\"\n\
    \        Compute percentiles of a distribution using various tail stopping rules.\n\
    \n        Parameters\n        ----------\n        k_depth: \"tukey\", \"proportion\"\
    , \"trustworthy\", or \"full\"\n            Stopping rule for choosing tail percentiled\
    \ to show:\n\n            - tukey: Show a similar number of outliers as in a conventional\
    \ boxplot.\n            - proportion: Show approximately `outlier_prop` outliers.\n\
    \            - trust_alpha: Use `trust_alpha` level for most extreme tail percentile.\n\
    \n        outlier_prop: float\n            Parameter for `k_depth=\"proportion\"\
    ` setting the expected outlier rate.\n        trust_alpha: float\n           \
    \ Parameter for `k_depth=\"trustworthy\"` setting the confidence threshold.\n\n\
    \        Notes\n        -----\n        Based on the proposal in this paper:\n\
    \        https://vita.had.co.nz/papers/letter-value-plot.pdf\n\n        \"\"\"\
    \n        k_options = [\"tukey\", \"proportion\", \"trustworthy\", \"full\"]\n\
    \        if isinstance(k_depth, str):\n            _check_argument(\"k_depth\"\
    , k_options, k_depth)\n        elif not isinstance(k_depth, int):\n          \
    \  err = (\n                \"The `k_depth` parameter must be either an integer\
    \ or string \"\n                f\"(one of {k_options}), not {k_depth!r}.\"\n\
    \            )\n            raise TypeError(err)\n\n        self.k_depth = k_depth\n\
    \        self.outlier_prop = outlier_prop\n        self.trust_alpha = trust_alpha\n\
    \n    def _compute_k(self, n):\n\n        # Select the depth, i.e. number of boxes\
    \ to draw, based on the method\n        if self.k_depth == \"full\":\n       \
    \     # extend boxes to 100% of the data\n            k = int(np.log2(n)) + 1\n\
    \        elif self.k_depth == \"tukey\":\n            # This results with 5-8\
    \ points in each tail\n            k = int(np.log2(n)) - 3\n        elif self.k_depth\
    \ == \"proportion\":\n            k = int(np.log2(n)) - int(np.log2(n * self.outlier_prop))\
    \ + 1\n        elif self.k_depth == \"trustworthy\":\n            normal_quantile_func\
    \ = np.vectorize(NormalDist().inv_cdf)\n            point_conf = 2 * normal_quantile_func(1\
    \ - self.trust_alpha / 2) ** 2\n            k = int(np.log2(n / point_conf)) +\
    \ 1\n        else:\n            # Allow having k directly specified as input\n\
    \            k = int(self.k_depth)\n\n        return max(k, 1)\n\n    def __call__(self,\
    \ x):\n        \"\"\"Evaluate the letter values.\"\"\"\n        k = self._compute_k(len(x))\n\
    \        exp = np.arange(k + 1, 1, -1), np.arange(2, k + 2)\n        levels =\
    \ k + 1 - np.concatenate([exp[0], exp[1][1:]])\n        percentiles = 100 * np.concatenate([0.5\
    \ ** exp[0], 1 - 0.5 ** exp[1]])\n        if self.k_depth == \"full\":\n     \
    \       percentiles[0] = 0\n            percentiles[-1] = 100\n        values\
    \ = np.percentile(x, percentiles)\n        fliers = np.asarray(x[(x < values.min())\
    \ | (x > values.max())])\n        median = np.percentile(x, 50)\n\n        return\
    \ {\n            \"k\": k,\n            \"levels\": levels,\n            \"percs\"\
    : percentiles,\n            \"values\": values,\n            \"fliers\": fliers,\n\
    \            \"median\": median,\n        }\n\n\ndef _percentile_interval(data,\
    \ width):\n    \"\"\"Return a percentile interval from data of a given width.\"\
    \"\"\n    edge = (100 - width) / 2\n    percentiles = edge, 100 - edge\n    return\
    \ np.nanpercentile(data, percentiles)\n\n\ndef _validate_errorbar_arg(arg):\n\
    \    \"\"\"Check type and value of errorbar argument and assign default level.\"\
    \"\"\n    DEFAULT_LEVELS = {\n        \"ci\": 95,\n        \"pi\": 95,\n     \
    \   \"se\": 1,\n        \"sd\": 1,\n    }\n\n    usage = \"`errorbar` must be\
    \ a callable, string, or (string, number) tuple\"\n\n    if arg is None:\n   \
    \     return None, None\n    elif callable(arg):\n        return arg, None\n \
    \   elif isinstance(arg, str):\n        method = arg\n        level = DEFAULT_LEVELS.get(method,\
    \ None)\n    else:\n        try:\n            method, level = arg\n        except\
    \ (ValueError, TypeError) as err:\n            raise err.__class__(usage) from\
    \ err\n\n    _check_argument(\"errorbar\", list(DEFAULT_LEVELS), method)\n   \
    \ if level is not None and not isinstance(level, Number):\n        raise TypeError(usage)\n\
    \n    return method, level\n\nOutput the complete test file, code only, no explanations.\n\
    ### Time\nCurrent time: 2025-03-14 17:11:51\n"
  role: user
