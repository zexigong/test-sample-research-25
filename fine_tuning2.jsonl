{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_cache\\test_cache.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport sys\nfrom numbers import Number\nfrom timeit import default_timer\n\nfrom dask.callbacks import Callback\n\noverhead = sys.getsizeof(1.23) * 4 + sys.getsizeof(()) * 4\n\n\nclass Cache(Callback):\n    \"\"\"Use cache for computation\n\n    Examples\n    --------\n\n    >>> cache = Cache(1e9)  # doctest: +SKIP\n\n    The cache can be used locally as a context manager around ``compute`` or\n    ``get`` calls:\n\n    >>> with cache:  # doctest: +SKIP\n    ...     result = x.compute()\n\n    You can also register a cache globally, so that it works for all\n    computations:\n\n    >>> cache.register()    # doctest: +SKIP\n    >>> cache.unregister()  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, cache, *args, **kwargs):\n        try:\n            import cachey\n        except ImportError as ex:\n            raise ImportError(\n                f'Cache requires cachey, \"{str(ex)}\" problem importing'\n            ) from ex\n        self._nbytes = cachey.nbytes\n        if isinstance(cache, Number):\n            cache = cachey.Cache(cache, *args, **kwargs)\n        else:\n            assert not args and not kwargs\n        self.cache = cache\n        self.starttimes = dict()\n\n    def _start(self, dsk):\n        self.durations = dict()\n        overlap = set(dsk) & set(self.cache.data)\n        for key in overlap:\n            dsk[key] = self.cache.data[key]\n\n    def _pretask(self, key, dsk, state):\n        self.starttimes[key] = default_timer()\n\n    def _posttask(self, key, value, dsk, state, id):\n        duration = default_timer() - self.starttimes[key]\n        deps = state[\"dependencies\"][key]\n        if deps:\n            duration += max(self.durations.get(k, 0) for k in deps)\n        self.durations[key] = duration\n        nb = self._nbytes(value) + overhead + sys.getsizeof(key) * 4\n        self.cache.put(key, value, cost=duration / nb / 1e9, nbytes=nb)\n\n    def _finish(self, dsk, state, errored):\n        self.starttimes.clear()\n        self.durations.clear()\n\n### Source File Dependency Files Content\n### Dependency File: callbacks.py\nfrom __future__ import annotations\n\nfrom collections.abc import Callable\nfrom contextlib import contextmanager\nfrom typing import ClassVar\n\n__all__ = [\"Callback\", \"add_callbacks\"]\n\n\nclass Callback:\n    \"\"\"Base class for using the callback mechanism\n\n    Create a callback with functions of the following signatures:\n\n    >>> def start(dsk):\n    ...     pass\n    >>> def start_state(dsk, state):\n    ...     pass\n    >>> def pretask(key, dsk, state):\n    ...     pass\n    >>> def posttask(key, result, dsk, state, worker_id):\n    ...     pass\n    >>> def finish(dsk, state, failed):\n    ...     pass\n\n    You may then construct a callback object with any number of them\n\n    >>> cb = Callback(pretask=pretask, finish=finish)\n\n    And use it either as a context manager over a compute/get call\n\n    >>> with cb:            # doctest: +SKIP\n    ...     x.compute()\n\n    Or globally with the ``register`` method\n\n    >>> cb.register()\n    >>> cb.unregister()\n\n    Alternatively subclass the ``Callback`` class with your own methods.\n\n    >>> class PrintKeys(Callback):\n    ...     def _pretask(self, key, dask, state):\n    ...         print(\"Computing: {0}!\".format(repr(key)))\n\n    >>> with PrintKeys():   # doctest: +SKIP\n    ...     x.compute()\n    \"\"\"\n\n    active: ClassVar[set[tuple[Callable | None, ...]]] = set()\n\n    def __init__(\n        self, start=None, start_state=None, pretask=None, posttask=None, finish=None\n    ):\n        if start:\n            self._start = start\n        if start_state:\n            self._start_state = start_state\n        if pretask:\n            self._pretask = pretask\n        if posttask:\n            self._posttask = posttask\n        if finish:\n            self._finish = finish\n\n    @property\n    def _callback(self) -> tuple[Callable | None, ...]:\n        fields = [\"_start\", \"_start_state\", \"_pretask\", \"_posttask\", \"_finish\"]\n        return tuple(getattr(self, i, None) for i in fields)\n\n    def __enter__(self):\n        self._cm = add_callbacks(self)\n        self._cm.__enter__()\n        return self\n\n    def __exit__(self, *args):\n        self._cm.__exit__(*args)\n\n    def register(self) -> None:\n        Callback.active.add(self._callback)\n\n    def unregister(self) -> None:\n        Callback.active.remove(self._callback)\n\n\ndef unpack_callbacks(cbs):\n    \"\"\"Take an iterable of callbacks, return a list of each callback.\"\"\"\n    if cbs:\n        return [[i for i in f if i] for f in zip(*cbs)]\n    else:\n        return [(), (), (), (), ()]\n\n\n@contextmanager\ndef local_callbacks(callbacks=None):\n    \"\"\"Allows callbacks to work with nested schedulers.\n\n    Callbacks will only be used by the first started scheduler they encounter.\n    This means that only the outermost scheduler will use global callbacks.\"\"\"\n    global_callbacks = callbacks is None\n    if global_callbacks:\n        callbacks, Callback.active = Callback.active, set()\n    try:\n        yield callbacks or ()\n    finally:\n        if global_callbacks:\n            Callback.active = callbacks\n\n\ndef normalize_callback(cb):\n    \"\"\"Normalizes a callback to a tuple\"\"\"\n    if isinstance(cb, Callback):\n        return cb._callback\n    elif isinstance(cb, tuple):\n        return cb\n    else:\n        raise TypeError(\"Callbacks must be either `Callback` or `tuple`\")\n\n\nclass add_callbacks:\n    \"\"\"Context manager for callbacks.\n\n    Takes several callbacks and applies them only in the enclosed context.\n    Callbacks can either be represented as a ``Callback`` object, or as a tuple\n    of length 4.\n\n    Examples\n    --------\n    >>> def pretask(key, dsk, state):\n    ...     print(\"Now running {0}\").format(key)\n    >>> callbacks = (None, pretask, None, None)\n    >>> with add_callbacks(callbacks):    # doctest: +SKIP\n    ...     res.compute()\n    \"\"\"\n\n    def __init__(self, *callbacks):\n        self.callbacks = [normalize_callback(c) for c in callbacks]\n        Callback.active.update(self.callbacks)\n\n    def __enter__(self):\n        return\n\n    def __exit__(self, type, value, traceback):\n        for c in self.callbacks:\n            Callback.active.discard(c)\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:36:31"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nfrom operator import add\nfrom time import sleep\n\nimport pytest\n\nfrom dask.cache import Cache\nfrom dask.callbacks import Callback\nfrom dask.local import get_sync\nfrom dask.threaded import get\n\ncachey = pytest.importorskip(\"cachey\")\n\n\nflag = []\n\n\ndef inc(x):\n    flag.append(x)\n    return x + 1\n\n\ndef test_cache():\n    c = cachey.Cache(10000)\n    cc = Cache(c)\n\n    with cc:\n        assert get({\"x\": (inc, 1)}, \"x\") == 2\n\n    assert flag == [1]\n    assert c.data[\"x\"] == 2\n\n    assert not cc.starttimes\n    assert not cc.durations\n\n    while flag:\n        flag.pop()\n    dsk = {\"x\": (inc, 1), \"y\": (inc, 2), \"z\": (add, \"x\", \"y\")}\n    with cc:\n        assert get(dsk, \"z\") == 5\n\n    assert flag == [2]  # no x present\n\n    assert not Callback.active\n\n\ndef test_cache_with_number():\n    c = Cache(10000, limit=1)\n    assert isinstance(c.cache, cachey.Cache)\n    assert c.cache.available_bytes == 10000\n    assert c.cache.limit == 1\n\n\ndef test_cache_correctness():\n    # https://github.com/dask/dask/issues/3631\n    c = Cache(10000)\n    pytest.importorskip(\"numpy\")\n    da = pytest.importorskip(\"dask.array\")\n    from numpy import ones, zeros\n\n    z = da.from_array(zeros(1), chunks=10)\n    o = da.from_array(ones(1), chunks=10)\n    with c:\n        assert (z.compute() == 0).all()\n        assert (o.compute() == 1).all()\n\n\ndef f(duration, size, *args):\n    sleep(duration)\n    return [0] * size\n\n\ndef test_prefer_cheap_dependent():\n    dsk = {\"x\": (f, 0.01, 10), \"y\": (f, 0.000001, 1, \"x\")}\n    c = Cache(10000)\n    with c:\n        get_sync(dsk, \"y\")\n\n    assert c.cache.scorer.cost[\"x\"] < c.cache.scorer.cost[\"y\"]\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_callbacks\\test_callbacks.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections.abc import Callable\nfrom contextlib import contextmanager\nfrom typing import ClassVar\n\n__all__ = [\"Callback\", \"add_callbacks\"]\n\n\nclass Callback:\n    \"\"\"Base class for using the callback mechanism\n\n    Create a callback with functions of the following signatures:\n\n    >>> def start(dsk):\n    ...     pass\n    >>> def start_state(dsk, state):\n    ...     pass\n    >>> def pretask(key, dsk, state):\n    ...     pass\n    >>> def posttask(key, result, dsk, state, worker_id):\n    ...     pass\n    >>> def finish(dsk, state, failed):\n    ...     pass\n\n    You may then construct a callback object with any number of them\n\n    >>> cb = Callback(pretask=pretask, finish=finish)\n\n    And use it either as a context manager over a compute/get call\n\n    >>> with cb:            # doctest: +SKIP\n    ...     x.compute()\n\n    Or globally with the ``register`` method\n\n    >>> cb.register()\n    >>> cb.unregister()\n\n    Alternatively subclass the ``Callback`` class with your own methods.\n\n    >>> class PrintKeys(Callback):\n    ...     def _pretask(self, key, dask, state):\n    ...         print(\"Computing: {0}!\".format(repr(key)))\n\n    >>> with PrintKeys():   # doctest: +SKIP\n    ...     x.compute()\n    \"\"\"\n\n    active: ClassVar[set[tuple[Callable | None, ...]]] = set()\n\n    def __init__(\n        self, start=None, start_state=None, pretask=None, posttask=None, finish=None\n    ):\n        if start:\n            self._start = start\n        if start_state:\n            self._start_state = start_state\n        if pretask:\n            self._pretask = pretask\n        if posttask:\n            self._posttask = posttask\n        if finish:\n            self._finish = finish\n\n    @property\n    def _callback(self) -> tuple[Callable | None, ...]:\n        fields = [\"_start\", \"_start_state\", \"_pretask\", \"_posttask\", \"_finish\"]\n        return tuple(getattr(self, i, None) for i in fields)\n\n    def __enter__(self):\n        self._cm = add_callbacks(self)\n        self._cm.__enter__()\n        return self\n\n    def __exit__(self, *args):\n        self._cm.__exit__(*args)\n\n    def register(self) -> None:\n        Callback.active.add(self._callback)\n\n    def unregister(self) -> None:\n        Callback.active.remove(self._callback)\n\n\ndef unpack_callbacks(cbs):\n    \"\"\"Take an iterable of callbacks, return a list of each callback.\"\"\"\n    if cbs:\n        return [[i for i in f if i] for f in zip(*cbs)]\n    else:\n        return [(), (), (), (), ()]\n\n\n@contextmanager\ndef local_callbacks(callbacks=None):\n    \"\"\"Allows callbacks to work with nested schedulers.\n\n    Callbacks will only be used by the first started scheduler they encounter.\n    This means that only the outermost scheduler will use global callbacks.\"\"\"\n    global_callbacks = callbacks is None\n    if global_callbacks:\n        callbacks, Callback.active = Callback.active, set()\n    try:\n        yield callbacks or ()\n    finally:\n        if global_callbacks:\n            Callback.active = callbacks\n\n\ndef normalize_callback(cb):\n    \"\"\"Normalizes a callback to a tuple\"\"\"\n    if isinstance(cb, Callback):\n        return cb._callback\n    elif isinstance(cb, tuple):\n        return cb\n    else:\n        raise TypeError(\"Callbacks must be either `Callback` or `tuple`\")\n\n\nclass add_callbacks:\n    \"\"\"Context manager for callbacks.\n\n    Takes several callbacks and applies them only in the enclosed context.\n    Callbacks can either be represented as a ``Callback`` object, or as a tuple\n    of length 4.\n\n    Examples\n    --------\n    >>> def pretask(key, dsk, state):\n    ...     print(\"Now running {0}\").format(key)\n    >>> callbacks = (None, pretask, None, None)\n    >>> with add_callbacks(callbacks):    # doctest: +SKIP\n    ...     res.compute()\n    \"\"\"\n\n    def __init__(self, *callbacks):\n        self.callbacks = [normalize_callback(c) for c in callbacks]\n        Callback.active.update(self.callbacks)\n\n    def __enter__(self):\n        return\n\n    def __exit__(self, type, value, traceback):\n        for c in self.callbacks:\n            Callback.active.discard(c)\n\n### Source File Dependency Files Content\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:36:31"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport pytest\n\nfrom dask.callbacks import Callback\nfrom dask.local import get_sync\nfrom dask.threaded import get as get_threaded\nfrom dask.utils_test import add\n\n\ndef test_start_callback():\n    flag = [False]\n\n    class MyCallback(Callback):\n        def _start(self, dsk):\n            flag[0] = True\n\n    with MyCallback():\n        get_sync({\"x\": 1}, \"x\")\n\n    assert flag[0] is True\n\n\ndef test_start_state_callback():\n    flag = [False]\n\n    class MyCallback(Callback):\n        def _start_state(self, dsk, state):\n            flag[0] = True\n            assert dsk[\"x\"]() == 1\n            assert len(state[\"cache\"]) == 1\n\n    with MyCallback():\n        get_sync({\"x\": 1}, \"x\")\n\n    assert flag[0] is True\n\n\ndef test_finish_always_called():\n    flag = [False]\n\n    class MyCallback(Callback):\n        def _finish(self, dsk, state, errored):\n            flag[0] = True\n            assert errored\n\n    dsk = {\"x\": (lambda: 1 / 0,)}\n\n    # `raise_on_exception=True`\n    try:\n        with MyCallback():\n            get_sync(dsk, \"x\")\n    except Exception as e:\n        assert isinstance(e, ZeroDivisionError)\n    assert flag[0]\n\n    # `raise_on_exception=False`\n    flag[0] = False\n    try:\n        with MyCallback():\n            get_threaded(dsk, \"x\")\n    except Exception as e:\n        assert isinstance(e, ZeroDivisionError)\n    assert flag[0]\n\n    # KeyboardInterrupt\n    def raise_keyboard():\n        raise KeyboardInterrupt()\n\n    dsk = {\"x\": (raise_keyboard,)}\n    flag[0] = False\n    with pytest.raises(KeyboardInterrupt):\n        with MyCallback():\n            get_sync(dsk, \"x\")\n    assert flag[0]\n\n\ndef test_nested_schedulers():\n    class MyCallback(Callback):\n        def _start(self, dsk):\n            self.dsk = dsk\n\n        def _pretask(self, key, dsk, state):\n            assert key in self.dsk\n\n    inner_callback = MyCallback()\n    inner_dsk = {\"x\": (add, 1, 2), \"y\": (add, \"x\", 3)}\n\n    def nested_call(x):\n        assert not Callback.active\n        with inner_callback:\n            return get_threaded(inner_dsk, \"y\") + x\n\n    outer_callback = MyCallback()\n    outer_dsk = {\"a\": (nested_call, 1), \"b\": (add, \"a\", 2)}\n\n    with outer_callback:\n        get_threaded(outer_dsk, \"b\")\n\n    assert not Callback.active\n    assert outer_callback.dsk.keys() == outer_dsk.keys()\n    assert inner_callback.dsk.keys() == inner_dsk.keys()\n    assert not Callback.active\n\n\ndef test_add_remove_mutates_not_replaces():\n    assert not Callback.active\n\n    with Callback():\n        assert Callback.active\n\n    assert not Callback.active\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_core\\test_core.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom collections.abc import Collection, Iterable, Mapping, MutableMapping\nfrom typing import Any, Literal, TypeVar, cast, overload\n\nimport toolz\n\nfrom dask._task_spec import DependenciesMapping, convert_legacy_graph, execute_graph\nfrom dask.typing import Graph, Key, NoDefault, no_default\n\n\ndef ishashable(x):\n    \"\"\"Is x hashable?\n\n    Examples\n    --------\n\n    >>> ishashable(1)\n    True\n    >>> ishashable([1])\n    False\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    try:\n        hash(x)\n        return True\n    except TypeError:\n        return False\n\n\ndef istask(x):\n    \"\"\"Is x a runnable task?\n\n    A task is a tuple with a callable first argument\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> istask((inc, 1))\n    True\n    >>> istask(1)\n    False\n    \"\"\"\n    from dask._task_spec import DataNode, GraphNode\n\n    if isinstance(x, GraphNode):\n        if isinstance(x, DataNode):\n            return False\n        return True\n    return type(x) is tuple and x and callable(x[0])\n\n\ndef preorder_traversal(task):\n    \"\"\"A generator to preorder-traverse a task.\"\"\"\n\n    for item in task:\n        if istask(item):\n            yield from preorder_traversal(item)\n        elif isinstance(item, list):\n            yield list\n            yield from preorder_traversal(item)\n        else:\n            yield item\n\n\ndef lists_to_tuples(res, keys):\n    if isinstance(keys, list):\n        return tuple(lists_to_tuples(r, k) for r, k in zip(res, keys))\n    return res\n\n\ndef _pack_result(result: Mapping, keys: list | Key) -> Any:\n    if isinstance(keys, list):\n        return tuple(_pack_result(result, k) for k in keys)\n    return result[keys]\n\n\ndef get(dsk: Mapping, out: list | Key, cache: MutableMapping | None = None) -> Any:\n    \"\"\"Get value from Dask\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': 1, 'y': (inc, 'x')}\n\n    >>> get(d, 'x')\n    1\n    >>> get(d, 'y')\n    2\n    \"\"\"\n    for k in flatten(out):\n        if k not in dsk:\n            raise KeyError(f\"{k} is not a key in the graph\")\n    if cache is None:\n        cache = {}\n\n    dsk2 = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    result = execute_graph(dsk2, cache, keys=set(flatten([out])))\n    return _pack_result(result, out)\n\n\ndef keys_in_tasks(keys: Collection[Key], tasks: Iterable[Any], as_list: bool = False):\n    \"\"\"Returns the keys in `keys` that are also in `tasks`\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> keys_in_tasks(dsk, ['x', 'y', 'j'])  # doctest: +SKIP\n    {'x', 'y'}\n    \"\"\"\n    from dask._task_spec import GraphNode\n\n    ret: list[Key] = []\n    while tasks:\n        work = []\n        for w in tasks:\n            typ = type(w)\n            if typ is tuple and w and callable(w[0]):  # istask(w)\n                work.extend(w[1:])\n            elif typ is list:\n                work.extend(w)\n            elif typ is dict:\n                work.extend(w.values())\n            elif isinstance(w, GraphNode):\n                work.extend(w.dependencies)\n            else:\n                try:\n                    if w in keys:\n                        ret.append(w)\n                except TypeError:  # not hashable\n                    pass\n        tasks = work\n    return ret if as_list else set(ret)\n\n\ndef iskey(key: object) -> bool:\n    \"\"\"Return True if the given object is a potential dask key; False otherwise.\n\n    The definition of a key in a Dask graph is any str, bytes, int, float, or tuple\n    thereof.\n\n    See Also\n    --------\n    ishashable\n    validate_key\n    dask.typing.Key\n    \"\"\"\n    typ = type(key)\n    if typ is tuple:\n        return all(iskey(i) for i in cast(tuple, key))\n    return typ in {bytes, int, float, str}\n\n\ndef validate_key(key: object) -> None:\n    \"\"\"Validate the format of a dask key.\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    if iskey(key):\n        return\n    typ = type(key)\n\n    if typ is tuple:\n        index = None\n        try:\n            for index, part in enumerate(cast(tuple, key)):  # noqa: B007\n                validate_key(part)\n        except TypeError as e:\n            raise TypeError(\n                f\"Composite key contains unexpected key type at {index=} ({key=!r})\"\n            ) from e\n    raise TypeError(f\"Unexpected key type {typ} ({key=!r})\")\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = ...,\n    task: Key | NoDefault = ...,\n    as_list: Literal[False] = ...,\n) -> set[Key]: ...\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None,\n    task: Key | NoDefault,\n    as_list: Literal[True],\n) -> list[Key]: ...\n\n\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = None,\n    task: Key | NoDefault = no_default,\n    as_list: bool = False,\n) -> set[Key] | list[Key]:\n    \"\"\"Get the immediate tasks on which this task depends\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> get_dependencies(dsk, 'x')\n    set()\n\n    >>> get_dependencies(dsk, 'y')\n    {'x'}\n\n    >>> get_dependencies(dsk, 'z')  # doctest: +SKIP\n    {'x', 'y'}\n\n    >>> get_dependencies(dsk, 'w')  # Only direct dependencies\n    {'z'}\n\n    >>> get_dependencies(dsk, 'a')  # Ignore non-keys\n    {'x'}\n\n    >>> get_dependencies(dsk, task=(inc, 'x'))  # provide tasks directly\n    {'x'}\n    \"\"\"\n    if key is not None:\n        arg = dsk[key]\n    elif task is not no_default:\n        arg = task\n    else:\n        raise ValueError(\"Provide either key or task\")\n\n    return keys_in_tasks(dsk, [arg], as_list=as_list)\n\n\ndef get_deps(dsk: Graph) -> tuple[dict[Key, set[Key]], dict[Key, set[Key]]]:\n    \"\"\"Get dependencies and dependents from dask dask graph\n\n    >>> inc = lambda x: x + 1\n    >>> dsk = {'a': 1, 'b': (inc, 'a'), 'c': (inc, 'b')}\n    >>> dependencies, dependents = get_deps(dsk)\n    >>> dependencies\n    {'a': set(), 'b': {'a'}, 'c': {'b'}}\n    >>> dependents  # doctest: +SKIP\n    {'a': {'b'}, 'b': {'c'}, 'c': set()}\n    \"\"\"\n    dependencies = {k: get_dependencies(dsk, task=v) for k, v in dsk.items()}\n    dependents = reverse_dict(dependencies)\n    return dependencies, dependents\n\n\ndef flatten(seq, container=list):\n    \"\"\"\n\n    >>> list(flatten([1]))\n    [1]\n\n    >>> list(flatten([[1, 2], [1, 2]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten([[[1], [2]], [[1], [2]]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten(((1, 2), (1, 2)))) # Don't flatten tuples\n    [(1, 2), (1, 2)]\n\n    >>> list(flatten((1, 2, [3, 4]))) # support heterogeneous\n    [1, 2, 3, 4]\n    \"\"\"\n    if isinstance(seq, str):\n        yield seq\n    else:\n        for item in seq:\n            if isinstance(item, container):\n                yield from flatten(item, container=container)\n            else:\n                yield item\n\n\nT_ = TypeVar(\"T_\")\n\n\ndef reverse_dict(d: Mapping[T_, Iterable[T_]]) -> dict[T_, set[T_]]:\n    \"\"\"\n\n    >>> a, b, c = 'abc'\n    >>> d = {a: [b, c], b: [c]}\n    >>> reverse_dict(d)  # doctest: +SKIP\n    {'a': set([]), 'b': set(['a']}, 'c': set(['a', 'b'])}\n    \"\"\"\n    result: defaultdict[T_, set[T_]] = defaultdict(set)\n    _add = set.add\n    for k, vals in d.items():\n        result[k]\n        for val in vals:\n            _add(result[val], k)\n    return dict(result)\n\n\ndef subs(task, key, val):\n    \"\"\"Perform a substitution on a task\n\n    Examples\n    --------\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> subs((inc, 'x'), 'x', 1)  # doctest: +ELLIPSIS\n    (<function inc at ...>, 1)\n    \"\"\"\n    type_task = type(task)\n    if not (type_task is tuple and task and callable(task[0])):  # istask(task):\n        try:\n            if type_task is type(key) and task == key:\n                return val\n        except Exception:\n            pass\n        if type_task is list:\n            return [subs(x, key, val) for x in task]\n        return task\n    newargs = []\n    hash_key = {key}\n    for arg in task[1:]:\n        type_arg = type(arg)\n        if type_arg is tuple and arg and callable(arg[0]):  # istask(task):\n            arg = subs(arg, key, val)\n        elif type_arg is list:\n            arg = [subs(x, key, val) for x in arg]\n        else:\n            try:\n                if arg in hash_key:  # Hash and equality match\n                    arg = val\n            except TypeError:  # not hashable\n                pass\n        newargs.append(arg)\n    return task[:1] + tuple(newargs)\n\n\ndef _toposort(dsk, keys=None, returncycle=False, dependencies=None):\n\n    # Stack-based depth-first search traversal.  This is based on Tarjan's\n    # method for topological sorting (see wikipedia for pseudocode)\n    if keys is None:\n        keys = dsk\n    elif not isinstance(keys, list):\n        keys = [keys]\n    if not returncycle:\n        ordered = []\n\n    # Nodes whose descendents have been completely explored.\n    # These nodes are guaranteed to not be part of a cycle.\n    completed = set()\n\n    # All nodes that have been visited in the current traversal.  Because\n    # we are doing depth-first search, going \"deeper\" should never result\n    # in visiting a node that has already been seen.  The `seen` and\n    # `completed` sets are mutually exclusive; it is okay to visit a node\n    # that has already been added to `completed`.\n    seen = set()\n\n    if dependencies is None:\n\n        dependencies = DependenciesMapping(dsk)\n\n    for key in keys:\n        if key in completed:\n            continue\n        nodes = [key]\n        while nodes:\n            # Keep current node on the stack until all descendants are visited\n            cur = nodes[-1]\n            if cur in completed:\n                # Already fully traversed descendants of cur\n                nodes.pop()\n                continue\n            seen.add(cur)\n\n            # Add direct descendants of cur to nodes stack\n            next_nodes = []\n            for nxt in dependencies[cur]:\n                if nxt not in completed:\n                    if nxt in seen:\n                        # Cycle detected!\n                        # Let's report only the nodes that directly participate in the cycle.\n                        # We use `priorities` below to greedily construct a short cycle.\n                        # Shorter cycles may exist.\n                        priorities = {}\n                        prev = nodes[-1]\n                        # Give priority to nodes that were seen earlier.\n                        while nodes[-1] != nxt:\n                            priorities[nodes.pop()] = -len(priorities)\n                        priorities[nxt] = -len(priorities)\n                        # We're going to get the cycle by walking backwards along dependents,\n                        # so calculate dependents only for the nodes in play.\n                        inplay = set(priorities)\n                        dependents = reverse_dict(\n                            {k: inplay.intersection(dependencies[k]) for k in inplay}\n                        )\n                        # Begin with the node that was seen twice and the node `prev` from\n                        # which we detected the cycle.\n                        cycle = [nodes.pop()]\n                        cycle.append(prev)\n                        while prev != cycle[0]:\n                            # Greedily take a step that takes us closest to completing the cycle.\n                            # This may not give us the shortest cycle, but we get *a* short cycle.\n                            deps = dependents[cycle[-1]]\n                            prev = min(deps, key=priorities.__getitem__)\n                            cycle.append(prev)\n                        cycle.reverse()\n\n                        if returncycle:\n                            return cycle\n                        else:\n                            cycle = \"->\".join(str(x) for x in cycle)\n                            raise RuntimeError(\"Cycle detected in Dask: %s\" % cycle)\n                    next_nodes.append(nxt)\n\n            if next_nodes:\n                nodes.extend(next_nodes)\n            else:\n                # cur has no more descendants to explore, so we're done with it\n                if not returncycle:\n                    ordered.append(cur)\n                completed.add(cur)\n                seen.remove(cur)\n                nodes.pop()\n    if returncycle:\n        return []\n    return ordered\n\n\ndef toposort(dsk, dependencies=None):\n    \"\"\"Return a list of keys of dask sorted in topological order.\"\"\"\n    return _toposort(dsk, dependencies=dependencies)\n\n\ndef getcycle(d, keys):\n    \"\"\"Return a list of nodes that form a cycle if Dask is not a DAG.\n\n    Returns an empty list if no cycle is found.\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': (inc, 'z'), 'y': (inc, 'x'), 'z': (inc, 'y')}\n    >>> getcycle(d, 'x')\n    ['x', 'z', 'y', 'x']\n\n    See Also\n    --------\n    isdag\n    \"\"\"\n    return _toposort(d, keys=keys, returncycle=True)\n\n\ndef isdag(d, keys):\n    \"\"\"Does Dask form a directed acyclic graph when calculating keys?\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> inc = lambda x: x + 1\n    >>> isdag({'x': 0, 'y': (inc, 'x')}, 'y')\n    True\n    >>> isdag({'x': (inc, 'y'), 'y': (inc, 'x')}, 'y')\n    False\n\n    See Also\n    --------\n    getcycle\n    \"\"\"\n    return not getcycle(d, keys)\n\n\nclass literal:\n    \"\"\"A small serializable object to wrap literal values without copying\"\"\"\n\n    __slots__ = (\"data\",)\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return \"literal<type=%s>\" % type(self.data).__name__\n\n    def __reduce__(self):\n        return (literal, (self.data,))\n\n    def __call__(self):\n        return self.data\n\n\ndef quote(x):\n    \"\"\"Ensure that this value remains this value in a dask graph\n\n    Some values in dask graph take on special meaning. Sometimes we want to\n    ensure that our data is not interpreted but remains literal.\n\n    >>> add = lambda x, y: x + y\n    >>> quote((add, 1, 2))\n    (literal<type=tuple>,)\n    \"\"\"\n    if istask(x) or type(x) is list or type(x) is dict:\n        return (literal(x),)\n    return x\n\n\ndef reshapelist(shape, seq):\n    \"\"\"Reshape iterator to nested shape\n\n    >>> reshapelist((2, 3), range(6))\n    [[0, 1, 2], [3, 4, 5]]\n    \"\"\"\n    if len(shape) == 1:\n        return list(seq)\n    else:\n        n = int(len(seq) / shape[0])\n        return [reshapelist(shape[1:], part) for part in toolz.partition(n, seq)]\n\n### Source File Dependency Files Content\n### Dependency File: typing.py\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Callable, Hashable, Mapping, Sequence\nfrom enum import Enum\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Literal,\n    Protocol,\n    TypeVar,\n    Union,\n    runtime_checkable,\n)\n\nif TYPE_CHECKING:\n    # IPython import is relatively slow. Avoid if not necessary\n    # TODO import from typing (requires Python >=3.10)\n    from typing import TypeAlias\n\n    from IPython.display import DisplayObject\n\nCollType = TypeVar(\"CollType\", bound=\"DaskCollection\")\nCollType_co = TypeVar(\"CollType_co\", bound=\"DaskCollection\", covariant=True)\nPostComputeCallable = Callable\n\n\nKey: TypeAlias = Union[str, bytes, int, float, tuple[\"Key\", ...]]\n# FIXME: This type is a little misleading. Low level graphs are often\n# MutableMappings but HLGs are not\nGraph: TypeAlias = Mapping[Key, Any]\n# Potentially nested list of Dask keys\nNestedKeys: TypeAlias = list[Union[Key, \"NestedKeys\"]]\n\n\nclass SchedulerGetCallable(Protocol):\n    \"\"\"Protocol defining the signature of a ``__dask_scheduler__`` callable.\"\"\"\n\n    def __call__(\n        self,\n        dsk: Graph,\n        keys: Sequence[Key] | Key,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Method called as the default scheduler for a collection.\n\n        Parameters\n        ----------\n        dsk :\n            The task graph.\n        keys :\n            Key(s) corresponding to the desired data.\n        **kwargs :\n            Additional arguments.\n\n        Returns\n        -------\n        Any\n            Result(s) associated with `keys`\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\nclass PostPersistCallable(Protocol[CollType_co]):\n    \"\"\"Protocol defining the signature of a ``__dask_postpersist__`` callable.\"\"\"\n\n    def __call__(\n        self,\n        dsk: Graph,\n        *args: Any,\n        rename: Mapping[str, str] | None = None,\n    ) -> CollType_co:\n        \"\"\"Method called to rebuild a persisted collection.\n\n        Parameters\n        ----------\n        dsk: Mapping\n            A mapping which contains at least the output keys returned\n            by __dask_keys__().\n        *args : Any\n            Additional optional arguments If no extra arguments are\n            necessary, it must be an empty tuple.\n        rename : Mapping[str, str], optional\n            If defined, it indicates that output keys may be changing\n            too; e.g. if the previous output of :meth:`__dask_keys__`\n            was ``[(\"a\", 0), (\"a\", 1)]``, after calling\n            ``rebuild(dsk, *extra_args, rename={\"a\": \"b\"})``\n            it must become ``[(\"b\", 0), (\"b\", 1)]``.\n            The ``rename`` mapping may not contain the collection\n            name(s); in such case the associated keys do not change.\n            It may contain replacements for unexpected names, which\n            must be ignored.\n\n        Returns\n        -------\n        Collection\n            An equivalent Dask collection with the same keys as\n            computed through a different graph.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\n@runtime_checkable\nclass DaskCollection(Protocol):\n    \"\"\"Protocol defining the interface of a Dask collection.\"\"\"\n\n    @abc.abstractmethod\n    def __dask_graph__(self) -> Graph:\n        \"\"\"The Dask task graph.\n\n        The core Dask collections (Array, DataFrame, Bag, and Delayed)\n        use a :py:class:`~dask.highlevelgraph.HighLevelGraph` to\n        represent the collection task graph. It is also possible to\n        represent the task graph as a low level graph using a Python\n        dictionary.\n\n        Returns\n        -------\n        Mapping\n            The Dask task graph. If the instance returns a\n            :py:class:`dask.highlevelgraph.HighLevelGraph` then the\n            :py:func:`__dask_layers__` method must be implemented, as\n            defined by the :py:class:`~dask.typing.HLGDaskCollection`\n            protocol.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_keys__(self) -> NestedKeys:\n        \"\"\"The output keys of the task graph.\n\n        Note that there are additional constraints on keys for a Dask\n        collection than those described in the :doc:`task graph\n        specification documentation <spec>`. These additional\n        constraints are described below.\n\n        All keys must either be non-empty strings or tuples where the first element is a\n        non-empty string, followed by zero or more arbitrary str, bytes, int, float, or\n        tuples thereof. The non-empty string is commonly known as the *collection name*.\n        All collections embedded in the dask package have exactly one name, but this is\n        not a requirement.\n\n        These are all valid outputs:\n\n        - ``[]``\n        - ``[\"x\", \"y\"]``\n        - ``[[(\"y\", \"a\", 0), (\"y\", \"a\", 1)], [(\"y\", \"b\", 0), (\"y\", \"b\", 1)]``\n\n        Returns\n        -------\n        list\n            A possibly nested list of keys that represent the outputs\n            of the graph. After computation, the results will be\n            returned in the same layout, with the keys replaced with\n            their corresponding outputs.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_postcompute__(self) -> tuple[PostComputeCallable, tuple]:\n        \"\"\"Finalizer function and optional arguments to construct final result.\n\n        Upon computation each key in the collection will have an in\n        memory result, the postcompute function combines each key's\n        result into a final in memory representation. For example,\n        dask.array.Array concatenates the arrays at each chunk into a\n        final in-memory array.\n\n        Returns\n        -------\n        PostComputeCallable\n            Callable that receives the sequence of the results of each\n            final key along with optional arguments. An example signature\n            would be ``finalize(results: Sequence[Any], *args)``.\n        tuple[Any, ...]\n            Optional arguments passed to the function following the\n            key results (the `*args` part of the\n            ``PostComputeCallable``. If no additional arguments are to\n            be passed then this must be an empty tuple.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_postpersist__(self) -> tuple[PostPersistCallable, tuple]:\n        \"\"\"Rebuilder function and optional arguments to construct a persisted collection.\n\n        See also the documentation for :py:class:`dask.typing.PostPersistCallable`.\n\n        Returns\n        -------\n        PostPersistCallable\n            Callable that rebuilds the collection. The signature\n            should be\n            ``rebuild(dsk: Mapping, *args: Any, rename: Mapping[str, str] | None)``\n            (as defined by the\n            :py:class:`~dask.typing.PostPersistCallable` protocol).\n            The callable should return an equivalent Dask collection\n            with the same keys as `self`, but with results that are\n            computed through a different graph. In the case of\n            :py:func:`dask.persist`, the new graph will have just the\n            output keys and the values already computed.\n        tuple[Any, ...]\n            Optional arguments passed to the rebuild callable. If no\n            additional arguments are to be passed then this must be an\n            empty tuple.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_tokenize__(self) -> Hashable:\n        \"\"\"Value that must fully represent the object.\"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    __dask_optimize__: Any\n    \"\"\"Given a graph and keys, return a new optimized graph.\n\n    This method can be either a ``staticmethod`` or a ``classmethod``,\n    but not an ``instancemethod``. For example implementations see the\n    definitions of ``__dask_optimize__`` in the core Dask collections:\n    ``dask.array.Array``, ``dask.dataframe.DataFrame``, etc.\n\n    Note that graphs and keys are merged before calling\n    ``__dask_optimize__``; as such, the graph and keys passed to\n    this method may represent more than one collection sharing the\n    same optimize method.\n\n    Parameters\n    ----------\n    dsk : Graph\n        The merged graphs from all collections sharing the same\n        ``__dask_optimize__`` method.\n    keys : Sequence[Key]\n        A list of the outputs from ``__dask_keys__`` from all\n        collections sharing the same ``__dask_optimize__`` method.\n    **kwargs : Any\n        Extra keyword arguments forwarded from the call to\n        ``compute`` or ``persist``. Can be used or ignored as\n        needed.\n\n    Returns\n    -------\n    MutableMapping\n        The optimized Dask graph.\n\n    \"\"\"\n\n    # FIXME: It is currently not possible to define a staticmethod from a callback protocol\n    # Also, the definition in `is_dask_collection` cannot be satisfied by a\n    # protocol / typing check\n    # staticmethod[SchedulerGetCallable]\n    __dask_scheduler__: staticmethod\n    \"\"\"The default scheduler ``get`` to use for this object.\n\n    Usually attached to the class as a staticmethod, e.g.:\n\n    >>> import dask.threaded\n    >>> class MyCollection:\n    ...     # Use the threaded scheduler by default\n    ...     __dask_scheduler__ = staticmethod(dask.threaded.get)\n\n    \"\"\"\n\n    @abc.abstractmethod\n    def compute(self, **kwargs: Any) -> Any:\n        \"\"\"Compute this dask collection.\n\n        This turns a lazy Dask collection into its in-memory\n        equivalent. For example a Dask array turns into a NumPy array\n        and a Dask dataframe turns into a Pandas dataframe. The entire\n        dataset must fit into memory before calling this operation.\n\n        Parameters\n        ----------\n        scheduler : string, optional\n            Which scheduler to use like \"threads\", \"synchronous\" or\n            \"processes\". If not provided, the default is to check the\n            global settings first, and then fall back to the\n            collection defaults.\n        optimize_graph : bool, optional\n            If True [default], the graph is optimized before\n            computation. Otherwise the graph is run as is. This can be\n            useful for debugging.\n        kwargs :\n            Extra keywords to forward to the scheduler function.\n\n        Returns\n        -------\n        The collection's computed result.\n\n        See Also\n        --------\n        dask.compute\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def persist(self: CollType, **kwargs: Any) -> CollType:\n        \"\"\"Persist this dask collection into memory\n\n        This turns a lazy Dask collection into a Dask collection with\n        the same metadata, but now with the results fully computed or\n        actively computing in the background.\n\n        The action of function differs significantly depending on the\n        active task scheduler. If the task scheduler supports\n        asynchronous computing, such as is the case of the\n        dask.distributed scheduler, then persist will return\n        *immediately* and the return value's task graph will contain\n        Dask Future objects. However if the task scheduler only\n        supports blocking computation then the call to persist will\n        *block* and the return value's task graph will contain\n        concrete Python results.\n\n        This function is particularly useful when using distributed\n        systems, because the results will be kept in distributed\n        memory, rather than returned to the local process as with\n        compute.\n\n        Parameters\n        ----------\n        scheduler : string, optional\n            Which scheduler to use like \"threads\", \"synchronous\" or\n            \"processes\". If not provided, the default is to check the\n            global settings first, and then fall back to the\n            collection defaults.\n        optimize_graph : bool, optional\n            If True [default], the graph is optimized before\n            computation. Otherwise the graph is run as is. This can be\n            useful for debugging.\n        **kwargs\n            Extra keywords to forward to the scheduler function.\n\n        Returns\n        -------\n        New dask collections backed by in-memory data\n\n        See Also\n        --------\n        dask.persist\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def visualize(\n        self,\n        filename: str = \"mydask\",\n        format: str | None = None,\n        optimize_graph: bool = False,\n        **kwargs: Any,\n    ) -> DisplayObject | None:\n        \"\"\"Render the computation of this object's task graph using graphviz.\n\n        Requires ``graphviz`` to be installed.\n\n        Parameters\n        ----------\n        filename : str or None, optional\n            The name of the file to write to disk. If the provided\n            `filename` doesn't include an extension, '.png' will be\n            used by default. If `filename` is None, no file will be\n            written, and we communicate with dot using only pipes.\n        format : {'png', 'pdf', 'dot', 'svg', 'jpeg', 'jpg'}, optional\n            Format in which to write output file. Default is 'png'.\n        optimize_graph : bool, optional\n            If True, the graph is optimized before rendering.\n            Otherwise, the graph is displayed as is. Default is False.\n        color: {None, 'order'}, optional\n            Options to color nodes. Provide ``cmap=`` keyword for\n            additional colormap\n        **kwargs\n           Additional keyword arguments to forward to ``to_graphviz``.\n\n        Examples\n        --------\n        >>> x.visualize(filename='dask.pdf')  # doctest: +SKIP\n        >>> x.visualize(filename='dask.pdf', color='order')  # doctest: +SKIP\n\n        Returns\n        -------\n        result : IPython.display.Image, IPython.display.SVG, or None\n            See dask.dot.dot_graph for more information.\n\n        See Also\n        --------\n        dask.visualize\n        dask.dot.dot_graph\n\n        Notes\n        -----\n        For more information on optimization see here:\n\n        https://docs.dask.org/en/latest/optimize.html\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\n@runtime_checkable\nclass HLGDaskCollection(DaskCollection, Protocol):\n    \"\"\"Protocol defining a Dask collection that uses HighLevelGraphs.\n\n    This protocol is nearly identical to\n    :py:class:`~dask.typing.DaskCollection`, with the addition of the\n    ``__dask_layers__`` method (required for collections backed by\n    high level graphs).\n\n    \"\"\"\n\n    @abc.abstractmethod\n    def __dask_layers__(self) -> Sequence[str]:\n        \"\"\"Names of the HighLevelGraph layers.\"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\nclass _NoDefault(Enum):\n    \"\"\"typing-aware constant to detect when the user omits a parameter and you can't use\n    None.\n\n    Copied from pandas._libs.lib._NoDefault.\n\n    Usage\n    -----\n    from dask.typing import NoDefault, no_default\n\n    def f(x: int | None | NoDefault = no_default) -> int:\n        if x is no_default:\n            ...\n    \"\"\"\n\n    no_default = \"NO_DEFAULT\"\n\n    def __repr__(self) -> str:\n        return \"<no_default>\"\n\n\nno_default = _NoDefault.no_default\nNoDefault = Literal[_NoDefault.no_default]\n\n\n### Dependency File: _task_spec.py\nfrom __future__ import annotations\n\n\"\"\" Task specification for dask\n\nThis module contains the task specification for dask. It is used to represent\nrunnable (task) and non-runnable (data) nodes in a dask graph.\n\nSimple examples of how to express tasks in dask\n-----------------------------------------------\n\n.. code-block:: python\n\n    func(\"a\", \"b\") ~ Task(\"key\", func, \"a\", \"b\")\n\n    [func(\"a\"), func(\"b\")] ~ [Task(\"key-1\", func, \"a\"), Task(\"key-2\", func, \"b\")]\n\n    {\"a\": func(\"b\")} ~ {\"a\": Task(\"a\", func, \"b\")}\n\n    \"literal-string\" ~ DataNode(\"key\", \"literal-string\")\n\n\nKeys, Aliases and TaskRefs\n-------------------------\n\nKeys are used to identify tasks in a dask graph. Every `GraphNode` instance has a\nkey attribute that _should_ reference the key in the dask graph.\n\n.. code-block:: python\n\n    {\"key\": Task(\"key\", func, \"a\")}\n\nReferencing other tasks is possible by using either one of `Alias` or a\n`TaskRef`.\n\n.. code-block:: python\n\n    # TaskRef can be used to provide the name of the reference explicitly\n    t = Task(\"key\", func, TaskRef(\"key-1\"))\n\n    # If a task is still in scope, the method `ref` can be used for convenience\n    t2 = Task(\"key2\", func2, t.ref())\n\n\nExecuting a task\n----------------\n\nA task can be executed by calling it with a dictionary of values. The values\nshould contain the dependencies of the task.\n\n.. code-block:: python\n\n    t = Task(\"key\", add, TaskRef(\"a\"), TaskRef(\"b\"))\n    assert t.dependencies == {\"a\", \"b\"}\n    t({\"a\": 1, \"b\": 2}) == 3\n\n\"\"\"\nimport itertools\nimport sys\nfrom collections import defaultdict\nfrom collections.abc import Callable, Container, Iterable, Mapping, MutableMapping\nfrom functools import partial\nfrom typing import Any, TypeVar, cast\n\nfrom dask.sizeof import sizeof\nfrom dask.typing import Key as KeyType\nfrom dask.utils import funcname, is_namedtuple_instance\n\n_T = TypeVar(\"_T\")\n_T_GraphNode = TypeVar(\"_T_GraphNode\", bound=\"GraphNode\")\n_T_Iterable = TypeVar(\"_T_Iterable\", bound=Iterable)\n\n\n# Ported from more-itertools\n# https://github.com/more-itertools/more-itertools/blob/c8153e2801ade2527f3a6c8b623afae93f5a1ce1/more_itertools/recipes.py#L944-L973\ndef _batched(iterable, n, *, strict=False):\n    \"\"\"Batch data into tuples of length *n*. If the number of items in\n    *iterable* is not divisible by *n*:\n    * The last batch will be shorter if *strict* is ``False``.\n    * :exc:`ValueError` will be raised if *strict* is ``True``.\n\n    >>> list(batched('ABCDEFG', 3))\n    [('A', 'B', 'C'), ('D', 'E', 'F'), ('G',)]\n\n    On Python 3.13 and above, this is an alias for :func:`itertools.batched`.\n    \"\"\"\n    if n < 1:\n        raise ValueError(\"n must be at least one\")\n    it = iter(iterable)\n    while batch := tuple(itertools.islice(it, n)):\n        if strict and len(batch) != n:\n            raise ValueError(\"batched(): incomplete batch\")\n        yield batch\n\n\nif sys.hexversion >= 0x30D00A2:\n\n    def batched(iterable, n, *, strict=False):\n        return itertools.batched(iterable, n, strict=strict)\n\nelse:\n    batched = _batched\n\n    batched.__doc__ = _batched.__doc__\n# End port\n\n\ndef identity(*args):\n    return args\n\n\ndef _identity_cast(*args, typ):\n    return typ(args)\n\n\n_anom_count = itertools.count()\n\n\ndef parse_input(obj: Any) -> object:\n    \"\"\"Tokenize user input into GraphNode objects\n\n    Note: This is similar to `convert_legacy_task` but does not\n    - compare any values to a global set of known keys to infer references/futures\n    - parse tuples and interprets them as runnable tasks\n    - Deal with SubgraphCallables\n\n    Parameters\n    ----------\n    obj : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    if isinstance(obj, GraphNode):\n        return obj\n\n    if isinstance(obj, TaskRef):\n        return Alias(obj.key)\n\n    if isinstance(obj, dict):\n        parsed_dict = {k: parse_input(v) for k, v in obj.items()}\n        if any(isinstance(v, GraphNode) for v in parsed_dict.values()):\n            return Dict(parsed_dict)\n\n    if isinstance(obj, (list, set, tuple)):\n        parsed_collection = tuple(parse_input(o) for o in obj)\n        if any(isinstance(o, GraphNode) for o in parsed_collection):\n            if isinstance(obj, list):\n                return List(*parsed_collection)\n            if isinstance(obj, set):\n                return Set(*parsed_collection)\n            if isinstance(obj, tuple):\n                if is_namedtuple_instance(obj):\n                    return _wrap_namedtuple_task(None, obj, parse_input)\n                return Tuple(*parsed_collection)\n\n    return obj\n\n\ndef _wrap_namedtuple_task(k, obj, parser):\n    if hasattr(obj, \"__getnewargs_ex__\"):\n        new_args, kwargs = obj.__getnewargs_ex__()\n        kwargs = {k: parser(v) for k, v in kwargs.items()}\n    elif hasattr(obj, \"__getnewargs__\"):\n        new_args = obj.__getnewargs__()\n        kwargs = {}\n\n    args_converted = parse_input(type(new_args)(map(parser, new_args)))\n\n    return Task(\n        k, partial(_instantiate_named_tuple, type(obj)), args_converted, Dict(kwargs)\n    )\n\n\ndef _instantiate_named_tuple(typ, args, kwargs):\n    return typ(*args, **kwargs)\n\n\nclass _MultiContainer(Container):\n    container: tuple\n    __slots__ = (\"container\",)\n\n    def __init__(self, *container):\n        self.container = container\n\n    def __contains__(self, o: object) -> bool:\n        for c in self.container:\n            if o in c:\n                return True\n        return False\n\n\nSubgraphType = None\n\n\ndef _execute_subgraph(inner_dsk, outkey, inkeys, *dependencies):\n    final = {}\n    final.update(inner_dsk)\n    for k, v in zip(inkeys, dependencies):\n        final[k] = DataNode(None, v)\n    res = execute_graph(final, keys=[outkey])\n    return res[outkey]\n\n\ndef convert_legacy_task(\n    key: KeyType | None,\n    task: _T,\n    all_keys: Container,\n) -> GraphNode | _T:\n    if isinstance(task, GraphNode):\n        return task\n\n    if type(task) is tuple and task and callable(task[0]):\n        func, args = task[0], task[1:]\n        new_args = []\n        new: object\n        for a in args:\n            if isinstance(a, dict):\n                new = Dict(a)\n            else:\n                new = convert_legacy_task(None, a, all_keys)\n            new_args.append(new)\n        return Task(key, func, *new_args)\n    try:\n        if isinstance(task, (bytes, int, float, str, tuple)):\n            if task in all_keys:\n                if key is None:\n                    return Alias(task)\n                else:\n                    return Alias(key, target=task)\n    except TypeError:\n        # Unhashable\n        pass\n\n    if isinstance(task, (list, tuple, set, frozenset)):\n        if is_namedtuple_instance(task):\n            return _wrap_namedtuple_task(\n                key,\n                task,\n                partial(\n                    convert_legacy_task,\n                    None,\n                    all_keys=all_keys,\n                ),\n            )\n        else:\n            parsed_args = tuple(convert_legacy_task(None, t, all_keys) for t in task)\n            if any(isinstance(a, GraphNode) for a in parsed_args):\n                return Task(key, _identity_cast, *parsed_args, typ=type(task))\n            else:\n                return cast(_T, type(task)(parsed_args))\n    elif isinstance(task, TaskRef):\n        if key is None:\n            return Alias(task.key)\n        else:\n            return Alias(key, target=task.key)\n    else:\n        return task\n\n\ndef convert_legacy_graph(\n    dsk: Mapping,\n    all_keys: Container | None = None,\n):\n    if all_keys is None:\n        all_keys = set(dsk)\n    new_dsk = {}\n    for k, arg in dsk.items():\n        t = convert_legacy_task(k, arg, all_keys)\n        if isinstance(t, Alias) and t.target == k:\n            continue\n        elif not isinstance(t, GraphNode):\n            t = DataNode(k, t)\n        new_dsk[k] = t\n    return new_dsk\n\n\ndef resolve_aliases(dsk: dict, keys: set, dependents: dict) -> dict:\n    \"\"\"Remove trivial sequential alias chains\n\n    Example:\n\n        dsk = {'x': 1, 'y': Alias('x'), 'z': Alias('y')}\n\n        resolve_aliases(dsk, {'z'}, {'x': {'y'}, 'y': {'z'}}) == {'z': 1}\n\n    \"\"\"\n    if not keys:\n        raise ValueError(\"No keys provided\")\n    dsk = dict(dsk)\n    work = list(keys)\n    seen = set()\n    while work:\n        k = work.pop()\n        if k in seen or k not in dsk:\n            continue\n        seen.add(k)\n        t = dsk[k]\n        if isinstance(t, Alias):\n            target_key = t.target\n            # Rules for when we allow to collapse an alias\n            # 1. The target key is not in the keys set. The keys set is what the\n            #    user is requesting and by collapsing we'd no longer be able to\n            #    return that result.\n            # 2. The target key is in fact part of dsk. If it isnt' this could\n            #    point to a persisted dependency and we cannot collapse it.\n            # 3. The target key has only one dependent which is the key we're\n            #    currently looking at. This means that there is a one to one\n            #    relation between this and the target key in which case we can\n            #    collapse them.\n            #    Note: If target was an alias as well, we could continue with\n            #    more advanced optimizations but this isn't implemented, yet\n            if (\n                target_key not in keys\n                and target_key in dsk\n                # Note: whenever we're performing a collapse, we're not updating\n                # the dependents. The length == 1 should still be sufficient for\n                # chains of these aliases\n                and len(dependents[target_key]) == 1\n            ):\n                tnew = dsk.pop(target_key).copy()\n\n                dsk[k] = tnew\n                tnew.key = k\n                if isinstance(tnew, Alias):\n                    work.append(k)\n                    seen.discard(k)\n                else:\n                    work.extend(tnew.dependencies)\n\n        work.extend(t.dependencies)\n    return dsk\n\n\nclass TaskRef:\n    val: KeyType\n    __slots__ = (\"key\",)\n\n    def __init__(self, key: KeyType):\n        self.key = key\n\n    def __str__(self):\n        return str(self.key)\n\n    def __repr__(self):\n        return f\"{type(self).__name__}({self.key!r})\"\n\n    def __hash__(self) -> int:\n        return hash(self.key)\n\n    def __eq__(self, value: object) -> bool:\n        if not isinstance(value, TaskRef):\n            return False\n        return self.key == value.key\n\n\nclass GraphNode:\n    key: KeyType\n    _dependencies: frozenset\n\n    __slots__ = tuple(__annotations__)\n\n    def ref(self):\n        return Alias(self.key)\n\n    def copy(self):\n        raise NotImplementedError\n\n    @property\n    def data_producer(self) -> bool:\n        return False\n\n    @property\n    def dependencies(self) -> frozenset:\n        return self._dependencies\n\n    @property\n    def block_fusion(self) -> bool:\n        return False\n\n    def _verify_values(self, values: tuple | dict) -> None:\n        if not self.dependencies:\n            return\n        if missing := set(self.dependencies) - set(values):\n            raise RuntimeError(f\"Not enough arguments provided: missing keys {missing}\")\n\n    def __call__(self, values) -> Any:\n        raise NotImplementedError(\"Not implemented\")\n\n    def __eq__(self, value: object) -> bool:\n        if type(value) is not type(self):\n            return False\n\n        from dask.tokenize import tokenize\n\n        return tokenize(self) == tokenize(value)\n\n    @property\n    def is_coro(self) -> bool:\n        return False\n\n    def __sizeof__(self) -> int:\n        return sum(\n            sizeof(getattr(self, sl)) for sl in type(self).__slots__\n        ) + sys.getsizeof(type(self))\n\n    def substitute(\n        self, subs: dict[KeyType, KeyType | GraphNode], key: KeyType | None = None\n    ) -> GraphNode:\n        \"\"\"Substitute a dependency with a new value. The new value either has to\n        be a new valid key or a GraphNode to replace the dependency entirely.\n\n        The GraphNode will not be mutated but instead a shallow copy will be\n        returned. The substitution will be performed eagerly.\n\n        Parameters\n        ----------\n        subs : dict[KeyType, KeyType  |  GraphNode]\n            The mapping describing the substitutions to be made.\n        key : KeyType | None, optional\n            The key of the new GraphNode object. If None provided, the key of\n            the old one will be reused.\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def fuse(*tasks: GraphNode, key: KeyType | None = None) -> GraphNode:\n        \"\"\"Fuse a set of tasks into a single task.\n\n        The tasks are fused into a single task that will execute the tasks in a\n        subgraph. The internal tasks are no longer accessible from the outside.\n\n        All provided tasks must form a valid subgraph that will reduce to a\n        single key. If multiple outputs are possible with the provided tasks, an\n        exception will be raised.\n\n        The tasks will not be rewritten but instead a new Task will be created\n        that will merely reference the old task objects. This way, Task objects\n        may be reused in multiple fused tasks.\n\n        Parameters\n        ----------\n        key : KeyType | None, optional\n            The key of the new Task object. If None provided, the key of the\n            final task will be used.\n\n        See also\n        --------\n        GraphNode.substitute : Easer substitution of dependencies\n        \"\"\"\n        if any(t.key is None for t in tasks):\n            raise ValueError(\"Cannot fuse tasks with missing keys\")\n        if len(tasks) == 1:\n            return tasks[0].substitute({}, key=key)\n        all_keys = set()\n        all_deps: set[KeyType] = set()\n        for t in tasks:\n            all_deps.update(t.dependencies)\n            all_keys.add(t.key)\n        external_deps = tuple(sorted(all_deps - all_keys, key=hash))\n        leafs = all_keys - all_deps\n        if len(leafs) > 1:\n            raise ValueError(f\"Cannot fuse tasks with multiple outputs {leafs}\")\n\n        outkey = leafs.pop()\n        return Task(\n            key or outkey,\n            _execute_subgraph,\n            {t.key: t for t in tasks},\n            outkey,\n            external_deps,\n            *(TaskRef(k) for k in external_deps),\n            _data_producer=any(t.data_producer for t in tasks),\n        )\n\n\n_no_deps: frozenset = frozenset()\n\n\nclass Alias(GraphNode):\n    target: KeyType\n    __slots__ = tuple(__annotations__)\n\n    def __init__(\n        self, key: KeyType | TaskRef, target: Alias | TaskRef | KeyType | None = None\n    ):\n        if isinstance(key, TaskRef):\n            key = key.key\n        self.key = key\n        if target is None:\n            target = key\n        if isinstance(target, Alias):\n            target = target.target\n        if isinstance(target, TaskRef):\n            target = target.key\n        self.target = target\n        self._dependencies = frozenset((self.target,))\n\n    def copy(self):\n        return Alias(self.key, self.target)\n\n    def substitute(\n        self, subs: dict[KeyType, KeyType | GraphNode], key: KeyType | None = None\n    ) -> GraphNode:\n        if self.key in subs or self.target in subs:\n            sub_key = subs.get(self.key, self.key)\n            val = subs.get(self.target, self.target)\n            if sub_key == self.key and val == self.target:\n                return self\n            if isinstance(val, GraphNode):\n                return val.substitute({}, key=key)\n            if key is None and isinstance(sub_key, GraphNode):\n                raise RuntimeError(\n                    f\"Invalid substitution encountered {self.key!r} -> {sub_key}\"\n                )\n            return Alias(key or sub_key, val)  # type: ignore\n        return self\n\n    def __dask_tokenize__(self):\n        return (type(self).__name__, self.key, self.target)\n\n    def __call__(self, values=()):\n        self._verify_values(values)\n        return values[self.target]\n\n    def __repr__(self):\n        if self.key != self.target:\n            return f\"Alias({self.key!r}->{self.target!r})\"\n        else:\n            return f\"Alias({self.key!r})\"\n\n    def __eq__(self, value: object) -> bool:\n        if not isinstance(value, Alias):\n            return False\n        if self.key != value.key:\n            return False\n        if self.target != value.target:\n            return False\n        return True\n\n\nclass DataNode(GraphNode):\n    value: Any\n    typ: type\n    __slots__ = tuple(__annotations__)\n\n    def __init__(self, key: Any, value: Any):\n        if key is None:\n            key = (type(value).__name__, next(_anom_count))\n        self.key = key\n        self.value = value\n        self.typ = type(value)\n        self._dependencies = _no_deps\n\n    @property\n    def data_producer(self) -> bool:\n        return True\n\n    def copy(self):\n        return DataNode(self.key, self.value)\n\n    def __call__(self, values=()):\n        return self.value\n\n    def __repr__(self):\n        return f\"DataNode({self.value!r})\"\n\n    def __reduce__(self):\n        return (DataNode, (self.key, self.value))\n\n    def __dask_tokenize__(self):\n        from dask.base import tokenize\n\n        return (type(self).__name__, tokenize(self.value))\n\n    def substitute(\n        self, subs: dict[KeyType, KeyType | GraphNode], key: KeyType | None = None\n    ) -> DataNode:\n        if key is not None and key != self.key:\n            return DataNode(key, self.value)\n        return self\n\n    def __iter__(self):\n        return iter(self.value)\n\n\ndef _call_recursively(obj, values):\n    if isinstance(obj, GraphNode):\n        return obj(values)\n    elif isinstance(obj, dict):\n        return {k: _call_recursively(v, values) for k, v in obj.items()}\n    elif isinstance(obj, (list, tuple, frozenset, set)):\n        return type(obj)(_call_recursively(v, values) for v in obj)\n    return obj\n\n\ndef _get_dependencies(obj: object) -> set | frozenset:\n    if isinstance(obj, GraphNode):\n        return obj.dependencies\n    elif isinstance(obj, dict):\n        if not obj:\n            return _no_deps\n        return set().union(*map(_get_dependencies, obj.values()))\n    elif isinstance(obj, (list, tuple, frozenset, set)):\n        if not obj:\n            return _no_deps\n        return set().union(*map(_get_dependencies, obj))\n    return _no_deps\n\n\nclass Task(GraphNode):\n    func: Callable\n    args: tuple\n    kwargs: dict\n    _data_producer: bool\n    _token: str | None\n    _is_coro: bool | None\n    _repr: str | None\n\n    __slots__ = tuple(__annotations__)\n\n    def __init__(\n        self,\n        key: Any,\n        func: Callable,\n        /,\n        *args: Any,\n        _data_producer: bool = False,\n        _dependencies: set | frozenset | None = None,\n        **kwargs: Any,\n    ):\n        self.key = key\n        self.func = func\n        if isinstance(func, Task):\n            raise TypeError(\"Cannot nest tasks\")\n        self.args = tuple(\n            Alias(obj.key) if isinstance(obj, TaskRef) else obj for obj in args\n        )\n        self.kwargs = {\n            k: Alias(v.key) if isinstance(v, TaskRef) else v for k, v in kwargs.items()\n        }\n        if _dependencies is None:\n            _dependencies = set()\n            for a in itertools.chain(self.args, self.kwargs.values()):\n                if isinstance(a, GraphNode):\n                    _dependencies.update(a.dependencies)\n        if _dependencies:\n            self._dependencies = frozenset(_dependencies)\n        else:\n            self._dependencies = _no_deps\n        self._is_coro = None\n        self._token = None\n        self._repr = None\n        self._data_producer = _data_producer\n\n    @property\n    def data_producer(self) -> bool:\n        return self._data_producer\n\n    def copy(self):\n        return type(self)(\n            self.key,\n            self.func,\n            *self.args,\n            _dependencies=self._dependencies,\n            **self.kwargs,\n        )\n\n    def __hash__(self):\n        return hash(self._get_token())\n\n    def _get_token(self) -> str:\n        if self._token:\n            return self._token\n        from dask.base import tokenize\n\n        self._token = tokenize(\n            (\n                type(self).__name__,\n                self.func,\n                self.args,\n                self.kwargs,\n            )\n        )\n        return self._token\n\n    def __dask_tokenize__(self):\n        return self._get_token()\n\n    def __repr__(self) -> str:\n        # When `Task` is deserialized the constructor will not run and\n        # `self._repr` is thus undefined.\n        if not hasattr(self, \"_repr\") or not self._repr:\n            head = funcname(self.func)\n            tail = \")\"\n            label_size = 40\n            args = self.args\n            kwargs = self.kwargs\n            if args or kwargs:\n                label_size2 = int(\n                    (label_size - len(head) - len(tail) - len(str(self.key)))\n                    // (len(args) + len(kwargs))\n                )\n            if args:\n                if label_size2 > 5:\n                    args_repr = \", \".join(repr(t) for t in args)\n                else:\n                    args_repr = \"...\"\n            else:\n                args_repr = \"\"\n            if kwargs:\n                if label_size2 > 5:\n                    kwargs_repr = \", \" + \", \".join(\n                        f\"{k}={repr(v)}\" for k, v in sorted(kwargs.items())\n                    )\n                else:\n                    kwargs_repr = \", ...\"\n            else:\n                kwargs_repr = \"\"\n            self._repr = f\"<Task {self.key!r} {head}({args_repr}{kwargs_repr}{tail}>\"\n        return self._repr\n\n    def __call__(self, values=()):\n        self._verify_values(values)\n        new_argspec = tuple(\n            a({k: values[k] for k in a.dependencies}) if isinstance(a, GraphNode) else a\n            for a in self.args\n        )\n        if self.kwargs:\n            kwargs = {\n                k: (\n                    kw({k: values[k] for k in kw.dependencies})\n                    if isinstance(kw, GraphNode)\n                    else kw\n                )\n                for k, kw in self.kwargs.items()\n            }\n            return self.func(*new_argspec, **kwargs)\n        return self.func(*new_argspec)\n\n    @property\n    def is_coro(self):\n        if self._is_coro is None:\n            # Note: Can't use cached_property on objects without __dict__\n            try:\n                from distributed.utils import iscoroutinefunction\n\n                self._is_coro = iscoroutinefunction(self.func)\n            except Exception:\n                self._is_coro = False\n        return self._is_coro\n\n    def substitute(\n        self, subs: dict[KeyType, KeyType | GraphNode], key: KeyType | None = None\n    ) -> Task:\n        subs_filtered = {\n            k: v for k, v in subs.items() if k in self.dependencies and k != v\n        }\n        if subs_filtered:\n            new_args = tuple(\n                a.substitute(subs_filtered) if isinstance(a, GraphNode) else a\n                for a in self.args\n            )\n            new_kwargs = {\n                k: v.substitute(subs_filtered) if isinstance(v, GraphNode) else v\n                for k, v in self.kwargs.items()\n            }\n            return type(self)(\n                key or self.key,\n                self.func,\n                *new_args,\n                _data_producer=self.data_producer,\n                **new_kwargs,\n            )\n        elif key is None or key == self.key:\n            return self\n        else:\n            # Rename\n            return type(self)(\n                key,\n                self.func,\n                *self.args,\n                _data_producer=self.data_producer,\n                **self.kwargs,\n            )\n\n\nclass NestedContainer(Task):\n    constructor: Callable\n    klass: type\n    __slots__ = tuple(__annotations__)\n\n    def __init__(\n        self,\n        /,\n        *args: Any,\n        _dependencies: set | frozenset | None = None,\n        **kwargs: Any,\n    ):\n        if len(args) == 1 and isinstance(args[0], self.klass):\n            args = args[0]  # type: ignore\n        super().__init__(\n            None,\n            self.to_container,\n            *args,\n            constructor=self.constructor,\n            _dependencies=_dependencies,\n            **kwargs,\n        )\n\n    def __repr__(self):\n        return f\"{type(self).__name__}({self.args})\"\n\n    def substitute(\n        self, subs: dict[KeyType, KeyType | GraphNode], key: KeyType | None = None\n    ) -> NestedContainer:\n        subs_filtered = {\n            k: v for k, v in subs.items() if k in self.dependencies and k != v\n        }\n        if not subs_filtered:\n            return self\n        return type(self)(\n            *(\n                a.substitute(subs_filtered) if isinstance(a, GraphNode) else a\n                for a in self.args\n            )\n        )\n\n    def __dask_tokenize__(self):\n        from dask.tokenize import tokenize\n\n        return (\n            type(self).__name__,\n            self.klass,\n            sorted(tokenize(a) for a in self.args),\n        )\n\n        return super().__dask_tokenize__()\n\n    @staticmethod\n    def to_container(*args, constructor):\n        return constructor(args)\n\n\nclass List(NestedContainer):\n    constructor = klass = list\n\n\nclass Tuple(NestedContainer):\n    constructor = klass = tuple\n\n\nclass Set(NestedContainer):\n    constructor = klass = set\n\n\nclass Dict(NestedContainer):\n    klass = dict\n\n    def __init__(\n        self, /, *args: Any, _dependencies: set | frozenset | None = None, **kwargs: Any\n    ):\n        if args:\n            if len(args) > 1:\n                raise ValueError(\"Dict can only take one positional argument\")\n            kwargs = args[0]\n        super().__init__(\n            *(itertools.chain(*kwargs.items())), _dependencies=_dependencies\n        )\n\n    def __repr__(self):\n        values = \", \".join(f\"{k}: {v}\" for k, v in batched(self.args, 2, strict=True))\n        return f\"Dict({values})\"\n\n    def substitute(\n        self, subs: dict[KeyType, KeyType | GraphNode], key: KeyType | None = None\n    ) -> Dict:\n        subs_filtered = {\n            k: v for k, v in subs.items() if k in self.dependencies and k != v\n        }\n        if not subs_filtered:\n            return self\n\n        new = {}\n        for k, v in batched(self.args, 2, strict=True):\n            v = v.substitute(subs_filtered) if isinstance(v, GraphNode) else v\n            new[k] = v\n        return type(self)(new)\n\n    @staticmethod\n    def constructor(args):\n        return dict(batched(args, 2, strict=True))\n\n\nclass DependenciesMapping(MutableMapping):\n    def __init__(self, dsk):\n        self.dsk = dsk\n        self._removed = set()\n        # Set a copy of dsk to avoid dct resizing\n        self._cache = dsk.copy()\n        self._cache.clear()\n\n    def __getitem__(self, key):\n        if (val := self._cache.get(key)) is not None:\n            return val\n        else:\n            v = self.dsk[key]\n            try:\n                deps = v.dependencies\n            except AttributeError:\n                from dask.core import get_dependencies\n\n                deps = get_dependencies(self.dsk, task=v)\n\n            if self._removed:\n                # deps is a frozenset but for good measure, let's not use -= since\n                # that _may_ perform an inplace mutation\n                deps = deps - self._removed\n            self._cache[key] = deps\n            return deps\n\n    def __iter__(self):\n        return iter(self.dsk)\n\n    def __delitem__(self, key: Any) -> None:\n        self._cache.clear()\n        self._removed.add(key)\n\n    def __setitem__(self, key: Any, value: Any) -> None:\n        raise NotImplementedError\n\n    def __len__(self) -> int:\n        return len(self.dsk)\n\n\nclass _DevNullMapping(MutableMapping):\n    def __getitem__(self, key):\n        raise KeyError(key)\n\n    def __setitem__(self, key, value):\n        pass\n\n    def __delitem__(self, key):\n        pass\n\n    def __len__(self):\n        return 0\n\n    def __iter__(self):\n        return iter(())\n\n\ndef execute_graph(\n    dsk: Iterable[GraphNode] | Mapping[KeyType, GraphNode],\n    cache: MutableMapping[KeyType, object] | None = None,\n    keys: Container[KeyType] | None = None,\n) -> MutableMapping[KeyType, object]:\n    \"\"\"Execute a given graph.\n\n    The graph is exceuted in topological order as defined by dask.order until\n    all leaf nodes, i.e. nodes without any dependents, are reached. The returned\n    dictionary contains the results of the leaf nodes.\n\n    If keys are required that are not part of the graph, they can be provided in the `cache` argument.\n\n    If `keys` is provided, the result will contain only values that are part of the `keys` set.\n\n    \"\"\"\n    if isinstance(dsk, (list, tuple, set, frozenset)):\n        dsk = {t.key: t for t in dsk}\n    else:\n        assert isinstance(dsk, dict)\n\n    refcount: defaultdict[KeyType, int] = defaultdict(int)\n    for vals in DependenciesMapping(dsk).values():\n        for val in vals:\n            refcount[val] += 1\n\n    cache = cache or {}\n    from dask.order import order\n\n    priorities = order(dsk)\n\n    for key, node in sorted(dsk.items(), key=lambda it: priorities[it[0]]):\n        cache[key] = node(cache)\n        for dep in node.dependencies:\n            refcount[dep] -= 1\n            if refcount[dep] == 0 and keys and dep not in keys:\n                del cache[dep]\n\n    return cache\n\n\ndef fuse_linear_task_spec(dsk, keys):\n    \"\"\"\n    keys are the keys from the graph that are requested by a computation. We\n    can't fuse those together.\n    \"\"\"\n    from dask.core import reverse_dict\n    from dask.optimization import default_fused_keys_renamer\n\n    keys = set(keys)\n    dependencies = DependenciesMapping(dsk)\n    dependents = reverse_dict(dependencies)\n\n    seen = set()\n    result = {}\n\n    for key in dsk:\n        if key in seen:\n            continue\n\n        seen.add(key)\n\n        deps = dependencies[key]\n        dependents_key = dependents[key]\n\n        if len(deps) != 1 and len(dependents_key) != 1 or dsk[key].block_fusion:\n            result[key] = dsk[key]\n            continue\n\n        linear_chain = [dsk[key]]\n        top_key = key\n\n        # Walk towards the leafs as long as the nodes have a single dependency\n        # and a single dependent, we can't fuse two nodes of an intermediate node\n        # is the source for 2 dependents\n        while len(deps) == 1:\n            (new_key,) = deps\n            if new_key in seen:\n                break\n            seen.add(new_key)\n            if new_key not in dsk:\n                # This can happen if a future is in the graph, the dependency mapping\n                # adds the key that is referenced by the future as a dependency\n                # see test_futures_to_delayed_array\n                break\n            if (\n                len(dependents[new_key]) != 1\n                or dsk[new_key].block_fusion\n                or new_key in keys\n            ):\n                result[new_key] = dsk[new_key]\n                break\n            # backwards comp for new names, temporary until is_rootish is removed\n            linear_chain.insert(0, dsk[new_key])\n            deps = dependencies[new_key]\n\n        # Walk the tree towards the root as long as the nodes have a single dependent\n        # and a single dependency, we can't fuse two nodes if node has multiple\n        # dependencies\n        while len(dependents_key) == 1 and top_key not in keys:\n            new_key = dependents_key.pop()\n            if new_key in seen:\n                break\n            seen.add(new_key)\n            if len(dependencies[new_key]) != 1 or dsk[new_key].block_fusion:\n                # Exit if the dependent has multiple dependencies, triangle\n                result[new_key] = dsk[new_key]\n                break\n            linear_chain.append(dsk[new_key])\n            top_key = new_key\n            dependents_key = dependents[new_key]\n\n        if len(linear_chain) == 1:\n            result[top_key] = linear_chain[0]\n        else:\n            # Renaming the keys is necessary to preserve the rootish detection for now\n            renamed_key = default_fused_keys_renamer([tsk.key for tsk in linear_chain])\n            result[renamed_key] = Task.fuse(*linear_chain, key=renamed_key)\n            if renamed_key != top_key:\n                # Having the same prefixes can result in the same key, i.e. getitem-hash -> getitem-hash\n                result[top_key] = Alias(top_key, target=renamed_key)\n    return result\n\n\ndef cull(\n    dsk: dict[KeyType, GraphNode], keys: Iterable[KeyType]\n) -> dict[KeyType, GraphNode]:\n    work = set(keys)\n    seen: set[KeyType] = set()\n    dsk2 = {}\n    wpop = work.pop\n    wupdate = work.update\n    sadd = seen.add\n    while work:\n        k = wpop()\n        if k in seen or k not in dsk:\n            continue\n        sadd(k)\n        dsk2[k] = v = dsk[k]\n        wupdate(v.dependencies)\n    return dsk2\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:36:31"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport pickle\nfrom collections import namedtuple\n\nimport pytest\n\nfrom dask.core import (\n    flatten,\n    get,\n    get_dependencies,\n    get_deps,\n    getcycle,\n    ishashable,\n    iskey,\n    istask,\n    literal,\n    preorder_traversal,\n    quote,\n    subs,\n    validate_key,\n)\nfrom dask.utils_test import GetFunctionTestMixin, add, inc\n\n\ndef contains(a, b):\n    \"\"\"\n\n    >>> contains({'x': 1, 'y': 2}, {'x': 1})\n    True\n    >>> contains({'x': 1, 'y': 2}, {'z': 3})\n    False\n    \"\"\"\n    return all(a.get(k) == v for k, v in b.items())\n\n\ndef test_ishashable():\n    class C:\n        pass\n\n    assert ishashable(\"x\")\n    assert ishashable(1)\n    assert ishashable(C())\n    assert ishashable((1, 2))\n    assert not ishashable([1, 2])\n    assert not ishashable({1: 2})\n\n\ndef test_iskey():\n    class C:\n        pass\n\n    assert iskey(\"x\")\n    assert iskey(1)\n    assert not iskey(C())  # Custom hashables can't be dask keys\n    assert not iskey((C(),))\n    assert iskey((1, 2))\n    assert iskey(())\n    assert iskey((\"x\",))\n    assert not iskey([1, 2])\n    assert not iskey({1, 2})\n    assert not iskey({1: 2})\n\n\ndef test_iskey_numpy_types():\n    np = pytest.importorskip(\"numpy\")\n    one = np.int64(1)\n    assert not iskey(one)\n    assert not iskey((\"foo\", one))\n\n\ndef test_validate_key():\n    validate_key(1)\n    validate_key((\"x\", 1))\n    with pytest.raises(TypeError, match=\"Unexpected key type.*list\"):\n        validate_key([\"x\", 1])\n\n    with pytest.raises(TypeError, match=\"unexpected key type at index=1\"):\n        validate_key((2, int))\n\n\ndef test_istask():\n    assert istask((inc, 1))\n    assert not istask(1)\n    assert not istask((1, 2))\n    f = namedtuple(\"f\", [\"x\", \"y\"])\n    assert not istask(f(sum, 2))\n\n\ndef test_preorder_traversal():\n    t = (add, 1, 2)\n    assert list(preorder_traversal(t)) == [add, 1, 2]\n    t = (add, (add, 1, 2), (add, 3, 4))\n    assert list(preorder_traversal(t)) == [add, add, 1, 2, add, 3, 4]\n    t = (add, (sum, [1, 2]), 3)\n    assert list(preorder_traversal(t)) == [add, sum, list, 1, 2, 3]\n\n\nclass TestGet(GetFunctionTestMixin):\n    get = staticmethod(get)\n\n\ndef test_GetFunctionTestMixin_class():\n    class TestCustomGetFail(GetFunctionTestMixin):\n        get = staticmethod(lambda x, y: 1)\n\n    custom_testget = TestCustomGetFail()\n    pytest.raises(AssertionError, custom_testget.test_get)\n\n    class TestCustomGetPass(GetFunctionTestMixin):\n        get = staticmethod(get)\n\n    custom_testget = TestCustomGetPass()\n    custom_testget.test_get()\n\n\ndef test_get_dependencies_nested():\n    dsk = {\"x\": 1, \"y\": 2, \"z\": (add, (inc, [[\"x\"]]), \"y\")}\n\n    assert get_dependencies(dsk, \"z\") == {\"x\", \"y\"}\n    assert sorted(get_dependencies(dsk, \"z\", as_list=True)) == [\"x\", \"y\"]\n\n\ndef test_get_dependencies_empty():\n    dsk = {\"x\": (inc,)}\n    assert get_dependencies(dsk, \"x\") == set()\n    assert get_dependencies(dsk, \"x\", as_list=True) == []\n\n\ndef test_get_dependencies_list():\n    dsk = {\"x\": 1, \"y\": 2, \"z\": [\"x\", [(inc, \"y\")]]}\n    assert get_dependencies(dsk, \"z\") == {\"x\", \"y\"}\n    assert sorted(get_dependencies(dsk, \"z\", as_list=True)) == [\"x\", \"y\"]\n\n\ndef test_get_dependencies_task():\n    dsk = {\"x\": 1, \"y\": 2, \"z\": [\"x\", [(inc, \"y\")]]}\n    assert get_dependencies(dsk, task=(inc, \"x\")) == {\"x\"}\n    assert get_dependencies(dsk, task=(inc, \"x\"), as_list=True) == [\"x\"]\n\n\ndef test_get_dependencies_nothing():\n    with pytest.raises(ValueError):\n        get_dependencies({})\n\n\ndef test_get_dependencies_many():\n    dsk = {\n        \"a\": [1, 2, 3],\n        \"b\": \"a\",\n        \"c\": [1, (inc, 1)],\n        \"d\": [(sum, \"c\")],\n        \"e\": [\"a\", \"b\", \"zzz\"],\n        \"f\": [[\"a\", \"b\"], 2, 3],\n    }\n\n    tasks = [dsk[k] for k in (\"d\", \"f\")]\n    s = get_dependencies(dsk, task=tasks)\n    assert s == {\"a\", \"b\", \"c\"}\n    s = get_dependencies(dsk, task=tasks, as_list=True)\n    assert sorted(s) == [\"a\", \"b\", \"c\"]\n\n    s = get_dependencies(dsk, task=[])\n    assert s == set()\n    s = get_dependencies(dsk, task=[], as_list=True)\n    assert s == []\n\n\ndef test_get_dependencies_task_none():\n    # Regression test for https://github.com/dask/distributed/issues/2756\n    dsk = {\"foo\": None}\n    assert get_dependencies(dsk, task=dsk[\"foo\"]) == set()\n\n\ndef test_get_deps():\n    dsk = {\n        \"a\": [1, 2, 3],\n        \"b\": \"a\",\n        \"c\": [1, (inc, 1)],\n        \"d\": [(sum, \"c\")],\n        \"e\": [\"b\", \"zzz\", \"b\"],\n        \"f\": [[\"a\", \"b\"], 2, 3],\n    }\n    dependencies, dependents = get_deps(dsk)\n    assert dependencies == {\n        \"a\": set(),\n        \"b\": {\"a\"},\n        \"c\": set(),\n        \"d\": {\"c\"},\n        \"e\": {\"b\"},\n        \"f\": {\"a\", \"b\"},\n    }\n    assert dependents == {\n        \"a\": {\"b\", \"f\"},\n        \"b\": {\"e\", \"f\"},\n        \"c\": {\"d\"},\n        \"d\": set(),\n        \"e\": set(),\n        \"f\": set(),\n    }\n\n\ndef test_flatten():\n    assert list(flatten(())) == []\n    assert list(flatten(\"foo\")) == [\"foo\"]\n\n\ndef test_subs():\n    assert subs((sum, [1, \"x\"]), \"x\", 2) == (sum, [1, 2])\n    assert subs((sum, [1, [\"x\"]]), \"x\", 2) == (sum, [1, [2]])\n\n\nclass MutateOnEq:\n    hit_eq = 0\n\n    def __eq__(self, other):\n        self.hit_eq += 1\n        return False\n\n\ndef test_subs_no_key_data_eq():\n    # Numpy throws a deprecation warning on bool(array == scalar), which\n    # pollutes the terminal. This test checks that `subs` never tries to\n    # compare keys (scalars) with values (which could be arrays)`subs` never\n    # tries to compare keys (scalars) with values (which could be arrays).\n    a = MutateOnEq()\n    subs(a, \"x\", 1)\n    assert a.hit_eq == 0\n    subs((add, a, \"x\"), \"x\", 1)\n    assert a.hit_eq == 0\n\n\ndef test_subs_with_unfriendly_eq():\n    try:\n        import numpy as np\n    except ImportError:\n        return\n    else:\n        task = (np.sum, np.array([1, 2]))\n        assert (subs(task, (4, 5), 1) == task) is True\n\n    class MyException(Exception):\n        pass\n\n    class F:\n        def __eq__(self, other):\n            raise MyException()\n\n    task = F()\n    assert subs(task, 1, 2) is task\n\n\ndef test_subs_with_surprisingly_friendly_eq():\n    try:\n        import pandas as pd\n    except ImportError:\n        return\n    else:\n        df = pd.DataFrame()\n        assert subs(df, \"x\", 1) is df\n\n\ndef test_subs_arbitrary_key():\n    key = (1.2, \"foo\", (3,))\n    assert subs((id, key), key, 1) == (id, 1)\n\n\ndef test_quote():\n    literals = [[1, 2, 3], (add, 1, 2), [1, [2, 3]], (add, 1, (add, 2, 3)), {\"x\": \"x\"}]\n\n    for l in literals:\n        assert get({\"x\": quote(l)}, \"x\") == l\n\n\ndef test_literal_serializable():\n    l = literal((add, 1, 2))\n    assert pickle.loads(pickle.dumps(l)).data == (add, 1, 2)\n\n\ndef test_getcycle():\n    dsk = {\n        0: [7, 13, 7, 9, 13, 3, 9, 18, 18, 17],\n        1: [14, 14, 12, 1, 9, 16, 4, 5, 9, 8],\n        2: [3, 1, 7, 7, 2, 0, 0, 6, 3, 2],\n        3: [4, 8, 3, 14, 15, 19, 14, 1, 9, 1],\n        4: [9, 13, 19, 4, 16, 8, 11, 1, 16, 1],\n        5: [9, 8, 12, 13, 10, 13, 19, 3, 18, 18],\n        6: [10, 2, 13, 16, 3, 12, 7, 16, 5, 17],\n        7: [16, 8, 6, 4, 4, 10, 3, 1, 13, 10],\n        8: [11, 4, 12, 10, 14, 6, 18, 15, 16, 12],\n        9: [17, 18, 5, 16, 19, 16, 3, 6, 16, 18],\n    }\n    assert len(getcycle(dsk, list(dsk))) <= 3  # 7->6->7\n    dsk = {\n        0: [1, 27, 6, 25, 30, 16, 1, 1, 35, 17],\n        1: [35, 22, 21, 31, 2, 28, 36, 0, 29, 29],\n        2: [14, 27, 10, 1, 38, 18, 28, 28, 6, 0],\n        3: [0, 37, 7, 16, 38, 28, 34, 13, 30, 10],\n        4: [22, 22, 27, 13, 29, 36, 22, 9, 39, 19],\n        5: [38, 7, 18, 17, 33, 5, 29, 11, 23, 30],\n        6: [3, 30, 28, 38, 4, 13, 14, 27, 29, 38],\n        7: [22, 27, 12, 2, 22, 16, 34, 15, 18, 16],\n        8: [36, 21, 24, 22, 28, 38, 21, 2, 4, 24],\n        9: [38, 32, 38, 7, 31, 34, 39, 20, 30, 18],\n    }\n    assert len(getcycle(dsk, list(dsk))) <= 4  # 0->1->2->0\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_dot\\test_dot.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport os\nimport re\nfrom functools import partial\n\nfrom dask.core import get_dependencies, ishashable, istask\nfrom dask.utils import apply, funcname, import_required, key_split\n\n\ndef task_label(task):\n    \"\"\"Label for a task on a dot graph.\n\n    Examples\n    --------\n    >>> from operator import add\n    >>> task_label((add, 1, 2))\n    'add'\n    >>> task_label((add, (add, 1, 2), 3))\n    'add(...)'\n    \"\"\"\n    func = task[0]\n    if func is apply:\n        func = task[1]\n    if hasattr(func, \"funcs\"):\n        if len(func.funcs) > 1:\n            return f\"{funcname(func.funcs[0])}(...)\"\n        else:\n            head = funcname(func.funcs[0])\n    else:\n        head = funcname(func)\n    if any(has_sub_tasks(i) for i in task[1:]):\n        return f\"{head}(...)\"\n    else:\n        return head\n\n\ndef has_sub_tasks(task):\n    \"\"\"Returns True if the task has sub tasks\"\"\"\n    if istask(task):\n        return True\n    elif isinstance(task, list):\n        return any(has_sub_tasks(i) for i in task)\n    else:\n        return False\n\n\ndef name(x):\n    try:\n        return str(hash(x))\n    except TypeError:\n        return str(hash(str(x)))\n\n\n_HASHPAT = re.compile(\"([0-9a-z]{32})\")\n_UUIDPAT = re.compile(\"([0-9a-z]{8}-[0-9a-z]{4}-[0-9a-z]{4}-[0-9a-z]{4}-[0-9a-z]{12})\")\n\n\ndef label(x, cache=None):\n    \"\"\"\n\n    >>> label('x')\n    'x'\n\n    >>> label(('x', 1))\n    \"('x', 1)\"\n\n    >>> from hashlib import md5\n    >>> x = 'x-%s-hello' % md5(b'1234').hexdigest()\n    >>> x\n    'x-81dc9bdb52d04dc20036dbd8313ed055-hello'\n\n    >>> label(x)\n    'x-#-hello'\n\n    >>> from uuid import uuid1\n    >>> x = 'x-%s-hello' % uuid1()\n    >>> x  # doctest: +SKIP\n    'x-4c1a3d7e-0b45-11e6-8334-54ee75105593-hello'\n\n    >>> label(x)\n    'x-#-hello'\n    \"\"\"\n    s = str(x)\n    for pattern in (_HASHPAT, _UUIDPAT):\n        m = re.search(pattern, s)\n        if m is not None:\n            for h in m.groups():\n                if cache is not None:\n                    n = cache.get(h, len(cache))\n                    label = f\"#{n}\"\n                    # cache will be overwritten destructively\n                    cache[h] = n\n                else:\n                    label = \"#\"\n                s = s.replace(h, label)\n    return s\n\n\ndef box_label(key, verbose=False):\n    \"\"\"Label boxes in graph by chunk index\n\n    >>> box_label(('x', 1, 2, 3))\n    '(1, 2, 3)'\n    >>> box_label(('x', 123))\n    '123'\n    >>> box_label('x')\n    ''\n    \"\"\"\n    if isinstance(key, tuple):\n        key = key[1:]\n        if len(key) == 1:\n            [key] = key\n        return str(key)\n    elif verbose:\n        return str(key)\n    else:\n        return \"\"\n\n\ndef to_graphviz(\n    dsk,\n    data_attributes=None,\n    function_attributes=None,\n    rankdir=\"BT\",\n    graph_attr=None,\n    node_attr=None,\n    edge_attr=None,\n    collapse_outputs=False,\n    verbose=False,\n    **kwargs,\n):\n    graphviz = import_required(\n        \"graphviz\",\n        \"Drawing dask graphs with the graphviz engine requires the `graphviz` \"\n        \"python library and the `graphviz` system library.\\n\\n\"\n        \"Please either conda or pip install as follows:\\n\\n\"\n        \"  conda install python-graphviz     # either conda install\\n\"\n        \"  python -m pip install graphviz    # or pip install and follow installation instructions\",\n    )\n\n    data_attributes = data_attributes or {}\n    function_attributes = function_attributes or {}\n    graph_attr = graph_attr or {}\n    node_attr = node_attr or {}\n    edge_attr = edge_attr or {}\n\n    graph_attr[\"rankdir\"] = rankdir\n    node_attr[\"fontname\"] = \"helvetica\"\n\n    graph_attr.update(kwargs)\n    g = graphviz.Digraph(\n        graph_attr=graph_attr, node_attr=node_attr, edge_attr=edge_attr\n    )\n\n    seen = set()\n    connected = set()\n\n    for k, v in dsk.items():\n        k_name = name(k)\n        if istask(v):\n            func_name = name((k, \"function\")) if not collapse_outputs else k_name\n            if collapse_outputs or func_name not in seen:\n                seen.add(func_name)\n                attrs = function_attributes.get(k, {}).copy()\n                attrs.setdefault(\"label\", key_split(k))\n                attrs.setdefault(\"shape\", \"circle\")\n                g.node(func_name, **attrs)\n            if not collapse_outputs:\n                g.edge(func_name, k_name)\n                connected.add(func_name)\n                connected.add(k_name)\n\n            for dep in get_dependencies(dsk, k):\n                dep_name = name(dep)\n                if dep_name not in seen:\n                    seen.add(dep_name)\n                    attrs = data_attributes.get(dep, {}).copy()\n                    attrs.setdefault(\"label\", box_label(dep, verbose))\n                    attrs.setdefault(\"shape\", \"box\")\n                    g.node(dep_name, **attrs)\n                g.edge(dep_name, func_name)\n                connected.add(dep_name)\n                connected.add(func_name)\n\n        elif ishashable(v) and v in dsk:\n            v_name = name(v)\n            g.edge(v_name, k_name)\n            connected.add(v_name)\n            connected.add(k_name)\n\n        if (not collapse_outputs or k_name in connected) and k_name not in seen:\n            seen.add(k_name)\n            attrs = data_attributes.get(k, {}).copy()\n            attrs.setdefault(\"label\", box_label(k, verbose))\n            attrs.setdefault(\"shape\", \"box\")\n            g.node(k_name, **attrs)\n    return g\n\n\nIPYTHON_IMAGE_FORMATS = frozenset([\"jpeg\", \"png\"])\nIPYTHON_NO_DISPLAY_FORMATS = frozenset([\"dot\", \"pdf\"])\n\n\ndef _get_display_cls(format):\n    \"\"\"\n    Get the appropriate IPython display class for `format`.\n\n    Returns `IPython.display.SVG` if format=='svg', otherwise\n    `IPython.display.Image`.\n\n    If IPython is not importable, return dummy function that swallows its\n    arguments and returns None.\n    \"\"\"\n    dummy = lambda *args, **kwargs: None\n    try:\n        import IPython.display as display\n    except ImportError:\n        # Can't return a display object if no IPython.\n        return dummy\n\n    if format in IPYTHON_NO_DISPLAY_FORMATS:\n        # IPython can't display this format natively, so just return None.\n        return dummy\n    elif format in IPYTHON_IMAGE_FORMATS:\n        # Partially apply `format` so that `Image` and `SVG` supply a uniform\n        # interface to the caller.\n        return partial(display.Image, format=format)\n    elif format == \"svg\":\n        return display.SVG\n    else:\n        raise ValueError(\"Unknown format '%s' passed to `dot_graph`\" % format)\n\n\ndef dot_graph(dsk, filename=\"mydask\", format=None, **kwargs):\n    \"\"\"\n    Render a task graph using dot.\n\n    If `filename` is not None, write a file to disk with the specified name and extension.\n    If no extension is specified, '.png' will be used by default.\n\n    Parameters\n    ----------\n    dsk : dict\n        The graph to display.\n    filename : str or None, optional\n        The name of the file to write to disk. If the provided `filename`\n        doesn't include an extension, '.png' will be used by default.\n        If `filename` is None, no file will be written, and we communicate\n        with dot using only pipes.  Default is 'mydask'.\n    format : {'png', 'pdf', 'dot', 'svg', 'jpeg', 'jpg'}, optional\n        Format in which to write output file.  Default is 'png'.\n    **kwargs\n        Additional keyword arguments to forward to `to_graphviz`.\n\n    Returns\n    -------\n    result : None or IPython.display.Image or IPython.display.SVG  (See below.)\n\n    Notes\n    -----\n    If IPython is installed, we return an IPython.display object in the\n    requested format.  If IPython is not installed, we just return None.\n\n    We always return None if format is 'pdf' or 'dot', because IPython can't\n    display these formats natively. Passing these formats with filename=None\n    will not produce any useful output.\n\n    See Also\n    --------\n    dask.dot.to_graphviz\n    \"\"\"\n    g = to_graphviz(dsk, **kwargs)\n    return graphviz_to_file(g, filename, format)\n\n\ndef graphviz_to_file(g, filename, format):\n    fmts = [\".png\", \".pdf\", \".dot\", \".svg\", \".jpeg\", \".jpg\"]\n\n    if (\n        format is None\n        and filename is not None\n        and any(filename.lower().endswith(fmt) for fmt in fmts)\n    ):\n        filename, format = os.path.splitext(filename)\n        format = format[1:].lower()\n\n    if format is None:\n        format = \"png\"\n\n    data = g.pipe(format=format)\n    if not data:\n        raise RuntimeError(\n            \"Graphviz failed to properly produce an image. \"\n            \"This probably means your installation of graphviz \"\n            \"is missing png support. See: \"\n            \"https://github.com/ContinuumIO/anaconda-issues/\"\n            \"issues/485 for more information.\"\n        )\n\n    display_cls = _get_display_cls(format)\n\n    if filename is None:\n        return display_cls(data=data)\n\n    full_filename = \".\".join([filename, format])\n    with open(full_filename, \"wb\") as f:\n        f.write(data)\n\n    return display_cls(filename=full_filename)\n\n\ndef _to_cytoscape_json(\n    dsk,\n    data_attributes=None,\n    function_attributes=None,\n    collapse_outputs=False,\n    verbose=False,\n    **kwargs,\n):\n    \"\"\"\n    Convert a dask graph to Cytoscape JSON:\n    https://js.cytoscape.org/#notation/elements-json\n    \"\"\"\n    nodes = []\n    edges = []\n    data = {\"nodes\": nodes, \"edges\": edges}\n\n    data_attributes = data_attributes or {}\n    function_attributes = function_attributes or {}\n\n    seen = set()\n    connected = set()\n\n    for k, v in dsk.items():\n        k_name = name(k)\n        if istask(v):\n            func_name = name((k, \"function\")) if not collapse_outputs else k_name\n            if collapse_outputs or func_name not in seen:\n                seen.add(func_name)\n                attrs = function_attributes.get(k, {}).copy()\n                nodes.append(\n                    {\n                        \"data\": {\n                            \"id\": func_name,\n                            \"label\": key_split(k),\n                            \"shape\": \"ellipse\",\n                            \"color\": \"gray\",\n                            **attrs,\n                        }\n                    }\n                )\n            if not collapse_outputs:\n                edges.append({\"data\": {\"source\": func_name, \"target\": k_name}})\n\n                connected.add(func_name)\n                connected.add(k_name)\n\n            for dep in get_dependencies(dsk, k):\n                dep_name = name(dep)\n                if dep_name not in seen:\n                    seen.add(dep_name)\n                    attrs = data_attributes.get(dep, {}).copy()\n                    nodes.append(\n                        {\n                            \"data\": {\n                                \"id\": dep_name,\n                                \"label\": box_label(dep, verbose),\n                                \"shape\": \"rectangle\",\n                                \"color\": \"gray\",\n                                **attrs,\n                            }\n                        }\n                    )\n                edges.append(\n                    {\n                        \"data\": {\n                            \"source\": dep_name,\n                            \"target\": func_name,\n                        }\n                    }\n                )\n                connected.add(dep_name)\n                connected.add(func_name)\n\n        elif ishashable(v) and v in dsk:\n            v_name = name(v)\n            edges.append(\n                {\n                    \"data\": {\n                        \"source\": v_name,\n                        \"target\": k_name,\n                    }\n                }\n            )\n            connected.add(v_name)\n            connected.add(k_name)\n\n        if (not collapse_outputs or k_name in connected) and k_name not in seen:\n            seen.add(k_name)\n            attrs = data_attributes.get(k, {}).copy()\n            nodes.append(\n                {\n                    \"data\": {\n                        \"id\": k_name,\n                        \"label\": box_label(k, verbose),\n                        \"shape\": \"rectangle\",\n                        \"color\": \"gray\",\n                        **attrs,\n                    }\n                }\n            )\n    return data\n\n\ndef cytoscape_graph(\n    dsk,\n    filename: str | None = \"mydask\",\n    format: str | None = None,\n    *,\n    rankdir: str = \"BT\",\n    node_sep: float = 10,\n    edge_sep: float = 10,\n    spacing_factor: float = 1,\n    node_style: dict[str, str] | None = None,\n    edge_style: dict[str, str] | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Create an ipycytoscape widget for a dask graph.\n\n    If `filename` is not None, write an HTML file to disk with the specified name.\n\n    This uses the Cytoscape dagre layout algorithm. Options for that are documented here:\n    https://github.com/cytoscape/cytoscape.js-dagre#api\n\n    Parameters\n    ----------\n    dsk : dict\n        The graph to display.\n    filename : str or None, optional\n        The name of the HTML file to write to disk.\n    format : str, optional\n        Not used in this engine.\n    rankdir: str\n        The direction in which to orient the visualization.\n    node_sep: float\n        The separation (in px) between nodes in the DAG layout.\n    edge_sep: float\n        The separation (in px) between edges in the DAG layout.\n    spacing_factor: float\n        An overall scaling factor to increase (>1) or decrease (<1) the spacing\n        of the layout.\n    node_style: dict[str, str], optional\n        A dictionary of style attributes for nodes (refer to Cytoscape JSON docs\n        for available options: https://js.cytoscape.org/#notation/elements-json)\n    edge_style: dict[str, str], optional\n        A dictionary of style attributes for edges (refer to Cytoscape JSON docs\n        for available options: https://js.cytoscape.org/#notation/elements-json)\n    **kwargs\n        Additional keyword arguments to forward to `_to_cytoscape_json`.\n\n    Returns\n    -------\n    result : ipycytoscape.CytoscapeWidget\n    \"\"\"\n    ipycytoscape = import_required(\n        \"ipycytoscape\",\n        \"Drawing dask graphs with the cytoscape engine requires the `ipycytoscape` \"\n        \"python library.\\n\\n\"\n        \"Please either conda or pip install as follows:\\n\\n\"\n        \"  conda install ipycytoscape            # either conda install\\n\"\n        \"  python -m pip install ipycytoscape    # or pip install\",\n    )\n\n    node_style = node_style or {}\n    edge_style = edge_style or {}\n\n    data = _to_cytoscape_json(dsk, **kwargs)\n    # TODO: it's not easy to programmatically increase the height of the widget.\n    # Ideally we would make it a bit bigger, but that will probably require upstreaming\n    # some fixes.\n    g = ipycytoscape.CytoscapeWidget(\n        layout={\"height\": \"400px\"},\n    )\n    g.set_layout(\n        name=\"dagre\",\n        rankDir=rankdir,\n        nodeSep=node_sep,\n        edgeSep=edge_sep,\n        spacingFactor=spacing_factor,\n        nodeDimensionsIncludeLabels=True,\n    )\n    g.graph.add_graph_from_json(\n        data,\n        directed=True,\n    )\n    g.set_style(\n        [\n            {\n                \"selector\": \"node\",\n                \"style\": {\n                    \"font-family\": \"helvetica\",\n                    \"font-size\": \"24px\",\n                    \"font-weight\": \"bold\",\n                    \"color\": \"black\",\n                    \"background-color\": \"#eee\",\n                    \"border-color\": \"data(color)\",\n                    \"border-width\": 4,\n                    \"opacity\": \"1.0\",\n                    \"text-valign\": \"center\",\n                    \"text-halign\": \"center\",\n                    \"label\": \"data(label)\",\n                    \"shape\": \"data(shape)\",\n                    \"width\": 64,\n                    \"height\": 64,\n                    **node_style,\n                },\n            },\n            {\n                \"selector\": \"edge\",\n                \"style\": {\n                    \"width\": 8,\n                    \"line-color\": \"gray\",\n                    \"target-arrow-shape\": \"triangle\",\n                    \"target-arrow-color\": \"gray\",\n                    \"curve-style\": \"bezier\",\n                    **edge_style,\n                },\n            },\n        ],\n    )\n    # Tweak the zoom sensitivity\n    z = g.zoom\n    g.max_zoom = z * 2.0\n    g.min_zoom = z / 10.0\n    g.wheel_sensitivity = 0.1\n\n    if filename is not None:\n        from ipywidgets.embed import embed_minimal_html\n\n        filename = filename if filename.endswith(\".html\") else filename + \".html\"\n        embed_minimal_html(filename, views=[g], title=\"Dask task graph\")\n    return g\n\n### Source File Dependency Files Content\n### Dependency File: core.py\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom collections.abc import Collection, Iterable, Mapping, MutableMapping\nfrom typing import Any, Literal, TypeVar, cast, overload\n\nimport toolz\n\nfrom dask._task_spec import DependenciesMapping, convert_legacy_graph, execute_graph\nfrom dask.typing import Graph, Key, NoDefault, no_default\n\n\ndef ishashable(x):\n    \"\"\"Is x hashable?\n\n    Examples\n    --------\n\n    >>> ishashable(1)\n    True\n    >>> ishashable([1])\n    False\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    try:\n        hash(x)\n        return True\n    except TypeError:\n        return False\n\n\ndef istask(x):\n    \"\"\"Is x a runnable task?\n\n    A task is a tuple with a callable first argument\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> istask((inc, 1))\n    True\n    >>> istask(1)\n    False\n    \"\"\"\n    from dask._task_spec import DataNode, GraphNode\n\n    if isinstance(x, GraphNode):\n        if isinstance(x, DataNode):\n            return False\n        return True\n    return type(x) is tuple and x and callable(x[0])\n\n\ndef preorder_traversal(task):\n    \"\"\"A generator to preorder-traverse a task.\"\"\"\n\n    for item in task:\n        if istask(item):\n            yield from preorder_traversal(item)\n        elif isinstance(item, list):\n            yield list\n            yield from preorder_traversal(item)\n        else:\n            yield item\n\n\ndef lists_to_tuples(res, keys):\n    if isinstance(keys, list):\n        return tuple(lists_to_tuples(r, k) for r, k in zip(res, keys))\n    return res\n\n\ndef _pack_result(result: Mapping, keys: list | Key) -> Any:\n    if isinstance(keys, list):\n        return tuple(_pack_result(result, k) for k in keys)\n    return result[keys]\n\n\ndef get(dsk: Mapping, out: list | Key, cache: MutableMapping | None = None) -> Any:\n    \"\"\"Get value from Dask\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': 1, 'y': (inc, 'x')}\n\n    >>> get(d, 'x')\n    1\n    >>> get(d, 'y')\n    2\n    \"\"\"\n    for k in flatten(out):\n        if k not in dsk:\n            raise KeyError(f\"{k} is not a key in the graph\")\n    if cache is None:\n        cache = {}\n\n    dsk2 = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    result = execute_graph(dsk2, cache, keys=set(flatten([out])))\n    return _pack_result(result, out)\n\n\ndef keys_in_tasks(keys: Collection[Key], tasks: Iterable[Any], as_list: bool = False):\n    \"\"\"Returns the keys in `keys` that are also in `tasks`\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> keys_in_tasks(dsk, ['x', 'y', 'j'])  # doctest: +SKIP\n    {'x', 'y'}\n    \"\"\"\n    from dask._task_spec import GraphNode\n\n    ret: list[Key] = []\n    while tasks:\n        work = []\n        for w in tasks:\n            typ = type(w)\n            if typ is tuple and w and callable(w[0]):  # istask(w)\n                work.extend(w[1:])\n            elif typ is list:\n                work.extend(w)\n            elif typ is dict:\n                work.extend(w.values())\n            elif isinstance(w, GraphNode):\n                work.extend(w.dependencies)\n            else:\n                try:\n                    if w in keys:\n                        ret.append(w)\n                except TypeError:  # not hashable\n                    pass\n        tasks = work\n    return ret if as_list else set(ret)\n\n\ndef iskey(key: object) -> bool:\n    \"\"\"Return True if the given object is a potential dask key; False otherwise.\n\n    The definition of a key in a Dask graph is any str, bytes, int, float, or tuple\n    thereof.\n\n    See Also\n    --------\n    ishashable\n    validate_key\n    dask.typing.Key\n    \"\"\"\n    typ = type(key)\n    if typ is tuple:\n        return all(iskey(i) for i in cast(tuple, key))\n    return typ in {bytes, int, float, str}\n\n\ndef validate_key(key: object) -> None:\n    \"\"\"Validate the format of a dask key.\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    if iskey(key):\n        return\n    typ = type(key)\n\n    if typ is tuple:\n        index = None\n        try:\n            for index, part in enumerate(cast(tuple, key)):  # noqa: B007\n                validate_key(part)\n        except TypeError as e:\n            raise TypeError(\n                f\"Composite key contains unexpected key type at {index=} ({key=!r})\"\n            ) from e\n    raise TypeError(f\"Unexpected key type {typ} ({key=!r})\")\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = ...,\n    task: Key | NoDefault = ...,\n    as_list: Literal[False] = ...,\n) -> set[Key]: ...\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None,\n    task: Key | NoDefault,\n    as_list: Literal[True],\n) -> list[Key]: ...\n\n\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = None,\n    task: Key | NoDefault = no_default,\n    as_list: bool = False,\n) -> set[Key] | list[Key]:\n    \"\"\"Get the immediate tasks on which this task depends\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> get_dependencies(dsk, 'x')\n    set()\n\n    >>> get_dependencies(dsk, 'y')\n    {'x'}\n\n    >>> get_dependencies(dsk, 'z')  # doctest: +SKIP\n    {'x', 'y'}\n\n    >>> get_dependencies(dsk, 'w')  # Only direct dependencies\n    {'z'}\n\n    >>> get_dependencies(dsk, 'a')  # Ignore non-keys\n    {'x'}\n\n    >>> get_dependencies(dsk, task=(inc, 'x'))  # provide tasks directly\n    {'x'}\n    \"\"\"\n    if key is not None:\n        arg = dsk[key]\n    elif task is not no_default:\n        arg = task\n    else:\n        raise ValueError(\"Provide either key or task\")\n\n    return keys_in_tasks(dsk, [arg], as_list=as_list)\n\n\ndef get_deps(dsk: Graph) -> tuple[dict[Key, set[Key]], dict[Key, set[Key]]]:\n    \"\"\"Get dependencies and dependents from dask dask graph\n\n    >>> inc = lambda x: x + 1\n    >>> dsk = {'a': 1, 'b': (inc, 'a'), 'c': (inc, 'b')}\n    >>> dependencies, dependents = get_deps(dsk)\n    >>> dependencies\n    {'a': set(), 'b': {'a'}, 'c': {'b'}}\n    >>> dependents  # doctest: +SKIP\n    {'a': {'b'}, 'b': {'c'}, 'c': set()}\n    \"\"\"\n    dependencies = {k: get_dependencies(dsk, task=v) for k, v in dsk.items()}\n    dependents = reverse_dict(dependencies)\n    return dependencies, dependents\n\n\ndef flatten(seq, container=list):\n    \"\"\"\n\n    >>> list(flatten([1]))\n    [1]\n\n    >>> list(flatten([[1, 2], [1, 2]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten([[[1], [2]], [[1], [2]]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten(((1, 2), (1, 2)))) # Don't flatten tuples\n    [(1, 2), (1, 2)]\n\n    >>> list(flatten((1, 2, [3, 4]))) # support heterogeneous\n    [1, 2, 3, 4]\n    \"\"\"\n    if isinstance(seq, str):\n        yield seq\n    else:\n        for item in seq:\n            if isinstance(item, container):\n                yield from flatten(item, container=container)\n            else:\n                yield item\n\n\nT_ = TypeVar(\"T_\")\n\n\ndef reverse_dict(d: Mapping[T_, Iterable[T_]]) -> dict[T_, set[T_]]:\n    \"\"\"\n\n    >>> a, b, c = 'abc'\n    >>> d = {a: [b, c], b: [c]}\n    >>> reverse_dict(d)  # doctest: +SKIP\n    {'a': set([]), 'b': set(['a']}, 'c': set(['a', 'b'])}\n    \"\"\"\n    result: defaultdict[T_, set[T_]] = defaultdict(set)\n    _add = set.add\n    for k, vals in d.items():\n        result[k]\n        for val in vals:\n            _add(result[val], k)\n    return dict(result)\n\n\ndef subs(task, key, val):\n    \"\"\"Perform a substitution on a task\n\n    Examples\n    --------\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> subs((inc, 'x'), 'x', 1)  # doctest: +ELLIPSIS\n    (<function inc at ...>, 1)\n    \"\"\"\n    type_task = type(task)\n    if not (type_task is tuple and task and callable(task[0])):  # istask(task):\n        try:\n            if type_task is type(key) and task == key:\n                return val\n        except Exception:\n            pass\n        if type_task is list:\n            return [subs(x, key, val) for x in task]\n        return task\n    newargs = []\n    hash_key = {key}\n    for arg in task[1:]:\n        type_arg = type(arg)\n        if type_arg is tuple and arg and callable(arg[0]):  # istask(task):\n            arg = subs(arg, key, val)\n        elif type_arg is list:\n            arg = [subs(x, key, val) for x in arg]\n        else:\n            try:\n                if arg in hash_key:  # Hash and equality match\n                    arg = val\n            except TypeError:  # not hashable\n                pass\n        newargs.append(arg)\n    return task[:1] + tuple(newargs)\n\n\ndef _toposort(dsk, keys=None, returncycle=False, dependencies=None):\n\n    # Stack-based depth-first search traversal.  This is based on Tarjan's\n    # method for topological sorting (see wikipedia for pseudocode)\n    if keys is None:\n        keys = dsk\n    elif not isinstance(keys, list):\n        keys = [keys]\n    if not returncycle:\n        ordered = []\n\n    # Nodes whose descendents have been completely explored.\n    # These nodes are guaranteed to not be part of a cycle.\n    completed = set()\n\n    # All nodes that have been visited in the current traversal.  Because\n    # we are doing depth-first search, going \"deeper\" should never result\n    # in visiting a node that has already been seen.  The `seen` and\n    # `completed` sets are mutually exclusive; it is okay to visit a node\n    # that has already been added to `completed`.\n    seen = set()\n\n    if dependencies is None:\n\n        dependencies = DependenciesMapping(dsk)\n\n    for key in keys:\n        if key in completed:\n            continue\n        nodes = [key]\n        while nodes:\n            # Keep current node on the stack until all descendants are visited\n            cur = nodes[-1]\n            if cur in completed:\n                # Already fully traversed descendants of cur\n                nodes.pop()\n                continue\n            seen.add(cur)\n\n            # Add direct descendants of cur to nodes stack\n            next_nodes = []\n            for nxt in dependencies[cur]:\n                if nxt not in completed:\n                    if nxt in seen:\n                        # Cycle detected!\n                        # Let's report only the nodes that directly participate in the cycle.\n                        # We use `priorities` below to greedily construct a short cycle.\n                        # Shorter cycles may exist.\n                        priorities = {}\n                        prev = nodes[-1]\n                        # Give priority to nodes that were seen earlier.\n                        while nodes[-1] != nxt:\n                            priorities[nodes.pop()] = -len(priorities)\n                        priorities[nxt] = -len(priorities)\n                        # We're going to get the cycle by walking backwards along dependents,\n                        # so calculate dependents only for the nodes in play.\n                        inplay = set(priorities)\n                        dependents = reverse_dict(\n                            {k: inplay.intersection(dependencies[k]) for k in inplay}\n                        )\n                        # Begin with the node that was seen twice and the node `prev` from\n                        # which we detected the cycle.\n                        cycle = [nodes.pop()]\n                        cycle.append(prev)\n                        while prev != cycle[0]:\n                            # Greedily take a step that takes us closest to completing the cycle.\n                            # This may not give us the shortest cycle, but we get *a* short cycle.\n                            deps = dependents[cycle[-1]]\n                            prev = min(deps, key=priorities.__getitem__)\n                            cycle.append(prev)\n                        cycle.reverse()\n\n                        if returncycle:\n                            return cycle\n                        else:\n                            cycle = \"->\".join(str(x) for x in cycle)\n                            raise RuntimeError(\"Cycle detected in Dask: %s\" % cycle)\n                    next_nodes.append(nxt)\n\n            if next_nodes:\n                nodes.extend(next_nodes)\n            else:\n                # cur has no more descendants to explore, so we're done with it\n                if not returncycle:\n                    ordered.append(cur)\n                completed.add(cur)\n                seen.remove(cur)\n                nodes.pop()\n    if returncycle:\n        return []\n    return ordered\n\n\ndef toposort(dsk, dependencies=None):\n    \"\"\"Return a list of keys of dask sorted in topological order.\"\"\"\n    return _toposort(dsk, dependencies=dependencies)\n\n\ndef getcycle(d, keys):\n    \"\"\"Return a list of nodes that form a cycle if Dask is not a DAG.\n\n    Returns an empty list if no cycle is found.\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': (inc, 'z'), 'y': (inc, 'x'), 'z': (inc, 'y')}\n    >>> getcycle(d, 'x')\n    ['x', 'z', 'y', 'x']\n\n    See Also\n    --------\n    isdag\n    \"\"\"\n    return _toposort(d, keys=keys, returncycle=True)\n\n\ndef isdag(d, keys):\n    \"\"\"Does Dask form a directed acyclic graph when calculating keys?\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> inc = lambda x: x + 1\n    >>> isdag({'x': 0, 'y': (inc, 'x')}, 'y')\n    True\n    >>> isdag({'x': (inc, 'y'), 'y': (inc, 'x')}, 'y')\n    False\n\n    See Also\n    --------\n    getcycle\n    \"\"\"\n    return not getcycle(d, keys)\n\n\nclass literal:\n    \"\"\"A small serializable object to wrap literal values without copying\"\"\"\n\n    __slots__ = (\"data\",)\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return \"literal<type=%s>\" % type(self.data).__name__\n\n    def __reduce__(self):\n        return (literal, (self.data,))\n\n    def __call__(self):\n        return self.data\n\n\ndef quote(x):\n    \"\"\"Ensure that this value remains this value in a dask graph\n\n    Some values in dask graph take on special meaning. Sometimes we want to\n    ensure that our data is not interpreted but remains literal.\n\n    >>> add = lambda x, y: x + y\n    >>> quote((add, 1, 2))\n    (literal<type=tuple>,)\n    \"\"\"\n    if istask(x) or type(x) is list or type(x) is dict:\n        return (literal(x),)\n    return x\n\n\ndef reshapelist(shape, seq):\n    \"\"\"Reshape iterator to nested shape\n\n    >>> reshapelist((2, 3), range(6))\n    [[0, 1, 2], [3, 4, 5]]\n    \"\"\"\n    if len(shape) == 1:\n        return list(seq)\n    else:\n        n = int(len(seq) / shape[0])\n        return [reshapelist(shape[1:], part) for part in toolz.partition(n, seq)]\n\n\n### Dependency File: utils.py\nfrom __future__ import annotations\n\nimport codecs\nimport functools\nimport inspect\nimport os\nimport re\nimport shutil\nimport sys\nimport tempfile\nimport types\nimport uuid\nimport warnings\nfrom collections.abc import Callable, Hashable, Iterable, Iterator, Mapping, Set\nfrom contextlib import contextmanager, nullcontext, suppress\nfrom datetime import datetime, timedelta\nfrom errno import ENOENT\nfrom functools import wraps\nfrom importlib import import_module\nfrom numbers import Integral, Number\nfrom operator import add\nfrom threading import Lock\nfrom typing import Any, ClassVar, Literal, TypeVar, cast, overload\nfrom weakref import WeakValueDictionary\n\nimport tlz as toolz\n\nfrom dask import config\nfrom dask.typing import no_default\n\nK = TypeVar(\"K\")\nV = TypeVar(\"V\")\nT = TypeVar(\"T\")\n\n# used in decorators to preserve the signature of the function it decorates\n# see https://mypy.readthedocs.io/en/stable/generics.html#declaring-decorators\nFuncType = Callable[..., Any]\nF = TypeVar(\"F\", bound=FuncType)\n\nsystem_encoding = sys.getdefaultencoding()\nif system_encoding == \"ascii\":\n    system_encoding = \"utf-8\"\n\n\ndef apply(func, args, kwargs=None):\n    \"\"\"Apply a function given its positional and keyword arguments.\n\n    Equivalent to ``func(*args, **kwargs)``\n    Most Dask users will never need to use the ``apply`` function.\n    It is typically only used by people who need to inject\n    keyword argument values into a low level Dask task graph.\n\n    Parameters\n    ----------\n    func : callable\n        The function you want to apply.\n    args : tuple\n        A tuple containing all the positional arguments needed for ``func``\n        (eg: ``(arg_1, arg_2, arg_3)``)\n    kwargs : dict, optional\n        A dictionary mapping the keyword arguments\n        (eg: ``{\"kwarg_1\": value, \"kwarg_2\": value}``\n\n    Examples\n    --------\n    >>> from dask.utils import apply\n    >>> def add(number, second_number=5):\n    ...     return number + second_number\n    ...\n    >>> apply(add, (10,), {\"second_number\": 2})  # equivalent to add(*args, **kwargs)\n    12\n\n    >>> task = apply(add, (10,), {\"second_number\": 2})\n    >>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\n    \"\"\"\n    if kwargs:\n        return func(*args, **kwargs)\n    else:\n        return func(*args)\n\n\ndef _deprecated(\n    *,\n    version: str | None = None,\n    after_version: str | None = None,\n    message: str | None = None,\n    use_instead: str | None = None,\n    category: type[Warning] = FutureWarning,\n):\n    \"\"\"Decorator to mark a function as deprecated\n\n    Parameters\n    ----------\n    version : str, optional\n        Version of Dask in which the function was deprecated. If specified, the version\n        will be included in the default warning message. This should no longer be used\n        after the introduction of automated versioning system.\n    after_version : str, optional\n        Version of Dask after which the function was deprecated. If specified, the\n        version will be included in the default warning message.\n    message : str, optional\n        Custom warning message to raise.\n    use_instead : str, optional\n        Name of function to use in place of the deprecated function.\n        If specified, this will be included in the default warning\n        message.\n    category : type[Warning], optional\n        Type of warning to raise. Defaults to ``FutureWarning``.\n\n    Examples\n    --------\n\n    >>> from dask.utils import _deprecated\n    >>> @_deprecated(after_version=\"X.Y.Z\", use_instead=\"bar\")\n    ... def foo():\n    ...     return \"baz\"\n    \"\"\"\n\n    def decorator(func):\n        if message is None:\n            msg = f\"{func.__name__} \"\n            if after_version is not None:\n                msg += f\"was deprecated after version {after_version} \"\n            elif version is not None:\n                msg += f\"was deprecated in version {version} \"\n            else:\n                msg += \"is deprecated \"\n            msg += \"and will be removed in a future release.\"\n\n            if use_instead is not None:\n                msg += f\" Please use {use_instead} instead.\"\n        else:\n            msg = message\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(msg, category=category, stacklevel=2)\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\ndef _deprecated_kwarg(\n    old_arg_name: str,\n    new_arg_name: str | None = None,\n    mapping: Mapping[Any, Any] | Callable[[Any], Any] | None = None,\n    stacklevel: int = 2,\n    comment: str | None = None,\n) -> Callable[[F], F]:\n    \"\"\"\n    Decorator to deprecate a keyword argument of a function.\n\n    Parameters\n    ----------\n    old_arg_name : str\n        Name of argument in function to deprecate\n    new_arg_name : str, optional\n        Name of preferred argument in function. Omit to warn that\n        ``old_arg_name`` keyword is deprecated.\n    mapping : dict or callable, optional\n        If mapping is present, use it to translate old arguments to\n        new arguments. A callable must do its own value checking;\n        values not found in a dict will be forwarded unchanged.\n    comment :  str, optional\n        Additional message to deprecation message. Useful to pass\n        on suggestions with the deprecation warning.\n\n    Examples\n    --------\n    The following deprecates 'cols', using 'columns' instead\n\n    >>> @_deprecated_kwarg(old_arg_name='cols', new_arg_name='columns')\n    ... def f(columns=''):\n    ...     print(columns)\n    ...\n    >>> f(columns='should work ok')\n    should work ok\n\n    >>> f(cols='should raise warning')  # doctest: +SKIP\n    FutureWarning: cols is deprecated, use columns instead\n      warnings.warn(msg, FutureWarning)\n    should raise warning\n\n    >>> f(cols='should error', columns=\"can\\'t pass do both\")  # doctest: +SKIP\n    TypeError: Can only specify 'cols' or 'columns', not both\n\n    >>> @_deprecated_kwarg('old', 'new', {'yes': True, 'no': False})\n    ... def f(new=False):\n    ...     print('yes!' if new else 'no!')\n    ...\n    >>> f(old='yes')  # doctest: +SKIP\n    FutureWarning: old='yes' is deprecated, use new=True instead\n      warnings.warn(msg, FutureWarning)\n    yes!\n\n    To raise a warning that a keyword will be removed entirely in the future\n\n    >>> @_deprecated_kwarg(old_arg_name='cols', new_arg_name=None)\n    ... def f(cols='', another_param=''):\n    ...     print(cols)\n    ...\n    >>> f(cols='should raise warning')  # doctest: +SKIP\n    FutureWarning: the 'cols' keyword is deprecated and will be removed in a\n    future version please takes steps to stop use of 'cols'\n    should raise warning\n    >>> f(another_param='should not raise warning')  # doctest: +SKIP\n    should not raise warning\n\n    >>> f(cols='should raise warning', another_param='')  # doctest: +SKIP\n    FutureWarning: the 'cols' keyword is deprecated and will be removed in a\n    future version please takes steps to stop use of 'cols'\n    should raise warning\n    \"\"\"\n    if mapping is not None and not hasattr(mapping, \"get\") and not callable(mapping):\n        raise TypeError(\n            \"mapping from old to new argument values must be dict or callable!\"\n        )\n\n    comment_ = f\"\\n{comment}\" or \"\"\n\n    def _deprecated_kwarg(func: F) -> F:\n        @wraps(func)\n        def wrapper(*args, **kwargs) -> Callable[..., Any]:\n            old_arg_value = kwargs.pop(old_arg_name, no_default)\n\n            if old_arg_value is not no_default:\n                if new_arg_name is None:\n                    msg = (\n                        f\"the {repr(old_arg_name)} keyword is deprecated and \"\n                        \"will be removed in a future version. Please take \"\n                        f\"steps to stop the use of {repr(old_arg_name)}\"\n                    ) + comment_\n                    warnings.warn(msg, FutureWarning, stacklevel=stacklevel)\n                    kwargs[old_arg_name] = old_arg_value\n                    return func(*args, **kwargs)\n\n                elif mapping is not None:\n                    if callable(mapping):\n                        new_arg_value = mapping(old_arg_value)\n                    else:\n                        new_arg_value = mapping.get(old_arg_value, old_arg_value)\n                    msg = (\n                        f\"the {old_arg_name}={repr(old_arg_value)} keyword is \"\n                        \"deprecated, use \"\n                        f\"{new_arg_name}={repr(new_arg_value)} instead.\"\n                    )\n                else:\n                    new_arg_value = old_arg_value\n                    msg = (\n                        f\"the {repr(old_arg_name)} keyword is deprecated, \"\n                        f\"use {repr(new_arg_name)} instead.\"\n                    )\n\n                warnings.warn(msg + comment_, FutureWarning, stacklevel=stacklevel)\n                if kwargs.get(new_arg_name) is not None:\n                    msg = (\n                        f\"Can only specify {repr(old_arg_name)} \"\n                        f\"or {repr(new_arg_name)}, not both.\"\n                    )\n                    raise TypeError(msg)\n                kwargs[new_arg_name] = new_arg_value\n            return func(*args, **kwargs)\n\n        return cast(F, wrapper)\n\n    return _deprecated_kwarg\n\n\ndef deepmap(func, *seqs):\n    \"\"\"Apply function inside nested lists\n\n    >>> inc = lambda x: x + 1\n    >>> deepmap(inc, [[1, 2], [3, 4]])\n    [[2, 3], [4, 5]]\n\n    >>> add = lambda x, y: x + y\n    >>> deepmap(add, [[1, 2], [3, 4]], [[10, 20], [30, 40]])\n    [[11, 22], [33, 44]]\n    \"\"\"\n    if isinstance(seqs[0], (list, Iterator)):\n        return [deepmap(func, *items) for items in zip(*seqs)]\n    else:\n        return func(*seqs)\n\n\n@_deprecated()\ndef homogeneous_deepmap(func, seq):\n    if not seq:\n        return seq\n    n = 0\n    tmp = seq\n    while isinstance(tmp, list):\n        n += 1\n        tmp = tmp[0]\n\n    return ndeepmap(n, func, seq)\n\n\ndef ndeepmap(n, func, seq):\n    \"\"\"Call a function on every element within a nested container\n\n    >>> def inc(x):\n    ...     return x + 1\n    >>> L = [[1, 2], [3, 4, 5]]\n    >>> ndeepmap(2, inc, L)\n    [[2, 3], [4, 5, 6]]\n    \"\"\"\n    if n == 1:\n        return [func(item) for item in seq]\n    elif n > 1:\n        return [ndeepmap(n - 1, func, item) for item in seq]\n    elif isinstance(seq, list):\n        return func(seq[0])\n    else:\n        return func(seq)\n\n\ndef import_required(mod_name, error_msg):\n    \"\"\"Attempt to import a required dependency.\n\n    Raises a RuntimeError if the requested module is not available.\n    \"\"\"\n    try:\n        return import_module(mod_name)\n    except ImportError as e:\n        raise RuntimeError(error_msg) from e\n\n\n@contextmanager\ndef tmpfile(extension=\"\", dir=None):\n    \"\"\"\n    Function to create and return a unique temporary file with the given extension, if provided.\n\n    Parameters\n    ----------\n    extension : str\n        The extension of the temporary file to be created\n    dir : str\n        If ``dir`` is not None, the file will be created in that directory; otherwise,\n        Python's default temporary directory is used.\n\n    Returns\n    -------\n    out : str\n        Path to the temporary file\n\n    See Also\n    --------\n    NamedTemporaryFile : Built-in alternative for creating temporary files\n    tmp_path : pytest fixture for creating a temporary directory unique to the test invocation\n\n    Notes\n    -----\n    This context manager is particularly useful on Windows for opening temporary files multiple times.\n    \"\"\"\n    extension = extension.lstrip(\".\")\n    if extension:\n        extension = \".\" + extension\n    handle, filename = tempfile.mkstemp(extension, dir=dir)\n    os.close(handle)\n    os.remove(filename)\n\n    try:\n        yield filename\n    finally:\n        if os.path.exists(filename):\n            with suppress(OSError):  # sometimes we can't remove a generated temp file\n                if os.path.isdir(filename):\n                    shutil.rmtree(filename)\n                else:\n                    os.remove(filename)\n\n\n@contextmanager\ndef tmpdir(dir=None):\n    \"\"\"\n    Function to create and return a unique temporary directory.\n\n    Parameters\n    ----------\n    dir : str\n        If ``dir`` is not None, the directory will be created in that directory; otherwise,\n        Python's default temporary directory is used.\n\n    Returns\n    -------\n    out : str\n        Path to the temporary directory\n\n    Notes\n    -----\n    This context manager is particularly useful on Windows for opening temporary directories multiple times.\n    \"\"\"\n    dirname = tempfile.mkdtemp(dir=dir)\n\n    try:\n        yield dirname\n    finally:\n        if os.path.exists(dirname):\n            if os.path.isdir(dirname):\n                with suppress(OSError):\n                    shutil.rmtree(dirname)\n            else:\n                with suppress(OSError):\n                    os.remove(dirname)\n\n\n@contextmanager\ndef filetext(text, extension=\"\", open=open, mode=\"w\"):\n    with tmpfile(extension=extension) as filename:\n        f = open(filename, mode=mode)\n        try:\n            f.write(text)\n        finally:\n            try:\n                f.close()\n            except AttributeError:\n                pass\n\n        yield filename\n\n\n@contextmanager\ndef changed_cwd(new_cwd):\n    old_cwd = os.getcwd()\n    os.chdir(new_cwd)\n    try:\n        yield\n    finally:\n        os.chdir(old_cwd)\n\n\n@contextmanager\ndef tmp_cwd(dir=None):\n    with tmpdir(dir) as dirname:\n        with changed_cwd(dirname):\n            yield dirname\n\n\nclass IndexCallable:\n    \"\"\"Provide getitem syntax for functions\n\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> I = IndexCallable(inc)\n    >>> I[3]\n    4\n    \"\"\"\n\n    __slots__ = (\"fn\",)\n\n    def __init__(self, fn):\n        self.fn = fn\n\n    def __getitem__(self, key):\n        return self.fn(key)\n\n\n@contextmanager\ndef filetexts(d, open=open, mode=\"t\", use_tmpdir=True):\n    \"\"\"Dumps a number of textfiles to disk\n\n    Parameters\n    ----------\n    d : dict\n        a mapping from filename to text like {'a.csv': '1,1\\n2,2'}\n\n    Since this is meant for use in tests, this context manager will\n    automatically switch to a temporary current directory, to avoid\n    race conditions when running tests in parallel.\n    \"\"\"\n    with tmp_cwd() if use_tmpdir else nullcontext():\n        for filename, text in d.items():\n            try:\n                os.makedirs(os.path.dirname(filename))\n            except OSError:\n                pass\n            f = open(filename, \"w\" + mode)\n            try:\n                f.write(text)\n            finally:\n                try:\n                    f.close()\n                except AttributeError:\n                    pass\n\n        yield list(d)\n\n        for filename in d:\n            if os.path.exists(filename):\n                with suppress(OSError):\n                    os.remove(filename)\n\n\ndef concrete(seq):\n    \"\"\"Make nested iterators concrete lists\n\n    >>> data = [[1, 2], [3, 4]]\n    >>> seq = iter(map(iter, data))\n    >>> concrete(seq)\n    [[1, 2], [3, 4]]\n    \"\"\"\n    if isinstance(seq, Iterator):\n        seq = list(seq)\n    if isinstance(seq, (tuple, list)):\n        seq = list(map(concrete, seq))\n    return seq\n\n\ndef pseudorandom(n: int, p, random_state=None):\n    \"\"\"Pseudorandom array of integer indexes\n\n    >>> pseudorandom(5, [0.5, 0.5], random_state=123)\n    array([1, 0, 0, 1, 1], dtype=int8)\n\n    >>> pseudorandom(10, [0.5, 0.2, 0.2, 0.1], random_state=5)\n    array([0, 2, 0, 3, 0, 1, 2, 1, 0, 0], dtype=int8)\n    \"\"\"\n    import numpy as np\n\n    p = list(p)\n    cp = np.cumsum([0] + p)\n    assert np.allclose(1, cp[-1])\n    assert len(p) < 256\n\n    if not isinstance(random_state, np.random.RandomState):\n        random_state = np.random.RandomState(random_state)\n\n    x = random_state.random_sample(n)\n    out = np.empty(n, dtype=\"i1\")\n\n    for i, (low, high) in enumerate(zip(cp[:-1], cp[1:])):\n        out[(x >= low) & (x < high)] = i\n    return out\n\n\ndef random_state_data(n: int, random_state=None) -> list:\n    \"\"\"Return a list of arrays that can initialize\n    ``np.random.RandomState``.\n\n    Parameters\n    ----------\n    n : int\n        Number of arrays to return.\n    random_state : int or np.random.RandomState, optional\n        If an int, is used to seed a new ``RandomState``.\n    \"\"\"\n    import numpy as np\n\n    if not all(\n        hasattr(random_state, attr) for attr in [\"normal\", \"beta\", \"bytes\", \"uniform\"]\n    ):\n        random_state = np.random.RandomState(random_state)\n\n    random_data = random_state.bytes(624 * n * 4)  # `n * 624` 32-bit integers\n    l = list(np.frombuffer(random_data, dtype=\"<u4\").reshape((n, -1)))\n    assert len(l) == n\n    return l\n\n\ndef is_integer(i) -> bool:\n    \"\"\"\n    >>> is_integer(6)\n    True\n    >>> is_integer(42.0)\n    True\n    >>> is_integer('abc')\n    False\n    \"\"\"\n    return isinstance(i, Integral) or (isinstance(i, float) and i.is_integer())\n\n\nONE_ARITY_BUILTINS = {\n    abs,\n    all,\n    any,\n    ascii,\n    bool,\n    bytearray,\n    bytes,\n    callable,\n    chr,\n    classmethod,\n    complex,\n    dict,\n    dir,\n    enumerate,\n    eval,\n    float,\n    format,\n    frozenset,\n    hash,\n    hex,\n    id,\n    int,\n    iter,\n    len,\n    list,\n    max,\n    min,\n    next,\n    oct,\n    open,\n    ord,\n    range,\n    repr,\n    reversed,\n    round,\n    set,\n    slice,\n    sorted,\n    staticmethod,\n    str,\n    sum,\n    tuple,\n    type,\n    vars,\n    zip,\n    memoryview,\n}\nMULTI_ARITY_BUILTINS = {\n    compile,\n    delattr,\n    divmod,\n    filter,\n    getattr,\n    hasattr,\n    isinstance,\n    issubclass,\n    map,\n    pow,\n    setattr,\n}\n\n\ndef getargspec(func):\n    \"\"\"Version of inspect.getargspec that works with partial and warps.\"\"\"\n    if isinstance(func, functools.partial):\n        return getargspec(func.func)\n\n    func = getattr(func, \"__wrapped__\", func)\n    if isinstance(func, type):\n        return inspect.getfullargspec(func.__init__)\n    else:\n        return inspect.getfullargspec(func)\n\n\ndef takes_multiple_arguments(func, varargs=True):\n    \"\"\"Does this function take multiple arguments?\n\n    >>> def f(x, y): pass\n    >>> takes_multiple_arguments(f)\n    True\n\n    >>> def f(x): pass\n    >>> takes_multiple_arguments(f)\n    False\n\n    >>> def f(x, y=None): pass\n    >>> takes_multiple_arguments(f)\n    False\n\n    >>> def f(*args): pass\n    >>> takes_multiple_arguments(f)\n    True\n\n    >>> class Thing:\n    ...     def __init__(self, a): pass\n    >>> takes_multiple_arguments(Thing)\n    False\n\n    \"\"\"\n    if func in ONE_ARITY_BUILTINS:\n        return False\n    elif func in MULTI_ARITY_BUILTINS:\n        return True\n\n    try:\n        spec = getargspec(func)\n    except Exception:\n        return False\n\n    try:\n        is_constructor = spec.args[0] == \"self\" and isinstance(func, type)\n    except Exception:\n        is_constructor = False\n\n    if varargs and spec.varargs:\n        return True\n\n    ndefaults = 0 if spec.defaults is None else len(spec.defaults)\n    return len(spec.args) - ndefaults - is_constructor > 1\n\n\ndef get_named_args(func) -> list[str]:\n    \"\"\"Get all non ``*args/**kwargs`` arguments for a function\"\"\"\n    s = inspect.signature(func)\n    return [\n        n\n        for n, p in s.parameters.items()\n        if p.kind in [p.POSITIONAL_OR_KEYWORD, p.POSITIONAL_ONLY, p.KEYWORD_ONLY]\n    ]\n\n\nclass Dispatch:\n    \"\"\"Simple single dispatch.\"\"\"\n\n    def __init__(self, name=None):\n        self._lookup = {}\n        self._lazy = {}\n        if name:\n            self.__name__ = name\n\n    def register(self, type, func=None):\n        \"\"\"Register dispatch of `func` on arguments of type `type`\"\"\"\n\n        def wrapper(func):\n            if isinstance(type, tuple):\n                for t in type:\n                    self.register(t, func)\n            else:\n                self._lookup[type] = func\n            return func\n\n        return wrapper(func) if func is not None else wrapper\n\n    def register_lazy(self, toplevel, func=None):\n        \"\"\"\n        Register a registration function which will be called if the\n        *toplevel* module (e.g. 'pandas') is ever loaded.\n        \"\"\"\n\n        def wrapper(func):\n            self._lazy[toplevel] = func\n            return func\n\n        return wrapper(func) if func is not None else wrapper\n\n    def dispatch(self, cls):\n        \"\"\"Return the function implementation for the given ``cls``\"\"\"\n        lk = self._lookup\n        for cls2 in cls.__mro__:\n            # Is a lazy registration function present?\n            toplevel, _, _ = cls2.__module__.partition(\".\")\n            try:\n                register = self._lazy[toplevel]\n            except KeyError:\n                pass\n            else:\n                register()\n                self._lazy.pop(toplevel, None)\n                return self.dispatch(cls)  # recurse\n            try:\n                impl = lk[cls2]\n            except KeyError:\n                pass\n            else:\n                if cls is not cls2:\n                    # Cache lookup\n                    lk[cls] = impl\n                return impl\n        raise TypeError(f\"No dispatch for {cls}\")\n\n    def __call__(self, arg, *args, **kwargs):\n        \"\"\"\n        Call the corresponding method based on type of argument.\n        \"\"\"\n        meth = self.dispatch(type(arg))\n        return meth(arg, *args, **kwargs)\n\n    @property\n    def __doc__(self):\n        try:\n            func = self.dispatch(object)\n            return func.__doc__\n        except TypeError:\n            return \"Single Dispatch for %s\" % self.__name__\n\n\ndef ensure_not_exists(filename) -> None:\n    \"\"\"\n    Ensure that a file does not exist.\n    \"\"\"\n    try:\n        os.unlink(filename)\n    except OSError as e:\n        if e.errno != ENOENT:\n            raise\n\n\ndef _skip_doctest(line):\n    # NumPy docstring contains cursor and comment only example\n    stripped = line.strip()\n    if stripped == \">>>\" or stripped.startswith(\">>> #\"):\n        return line\n    elif \">>>\" in stripped and \"+SKIP\" not in stripped:\n        if \"# doctest:\" in line:\n            return line + \", +SKIP\"\n        else:\n            return line + \"  # doctest: +SKIP\"\n    else:\n        return line\n\n\ndef skip_doctest(doc):\n    if doc is None:\n        return \"\"\n    return \"\\n\".join([_skip_doctest(line) for line in doc.split(\"\\n\")])\n\n\ndef extra_titles(doc):\n    lines = doc.split(\"\\n\")\n    titles = {\n        i: lines[i].strip()\n        for i in range(len(lines) - 1)\n        if lines[i + 1].strip() and all(c == \"-\" for c in lines[i + 1].strip())\n    }\n\n    seen = set()\n    for i, title in sorted(titles.items()):\n        if title in seen:\n            new_title = \"Extra \" + title\n            lines[i] = lines[i].replace(title, new_title)\n            lines[i + 1] = lines[i + 1].replace(\"-\" * len(title), \"-\" * len(new_title))\n        else:\n            seen.add(title)\n\n    return \"\\n\".join(lines)\n\n\ndef ignore_warning(doc, cls, name, extra=\"\", skipblocks=0, inconsistencies=None):\n    \"\"\"Expand docstring by adding disclaimer and extra text\"\"\"\n    import inspect\n\n    if inspect.isclass(cls):\n        l1 = \"This docstring was copied from {}.{}.{}.\\n\\n\".format(\n            cls.__module__,\n            cls.__name__,\n            name,\n        )\n    else:\n        l1 = f\"This docstring was copied from {cls.__name__}.{name}.\\n\\n\"\n    l2 = \"Some inconsistencies with the Dask version may exist.\"\n\n    i = doc.find(\"\\n\\n\")\n    if i != -1:\n        # Insert our warning\n        head = doc[: i + 2]\n        tail = doc[i + 2 :]\n        while skipblocks > 0:\n            i = tail.find(\"\\n\\n\")\n            head = tail[: i + 2]\n            tail = tail[i + 2 :]\n            skipblocks -= 1\n        # Indentation of next line\n        indent = re.match(r\"\\s*\", tail).group(0)\n        # Insert the warning, indented, with a blank line before and after\n        if extra:\n            more = [indent, extra.rstrip(\"\\n\") + \"\\n\\n\"]\n        else:\n            more = []\n        if inconsistencies is not None:\n            l3 = f\"Known inconsistencies: \\n {inconsistencies}\"\n            bits = [head, indent, l1, l2, \"\\n\\n\", l3, \"\\n\\n\"] + more + [tail]\n        else:\n            bits = [head, indent, l1, indent, l2, \"\\n\\n\"] + more + [tail]\n        doc = \"\".join(bits)\n\n    return doc\n\n\ndef unsupported_arguments(doc, args):\n    \"\"\"Mark unsupported arguments with a disclaimer\"\"\"\n    lines = doc.split(\"\\n\")\n    for arg in args:\n        subset = [\n            (i, line)\n            for i, line in enumerate(lines)\n            if re.match(r\"^\\s*\" + arg + \" ?:\", line)\n        ]\n        if len(subset) == 1:\n            [(i, line)] = subset\n            lines[i] = line + \"  (Not supported in Dask)\"\n    return \"\\n\".join(lines)\n\n\ndef _derived_from(\n    cls, method, ua_args=None, extra=\"\", skipblocks=0, inconsistencies=None\n):\n    \"\"\"Helper function for derived_from to ease testing\"\"\"\n    ua_args = ua_args or []\n\n    # do not use wraps here, as it hides keyword arguments displayed\n    # in the doc\n    original_method = getattr(cls, method.__name__)\n\n    doc = getattr(original_method, \"__doc__\", None)\n\n    if isinstance(original_method, property):\n        # some things like SeriesGroupBy.unique are generated.\n        original_method = original_method.fget\n        if not doc:\n            doc = getattr(original_method, \"__doc__\", None)\n\n    if isinstance(original_method, functools.cached_property):\n        original_method = original_method.func\n        if not doc:\n            doc = getattr(original_method, \"__doc__\", None)\n\n    if doc is None:\n        doc = \"\"\n\n    # pandas DataFrame/Series sometimes override methods without setting __doc__\n    if not doc and cls.__name__ in {\"DataFrame\", \"Series\"}:\n        for obj in cls.mro():\n            obj_method = getattr(obj, method.__name__, None)\n            if obj_method is not None and obj_method.__doc__:\n                doc = obj_method.__doc__\n                break\n\n    # Insert disclaimer that this is a copied docstring\n    if doc:\n        doc = ignore_warning(\n            doc,\n            cls,\n            method.__name__,\n            extra=extra,\n            skipblocks=skipblocks,\n            inconsistencies=inconsistencies,\n        )\n    elif extra:\n        doc += extra.rstrip(\"\\n\") + \"\\n\\n\"\n\n    # Mark unsupported arguments\n    try:\n        method_args = get_named_args(method)\n        original_args = get_named_args(original_method)\n        not_supported = [m for m in original_args if m not in method_args]\n    except ValueError:\n        not_supported = []\n    if len(ua_args) > 0:\n        not_supported.extend(ua_args)\n    if len(not_supported) > 0:\n        doc = unsupported_arguments(doc, not_supported)\n\n    doc = skip_doctest(doc)\n    doc = extra_titles(doc)\n\n    return doc\n\n\ndef derived_from(\n    original_klass, version=None, ua_args=None, skipblocks=0, inconsistencies=None\n):\n    \"\"\"Decorator to attach original class's docstring to the wrapped method.\n\n    The output structure will be: top line of docstring, disclaimer about this\n    being auto-derived, any extra text associated with the method being patched,\n    the body of the docstring and finally, the list of keywords that exist in\n    the original method but not in the dask version.\n\n    Parameters\n    ----------\n    original_klass: type\n        Original class which the method is derived from\n    version : str\n        Original package version which supports the wrapped method\n    ua_args : list\n        List of keywords which Dask doesn't support. Keywords existing in\n        original but not in Dask will automatically be added.\n    skipblocks : int\n        How many text blocks (paragraphs) to skip from the start of the\n        docstring. Useful for cases where the target has extra front-matter.\n    inconsistencies: list\n        List of known inconsistencies with method whose docstrings are being\n        copied.\n    \"\"\"\n    ua_args = ua_args or []\n\n    def wrapper(method):\n        try:\n            extra = getattr(method, \"__doc__\", None) or \"\"\n            method.__doc__ = _derived_from(\n                original_klass,\n                method,\n                ua_args=ua_args,\n                extra=extra,\n                skipblocks=skipblocks,\n                inconsistencies=inconsistencies,\n            )\n            return method\n\n        except AttributeError:\n            module_name = original_klass.__module__.split(\".\")[0]\n\n            @functools.wraps(method)\n            def wrapped(*args, **kwargs):\n                msg = f\"Base package doesn't support '{method.__name__}'.\"\n                if version is not None:\n                    msg2 = \" Use {0} {1} or later to use this method.\"\n                    msg += msg2.format(module_name, version)\n                raise NotImplementedError(msg)\n\n            return wrapped\n\n    return wrapper\n\n\ndef funcname(func) -> str:\n    \"\"\"Get the name of a function.\"\"\"\n    # functools.partial\n    if isinstance(func, functools.partial):\n        return funcname(func.func)\n    # methodcaller\n    if isinstance(func, methodcaller):\n        return func.method[:50]\n\n    module_name = getattr(func, \"__module__\", None) or \"\"\n    type_name = getattr(type(func), \"__name__\", None) or \"\"\n\n    # toolz.curry\n    if \"toolz\" in module_name and \"curry\" == type_name:\n        return func.func_name[:50]\n    # multipledispatch objects\n    if \"multipledispatch\" in module_name and \"Dispatcher\" == type_name:\n        return func.name[:50]\n    # numpy.vectorize objects\n    if \"numpy\" in module_name and \"vectorize\" == type_name:\n        return (\"vectorize_\" + funcname(func.pyfunc))[:50]\n\n    # All other callables\n    try:\n        name = func.__name__\n        if name == \"<lambda>\":\n            return \"lambda\"\n        return name[:50]\n    except AttributeError:\n        return str(func)[:50]\n\n\ndef typename(typ: Any, short: bool = False) -> str:\n    \"\"\"\n    Return the name of a type\n\n    Examples\n    --------\n    >>> typename(int)\n    'int'\n\n    >>> from dask.core import literal\n    >>> typename(literal)\n    'dask.core.literal'\n    >>> typename(literal, short=True)\n    'dask.literal'\n    \"\"\"\n    if not isinstance(typ, type):\n        return typename(type(typ))\n    try:\n        if not typ.__module__ or typ.__module__ == \"builtins\":\n            return typ.__name__\n        else:\n            if short:\n                module, *_ = typ.__module__.split(\".\")\n            else:\n                module = typ.__module__\n            return module + \".\" + typ.__name__\n    except AttributeError:\n        return str(typ)\n\n\ndef ensure_bytes(s) -> bytes:\n    \"\"\"Attempt to turn `s` into bytes.\n\n    Parameters\n    ----------\n    s : Any\n        The object to be converted. Will correctly handled\n        * str\n        * bytes\n        * objects implementing the buffer protocol (memoryview, ndarray, etc.)\n\n    Returns\n    -------\n    b : bytes\n\n    Raises\n    ------\n    TypeError\n        When `s` cannot be converted\n\n    Examples\n    --------\n    >>> ensure_bytes('123')\n    b'123'\n    >>> ensure_bytes(b'123')\n    b'123'\n    >>> ensure_bytes(bytearray(b'123'))\n    b'123'\n    \"\"\"\n    if isinstance(s, bytes):\n        return s\n    elif hasattr(s, \"encode\"):\n        return s.encode()\n    else:\n        try:\n            return bytes(s)\n        except Exception as e:\n            raise TypeError(\n                f\"Object {s} is neither a bytes object nor can be encoded to bytes\"\n            ) from e\n\n\ndef ensure_unicode(s) -> str:\n    \"\"\"Turn string or bytes to string\n\n    >>> ensure_unicode('123')\n    '123'\n    >>> ensure_unicode(b'123')\n    '123'\n    \"\"\"\n    if isinstance(s, str):\n        return s\n    elif hasattr(s, \"decode\"):\n        return s.decode()\n    else:\n        try:\n            return codecs.decode(s)\n        except Exception as e:\n            raise TypeError(\n                f\"Object {s} is neither a str object nor can be decoded to str\"\n            ) from e\n\n\ndef digit(n, k, base):\n    \"\"\"\n\n    >>> digit(1234, 0, 10)\n    4\n    >>> digit(1234, 1, 10)\n    3\n    >>> digit(1234, 2, 10)\n    2\n    >>> digit(1234, 3, 10)\n    1\n    \"\"\"\n    return n // base**k % base\n\n\ndef insert(tup, loc, val):\n    \"\"\"\n\n    >>> insert(('a', 'b', 'c'), 0, 'x')\n    ('x', 'b', 'c')\n    \"\"\"\n    L = list(tup)\n    L[loc] = val\n    return tuple(L)\n\n\ndef memory_repr(num):\n    for x in [\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n        if num < 1024.0:\n            return f\"{num:3.1f} {x}\"\n        num /= 1024.0\n\n\ndef asciitable(columns, rows):\n    \"\"\"Formats an ascii table for given columns and rows.\n\n    Parameters\n    ----------\n    columns : list\n        The column names\n    rows : list of tuples\n        The rows in the table. Each tuple must be the same length as\n        ``columns``.\n    \"\"\"\n    rows = [tuple(str(i) for i in r) for r in rows]\n    columns = tuple(str(i) for i in columns)\n    widths = tuple(max(max(map(len, x)), len(c)) for x, c in zip(zip(*rows), columns))\n    row_template = (\"|\" + (\" %%-%ds |\" * len(columns))) % widths\n    header = row_template % tuple(columns)\n    bar = \"+%s+\" % \"+\".join(\"-\" * (w + 2) for w in widths)\n    data = \"\\n\".join(row_template % r for r in rows)\n    return \"\\n\".join([bar, header, bar, data, bar])\n\n\ndef put_lines(buf, lines):\n    if any(not isinstance(x, str) for x in lines):\n        lines = [str(x) for x in lines]\n    buf.write(\"\\n\".join(lines))\n\n\n_method_cache: dict[str, methodcaller] = {}\n\n\nclass methodcaller:\n    \"\"\"\n    Return a callable object that calls the given method on its operand.\n\n    Unlike the builtin `operator.methodcaller`, instances of this class are\n    cached and arguments are passed at call time instead of build time.\n    \"\"\"\n\n    __slots__ = (\"method\",)\n    method: str\n\n    @property\n    def func(self) -> str:\n        # For `funcname` to work\n        return self.method\n\n    def __new__(cls, method: str):\n        try:\n            return _method_cache[method]\n        except KeyError:\n            self = object.__new__(cls)\n            self.method = method\n            _method_cache[method] = self\n            return self\n\n    def __call__(self, __obj, *args, **kwargs):\n        return getattr(__obj, self.method)(*args, **kwargs)\n\n    def __reduce__(self):\n        return (methodcaller, (self.method,))\n\n    def __str__(self):\n        return f\"<{self.__class__.__name__}: {self.method}>\"\n\n    __repr__ = __str__\n\n\nclass itemgetter:\n    \"\"\"Variant of operator.itemgetter that supports equality tests\"\"\"\n\n    __slots__ = (\"index\",)\n\n    def __init__(self, index):\n        self.index = index\n\n    def __call__(self, x):\n        return x[self.index]\n\n    def __reduce__(self):\n        return (itemgetter, (self.index,))\n\n    def __eq__(self, other):\n        return type(self) is type(other) and self.index == other.index\n\n\nclass MethodCache:\n    \"\"\"Attribute access on this object returns a methodcaller for that\n    attribute.\n\n    Examples\n    --------\n    >>> a = [1, 3, 3]\n    >>> M.count(a, 3) == a.count(3)\n    True\n    \"\"\"\n\n    def __getattr__(self, item):\n        return methodcaller(item)\n\n    def __dir__(self):\n        return list(_method_cache)\n\n\nM = MethodCache()\n\n\nclass SerializableLock:\n    \"\"\"A Serializable per-process Lock\n\n    This wraps a normal ``threading.Lock`` object and satisfies the same\n    interface.  However, this lock can also be serialized and sent to different\n    processes.  It will not block concurrent operations between processes (for\n    this you should look at ``multiprocessing.Lock`` or ``locket.lock_file``\n    but will consistently deserialize into the same lock.\n\n    So if we make a lock in one process::\n\n        lock = SerializableLock()\n\n    And then send it over to another process multiple times::\n\n        bytes = pickle.dumps(lock)\n        a = pickle.loads(bytes)\n        b = pickle.loads(bytes)\n\n    Then the deserialized objects will operate as though they were the same\n    lock, and collide as appropriate.\n\n    This is useful for consistently protecting resources on a per-process\n    level.\n\n    The creation of locks is itself not threadsafe.\n    \"\"\"\n\n    _locks: ClassVar[WeakValueDictionary[Hashable, Lock]] = WeakValueDictionary()\n    token: Hashable\n    lock: Lock\n\n    def __init__(self, token: Hashable | None = None):\n        self.token = token or str(uuid.uuid4())\n        if self.token in SerializableLock._locks:\n            self.lock = SerializableLock._locks[self.token]\n        else:\n            self.lock = Lock()\n            SerializableLock._locks[self.token] = self.lock\n\n    def acquire(self, *args, **kwargs):\n        return self.lock.acquire(*args, **kwargs)\n\n    def release(self, *args, **kwargs):\n        return self.lock.release(*args, **kwargs)\n\n    def __enter__(self):\n        self.lock.__enter__()\n\n    def __exit__(self, *args):\n        self.lock.__exit__(*args)\n\n    def locked(self):\n        return self.lock.locked()\n\n    def __getstate__(self):\n        return self.token\n\n    def __setstate__(self, token):\n        self.__init__(token)\n\n    def __str__(self):\n        return f\"<{self.__class__.__name__}: {self.token}>\"\n\n    __repr__ = __str__\n\n\ndef get_scheduler_lock(collection=None, scheduler=None):\n    \"\"\"Get an instance of the appropriate lock for a certain situation based on\n    scheduler used.\"\"\"\n    from dask import multiprocessing\n    from dask.base import get_scheduler\n\n    actual_get = get_scheduler(collections=[collection], scheduler=scheduler)\n\n    if actual_get == multiprocessing.get:\n        return multiprocessing.get_context().Manager().Lock()\n    else:\n        # if this is a distributed client, we need to lock on\n        # the level between processes, SerializableLock won't work\n        try:\n            import distributed.lock\n            from distributed.worker import get_client\n\n            client = get_client()\n        except (ImportError, ValueError):\n            pass\n        else:\n            if actual_get == client.get:\n                return distributed.lock.Lock()\n\n    return SerializableLock()\n\n\ndef ensure_dict(d: Mapping[K, V], *, copy: bool = False) -> dict[K, V]:\n    \"\"\"Convert a generic Mapping into a dict.\n    Optimize use case of :class:`~dask.highlevelgraph.HighLevelGraph`.\n\n    Parameters\n    ----------\n    d : Mapping\n    copy : bool\n        If True, guarantee that the return value is always a shallow copy of d;\n        otherwise it may be the input itself.\n    \"\"\"\n    if type(d) is dict:\n        return d.copy() if copy else d\n    try:\n        layers = d.layers  # type: ignore\n    except AttributeError:\n        return dict(d)\n\n    result = {}\n    for layer in toolz.unique(layers.values(), key=id):\n        result.update(layer)\n    return result\n\n\ndef ensure_set(s: Set[T], *, copy: bool = False) -> set[T]:\n    \"\"\"Convert a generic Set into a set.\n\n    Parameters\n    ----------\n    s : Set\n    copy : bool\n        If True, guarantee that the return value is always a shallow copy of s;\n        otherwise it may be the input itself.\n    \"\"\"\n    if type(s) is set:\n        return s.copy() if copy else s\n    return set(s)\n\n\nclass OperatorMethodMixin:\n    \"\"\"A mixin for dynamically implementing operators\"\"\"\n\n    __slots__ = ()\n\n    @classmethod\n    def _bind_operator(cls, op):\n        \"\"\"bind operator to this class\"\"\"\n        name = op.__name__\n\n        if name.endswith(\"_\"):\n            # for and_ and or_\n            name = name[:-1]\n        elif name == \"inv\":\n            name = \"invert\"\n\n        meth = f\"__{name}__\"\n\n        if name in (\"abs\", \"invert\", \"neg\", \"pos\"):\n            setattr(cls, meth, cls._get_unary_operator(op))\n        else:\n            setattr(cls, meth, cls._get_binary_operator(op))\n\n            if name in (\"eq\", \"gt\", \"ge\", \"lt\", \"le\", \"ne\", \"getitem\"):\n                return\n\n            rmeth = f\"__r{name}__\"\n            setattr(cls, rmeth, cls._get_binary_operator(op, inv=True))\n\n    @classmethod\n    def _get_unary_operator(cls, op):\n        \"\"\"Must return a method used by unary operator\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def _get_binary_operator(cls, op, inv=False):\n        \"\"\"Must return a method used by binary operator\"\"\"\n        raise NotImplementedError\n\n\ndef partial_by_order(*args, **kwargs):\n    \"\"\"\n\n    >>> from operator import add\n    >>> partial_by_order(5, function=add, other=[(1, 10)])\n    15\n    \"\"\"\n    function = kwargs.pop(\"function\")\n    other = kwargs.pop(\"other\")\n    args2 = list(args)\n    for i, arg in other:\n        args2.insert(i, arg)\n    return function(*args2, **kwargs)\n\n\ndef is_arraylike(x) -> bool:\n    \"\"\"Is this object a numpy array or something similar?\n\n    This function tests specifically for an object that already has\n    array attributes (e.g. np.ndarray, dask.array.Array, cupy.ndarray,\n    sparse.COO), **NOT** for something that can be coerced into an\n    array object (e.g. Python lists and tuples). It is meant for dask\n    developers and developers of downstream libraries.\n\n    Note that this function does not correspond with NumPy's\n    definition of array_like, which includes any object that can be\n    coerced into an array (see definition in the NumPy glossary):\n    https://numpy.org/doc/stable/glossary.html\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> is_arraylike(np.ones(5))\n    True\n    >>> is_arraylike(np.ones(()))\n    True\n    >>> is_arraylike(5)\n    False\n    >>> is_arraylike('cat')\n    False\n    \"\"\"\n    from dask.base import is_dask_collection\n\n    is_duck_array = hasattr(x, \"__array_function__\") or hasattr(x, \"__array_ufunc__\")\n\n    return bool(\n        hasattr(x, \"shape\")\n        and isinstance(x.shape, tuple)\n        and hasattr(x, \"dtype\")\n        and not any(is_dask_collection(n) for n in x.shape)\n        # We special case scipy.sparse and cupyx.scipy.sparse arrays as having partial\n        # support for them is useful in scenarios where we mostly call `map_partitions`\n        # or `map_blocks` with scikit-learn functions on dask arrays and dask dataframes.\n        # https://github.com/dask/dask/pull/3738\n        and (is_duck_array or \"scipy.sparse\" in typename(type(x)))\n    )\n\n\ndef is_dataframe_like(df) -> bool:\n    \"\"\"Looks like a Pandas DataFrame\"\"\"\n    if (df.__class__.__module__, df.__class__.__name__) == (\n        \"pandas.core.frame\",\n        \"DataFrame\",\n    ):\n        # fast exec for most likely input\n        return True\n    typ = df.__class__\n    return (\n        all(hasattr(typ, name) for name in (\"groupby\", \"head\", \"merge\", \"mean\"))\n        and all(hasattr(df, name) for name in (\"dtypes\", \"columns\"))\n        and not any(hasattr(typ, name) for name in (\"name\", \"dtype\"))\n    )\n\n\ndef is_series_like(s) -> bool:\n    \"\"\"Looks like a Pandas Series\"\"\"\n    typ = s.__class__\n    return (\n        all(hasattr(typ, name) for name in (\"groupby\", \"head\", \"mean\"))\n        and all(hasattr(s, name) for name in (\"dtype\", \"name\"))\n        and \"index\" not in typ.__name__.lower()\n    )\n\n\ndef is_index_like(s) -> bool:\n    \"\"\"Looks like a Pandas Index\"\"\"\n    typ = s.__class__\n    return (\n        all(hasattr(s, name) for name in (\"name\", \"dtype\"))\n        and \"index\" in typ.__name__.lower()\n    )\n\n\ndef is_cupy_type(x) -> bool:\n    # TODO: avoid explicit reference to CuPy\n    return \"cupy\" in str(type(x))\n\n\ndef natural_sort_key(s: str) -> list[str | int]:\n    \"\"\"\n    Sorting `key` function for performing a natural sort on a collection of\n    strings\n\n    See https://en.wikipedia.org/wiki/Natural_sort_order\n\n    Parameters\n    ----------\n    s : str\n        A string that is an element of the collection being sorted\n\n    Returns\n    -------\n    tuple[str or int]\n        Tuple of the parts of the input string where each part is either a\n        string or an integer\n\n    Examples\n    --------\n    >>> a = ['f0', 'f1', 'f2', 'f8', 'f9', 'f10', 'f11', 'f19', 'f20', 'f21']\n    >>> sorted(a)\n    ['f0', 'f1', 'f10', 'f11', 'f19', 'f2', 'f20', 'f21', 'f8', 'f9']\n    >>> sorted(a, key=natural_sort_key)\n    ['f0', 'f1', 'f2', 'f8', 'f9', 'f10', 'f11', 'f19', 'f20', 'f21']\n    \"\"\"\n    return [int(part) if part.isdigit() else part for part in re.split(r\"(\\d+)\", s)]\n\n\ndef parse_bytes(s: float | str) -> int:\n    \"\"\"Parse byte string to numbers\n\n    >>> from dask.utils import parse_bytes\n    >>> parse_bytes('100')\n    100\n    >>> parse_bytes('100 MB')\n    100000000\n    >>> parse_bytes('100M')\n    100000000\n    >>> parse_bytes('5kB')\n    5000\n    >>> parse_bytes('5.4 kB')\n    5400\n    >>> parse_bytes('1kiB')\n    1024\n    >>> parse_bytes('1e6')\n    1000000\n    >>> parse_bytes('1e6 kB')\n    1000000000\n    >>> parse_bytes('MB')\n    1000000\n    >>> parse_bytes(123)\n    123\n    >>> parse_bytes('5 foos')\n    Traceback (most recent call last):\n        ...\n    ValueError: Could not interpret 'foos' as a byte unit\n    \"\"\"\n    if isinstance(s, (int, float)):\n        return int(s)\n    s = s.replace(\" \", \"\")\n    if not any(char.isdigit() for char in s):\n        s = \"1\" + s\n\n    for i in range(len(s) - 1, -1, -1):\n        if not s[i].isalpha():\n            break\n    index = i + 1\n\n    prefix = s[:index]\n    suffix = s[index:]\n\n    try:\n        n = float(prefix)\n    except ValueError as e:\n        raise ValueError(\"Could not interpret '%s' as a number\" % prefix) from e\n\n    try:\n        multiplier = byte_sizes[suffix.lower()]\n    except KeyError as e:\n        raise ValueError(\"Could not interpret '%s' as a byte unit\" % suffix) from e\n\n    result = n * multiplier\n    return int(result)\n\n\nbyte_sizes = {\n    \"kB\": 10**3,\n    \"MB\": 10**6,\n    \"GB\": 10**9,\n    \"TB\": 10**12,\n    \"PB\": 10**15,\n    \"KiB\": 2**10,\n    \"MiB\": 2**20,\n    \"GiB\": 2**30,\n    \"TiB\": 2**40,\n    \"PiB\": 2**50,\n    \"B\": 1,\n    \"\": 1,\n}\nbyte_sizes = {k.lower(): v for k, v in byte_sizes.items()}\nbyte_sizes.update({k[0]: v for k, v in byte_sizes.items() if k and \"i\" not in k})\nbyte_sizes.update({k[:-1]: v for k, v in byte_sizes.items() if k and \"i\" in k})\n\n\ndef format_time(n: float) -> str:\n    \"\"\"format integers as time\n\n    >>> from dask.utils import format_time\n    >>> format_time(1)\n    '1.00 s'\n    >>> format_time(0.001234)\n    '1.23 ms'\n    >>> format_time(0.00012345)\n    '123.45 us'\n    >>> format_time(123.456)\n    '123.46 s'\n    >>> format_time(1234.567)\n    '20m 34s'\n    >>> format_time(12345.67)\n    '3hr 25m'\n    >>> format_time(123456.78)\n    '34hr 17m'\n    >>> format_time(1234567.89)\n    '14d 6hr'\n    \"\"\"\n    if n > 24 * 60 * 60 * 2:\n        d = int(n / 3600 / 24)\n        h = int((n - d * 3600 * 24) / 3600)\n        return f\"{d}d {h}hr\"\n    if n > 60 * 60 * 2:\n        h = int(n / 3600)\n        m = int((n - h * 3600) / 60)\n        return f\"{h}hr {m}m\"\n    if n > 60 * 10:\n        m = int(n / 60)\n        s = int(n - m * 60)\n        return f\"{m}m {s}s\"\n    if n >= 1:\n        return \"%.2f s\" % n\n    if n >= 1e-3:\n        return \"%.2f ms\" % (n * 1e3)\n    return \"%.2f us\" % (n * 1e6)\n\n\ndef format_time_ago(n: datetime) -> str:\n    \"\"\"Calculate a '3 hours ago' type string from a Python datetime.\n\n    Examples\n    --------\n    >>> from datetime import datetime, timedelta\n\n    >>> now = datetime.now()\n    >>> format_time_ago(now)\n    'Just now'\n\n    >>> past = datetime.now() - timedelta(minutes=1)\n    >>> format_time_ago(past)\n    '1 minute ago'\n\n    >>> past = datetime.now() - timedelta(minutes=2)\n    >>> format_time_ago(past)\n    '2 minutes ago'\n\n    >>> past = datetime.now() - timedelta(hours=1)\n    >>> format_time_ago(past)\n    '1 hour ago'\n\n    >>> past = datetime.now() - timedelta(hours=6)\n    >>> format_time_ago(past)\n    '6 hours ago'\n\n    >>> past = datetime.now() - timedelta(days=1)\n    >>> format_time_ago(past)\n    '1 day ago'\n\n    >>> past = datetime.now() - timedelta(days=5)\n    >>> format_time_ago(past)\n    '5 days ago'\n\n    >>> past = datetime.now() - timedelta(days=8)\n    >>> format_time_ago(past)\n    '1 week ago'\n\n    >>> past = datetime.now() - timedelta(days=16)\n    >>> format_time_ago(past)\n    '2 weeks ago'\n\n    >>> past = datetime.now() - timedelta(days=190)\n    >>> format_time_ago(past)\n    '6 months ago'\n\n    >>> past = datetime.now() - timedelta(days=800)\n    >>> format_time_ago(past)\n    '2 years ago'\n\n    \"\"\"\n    units = {\n        \"years\": lambda diff: diff.days / 365,\n        \"months\": lambda diff: diff.days / 30.436875,  # Average days per month\n        \"weeks\": lambda diff: diff.days / 7,\n        \"days\": lambda diff: diff.days,\n        \"hours\": lambda diff: diff.seconds / 3600,\n        \"minutes\": lambda diff: diff.seconds % 3600 / 60,\n    }\n    diff = datetime.now() - n\n    for unit in units:\n        dur = int(units[unit](diff))\n        if dur > 0:\n            if dur == 1:  # De-pluralize\n                unit = unit[:-1]\n            return f\"{dur} {unit} ago\"\n    return \"Just now\"\n\n\ndef format_bytes(n: int) -> str:\n    \"\"\"Format bytes as text\n\n    >>> from dask.utils import format_bytes\n    >>> format_bytes(1)\n    '1 B'\n    >>> format_bytes(1234)\n    '1.21 kiB'\n    >>> format_bytes(12345678)\n    '11.77 MiB'\n    >>> format_bytes(1234567890)\n    '1.15 GiB'\n    >>> format_bytes(1234567890000)\n    '1.12 TiB'\n    >>> format_bytes(1234567890000000)\n    '1.10 PiB'\n\n    For all values < 2**60, the output is always <= 10 characters.\n    \"\"\"\n    for prefix, k in (\n        (\"Pi\", 2**50),\n        (\"Ti\", 2**40),\n        (\"Gi\", 2**30),\n        (\"Mi\", 2**20),\n        (\"ki\", 2**10),\n    ):\n        if n >= k * 0.9:\n            return f\"{n / k:.2f} {prefix}B\"\n    return f\"{n} B\"\n\n\ntimedelta_sizes = {\n    \"s\": 1,\n    \"ms\": 1e-3,\n    \"us\": 1e-6,\n    \"ns\": 1e-9,\n    \"m\": 60,\n    \"h\": 3600,\n    \"d\": 3600 * 24,\n    \"w\": 7 * 3600 * 24,\n}\n\ntds2 = {\n    \"second\": 1,\n    \"minute\": 60,\n    \"hour\": 60 * 60,\n    \"day\": 60 * 60 * 24,\n    \"week\": 7 * 60 * 60 * 24,\n    \"millisecond\": 1e-3,\n    \"microsecond\": 1e-6,\n    \"nanosecond\": 1e-9,\n}\ntds2.update({k + \"s\": v for k, v in tds2.items()})\ntimedelta_sizes.update(tds2)\ntimedelta_sizes.update({k.upper(): v for k, v in timedelta_sizes.items()})\n\n\n@overload\ndef parse_timedelta(s: None, default: str | Literal[False] = \"seconds\") -> None: ...\n\n\n@overload\ndef parse_timedelta(\n    s: str | float | timedelta, default: str | Literal[False] = \"seconds\"\n) -> float: ...\n\n\ndef parse_timedelta(s, default=\"seconds\"):\n    \"\"\"Parse timedelta string to number of seconds\n\n    Parameters\n    ----------\n    s : str, float, timedelta, or None\n    default: str or False, optional\n        Unit of measure if s  does not specify one. Defaults to seconds.\n        Set to False to require s to explicitly specify its own unit.\n\n    Examples\n    --------\n    >>> from datetime import timedelta\n    >>> from dask.utils import parse_timedelta\n    >>> parse_timedelta('3s')\n    3\n    >>> parse_timedelta('3.5 seconds')\n    3.5\n    >>> parse_timedelta('300ms')\n    0.3\n    >>> parse_timedelta(timedelta(seconds=3))  # also supports timedeltas\n    3\n    \"\"\"\n    if s is None:\n        return None\n    if isinstance(s, timedelta):\n        s = s.total_seconds()\n        return int(s) if int(s) == s else s\n    if isinstance(s, Number):\n        s = str(s)\n    s = s.replace(\" \", \"\")\n    if not s[0].isdigit():\n        s = \"1\" + s\n\n    for i in range(len(s) - 1, -1, -1):\n        if not s[i].isalpha():\n            break\n    index = i + 1\n\n    prefix = s[:index]\n    suffix = s[index:] or default\n    if suffix is False:\n        raise ValueError(f\"Missing time unit: {s}\")\n    if not isinstance(suffix, str):\n        raise TypeError(f\"default must be str or False, got {default!r}\")\n\n    n = float(prefix)\n\n    try:\n        multiplier = timedelta_sizes[suffix.lower()]\n    except KeyError:\n        valid_units = \", \".join(timedelta_sizes.keys())\n        raise KeyError(\n            f\"Invalid time unit: {suffix}. Valid units are: {valid_units}\"\n        ) from None\n\n    result = n * multiplier\n    if int(result) == result:\n        result = int(result)\n    return result\n\n\ndef has_keyword(func, keyword):\n    try:\n        return keyword in inspect.signature(func).parameters\n    except Exception:\n        return False\n\n\ndef ndimlist(seq):\n    if not isinstance(seq, (list, tuple)):\n        return 0\n    elif not seq:\n        return 1\n    else:\n        return 1 + ndimlist(seq[0])\n\n\ndef iter_chunks(sizes, max_size):\n    \"\"\"Split sizes into chunks of total max_size each\n\n    Parameters\n    ----------\n    sizes : iterable of numbers\n        The sizes to be chunked\n    max_size : number\n        Maximum total size per chunk.\n        It must be greater or equal than each size in sizes\n    \"\"\"\n    chunk, chunk_sum = [], 0\n    iter_sizes = iter(sizes)\n    size = next(iter_sizes, None)\n    while size is not None:\n        assert size <= max_size\n        if chunk_sum + size <= max_size:\n            chunk.append(size)\n            chunk_sum += size\n            size = next(iter_sizes, None)\n        else:\n            assert chunk\n            yield chunk\n            chunk, chunk_sum = [], 0\n    if chunk:\n        yield chunk\n\n\nhex_pattern = re.compile(\"[a-f]+\")\n\n\n@functools.lru_cache(100000)\ndef key_split(s):\n    \"\"\"\n    >>> key_split('x')\n    'x'\n    >>> key_split('x-1')\n    'x'\n    >>> key_split('x-1-2-3')\n    'x'\n    >>> key_split(('x-2', 1))\n    'x'\n    >>> key_split(\"('x-2', 1)\")\n    'x'\n    >>> key_split(\"('x', 1)\")\n    'x'\n    >>> key_split('hello-world-1')\n    'hello-world'\n    >>> key_split(b'hello-world-1')\n    'hello-world'\n    >>> key_split('ae05086432ca935f6eba409a8ecd4896')\n    'data'\n    >>> key_split('<module.submodule.myclass object at 0xdaf372')\n    'myclass'\n    >>> key_split(None)\n    'Other'\n    >>> key_split('x-abcdefab')  # ignores hex\n    'x'\n    >>> key_split('_(x)')  # strips unpleasant characters\n    'x'\n    \"\"\"\n    # If we convert the key, recurse to utilize LRU cache better\n    if type(s) is bytes:\n        return key_split(s.decode())\n    if type(s) is tuple:\n        return key_split(s[0])\n    try:\n        words = s.split(\"-\")\n        if not words[0][0].isalpha():\n            result = words[0].split(\",\")[0].strip(\"_'()\\\"\")\n        else:\n            result = words[0]\n        for word in words[1:]:\n            if word.isalpha() and not (\n                len(word) == 8 and hex_pattern.match(word) is not None\n            ):\n                result += \"-\" + word\n            else:\n                break\n        if len(result) == 32 and re.match(r\"[a-f0-9]{32}\", result):\n            return \"data\"\n        else:\n            if result[0] == \"<\":\n                result = result.strip(\"<>\").split()[0].split(\".\")[-1]\n            return sys.intern(result)\n    except Exception:\n        return \"Other\"\n\n\ndef stringify(obj, exclusive: Iterable | None = None):\n    \"\"\"Convert an object to a string\n\n    If ``exclusive`` is specified, search through `obj` and convert\n    values that are in ``exclusive``.\n\n    Note that when searching through dictionaries, only values are\n    converted, not the keys.\n\n    Parameters\n    ----------\n    obj : Any\n        Object (or values within) to convert to string\n    exclusive: Iterable, optional\n        Set of values to search for when converting values to strings\n\n    Returns\n    -------\n    result : type(obj)\n        Stringified copy of ``obj`` or ``obj`` itself if it is already a\n        string or bytes.\n\n    Examples\n    --------\n    >>> stringify(b'x')\n    b'x'\n    >>> stringify('x')\n    'x'\n    >>> stringify({('a',0):('a',0), ('a',1): ('a',1)})\n    \"{('a', 0): ('a', 0), ('a', 1): ('a', 1)}\"\n    >>> stringify({('a',0):('a',0), ('a',1): ('a',1)}, exclusive={('a',0)})\n    {('a', 0): \"('a', 0)\", ('a', 1): ('a', 1)}\n    \"\"\"\n\n    typ = type(obj)\n    if typ is str or typ is bytes:\n        return obj\n    elif exclusive is None:\n        return str(obj)\n\n    if typ is list:\n        return [stringify(v, exclusive) for v in obj]\n    if typ is dict:\n        return {k: stringify(v, exclusive) for k, v in obj.items()}\n    try:\n        if obj in exclusive:\n            return stringify(obj)\n    except TypeError:  # `obj` not hashable\n        pass\n    if typ is tuple:  # If the tuple itself isn't a key, check its elements\n        return tuple(stringify(v, exclusive) for v in obj)\n    return obj\n\n\nclass cached_property(functools.cached_property):\n    \"\"\"Read only version of functools.cached_property.\"\"\"\n\n    def __set__(self, instance, val):\n        \"\"\"Raise an error when attempting to set a cached property.\"\"\"\n        raise AttributeError(\"Can't set attribute\")\n\n\nclass _HashIdWrapper:\n    \"\"\"Hash and compare a wrapped object by identity instead of value\"\"\"\n\n    def __init__(self, wrapped):\n        self.wrapped = wrapped\n\n    def __eq__(self, other):\n        if not isinstance(other, _HashIdWrapper):\n            return NotImplemented\n        return self.wrapped is other.wrapped\n\n    def __ne__(self, other):\n        if not isinstance(other, _HashIdWrapper):\n            return NotImplemented\n        return self.wrapped is not other.wrapped\n\n    def __hash__(self):\n        return id(self.wrapped)\n\n\n@functools.lru_cache\ndef _cumsum(seq, initial_zero):\n    if isinstance(seq, _HashIdWrapper):\n        seq = seq.wrapped\n    if initial_zero:\n        return tuple(toolz.accumulate(add, seq, 0))\n    else:\n        return tuple(toolz.accumulate(add, seq))\n\n\n@functools.lru_cache\ndef _max(seq):\n    if isinstance(seq, _HashIdWrapper):\n        seq = seq.wrapped\n    return max(seq)\n\n\ndef cached_max(seq):\n    \"\"\"Compute max with caching.\n\n    Caching is by the identity of `seq` rather than the value. It is thus\n    important that `seq` is a tuple of immutable objects, and this function\n    is intended for use where `seq` is a value that will persist (generally\n    block sizes).\n\n    Parameters\n    ----------\n    seq : tuple\n        Values to reduce\n\n    Returns\n    -------\n    tuple\n    \"\"\"\n    assert isinstance(seq, tuple)\n    # Look up by identity first, to avoid a linear-time __hash__\n    # if we've seen this tuple object before.\n    result = _max(_HashIdWrapper(seq))\n    return result\n\n\ndef cached_cumsum(seq, initial_zero=False):\n    \"\"\"Compute :meth:`toolz.accumulate` with caching.\n\n    Caching is by the identify of `seq` rather than the value. It is thus\n    important that `seq` is a tuple of immutable objects, and this function\n    is intended for use where `seq` is a value that will persist (generally\n    block sizes).\n\n    Parameters\n    ----------\n    seq : tuple\n        Values to cumulatively sum.\n    initial_zero : bool, optional\n        If true, the return value is prefixed with a zero.\n\n    Returns\n    -------\n    tuple\n    \"\"\"\n    if isinstance(seq, tuple):\n        # Look up by identity first, to avoid a linear-time __hash__\n        # if we've seen this tuple object before.\n        result = _cumsum(_HashIdWrapper(seq), initial_zero)\n    else:\n        # Construct a temporary tuple, and look up by value.\n        result = _cumsum(tuple(seq), initial_zero)\n    return result\n\n\ndef show_versions() -> None:\n    \"\"\"Provide version information for bug reports.\"\"\"\n\n    from json import dumps\n    from platform import uname\n    from sys import stdout, version_info\n\n    from dask._compatibility import importlib_metadata\n\n    try:\n        from distributed import __version__ as distributed_version\n    except ImportError:\n        distributed_version = None\n\n    from dask import __version__ as dask_version\n\n    deps = [\n        \"numpy\",\n        \"pandas\",\n        \"cloudpickle\",\n        \"fsspec\",\n        \"bokeh\",\n        \"pyarrow\",\n        \"zarr\",\n    ]\n\n    result: dict[str, str | None] = {\n        # note: only major, minor, micro are extracted\n        \"Python\": \".\".join([str(i) for i in version_info[:3]]),\n        \"Platform\": uname().system,\n        \"dask\": dask_version,\n        \"distributed\": distributed_version,\n    }\n\n    for modname in deps:\n        try:\n            result[modname] = importlib_metadata.version(modname)\n        except importlib_metadata.PackageNotFoundError:\n            result[modname] = None\n\n    stdout.writelines(dumps(result, indent=2))\n\n    return\n\n\ndef maybe_pluralize(count, noun, plural_form=None):\n    \"\"\"Pluralize a count-noun string pattern when necessary\"\"\"\n    if count == 1:\n        return f\"{count} {noun}\"\n    else:\n        return f\"{count} {plural_form or noun + 's'}\"\n\n\ndef is_namedtuple_instance(obj: Any) -> bool:\n    \"\"\"Returns True if obj is an instance of a namedtuple.\n\n    Note: This function checks for the existence of the methods and\n    attributes that make up the namedtuple API, so it will return True\n    IFF obj's type implements that API.\n    \"\"\"\n    return (\n        isinstance(obj, tuple)\n        and hasattr(obj, \"_make\")\n        and hasattr(obj, \"_asdict\")\n        and hasattr(obj, \"_replace\")\n        and hasattr(obj, \"_fields\")\n        and hasattr(obj, \"_field_defaults\")\n    )\n\n\ndef get_default_shuffle_method() -> str:\n    if d := config.get(\"dataframe.shuffle.method\", None):\n        return d\n    try:\n        from distributed import default_client\n\n        default_client()\n    except (ImportError, ValueError):\n        return \"disk\"\n\n    try:\n        from distributed.shuffle import check_minimal_arrow_version\n\n        check_minimal_arrow_version()\n    except ModuleNotFoundError:\n        return \"tasks\"\n    return \"p2p\"\n\n\ndef get_meta_library(like):\n    if hasattr(like, \"_meta\"):\n        like = like._meta\n\n    return import_module(typename(like).partition(\".\")[0])\n\n\nclass shorten_traceback:\n    \"\"\"Context manager that removes irrelevant stack elements from traceback.\n\n    * omits frames from modules that match `admin.traceback.shorten`\n    * always keeps the first and last frame.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: types.TracebackType | None,\n    ) -> None:\n        if exc_val and exc_tb:\n            exc_val.__traceback__ = self.shorten(exc_tb)\n\n    @staticmethod\n    def shorten(exc_tb: types.TracebackType) -> types.TracebackType:\n        paths = config.get(\"admin.traceback.shorten\")\n        if not paths:\n            return exc_tb\n\n        exp = re.compile(\".*(\" + \"|\".join(paths) + \")\")\n        curr: types.TracebackType | None = exc_tb\n        prev: types.TracebackType | None = None\n\n        while curr:\n            if prev is None:\n                prev = curr  # first frame\n            elif not curr.tb_next:\n                # always keep last frame\n                prev.tb_next = curr\n                prev = prev.tb_next\n            elif not exp.match(curr.tb_frame.f_code.co_filename):\n                # keep if module is not listed in config\n                prev.tb_next = curr\n                prev = curr\n            curr = curr.tb_next\n\n        # Uncomment to remove the first frame, which is something you don't want to keep\n        # if it matches the regexes. Requires Python >=3.11.\n        # if exc_tb.tb_next and exp.match(exc_tb.tb_frame.f_code.co_filename):\n        #     return exc_tb.tb_next\n\n        return exc_tb\n\n\ndef unzip(ls, nout):\n    \"\"\"Unzip a list of lists into ``nout`` outputs.\"\"\"\n    out = list(zip(*ls))\n    if not out:\n        out = [()] * nout\n    return out\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:36:31"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport copy\nimport os\nimport re\nimport sys\nfrom functools import partial\nfrom operator import add, neg\n\nimport pytest\n\nipycytoscape = pytest.importorskip(\"ipycytoscape\")\nfrom dask.dot import _to_cytoscape_json, cytoscape_graph\n\nif sys.flags.optimize != 2:\n    pytest.importorskip(\"graphviz\")\n    from dask.dot import dot_graph, label, task_label, to_graphviz\nelse:\n    pytestmark = pytest.mark.skipif(\n        True, reason=\"graphviz exception with Python -OO flag\"\n    )\n\nfrom dask import delayed\nfrom dask.utils import ensure_not_exists\n\ntry:\n    from IPython.display import SVG, Image\nexcept ImportError:\n    ipython_not_installed = True\n    Image = None\n    SVG = None\nelse:\n    ipython_not_installed = False\nipython_not_installed_mark = pytest.mark.skipif(\n    ipython_not_installed, reason=\"IPython not installed\"\n)\n\n\n# Since graphviz doesn't store a graph, we need to parse the output\nlabel_re = re.compile(r\".*\\[label=(.*?) shape=(.*?)\\]\")\n\n\ndef get_label(line):\n    m = label_re.match(line)\n    if m:\n        return m.group(1)\n\n\ndef get_shape(line):\n    m = label_re.match(line)\n    if m:\n        return m.group(2)\n\n\ndsk = {\n    \"a\": 1,\n    \"b\": 2,\n    \"c\": (neg, \"a\"),\n    \"d\": (neg, \"b\"),\n    \"e\": (add, \"c\", \"d\"),\n    \"f\": (sum, [\"a\", \"e\"]),\n}\n\n\ndef test_task_label():\n    assert task_label((partial(add, 1), 1)) == \"add\"\n    assert task_label((add, 1)) == \"add\"\n    assert task_label((add, (add, 1, 2))) == \"add(...)\"\n\n\ndef test_label():\n    assert label(\"x\") == \"x\"\n    assert label(\"elemwise-ffcd9aa2231d466b5aa91e8bfa9e9487\") == \"elemwise-#\"\n\n    cache = {}\n    result = label(\"elemwise-ffcd9aa2231d466b5aa91e8bfa9e9487\", cache=cache)\n    assert result == \"elemwise-#0\"\n    # cached\n    result = label(\"elemwise-ffcd9aa2231d466b5aa91e8bfa9e9487\", cache=cache)\n    assert result == \"elemwise-#0\"\n    assert len(cache) == 1\n\n    result = label(\"elemwise-e890b510984f344edea9a5e5fe05c0db\", cache=cache)\n    assert result == \"elemwise-#1\"\n    assert len(cache) == 2\n\n    result = label(\"elemwise-ffcd9aa2231d466b5aa91e8bfa9e9487\", cache=cache)\n    assert result == \"elemwise-#0\"\n    assert len(cache) == 2\n\n    assert label(\"x\", cache=cache) == \"x\"\n    assert len(cache) == 2\n\n\ndef test_to_graphviz():\n    g = to_graphviz(dsk)\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 10  # 10 nodes total\n    assert set(labels) == {\"c\", \"d\", \"e\", \"f\", '\"\"'}\n    shapes = list(filter(None, map(get_shape, g.body)))\n    assert set(shapes) == {\"box\", \"circle\"}\n\n\ndef test_to_graphviz_custom():\n    g = to_graphviz(\n        dsk,\n        data_attributes={\"a\": {\"shape\": \"square\"}},\n        function_attributes={\"c\": {\"label\": \"neg_c\", \"shape\": \"ellipse\"}},\n    )\n    labels = set(filter(None, map(get_label, g.body)))\n    assert labels == {\"neg_c\", \"d\", \"e\", \"f\", '\"\"'}\n    shapes = list(filter(None, map(get_shape, g.body)))\n    assert set(shapes) == {\"box\", \"circle\", \"square\", \"ellipse\"}\n\n\ndef test_cytoscape_graph_custom(tmp_path):\n    # Check that custom styling and layout propagates through\n    g = cytoscape_graph(\n        dsk,\n        filename=os.fsdecode(tmp_path / \"mydask.html\"),\n        rankdir=\"LR\",\n        node_sep=20,\n        edge_sep=30,\n        spacing_factor=2,\n        edge_style={\"line-color\": \"red\"},\n        node_style={\"background-color\": \"green\"},\n    )\n    sty = g.cytoscape_style\n    layout = g.cytoscape_layout\n    node_sty = next(s for s in sty if s[\"selector\"] == \"node\")[\"style\"]\n    edge_sty = next(s for s in sty if s[\"selector\"] == \"edge\")[\"style\"]\n\n    assert edge_sty[\"line-color\"] == \"red\"\n    assert node_sty[\"background-color\"] == \"green\"\n    assert layout[\"rankDir\"] == \"LR\"\n    assert layout[\"nodeSep\"] == 20\n    assert layout[\"edgeSep\"] == 30\n    assert layout[\"spacingFactor\"] == 2\n\n\ndef test_cytoscape_graph_color():\n    pytest.importorskip(\"matplotlib.pyplot\")\n    from dask.delayed import Delayed\n\n    g = Delayed(\"f\", dsk).visualize(engine=\"cytoscape\")\n    init_color = g.graph.nodes[0].data[\"color\"]\n    # Check that we changed the semantic color\n    g = Delayed(\"f\", dsk).visualize(engine=\"cytoscape\", color=\"order\")\n    assert any(n.data[\"color\"] != init_color for n in g.graph.nodes)\n\n\ndef test_to_graphviz_attributes():\n    assert to_graphviz(dsk).graph_attr[\"rankdir\"] == \"BT\"\n    assert to_graphviz(dsk, rankdir=\"LR\").graph_attr[\"rankdir\"] == \"LR\"\n    assert to_graphviz(dsk, node_attr={\"color\": \"white\"}).node_attr[\"color\"] == \"white\"\n    assert to_graphviz(dsk, edge_attr={\"color\": \"white\"}).edge_attr[\"color\"] == \"white\"\n\n\ndef test_aliases():\n    g = to_graphviz({\"x\": 1, \"y\": \"x\"})\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 2\n    assert len(g.body) - len(labels) == 1  # Single edge\n\n\ndef test_to_graphviz_verbose():\n    g = to_graphviz(dsk, verbose=True)\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 10  # 10 nodes total\n    assert set(labels) == {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n    shapes = list(filter(None, map(get_shape, g.body)))\n    assert set(shapes) == {\"box\", \"circle\"}\n\n\ndef test__to_cytoscape_json_verbose():\n    data = _to_cytoscape_json(dsk, verbose=True)\n    labels = list(map(lambda x: x[\"data\"][\"label\"], data[\"nodes\"]))\n    assert len(labels) == 10\n    assert set(labels) == {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n    shapes = list(map(lambda x: x[\"data\"][\"shape\"], data[\"nodes\"]))\n    assert set(shapes) == {\"ellipse\", \"rectangle\"}\n\n\ndef test_to_graphviz_collapse_outputs():\n    g = to_graphviz(dsk, collapse_outputs=True)\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 6  # 6 nodes total\n    assert set(labels) == {\"c\", \"d\", \"e\", \"f\", '\"\"'}\n    shapes = list(filter(None, map(get_shape, g.body)))\n    assert set(shapes) == {\"box\", \"circle\"}\n\n\ndef test__to_cytoscape_json_collapse_outputs():\n    data = _to_cytoscape_json(dsk, collapse_outputs=True)\n    labels = list(map(lambda x: x[\"data\"][\"label\"], data[\"nodes\"]))\n    assert len(labels) == 6  # 6 nodes total\n    assert set(labels) == {\"c\", \"d\", \"e\", \"f\", \"\"}\n    shapes = list(map(lambda x: x[\"data\"][\"shape\"], data[\"nodes\"]))\n    assert set(shapes) == {\"ellipse\", \"rectangle\"}\n\n\ndef test_to_graphviz_collapse_outputs_and_verbose():\n    g = to_graphviz(dsk, collapse_outputs=True, verbose=True)\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 6  # 6 nodes total\n    assert set(labels) == {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n    shapes = list(filter(None, map(get_shape, g.body)))\n    assert set(shapes) == {\"box\", \"circle\"}\n\n\ndef test__to_cytoscape_json_collapse_outputs_and_verbose():\n    data = _to_cytoscape_json(dsk, collapse_outputs=True, verbose=True)\n    labels = list(map(lambda x: x[\"data\"][\"label\"], data[\"nodes\"]))\n    assert len(labels) == 6  # 6 nodes total\n    assert set(labels) == {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n    shapes = list(map(lambda x: x[\"data\"][\"shape\"], data[\"nodes\"]))\n    assert set(shapes) == {\"ellipse\", \"rectangle\"}\n\n\ndef test_to_graphviz_with_unconnected_node():\n    dsk2 = dsk.copy()\n    dsk2[\"g\"] = 3\n    g = to_graphviz(dsk2, verbose=True)\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 11  # 11 nodes total\n    assert set(labels) == {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"}\n\n    g = to_graphviz(dsk, verbose=True, collapse_outputs=True)\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 6  # 6 nodes total\n    assert set(labels) == {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n\n\n@pytest.mark.parametrize(\n    \"format,typ\",\n    [\n        pytest.param(\"png\", Image, marks=ipython_not_installed_mark),\n        pytest.param(\n            \"jpeg\",\n            Image,\n            marks=pytest.mark.xfail(\n                reason=\"jpeg not always supported in dot\", strict=False\n            ),\n        ),\n        (\"dot\", type(None)),\n        (\"pdf\", type(None)),\n        pytest.param(\"svg\", SVG, marks=ipython_not_installed_mark),\n    ],\n)\n@pytest.mark.xfail(\n    sys.platform == \"win32\",\n    reason=\"graphviz/pango on conda-forge currently broken for windows\",\n    strict=False,\n)\ndef test_dot_graph(tmpdir, format, typ):\n    # Use a name that the shell would interpret specially to ensure that we're\n    # not vulnerable to shell injection when interacting with `dot`.\n    filename = str(tmpdir.join(\"$(touch should_not_get_created.txt)\"))\n\n    target = \".\".join([filename, format])\n    ensure_not_exists(target)\n    try:\n        result = dot_graph(dsk, filename=filename, format=format)\n\n        assert not os.path.exists(\"should_not_get_created.txt\")\n        assert os.path.isfile(target)\n        assert isinstance(result, typ)\n    finally:\n        ensure_not_exists(target)\n\n\n@pytest.mark.parametrize(\n    \"format,typ\",\n    [\n        pytest.param(\"png\", Image, marks=ipython_not_installed_mark),\n        pytest.param(\n            \"jpeg\",\n            Image,\n            marks=pytest.mark.xfail(\n                reason=\"jpeg not always supported in dot\", strict=False\n            ),\n        ),\n        (\"dot\", type(None)),\n        (\"pdf\", type(None)),\n        pytest.param(\"svg\", SVG, marks=ipython_not_installed_mark),\n    ],\n)\n@pytest.mark.xfail(\n    sys.platform == \"win32\",\n    reason=\"graphviz/pango on conda-forge currently broken for windows\",\n    strict=False,\n)\ndef test_dot_graph_no_filename(tmpdir, format, typ):\n    before = tmpdir.listdir()\n    result = dot_graph(dsk, filename=None, format=format)\n    # We shouldn't write any files if filename is None.\n    after = tmpdir.listdir()\n    assert before == after\n    assert isinstance(result, typ)\n\n\n@ipython_not_installed_mark\n@pytest.mark.xfail(\n    sys.platform == \"win32\",\n    reason=\"graphviz/pango on conda-forge currently broken for windows\",\n    strict=False,\n)\ndef test_dot_graph_defaults():\n    # Test with default args.\n    default_name = \"mydask\"\n    default_format = \"png\"\n    target = \".\".join([default_name, default_format])\n\n    ensure_not_exists(target)\n    try:\n        result = dot_graph(dsk)\n        assert os.path.isfile(target)\n        assert isinstance(result, Image)\n    finally:\n        ensure_not_exists(target)\n\n\ndef test_cytoscape_graph(tmpdir):\n    target = str(tmpdir.join(\"mydask.html\"))\n    ensure_not_exists(target)\n    try:\n        result = cytoscape_graph(dsk, target)\n        assert os.path.isfile(target)\n        assert isinstance(result, ipycytoscape.CytoscapeWidget)\n    finally:\n        ensure_not_exists(target)\n\n\n@pytest.mark.parametrize(\n    \"filename,format,target,expected_result_type\",\n    [\n        pytest.param(\n            \"mydaskpdf\", \"svg\", \"mydaskpdf.svg\", SVG, marks=ipython_not_installed_mark\n        ),\n        (\"mydask.pdf\", None, \"mydask.pdf\", type(None)),\n        pytest.param(\n            \"mydask.pdf\", \"svg\", \"mydask.pdf.svg\", SVG, marks=ipython_not_installed_mark\n        ),\n        pytest.param(\n            \"mydaskpdf\", None, \"mydaskpdf.png\", Image, marks=ipython_not_installed_mark\n        ),\n        pytest.param(\n            \"mydask.pdf.svg\",\n            None,\n            \"mydask.pdf.svg\",\n            SVG,\n            marks=ipython_not_installed_mark,\n        ),\n    ],\n)\n@pytest.mark.xfail(\n    sys.platform == \"win32\",\n    reason=\"graphviz/pango on conda-forge currently broken for windows\",\n    strict=False,\n)\ndef test_filenames_and_formats(tmpdir, filename, format, target, expected_result_type):\n    result = dot_graph(dsk, filename=str(tmpdir.join(filename)), format=format)\n    assert tmpdir.join(target).exists()\n    assert isinstance(result, expected_result_type)\n\n\ndef test_delayed_kwargs_apply():\n    def f(x, y=True):\n        return x + y\n\n    x = delayed(f)(1, y=2)\n    label = task_label(x.dask[x.key])\n    assert \"f\" in label\n    assert \"apply\" not in label\n\n\n@pytest.mark.parametrize(\"viz_func\", [to_graphviz, _to_cytoscape_json])\ndef test_immutable_attributes(viz_func):\n    def inc(x):\n        return x + 1\n\n    dsk = {\"a\": (inc, 1), \"b\": (inc, 2), \"c\": (add, \"a\", \"b\")}\n    attrs_func = {\"a\": {}}\n    attrs_data = {\"b\": {}}\n    attrs_func_test = copy.deepcopy(attrs_func)\n    attrs_data_test = copy.deepcopy(attrs_data)\n\n    viz_func(\n        dsk,\n        function_attributes=attrs_func,\n        data_attributes=attrs_data,\n    )\n\n    assert attrs_func_test == attrs_func\n    assert attrs_data_test == attrs_data\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_local\\test_local.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"\nAsynchronous Shared-Memory Scheduler for Dask Graphs.\n\nThis scheduler coordinates several workers to execute tasks in a dask graph in\nparallel.  It depends on a ``concurrent.futures.Executor``\nand a corresponding Queue for worker-to-scheduler communication.\n\nIt tries to execute tasks in an order which maintains a small memory footprint\nthroughout execution.  It does this by running tasks that allow us to release\ndata resources.\n\n\nTask Selection Policy\n=====================\n\nWhen we complete a task we add more data in to our set of available data; this\nnew data makes new tasks available.  We preferentially choose tasks that were\njust made available in a last-in-first-out fashion.  We implement this as a\nsimple stack.  This results in more depth-first rather than breadth first\nbehavior which encourages us to process batches of data to completion before\nstarting in on new data when possible.\n\nWhen the addition of new data readies multiple tasks simultaneously we add\ntasks to the stack in sorted order so that tasks with greater keynames are run\nfirst.  This can be handy to break ties in a predictable fashion.\n\n\nState\n=====\n\nMany functions pass around a ``state`` variable that holds the current state of\nthe computation.  This variable consists of several other dictionaries and\nsets, explained below.\n\nConstant state\n--------------\n\n1.  dependencies: {x: [a, b ,c]} a,b,c, must be run before x\n2.  dependents: {a: [x, y]} a must run before x or y\n\nChanging state\n--------------\n\n### Data\n\n1.  cache: available concrete data.  {key: actual-data}\n2.  released: data that we've seen, used, and released because it is no longer\n    needed\n\n### Jobs\n\n1.  ready: A fifo stack of ready-to-run tasks\n2.  running: A set of tasks currently in execution\n3.  finished: A set of finished tasks\n4.  waiting: which tasks are still waiting on others :: {key: {keys}}\n    Real-time equivalent of dependencies\n5.  waiting_data: available data to yet-to-be-run-tasks :: {key: {keys}}\n    Real-time equivalent of dependents\n\n\nExamples\n--------\n\n>>> import pprint  # doctest: +SKIP\n>>> inc = lambda x: x + 1\n>>> add = lambda x, y: x + y\n>>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n>>> pprint.pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n{'cache': {'x': 1, 'y': 2},\n 'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n 'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n 'finished': set(),\n 'ready': ['z'],\n 'released': set(),\n 'running': set(),\n 'waiting': {'w': {'z'}},\n 'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n\nOptimizations\n=============\n\nWe build this scheduler with out-of-core array operations in mind.  To this end\nwe have encoded some particular optimizations.\n\nCompute to release data\n-----------------------\n\nWhen we choose a new task to execute we often have many options.  Policies at\nthis stage are cheap and can significantly impact performance.  One could\nimagine policies that expose parallelism, drive towards a particular output,\netc..\n\nOur current policy is to run tasks that were most recently made available.\n\n\nInlining computations\n---------------------\n\nWe hold on to intermediate computations either in memory or on disk.\n\nFor very cheap computations that may emit new copies of the data, like\n``np.transpose`` or possibly even ``x + 1`` we choose not to store these as\nseparate pieces of data / tasks.  Instead we combine them with the computations\nthat require them.  This may result in repeated computation but saves\nsignificantly on space and computation complexity.\n\nSee the function ``inline_functions`` for more information.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections.abc import Mapping, Sequence\nfrom concurrent.futures import Executor, Future\nfrom functools import partial\nfrom queue import Empty, Queue\n\nfrom dask import config\nfrom dask._task_spec import DataNode, DependenciesMapping, convert_legacy_graph\nfrom dask.callbacks import local_callbacks, unpack_callbacks\nfrom dask.core import flatten, get_dependencies, reverse_dict\nfrom dask.order import order\nfrom dask.typing import Key\n\nif os.name == \"nt\":\n    # Python 3 windows Queue.get doesn't handle interrupts properly. To\n    # workaround this we poll at a sufficiently large interval that it\n    # shouldn't affect performance, but small enough that users trying to kill\n    # an application shouldn't care.\n    def queue_get(q):\n        while True:\n            try:\n                return q.get(block=True, timeout=0.1)\n            except Empty:\n                pass\n\nelse:\n\n    def queue_get(q):\n        return q.get()\n\n\ndef start_state_from_dask(dsk, cache=None, sortkey=None):\n    \"\"\"Start state from a dask\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n    >>> from pprint import pprint  # doctest: +SKIP\n    >>> pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n    {'cache': {'x': 1, 'y': 2},\n     'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n     'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n     'finished': set(),\n     'ready': ['z'],\n     'released': set(),\n     'running': set(),\n     'waiting': {'w': {'z'}},\n     'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n    \"\"\"\n    if sortkey is None:\n        sortkey = order(dsk).get\n    if cache is None:\n        cache = config.get(\"cache\", None)\n    if cache is None:\n        cache = dict()\n\n    dsk = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    data_keys = set()\n    for k, v in dsk.items():\n        if isinstance(v, DataNode):\n            cache[k] = v()\n            data_keys.add(k)\n\n    dsk2 = dsk.copy()\n    dsk2.update(cache)\n\n    dependencies = DependenciesMapping(dsk)\n    waiting = {k: set(v) for k, v in dependencies.items() if k not in data_keys}\n\n    dependents = reverse_dict(dependencies)\n    for a in cache:\n        for b in dependents.get(a, ()):\n            waiting[b].remove(a)\n    waiting_data = {k: v.copy() for k, v in dependents.items() if v}\n\n    ready_set = {k for k, v in waiting.items() if not v}\n    ready = sorted(ready_set, key=sortkey, reverse=True)\n    waiting = {k: v for k, v in waiting.items() if v}\n\n    state = {\n        \"dependencies\": dependencies,\n        \"dependents\": dependents,\n        \"waiting\": waiting,\n        \"waiting_data\": waiting_data,\n        \"cache\": cache,\n        \"ready\": ready,\n        \"running\": set(),\n        \"finished\": set(),\n        \"released\": set(),\n    }\n\n    return state\n\n\n\"\"\"\nRunning tasks\n-------------\n\nWhen we execute tasks we both\n\n1.  Perform the actual work of collecting the appropriate data and calling the function\n2.  Manage administrative state to coordinate with the scheduler\n\"\"\"\n\n\ndef execute_task(key, task_info, dumps, loads, get_id, pack_exception):\n    \"\"\"\n    Compute task and handle all administration\n\n    See Also\n    --------\n    _execute_task : actually execute task\n    \"\"\"\n    try:\n        task, data = loads(task_info)\n        result = task(data)\n        id = get_id()\n        result = dumps((result, id))\n        failed = False\n    except BaseException as e:  # noqa: B036\n        result = pack_exception(e, dumps)\n        failed = True\n    return key, result, failed\n\n\ndef batch_execute_tasks(it):\n    \"\"\"\n    Batch computing of multiple tasks with `execute_task`\n    \"\"\"\n    return [execute_task(*a) for a in it]\n\n\ndef release_data(key, state, delete=True):\n    \"\"\"Remove data from temporary storage\n\n    See Also\n    --------\n    finish_task\n    \"\"\"\n    if key in state[\"waiting_data\"]:\n        assert not state[\"waiting_data\"][key]\n        del state[\"waiting_data\"][key]\n\n    state[\"released\"].add(key)\n\n    if delete:\n        del state[\"cache\"][key]\n\n\ndef finish_task(\n    dsk, key, state, results, sortkey, delete=True, release_data=release_data\n):\n    \"\"\"\n    Update execution state after a task finishes\n\n    Mutates.  This should run atomically (with a lock).\n    \"\"\"\n    for dep in sorted(state[\"dependents\"][key], key=sortkey, reverse=True):\n        s = state[\"waiting\"][dep]\n        s.remove(key)\n        if not s:\n            del state[\"waiting\"][dep]\n            state[\"ready\"].append(dep)\n\n    for dep in state[\"dependencies\"][key]:\n        if dep in state[\"waiting_data\"]:\n            s = state[\"waiting_data\"][dep]\n            s.remove(key)\n            if not s and dep not in results:\n                release_data(dep, state, delete=delete)\n        elif delete and dep not in results:\n            release_data(dep, state, delete=delete)\n\n    state[\"finished\"].add(key)\n    state[\"running\"].remove(key)\n\n    return state\n\n\ndef nested_get(ind, coll):\n    \"\"\"Get nested index from collection\n\n    Examples\n    --------\n\n    >>> nested_get(1, 'abc')\n    'b'\n    >>> nested_get([1, 0], 'abc')\n    ('b', 'a')\n    >>> nested_get([[1, 0], [0, 1]], 'abc')\n    (('b', 'a'), ('a', 'b'))\n    \"\"\"\n    if isinstance(ind, list):\n        return tuple(nested_get(i, coll) for i in ind)\n    else:\n        return coll[ind]\n\n\ndef default_get_id():\n    \"\"\"Default get_id\"\"\"\n    return None\n\n\ndef default_pack_exception(e, dumps):\n    raise\n\n\ndef reraise(exc, tb=None):\n    if exc.__traceback__ is not tb:\n        raise exc.with_traceback(tb)\n    raise exc\n\n\ndef identity(x):\n    \"\"\"Identity function. Returns x.\n\n    >>> identity(3)\n    3\n    \"\"\"\n    return x\n\n\n\"\"\"\nTask Selection\n--------------\n\nWe often have a choice among many tasks to run next.  This choice is both\ncheap and can significantly impact performance.\n\nWe currently select tasks that have recently been made ready.  We hope that\nthis first-in-first-out policy reduces memory footprint\n\"\"\"\n\n\"\"\"\n`get`\n-----\n\nThe main function of the scheduler.  Get is the main entry point.\n\"\"\"\n\n\ndef get_async(\n    submit,\n    num_workers,\n    dsk,\n    result,\n    cache=None,\n    get_id=default_get_id,\n    rerun_exceptions_locally=None,\n    pack_exception=default_pack_exception,\n    raise_exception=reraise,\n    callbacks=None,\n    dumps=identity,\n    loads=identity,\n    chunksize=None,\n    **kwargs,\n):\n    \"\"\"Asynchronous get function\n\n    This is a general version of various asynchronous schedulers for dask.  It\n    takes a ``concurrent.futures.Executor.submit`` function to form a more\n    specific ``get`` method that walks through the dask array with parallel\n    workers, avoiding repeat computation and minimizing memory use.\n\n    Parameters\n    ----------\n    submit : function\n        A ``concurrent.futures.Executor.submit`` function\n    num_workers : int\n        The number of workers that task submissions can be spread over\n    dsk : dict\n        A dask dictionary specifying a workflow\n    result : key or list of keys\n        Keys corresponding to desired data\n    cache : dict-like, optional\n        Temporary storage of results\n    get_id : callable, optional\n        Function to return the worker id, takes no arguments. Examples are\n        `threading.current_thread` and `multiprocessing.current_process`.\n    rerun_exceptions_locally : bool, optional\n        Whether to rerun failing tasks in local process to enable debugging\n        (False by default)\n    pack_exception : callable, optional\n        Function to take an exception and ``dumps`` method, and return a\n        serialized tuple of ``(exception, traceback)`` to send back to the\n        scheduler. Default is to just raise the exception.\n    raise_exception : callable, optional\n        Function that takes an exception and a traceback, and raises an error.\n    callbacks : tuple or list of tuples, optional\n        Callbacks are passed in as tuples of length 5. Multiple sets of\n        callbacks may be passed in as a list of tuples. For more information,\n        see the dask.diagnostics documentation.\n    dumps: callable, optional\n        Function to serialize task data and results to communicate between\n        worker and parent.  Defaults to identity.\n    loads: callable, optional\n        Inverse function of `dumps`.  Defaults to identity.\n    chunksize: int, optional\n        Size of chunks to use when dispatching work. Defaults to 1.\n        If -1, will be computed to evenly divide ready work across workers.\n\n    See Also\n    --------\n    threaded.get\n    \"\"\"\n    chunksize = chunksize or config.get(\"chunksize\", 1)\n\n    queue = Queue()\n\n    if isinstance(result, list):\n        result_flat = set(flatten(result))\n    else:\n        result_flat = {result}\n    results = set(result_flat)\n\n    dsk = dict(convert_legacy_graph(dsk))\n    with local_callbacks(callbacks) as callbacks:\n        _, _, pretask_cbs, posttask_cbs, _ = unpack_callbacks(callbacks)\n        started_cbs = []\n        succeeded = False\n        # if start_state_from_dask fails, we will have something\n        # to pass to the final block.\n        state = {}\n        try:\n            for cb in callbacks:\n                if cb[0]:\n                    cb[0](dsk)\n                started_cbs.append(cb)\n\n            keyorder = order(dsk)\n\n            state = start_state_from_dask(dsk, cache=cache, sortkey=keyorder.get)\n\n            for _, start_state, _, _, _ in callbacks:\n                if start_state:\n                    start_state(dsk, state)\n\n            if rerun_exceptions_locally is None:\n                rerun_exceptions_locally = config.get(\"rerun_exceptions_locally\", False)\n\n            if state[\"waiting\"] and not state[\"ready\"]:\n                raise ValueError(\"Found no accessible jobs in dask\")\n\n            def fire_tasks(chunksize):\n                \"\"\"Fire off a task to the thread pool\"\"\"\n                # Determine chunksize and/or number of tasks to submit\n                nready = len(state[\"ready\"])\n                if chunksize == -1:\n                    ntasks = nready\n                    chunksize = -(ntasks // -num_workers)\n                else:\n                    used_workers = -(len(state[\"running\"]) // -chunksize)\n                    avail_workers = max(num_workers - used_workers, 0)\n                    ntasks = min(nready, chunksize * avail_workers)\n\n                # Prep all ready tasks for submission\n                args = []\n                for _ in range(ntasks):\n                    # Get the next task to compute (most recently added)\n                    key = state[\"ready\"].pop()\n                    # Notify task is running\n                    state[\"running\"].add(key)\n                    for f in pretask_cbs:\n                        f(key, dsk, state)\n\n                    # Prep args to send\n                    data = {\n                        dep: state[\"cache\"][dep] for dep in get_dependencies(dsk, key)\n                    }\n                    args.append(\n                        (\n                            key,\n                            dumps((dsk[key], data)),\n                            dumps,\n                            loads,\n                            get_id,\n                            pack_exception,\n                        )\n                    )\n\n                # Batch submit\n                for i in range(-(len(args) // -chunksize)):\n                    each_args = args[i * chunksize : (i + 1) * chunksize]\n                    if not each_args:\n                        break\n                    fut = submit(batch_execute_tasks, each_args)\n                    fut.add_done_callback(queue.put)\n\n            # Main loop, wait on tasks to finish, insert new ones\n            while state[\"waiting\"] or state[\"ready\"] or state[\"running\"]:\n                fire_tasks(chunksize)\n                for key, res_info, failed in queue_get(queue).result():\n                    if failed:\n                        exc, tb = loads(res_info)\n                        if rerun_exceptions_locally:\n                            data = {\n                                dep: state[\"cache\"][dep]\n                                for dep in get_dependencies(dsk, key)\n                            }\n                            task = dsk[key]\n                            task(data)  # Re-execute locally\n                        else:\n                            raise_exception(exc, tb)\n                    res, worker_id = loads(res_info)\n                    state[\"cache\"][key] = res\n                    finish_task(dsk, key, state, results, keyorder.get)\n                    for f in posttask_cbs:\n                        f(key, res, dsk, state, worker_id)\n\n            succeeded = True\n\n        finally:\n            for _, _, _, _, finish in started_cbs:\n                if finish:\n                    finish(dsk, state, not succeeded)\n\n    return nested_get(result, state[\"cache\"])\n\n\n\"\"\" Synchronous concrete version of get_async\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a\nsequential one.  This is useful for debugging and for code dominated by the\nGIL\n\"\"\"\n\n\nclass SynchronousExecutor(Executor):\n    _max_workers = 1\n\n    def submit(self, fn, *args, **kwargs):\n        fut = Future()\n        try:\n            fut.set_result(fn(*args, **kwargs))\n        except BaseException as e:  # noqa: B036\n            fut.set_exception(e)\n        return fut\n\n\nsynchronous_executor = SynchronousExecutor()\n\n\ndef get_sync(dsk: Mapping, keys: Sequence[Key] | Key, **kwargs):\n    \"\"\"A naive synchronous version of get_async\n\n    Can be useful for debugging.\n    \"\"\"\n    kwargs.pop(\"num_workers\", None)  # if num_workers present, remove it\n    return get_async(\n        synchronous_executor.submit,\n        synchronous_executor._max_workers,\n        dsk,\n        keys,\n        **kwargs,\n    )\n\n\n\"\"\" Adaptor for ``multiprocessing.Pool`` instances\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a wrapper\nclass for ``multiprocessing.Pool`` instances so we can treat them like\n``concurrent.futures.Executor`` instances instead.\n\nThis is mainly useful for legacy use cases or users that prefer\n``multiprocessing.Pool``.\n\"\"\"\n\n\nclass MultiprocessingPoolExecutor(Executor):\n    def __init__(self, pool):\n        self.pool = pool\n        self._max_workers = len(pool._pool)\n\n    def submit(self, fn, *args, **kwargs):\n        return submit_apply_async(self.pool.apply_async, fn, *args, **kwargs)\n\n\ndef submit_apply_async(apply_async, fn, *args, **kwargs):\n    fut = Future()\n    apply_async(fn, args, kwargs, fut.set_result, fut.set_exception)\n    return fut\n\n\ndef get_apply_async(apply_async, num_workers, *args, **kwargs):\n    return get_async(\n        partial(submit_apply_async, apply_async), num_workers, *args, **kwargs\n    )\n\n\ndef sortkey(item):\n    \"\"\"Sorting key function that is robust to different types\n\n    Both strings and tuples are common key types in dask graphs.\n    However In Python 3 one can not compare strings with tuples directly.\n    This function maps many types to a form where they can be compared\n\n    Examples\n    --------\n    >>> sortkey('Hello')\n    ('str', 'Hello')\n\n    >>> sortkey(('x', 1))\n    ('tuple', ('x', 1))\n    \"\"\"\n    return (type(item).__name__, item)\n\n### Source File Dependency Files Content\n### Dependency File: callbacks.py\nfrom __future__ import annotations\n\nfrom collections.abc import Callable\nfrom contextlib import contextmanager\nfrom typing import ClassVar\n\n__all__ = [\"Callback\", \"add_callbacks\"]\n\n\nclass Callback:\n    \"\"\"Base class for using the callback mechanism\n\n    Create a callback with functions of the following signatures:\n\n    >>> def start(dsk):\n    ...     pass\n    >>> def start_state(dsk, state):\n    ...     pass\n    >>> def pretask(key, dsk, state):\n    ...     pass\n    >>> def posttask(key, result, dsk, state, worker_id):\n    ...     pass\n    >>> def finish(dsk, state, failed):\n    ...     pass\n\n    You may then construct a callback object with any number of them\n\n    >>> cb = Callback(pretask=pretask, finish=finish)\n\n    And use it either as a context manager over a compute/get call\n\n    >>> with cb:            # doctest: +SKIP\n    ...     x.compute()\n\n    Or globally with the ``register`` method\n\n    >>> cb.register()\n    >>> cb.unregister()\n\n    Alternatively subclass the ``Callback`` class with your own methods.\n\n    >>> class PrintKeys(Callback):\n    ...     def _pretask(self, key, dask, state):\n    ...         print(\"Computing: {0}!\".format(repr(key)))\n\n    >>> with PrintKeys():   # doctest: +SKIP\n    ...     x.compute()\n    \"\"\"\n\n    active: ClassVar[set[tuple[Callable | None, ...]]] = set()\n\n    def __init__(\n        self, start=None, start_state=None, pretask=None, posttask=None, finish=None\n    ):\n        if start:\n            self._start = start\n        if start_state:\n            self._start_state = start_state\n        if pretask:\n            self._pretask = pretask\n        if posttask:\n            self._posttask = posttask\n        if finish:\n            self._finish = finish\n\n    @property\n    def _callback(self) -> tuple[Callable | None, ...]:\n        fields = [\"_start\", \"_start_state\", \"_pretask\", \"_posttask\", \"_finish\"]\n        return tuple(getattr(self, i, None) for i in fields)\n\n    def __enter__(self):\n        self._cm = add_callbacks(self)\n        self._cm.__enter__()\n        return self\n\n    def __exit__(self, *args):\n        self._cm.__exit__(*args)\n\n    def register(self) -> None:\n        Callback.active.add(self._callback)\n\n    def unregister(self) -> None:\n        Callback.active.remove(self._callback)\n\n\ndef unpack_callbacks(cbs):\n    \"\"\"Take an iterable of callbacks, return a list of each callback.\"\"\"\n    if cbs:\n        return [[i for i in f if i] for f in zip(*cbs)]\n    else:\n        return [(), (), (), (), ()]\n\n\n@contextmanager\ndef local_callbacks(callbacks=None):\n    \"\"\"Allows callbacks to work with nested schedulers.\n\n    Callbacks will only be used by the first started scheduler they encounter.\n    This means that only the outermost scheduler will use global callbacks.\"\"\"\n    global_callbacks = callbacks is None\n    if global_callbacks:\n        callbacks, Callback.active = Callback.active, set()\n    try:\n        yield callbacks or ()\n    finally:\n        if global_callbacks:\n            Callback.active = callbacks\n\n\ndef normalize_callback(cb):\n    \"\"\"Normalizes a callback to a tuple\"\"\"\n    if isinstance(cb, Callback):\n        return cb._callback\n    elif isinstance(cb, tuple):\n        return cb\n    else:\n        raise TypeError(\"Callbacks must be either `Callback` or `tuple`\")\n\n\nclass add_callbacks:\n    \"\"\"Context manager for callbacks.\n\n    Takes several callbacks and applies them only in the enclosed context.\n    Callbacks can either be represented as a ``Callback`` object, or as a tuple\n    of length 4.\n\n    Examples\n    --------\n    >>> def pretask(key, dsk, state):\n    ...     print(\"Now running {0}\").format(key)\n    >>> callbacks = (None, pretask, None, None)\n    >>> with add_callbacks(callbacks):    # doctest: +SKIP\n    ...     res.compute()\n    \"\"\"\n\n    def __init__(self, *callbacks):\n        self.callbacks = [normalize_callback(c) for c in callbacks]\n        Callback.active.update(self.callbacks)\n\n    def __enter__(self):\n        return\n\n    def __exit__(self, type, value, traceback):\n        for c in self.callbacks:\n            Callback.active.discard(c)\n\n\n### Dependency File: core.py\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom collections.abc import Collection, Iterable, Mapping, MutableMapping\nfrom typing import Any, Literal, TypeVar, cast, overload\n\nimport toolz\n\nfrom dask._task_spec import DependenciesMapping, convert_legacy_graph, execute_graph\nfrom dask.typing import Graph, Key, NoDefault, no_default\n\n\ndef ishashable(x):\n    \"\"\"Is x hashable?\n\n    Examples\n    --------\n\n    >>> ishashable(1)\n    True\n    >>> ishashable([1])\n    False\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    try:\n        hash(x)\n        return True\n    except TypeError:\n        return False\n\n\ndef istask(x):\n    \"\"\"Is x a runnable task?\n\n    A task is a tuple with a callable first argument\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> istask((inc, 1))\n    True\n    >>> istask(1)\n    False\n    \"\"\"\n    from dask._task_spec import DataNode, GraphNode\n\n    if isinstance(x, GraphNode):\n        if isinstance(x, DataNode):\n            return False\n        return True\n    return type(x) is tuple and x and callable(x[0])\n\n\ndef preorder_traversal(task):\n    \"\"\"A generator to preorder-traverse a task.\"\"\"\n\n    for item in task:\n        if istask(item):\n            yield from preorder_traversal(item)\n        elif isinstance(item, list):\n            yield list\n            yield from preorder_traversal(item)\n        else:\n            yield item\n\n\ndef lists_to_tuples(res, keys):\n    if isinstance(keys, list):\n        return tuple(lists_to_tuples(r, k) for r, k in zip(res, keys))\n    return res\n\n\ndef _pack_result(result: Mapping, keys: list | Key) -> Any:\n    if isinstance(keys, list):\n        return tuple(_pack_result(result, k) for k in keys)\n    return result[keys]\n\n\ndef get(dsk: Mapping, out: list | Key, cache: MutableMapping | None = None) -> Any:\n    \"\"\"Get value from Dask\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': 1, 'y': (inc, 'x')}\n\n    >>> get(d, 'x')\n    1\n    >>> get(d, 'y')\n    2\n    \"\"\"\n    for k in flatten(out):\n        if k not in dsk:\n            raise KeyError(f\"{k} is not a key in the graph\")\n    if cache is None:\n        cache = {}\n\n    dsk2 = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    result = execute_graph(dsk2, cache, keys=set(flatten([out])))\n    return _pack_result(result, out)\n\n\ndef keys_in_tasks(keys: Collection[Key], tasks: Iterable[Any], as_list: bool = False):\n    \"\"\"Returns the keys in `keys` that are also in `tasks`\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> keys_in_tasks(dsk, ['x', 'y', 'j'])  # doctest: +SKIP\n    {'x', 'y'}\n    \"\"\"\n    from dask._task_spec import GraphNode\n\n    ret: list[Key] = []\n    while tasks:\n        work = []\n        for w in tasks:\n            typ = type(w)\n            if typ is tuple and w and callable(w[0]):  # istask(w)\n                work.extend(w[1:])\n            elif typ is list:\n                work.extend(w)\n            elif typ is dict:\n                work.extend(w.values())\n            elif isinstance(w, GraphNode):\n                work.extend(w.dependencies)\n            else:\n                try:\n                    if w in keys:\n                        ret.append(w)\n                except TypeError:  # not hashable\n                    pass\n        tasks = work\n    return ret if as_list else set(ret)\n\n\ndef iskey(key: object) -> bool:\n    \"\"\"Return True if the given object is a potential dask key; False otherwise.\n\n    The definition of a key in a Dask graph is any str, bytes, int, float, or tuple\n    thereof.\n\n    See Also\n    --------\n    ishashable\n    validate_key\n    dask.typing.Key\n    \"\"\"\n    typ = type(key)\n    if typ is tuple:\n        return all(iskey(i) for i in cast(tuple, key))\n    return typ in {bytes, int, float, str}\n\n\ndef validate_key(key: object) -> None:\n    \"\"\"Validate the format of a dask key.\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    if iskey(key):\n        return\n    typ = type(key)\n\n    if typ is tuple:\n        index = None\n        try:\n            for index, part in enumerate(cast(tuple, key)):  # noqa: B007\n                validate_key(part)\n        except TypeError as e:\n            raise TypeError(\n                f\"Composite key contains unexpected key type at {index=} ({key=!r})\"\n            ) from e\n    raise TypeError(f\"Unexpected key type {typ} ({key=!r})\")\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = ...,\n    task: Key | NoDefault = ...,\n    as_list: Literal[False] = ...,\n) -> set[Key]: ...\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None,\n    task: Key | NoDefault,\n    as_list: Literal[True],\n) -> list[Key]: ...\n\n\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = None,\n    task: Key | NoDefault = no_default,\n    as_list: bool = False,\n) -> set[Key] | list[Key]:\n    \"\"\"Get the immediate tasks on which this task depends\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> get_dependencies(dsk, 'x')\n    set()\n\n    >>> get_dependencies(dsk, 'y')\n    {'x'}\n\n    >>> get_dependencies(dsk, 'z')  # doctest: +SKIP\n    {'x', 'y'}\n\n    >>> get_dependencies(dsk, 'w')  # Only direct dependencies\n    {'z'}\n\n    >>> get_dependencies(dsk, 'a')  # Ignore non-keys\n    {'x'}\n\n    >>> get_dependencies(dsk, task=(inc, 'x'))  # provide tasks directly\n    {'x'}\n    \"\"\"\n    if key is not None:\n        arg = dsk[key]\n    elif task is not no_default:\n        arg = task\n    else:\n        raise ValueError(\"Provide either key or task\")\n\n    return keys_in_tasks(dsk, [arg], as_list=as_list)\n\n\ndef get_deps(dsk: Graph) -> tuple[dict[Key, set[Key]], dict[Key, set[Key]]]:\n    \"\"\"Get dependencies and dependents from dask dask graph\n\n    >>> inc = lambda x: x + 1\n    >>> dsk = {'a': 1, 'b': (inc, 'a'), 'c': (inc, 'b')}\n    >>> dependencies, dependents = get_deps(dsk)\n    >>> dependencies\n    {'a': set(), 'b': {'a'}, 'c': {'b'}}\n    >>> dependents  # doctest: +SKIP\n    {'a': {'b'}, 'b': {'c'}, 'c': set()}\n    \"\"\"\n    dependencies = {k: get_dependencies(dsk, task=v) for k, v in dsk.items()}\n    dependents = reverse_dict(dependencies)\n    return dependencies, dependents\n\n\ndef flatten(seq, container=list):\n    \"\"\"\n\n    >>> list(flatten([1]))\n    [1]\n\n    >>> list(flatten([[1, 2], [1, 2]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten([[[1], [2]], [[1], [2]]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten(((1, 2), (1, 2)))) # Don't flatten tuples\n    [(1, 2), (1, 2)]\n\n    >>> list(flatten((1, 2, [3, 4]))) # support heterogeneous\n    [1, 2, 3, 4]\n    \"\"\"\n    if isinstance(seq, str):\n        yield seq\n    else:\n        for item in seq:\n            if isinstance(item, container):\n                yield from flatten(item, container=container)\n            else:\n                yield item\n\n\nT_ = TypeVar(\"T_\")\n\n\ndef reverse_dict(d: Mapping[T_, Iterable[T_]]) -> dict[T_, set[T_]]:\n    \"\"\"\n\n    >>> a, b, c = 'abc'\n    >>> d = {a: [b, c], b: [c]}\n    >>> reverse_dict(d)  # doctest: +SKIP\n    {'a': set([]), 'b': set(['a']}, 'c': set(['a', 'b'])}\n    \"\"\"\n    result: defaultdict[T_, set[T_]] = defaultdict(set)\n    _add = set.add\n    for k, vals in d.items():\n        result[k]\n        for val in vals:\n            _add(result[val], k)\n    return dict(result)\n\n\ndef subs(task, key, val):\n    \"\"\"Perform a substitution on a task\n\n    Examples\n    --------\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> subs((inc, 'x'), 'x', 1)  # doctest: +ELLIPSIS\n    (<function inc at ...>, 1)\n    \"\"\"\n    type_task = type(task)\n    if not (type_task is tuple and task and callable(task[0])):  # istask(task):\n        try:\n            if type_task is type(key) and task == key:\n                return val\n        except Exception:\n            pass\n        if type_task is list:\n            return [subs(x, key, val) for x in task]\n        return task\n    newargs = []\n    hash_key = {key}\n    for arg in task[1:]:\n        type_arg = type(arg)\n        if type_arg is tuple and arg and callable(arg[0]):  # istask(task):\n            arg = subs(arg, key, val)\n        elif type_arg is list:\n            arg = [subs(x, key, val) for x in arg]\n        else:\n            try:\n                if arg in hash_key:  # Hash and equality match\n                    arg = val\n            except TypeError:  # not hashable\n                pass\n        newargs.append(arg)\n    return task[:1] + tuple(newargs)\n\n\ndef _toposort(dsk, keys=None, returncycle=False, dependencies=None):\n\n    # Stack-based depth-first search traversal.  This is based on Tarjan's\n    # method for topological sorting (see wikipedia for pseudocode)\n    if keys is None:\n        keys = dsk\n    elif not isinstance(keys, list):\n        keys = [keys]\n    if not returncycle:\n        ordered = []\n\n    # Nodes whose descendents have been completely explored.\n    # These nodes are guaranteed to not be part of a cycle.\n    completed = set()\n\n    # All nodes that have been visited in the current traversal.  Because\n    # we are doing depth-first search, going \"deeper\" should never result\n    # in visiting a node that has already been seen.  The `seen` and\n    # `completed` sets are mutually exclusive; it is okay to visit a node\n    # that has already been added to `completed`.\n    seen = set()\n\n    if dependencies is None:\n\n        dependencies = DependenciesMapping(dsk)\n\n    for key in keys:\n        if key in completed:\n            continue\n        nodes = [key]\n        while nodes:\n            # Keep current node on the stack until all descendants are visited\n            cur = nodes[-1]\n            if cur in completed:\n                # Already fully traversed descendants of cur\n                nodes.pop()\n                continue\n            seen.add(cur)\n\n            # Add direct descendants of cur to nodes stack\n            next_nodes = []\n            for nxt in dependencies[cur]:\n                if nxt not in completed:\n                    if nxt in seen:\n                        # Cycle detected!\n                        # Let's report only the nodes that directly participate in the cycle.\n                        # We use `priorities` below to greedily construct a short cycle.\n                        # Shorter cycles may exist.\n                        priorities = {}\n                        prev = nodes[-1]\n                        # Give priority to nodes that were seen earlier.\n                        while nodes[-1] != nxt:\n                            priorities[nodes.pop()] = -len(priorities)\n                        priorities[nxt] = -len(priorities)\n                        # We're going to get the cycle by walking backwards along dependents,\n                        # so calculate dependents only for the nodes in play.\n                        inplay = set(priorities)\n                        dependents = reverse_dict(\n                            {k: inplay.intersection(dependencies[k]) for k in inplay}\n                        )\n                        # Begin with the node that was seen twice and the node `prev` from\n                        # which we detected the cycle.\n                        cycle = [nodes.pop()]\n                        cycle.append(prev)\n                        while prev != cycle[0]:\n                            # Greedily take a step that takes us closest to completing the cycle.\n                            # This may not give us the shortest cycle, but we get *a* short cycle.\n                            deps = dependents[cycle[-1]]\n                            prev = min(deps, key=priorities.__getitem__)\n                            cycle.append(prev)\n                        cycle.reverse()\n\n                        if returncycle:\n                            return cycle\n                        else:\n                            cycle = \"->\".join(str(x) for x in cycle)\n                            raise RuntimeError(\"Cycle detected in Dask: %s\" % cycle)\n                    next_nodes.append(nxt)\n\n            if next_nodes:\n                nodes.extend(next_nodes)\n            else:\n                # cur has no more descendants to explore, so we're done with it\n                if not returncycle:\n                    ordered.append(cur)\n                completed.add(cur)\n                seen.remove(cur)\n                nodes.pop()\n    if returncycle:\n        return []\n    return ordered\n\n\ndef toposort(dsk, dependencies=None):\n    \"\"\"Return a list of keys of dask sorted in topological order.\"\"\"\n    return _toposort(dsk, dependencies=dependencies)\n\n\ndef getcycle(d, keys):\n    \"\"\"Return a list of nodes that form a cycle if Dask is not a DAG.\n\n    Returns an empty list if no cycle is found.\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': (inc, 'z'), 'y': (inc, 'x'), 'z': (inc, 'y')}\n    >>> getcycle(d, 'x')\n    ['x', 'z', 'y', 'x']\n\n    See Also\n    --------\n    isdag\n    \"\"\"\n    return _toposort(d, keys=keys, returncycle=True)\n\n\ndef isdag(d, keys):\n    \"\"\"Does Dask form a directed acyclic graph when calculating keys?\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> inc = lambda x: x + 1\n    >>> isdag({'x': 0, 'y': (inc, 'x')}, 'y')\n    True\n    >>> isdag({'x': (inc, 'y'), 'y': (inc, 'x')}, 'y')\n    False\n\n    See Also\n    --------\n    getcycle\n    \"\"\"\n    return not getcycle(d, keys)\n\n\nclass literal:\n    \"\"\"A small serializable object to wrap literal values without copying\"\"\"\n\n    __slots__ = (\"data\",)\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return \"literal<type=%s>\" % type(self.data).__name__\n\n    def __reduce__(self):\n        return (literal, (self.data,))\n\n    def __call__(self):\n        return self.data\n\n\ndef quote(x):\n    \"\"\"Ensure that this value remains this value in a dask graph\n\n    Some values in dask graph take on special meaning. Sometimes we want to\n    ensure that our data is not interpreted but remains literal.\n\n    >>> add = lambda x, y: x + y\n    >>> quote((add, 1, 2))\n    (literal<type=tuple>,)\n    \"\"\"\n    if istask(x) or type(x) is list or type(x) is dict:\n        return (literal(x),)\n    return x\n\n\ndef reshapelist(shape, seq):\n    \"\"\"Reshape iterator to nested shape\n\n    >>> reshapelist((2, 3), range(6))\n    [[0, 1, 2], [3, 4, 5]]\n    \"\"\"\n    if len(shape) == 1:\n        return list(seq)\n    else:\n        n = int(len(seq) / shape[0])\n        return [reshapelist(shape[1:], part) for part in toolz.partition(n, seq)]\n\n\n### Dependency File: order.py\nfrom __future__ import annotations\n\nr\"\"\" Static order of nodes in dask graph\n\nDask makes decisions on what tasks to prioritize both\n\n*  Dynamically at runtime\n*  Statically before runtime\n\nDynamically we prefer to run tasks that were just made available.  However when\nseveral tasks become available at the same time we have an opportunity to break\nties in an intelligent way\n\n        d\n        |\n    b   c\n     \\ /\n      a\n\nFor example after we finish ``a`` we can choose to run either ``b`` or ``c``\nnext.  Making small decisions like this can greatly affect our performance,\nespecially because the order in which we run tasks affects the order in which\nwe can release memory, which operationally we find to have a large affect on\nmany computation.  We want to run tasks in such a way that we keep only a small\namount of data in memory at any given time.\n\n\nStatic Ordering\n---------------\n\nAnd so we create a total ordering over all nodes to serve as a tie breaker.  We\nrepresent this ordering with a dictionary mapping keys to integer values.\nLower scores have higher priority.  These scores correspond to the order in\nwhich a sequential scheduler would visit each node.\n\n    {'a': 0,\n     'c': 1,\n     'd': 2,\n     'b': 3}\n\nThere are several ways in which we might order our keys.  This is a nuanced\nprocess that has to take into account many different kinds of workflows, and\noperate efficiently in linear time.  We strongly recommend that readers look at\nthe docstrings of tests in dask/tests/test_order.py.  These tests usually have\ngraph types laid out very carefully to show the kinds of situations that often\narise, and the order we would like to be determined.\n\n\"\"\"\nfrom collections import defaultdict, deque, namedtuple\nfrom collections.abc import Callable, Iterable, Mapping, MutableMapping\nfrom typing import Any, Literal, NamedTuple, overload\n\nfrom dask._task_spec import DataNode, DependenciesMapping\nfrom dask.core import get_deps, getcycle, istask, reverse_dict\nfrom dask.typing import Key\n\n\nclass Order(NamedTuple):\n    priority: int\n    critical_path: float | int\n\n\n@overload\ndef order(\n    dsk: Mapping[Key, Any],\n    dependencies: Mapping[Key, set[Key]] | None = None,\n    *,\n    return_stats: Literal[True],\n) -> dict[Key, Order]: ...\n\n\n@overload\ndef order(\n    dsk: Mapping[Key, Any],\n    dependencies: Mapping[Key, set[Key]] | None = None,\n    *,\n    return_stats: Literal[False] = False,\n) -> dict[Key, int]: ...\n\n\ndef order(\n    dsk: Mapping[Key, Any],\n    dependencies: Mapping[Key, set[Key]] | None = None,\n    *,\n    return_stats: bool = False,\n) -> dict[Key, Order] | dict[Key, int]:\n    \"\"\"Order nodes in dask graph\n\n    This produces an ordering over our tasks that we use to break ties when\n    executing.  We do this ahead of time to reduce a bit of stress on the\n    scheduler and also to assist in static analysis.\n\n    This currently traverses the graph as a single-threaded scheduler would\n    traverse it.\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'a': 1, 'b': 2, 'c': (inc, 'a'), 'd': (add, 'b', 'c')}\n    >>> order(dsk)\n    {'a': 0, 'c': 1, 'b': 2, 'd': 3}\n    \"\"\"\n    if not dsk:\n        return {}\n\n    dsk = dict(dsk)\n\n    dependencies = DependenciesMapping(dsk)\n    dependents = reverse_dict(dependencies)\n\n    external_keys = set()\n    if len(dependents) != len(dependencies):\n        # There are external keys. Let's add a DataNode to ensure the algo below\n        # is encountering a complete graph. Those artificial data nodes will be\n        # ignored when assigning results\n        for k in dependents:\n            if k not in dependencies:\n                external_keys.add(k)\n                dsk[k] = DataNode(k, object())\n\n    expected_len = len(dsk)\n    leaf_nodes = {k for k, v in dependents.items() if not v}\n    root_nodes = {k for k, v in dependencies.items() if not v}\n\n    result: dict[Key, Order | int] = {}\n\n    # Normalize the graph by removing leaf nodes that are not actual tasks, see\n    # for instance da.store where the task is merely an alias\n    # to multiple keys, i.e. [key1, key2, ...,]\n\n    # Similarly, we are removing root nodes that are pure data tasks. Those task\n    # are embedded in the run_spec of a task and are not runnable. We have to\n    # assign a priority but their priority has no impact on performance.\n    # The removal of those tasks typically transforms the graph topology in a\n    # way that is simpler to handle\n    all_tasks = False\n    n_removed_leaves = 0\n    requires_data_task = defaultdict(set)\n\n    while not all_tasks:\n        all_tasks = True\n        for leaf in list(leaf_nodes):\n            if leaf in root_nodes:\n                continue\n            if (\n                not istask(dsk[leaf])\n                # Having a linear chain is fine\n                and len(dependencies[leaf]) > 1\n            ):\n                all_tasks = False\n                # Put non-tasks at the very end since they are merely aliases\n                # and have no impact on performance at all\n                prio = len(dsk) - 1 - n_removed_leaves\n                if return_stats:\n                    result[leaf] = Order(prio, -1)\n                else:\n                    result[leaf] = prio\n                n_removed_leaves += 1\n                leaf_nodes.remove(leaf)\n                for dep in dependencies[leaf]:\n                    dependents[dep].remove(leaf)\n                    if not dependents[dep]:\n                        leaf_nodes.add(dep)\n                del dsk[leaf]\n                del dependencies[leaf]\n                del dependents[leaf]\n\n        for root in list(root_nodes):\n            if root in leaf_nodes:\n                continue\n            deps_root = dependents[root]\n            if not istask(dsk[root]) and len(deps_root) > 1:\n                del dsk[root]\n                del dependencies[root]\n                root_nodes.remove(root)\n                del dependents[root]\n                for dep in deps_root:\n                    requires_data_task[dep].add(root)\n                    if not dependencies[dep]:\n                        root_nodes.add(dep)\n\n    num_needed, total_dependencies = ndependencies(dependencies, dependents)\n    if len(total_dependencies) != len(dsk):\n        cycle = getcycle(dsk, None)\n        raise RuntimeError(\n            \"Cycle detected between the following keys:\\n  -> %s\"\n            % \"\\n  -> \".join(str(x) for x in cycle)\n        )\n    assert dependencies is not None\n    roots_connected, max_dependents = _connecting_to_roots(dependencies, dependents)\n    leafs_connected, _ = _connecting_to_roots(dependents, dependencies)\n    i = 0\n\n    runnable_hull = set()\n    reachable_hull = set()\n\n    runnable = []\n\n    known_runnable_paths: dict[Key, list[list[Key]]] = {}\n    known_runnable_paths_pop = known_runnable_paths.pop\n\n    crit_path_counter = 0\n    scrit_path: set[Key] = set()\n    _crit_path_counter_offset: int | float = 0\n\n    _sort_keys_cache: dict[Key, tuple[int, int, int, int, str]] = {}\n\n    leafs_connected_to_loaded_roots: set[Key] = set()\n    processed_roots = set()\n\n    def sort_key(x: Key) -> tuple[int, int, int, int, str]:\n        try:\n            return _sort_keys_cache[x]\n        except KeyError:\n            assert dependencies is not None\n            _sort_keys_cache[x] = rv = (\n                total_dependencies[x],\n                len(dependencies[x]),\n                len(roots_connected[x]),\n                -max_dependents[x],\n                # Converting to str is actually faster than creating some\n                # wrapper class and comparisons that come this far are\n                # relatively rare so we prefer fast init over fast comparison\n                str(x),\n            )\n            return rv\n\n    def add_to_result(item: Key) -> None:\n        nonlocal crit_path_counter\n        # Earlier versions recursed into this method but this could cause\n        # recursion depth errors. This is the only reason for the while loop\n        next_items = [item]\n        nonlocal i\n        while next_items:\n            item = next_items.pop()\n            runnable_hull.discard(item)\n            reachable_hull.discard(item)\n            leaf_nodes.discard(item)\n            if item in result:\n                continue\n\n            while requires_data_task[item]:\n                add_to_result(requires_data_task[item].pop())\n            if return_stats:\n                result[item] = Order(i, crit_path_counter - _crit_path_counter_offset)\n            else:\n                result[item] = i\n            if item not in external_keys:\n                i += 1\n            if item in root_nodes:\n                processed_roots.add(item)\n            # Note: This is a `set` and therefore this introduces a certain\n            # randomness. However, this randomness should not have any impact on\n            # the final result since the `process_runnable` should produce\n            # equivalent results regardless of the order in which runnable is\n            # populated (not identical but equivalent)\n            for dep in dependents.get(item, ()):\n                num_needed[dep] -= 1\n                reachable_hull.add(dep)\n                if not num_needed[dep]:\n                    if len(dependents[item]) == 1:\n                        next_items.append(dep)\n                    else:\n                        runnable.append(dep)\n\n    def _with_offset(func: Callable[..., None]) -> Callable[..., None]:\n        # This decorator is only used to reduce indentation levels. The offset\n        # is purely cosmetical and used for some visualizations and I haven't\n        # settled on how to implement this best so I didn't want to have large\n        # indentations that make things harder to read\n        nonlocal _crit_path_counter_offset\n\n        def wrapper(*args: Any, **kwargs: Any) -> None:\n            nonlocal _crit_path_counter_offset\n            _crit_path_counter_offset = 0.5\n            try:\n                func(*args, **kwargs)\n            finally:\n                _crit_path_counter_offset = 0\n\n        return wrapper\n\n    @_with_offset\n    def process_runnables() -> None:\n        \"\"\"Compute all currently runnable paths and either cache or execute them\n\n        This is designed to ensure we are running tasks that are free to execute\n        (e.g. the result of a splitter task) not too eagerly. If we executed\n        such free tasks too early we'd be walking the graph in a too wide /\n        breadth first fashion that is not optimal. If instead we were to only\n        execute them once they are needed for a final result, this can cause\n        very high memory pressure since valuable reducers are executed too\n        late.\n\n        The strategy here is to take all runnable tasks and walk forwards until\n        we hit a reducer node (i.e. a node with more than one dependency). We\n        will remember/cache the path to this reducer node.\n        If this path leads to a leaf or if we find enough runnable paths for a\n        reducer to be runnable, we will execute the path.\n\n        If instead of a reducer a splitter is encountered that is runnable, we\n        will follow its splitter paths individually and apply the same logic to\n        each branch.\n        \"\"\"\n        while runnable:\n            candidates = runnable.copy()\n            runnable.clear()\n            while candidates:\n                key = candidates.pop()\n                if key in runnable_hull or key in result:\n                    continue\n                if key in leaf_nodes:\n                    add_to_result(key)\n                    continue\n                path = [key]\n                branches = deque([(0, path)])\n\n                while branches:\n                    nsplits, path = branches.popleft()\n                    while True:\n                        # Loop invariant. Too expensive to compute at runtime\n                        # assert not set(known_runnable_paths).intersection(runnable_hull)\n                        current = path[-1]\n                        runnable_hull.add(current)\n                        deps_downstream = dependents[current]\n                        deps_upstream = dependencies[current]\n                        if not deps_downstream:\n                            # FIXME: The fact that it is possible for\n                            # num_needed[current] == 0 means we're doing some\n                            # work twice\n                            if num_needed[current] <= 1:\n                                for k in path:\n                                    add_to_result(k)\n                            else:\n                                runnable_hull.discard(current)\n                        elif len(path) == 1 or len(deps_upstream) == 1:\n                            if len(deps_downstream) > 1:\n                                nsplits += 1\n                                for d in sorted(deps_downstream, key=sort_key):\n                                    # This ensures we're only considering splitters\n                                    # that are genuinely splitting and not\n                                    # interleaving\n                                    if len(dependencies[d]) == 1:\n                                        branch = path.copy()\n                                        branch.append(d)\n                                        branches.append((nsplits, branch))\n                                break\n                            path.extend(deps_downstream)\n                            continue\n                        elif current in known_runnable_paths:\n                            known_runnable_paths[current].append(path)\n                            runnable_hull.discard(current)\n                            if (\n                                len(known_runnable_paths[current])\n                                >= num_needed[current]\n                            ):\n                                pruned_branches: deque[list[Key]] = deque()\n                                for path in known_runnable_paths_pop(current):\n                                    if path[-2] not in result:\n                                        pruned_branches.append(path)\n                                if len(pruned_branches) < num_needed[current]:\n                                    known_runnable_paths[current] = list(\n                                        pruned_branches\n                                    )\n                                else:\n                                    if nsplits > 1:\n                                        path = []\n                                        for pruned in pruned_branches:\n                                            path.extend(pruned)\n                                        branches.append((nsplits - 1, path))\n                                        break\n\n                                    while pruned_branches:\n                                        path = pruned_branches.popleft()\n                                        for k in path:\n                                            if num_needed[k]:\n                                                pruned_branches.append(path)\n                                                break\n                                            add_to_result(k)\n                        else:\n                            if (\n                                len(dependencies[current]) > 1\n                                and num_needed[current] <= 1\n                            ):\n                                for k in path:\n                                    add_to_result(k)\n                            else:\n                                known_runnable_paths[current] = [path]\n                                runnable_hull.discard(current)\n                        break\n\n    # Pick strategy\n    # Note: We're trying to be smart here by picking a strategy on how to\n    # determine the critical path. This is not always clear and we may want to\n    # consider just calculating both orderings and picking the one with less\n    # pressure. The only concern to this would be performance but at time of\n    # writing, the most expensive part of ordering is the prep work (mostly\n    # connected roots + sort_key) which can be reused for multiple orderings.\n\n    def get_target(longest_path: bool = False) -> Key:\n        # Some topologies benefit if the node with the most dependencies\n        # is used as first choice, others benefit from the opposite.\n        candidates = leaf_nodes\n        skey: Callable = sort_key\n        if reachable_hull:\n            skey = lambda k: (num_needed[k], sort_key(k))\n\n        all_leafs_accessible = len(leafs_connected_to_loaded_roots) >= len(candidates)\n        if reachable_hull and not all_leafs_accessible:\n            # Avoid this branch if we can already access all leafs. Just pick\n            # one of them without the expensive selection process in this case.\n\n            candidates = reachable_hull & candidates\n            if not candidates:\n                # We can't reach a leaf node directly, but we still have nodes\n                # with results in memory, these notes can inform our path towards\n                # a new preferred leaf node.\n                for r in processed_roots:\n                    leafs_connected_to_loaded_roots.update(leafs_connected[r])\n                processed_roots.clear()\n\n                leafs_connected_to_loaded_roots.intersection_update(leaf_nodes)\n                candidates = leafs_connected_to_loaded_roots\n            else:\n                leafs_connected_to_loaded_roots.update(candidates)\n\n        elif not reachable_hull:\n            # We reach a new and independent branch, so throw away previous branch\n            leafs_connected_to_loaded_roots.clear()\n\n        if longest_path and (not reachable_hull or all_leafs_accessible):\n            return leaf_nodes_sorted.pop()\n        else:\n            # FIXME: This can be very expensive\n            return min(candidates, key=skey)\n\n    def use_longest_path() -> bool:\n        size = 0\n        # Heavy reducer / splitter topologies often benefit from a very\n        # traditional critical path that expresses the longest chain of\n        # tasks.\n        if abs(len(root_nodes) - len(leaf_nodes)) / len(root_nodes) < 0.8:\n            # If the graph stays about the same, we are checking for symmetry\n            # and choose a \"quickest path first\" approach if the graph appears\n            # to be asymmetrical\n            for r in root_nodes:\n                if not size:\n                    size = len(leafs_connected[r])\n                elif size != len(leafs_connected[r]):\n                    return False\n\n        return True\n\n    longest_path = use_longest_path()\n    leaf_nodes_sorted = []\n    if longest_path:\n        leaf_nodes_sorted = sorted(leaf_nodes, key=sort_key, reverse=False)\n\n    # *************************************************************************\n    # CORE ALGORITHM STARTS HERE\n    #\n    # 0. Nomenclature\n    #\n    #   - roots: Nodes that have no dependencies (i.e. typically data producers)\n    #   - leafs: Nodes that have no dependents (i.e. user requested keys)\n    #   - critical_path: The strategic path through the graph.\n    #   - walking forwards: Starting from a root node we walk the graph as if we\n    #     were to compute the individual nodes, i.e. along dependents\n    #   - walking backwards: Starting from a leaf node we walk the graph in\n    #     reverse direction, i.e. along dependencies\n    #   - runnable: Nodes that are ready to be computed because all their\n    #     dependencies are in result\n    #   - runnable_hull: Nodes that could be reached and executed without\n    #     \"walking back\". This typically means that these are tasks than can be\n    #     executed without loading additional data/executing additional root\n    #     nodes\n    #  -  reachable_hull: Nodes that are touching the result, i.e. all nodes in\n    #     reachable_hull have at least one dependency in result\n    #\n    # A. Build the critical path\n    #\n    #   To build the critical path we will use a provided `get_target` function\n    #   that returns a node that is anywhere in the graph, typically a leaf\n    #   node. This node is not required to be runnable. We will walk the graph\n    #   backwards, i.e. from leafs to roots and append nodes to the graph as we\n    #   go. The critical path is a\n    #   linear path in the graph. While this is a viable strategy, it is not\n    #   required for the critical path to be a classical \"longest path\" but it\n    #   can define any route through the graph that should be considered as top\n    #   priority.\n    #\n    #   1. Determine the target node by calling ``get_target`` and append the\n    #      target to the critical path stack\n    #   2. Take the _most valuable_ (max given a `sort_key`) of its dependents\n    #      and append it to the critical path stack. This key is the new target.\n    #   3. Repeat step 2 until we reach a node that has no dependencies and is\n    #      therefore runnable\n    #\n    # B. Walk the critical path\n    #\n    #   Only the first element of the critical path is an actually runnable node\n    #   and this is where we're starting the sort. Strategically, this is the\n    #   most important goal to achieve but since not all of the nodes are\n    #   immediately runnable we have to walk back and compute other nodes first\n    #   before we can unlock the critical path. This typically requires us also\n    #   to load more data / run more root tasks.\n    #   While walking the critical path we will also unlock non-critical tasks\n    #   that could be run but are not contributing to our strategic goal. Under\n    #   certain circumstances, those runnable tasks are allowed to be run right\n    #   away to reduce memory pressure. This is described in more detail in\n    #   `process_runnable`.\n    #   Given this, the algorithm is as follows:\n    #\n    #   1. Pop the first element of the critical path\n    #   2a. If the node is already in the result, continue\n    #   2b. If the node is not runnable, we will put it back on the stack and\n    #       put all its dependencies on the stack and continue with step 1. This\n    #       is what we refer to as \"walking back\"\n    #   2c. Else, we add the node to the result\n    #   3.  If we previously had to walk back we will consider running\n    #       non-critical tasks (by calling process_runnables)\n    #   4a. If critical path is not empty, repeat step 1\n    #   4b. Go back to A.) and build a new critical path given a new target that\n    #       accounts for the already computed nodes.\n    #\n    # *************************************************************************\n\n    critical_path: list[Key] = []\n    cpath_append = critical_path.append\n    scpath_add = scrit_path.add\n\n    def path_append(item: Key) -> None:\n        cpath_append(item)\n        scpath_add(item)\n\n    scpath_update = scrit_path.update\n    cpath_extend = critical_path.extend\n\n    def path_extend(items: Iterable[Key]) -> None:\n        cpath_extend(items)\n        scpath_update(items)\n\n    cpath_pop = critical_path.pop\n    scpath_discard = scrit_path.discard\n\n    def path_pop() -> Key:\n        item = cpath_pop()\n        scpath_discard(item)\n        return item\n\n    while len(result) < expected_len:\n        crit_path_counter += 1\n        assert not critical_path\n        assert not scrit_path\n\n        # A. Build the critical path\n        target = get_target(longest_path=longest_path)\n        next_deps = dependencies[target]\n        path_append(target)\n\n        while next_deps:\n            item = max(next_deps, key=sort_key)\n            path_append(item)\n            next_deps = dependencies[item].difference(result)\n            path_extend(next_deps)\n\n        # B. Walk the critical path\n\n        walked_back = False\n        while critical_path:\n            item = path_pop()\n            if item in result:\n                continue\n            if num_needed[item]:\n                path_append(item)\n                deps = dependencies[item].difference(result)\n                unknown: list[Key] = []\n                known: list[Key] = []\n                k_append = known.append\n                uk_append = unknown.append\n                for d in sorted(deps, key=sort_key):\n                    if d in known_runnable_paths:\n                        k_append(d)\n                    else:\n                        uk_append(d)\n                if len(unknown) > 1:\n                    walked_back = True\n\n                for d in unknown:\n                    path_append(d)\n                for d in known:\n                    for path in known_runnable_paths_pop(d):\n                        path_extend(reversed(path))\n\n                del deps\n                continue\n            else:\n                if walked_back and len(runnable) < len(critical_path):\n                    process_runnables()\n                add_to_result(item)\n        process_runnables()\n\n    assert len(result) == expected_len\n    for k in external_keys:\n        del result[k]\n    return result  # type: ignore\n\n\ndef _connecting_to_roots(\n    dependencies: Mapping[Key, set[Key]], dependents: Mapping[Key, set[Key]]\n) -> tuple[dict[Key, frozenset[Key]], dict[Key, int]]:\n    \"\"\"Determine for every node which root nodes are connected to it (i.e.\n    ancestors). If arguments of dependencies and dependents are switched, this\n    can also be used to determine which leaf nodes are connected to which node\n    (i.e. descendants).\n\n    Also computes a weight that is defined as (cheaper to compute here)\n\n            `max(len(dependents[k]) for k in connected_roots[key])`\n\n    \"\"\"\n    result = {}\n    current = []\n    num_needed = {k: len(v) for k, v in dependencies.items() if v}\n    max_dependents = {}\n    roots = set()\n    for k, v in dependencies.items():\n        if not v:\n            # Note: Hashing the full keys is relatively expensive. Hashing\n            # integers would be much faster so this could be sped up by just\n            # introducing a counter here. However, the order algorithm is also\n            # sometimes interested in the actual keys and the only way to\n            # benefit from the speedup of using integers would be to convert\n            # this back on demand which makes the code very hard to read.\n            roots.add(k)\n            result[k] = frozenset({k})\n            deps = dependents[k]\n            max_dependents[k] = len(deps)\n            for child in deps:\n                num_needed[child] -= 1\n                if not num_needed[child]:\n                    current.append(child)\n\n    dedup_mapping: dict[frozenset[Key], frozenset[Key]] = {}\n    while current:\n        key = current.pop()\n        if key in result:\n            continue\n        for parent in dependents[key]:\n            num_needed[parent] -= 1\n            if not num_needed[parent]:\n                current.append(parent)\n        # At some point, all the roots are the same, particularly for dense\n        # graphs. We don't want to create new sets over and over again\n        transitive_deps = []\n        transitive_deps_ids = set()\n        max_dependents_key = list()\n        for child in dependencies[key]:\n            r_child = result[child]\n            if id(r_child) in transitive_deps_ids:\n                continue\n            transitive_deps.append(r_child)\n            transitive_deps_ids.add(id(r_child))\n            max_dependents_key.append(max_dependents[child])\n\n        max_dependents[key] = max(max_dependents_key)\n        if len(transitive_deps_ids) == 1:\n            result[key] = transitive_deps[0]\n        else:\n            prev: set | frozenset | None = None\n            new_set = None\n            for tdeps in transitive_deps:\n                if prev is None:\n                    prev = tdeps\n                    continue\n                if tdeps.issubset(prev):\n                    continue\n                if new_set is None:\n                    prev = new_set = set(prev)\n                prev.update(tdeps)\n            if new_set is not None:\n                # frozenset is unfortunately triggering a copy. In the event of\n                # a cache hit, this is wasted time but we can't hash the set\n                # otherwise (unless we did it manually) and can therefore not\n                # deduplicate without this copy\n                frozen_res = frozenset(new_set)\n                try:\n                    result[key] = dedup_mapping[frozen_res]\n                except KeyError:\n                    dedup_mapping[frozen_res] = frozen_res\n                    result[key] = frozen_res\n            else:\n                result[key] = prev  # type: ignore\n    del dedup_mapping\n\n    empty_set: frozenset[Key] = frozenset()\n    for r in roots:\n        result[r] = empty_set\n    return result, max_dependents\n\n\ndef ndependencies(\n    dependencies: Mapping[Key, set[Key]], dependents: Mapping[Key, set[Key]]\n) -> tuple[dict[Key, int], dict[Key, int]]:\n    \"\"\"Number of total data elements on which this key depends\n\n    For each key we return the number of tasks that must be run for us to run\n    this task.\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> dsk = {'a': 1, 'b': (inc, 'a'), 'c': (inc, 'b')}\n    >>> dependencies, dependents = get_deps(dsk)\n    >>> num_dependencies, total_dependencies = ndependencies(dependencies, dependents)\n    >>> sorted(total_dependencies.items())\n    [('a', 1), ('b', 2), ('c', 3)]\n\n    Returns\n    -------\n    num_dependencies: Dict[key, int]\n    total_dependencies: Dict[key, int]\n    \"\"\"\n    num_needed = {}\n    result = {}\n    for k, v in dependencies.items():\n        num_needed[k] = len(v)\n        if not v:\n            result[k] = 1\n\n    num_dependencies = num_needed.copy()\n    current: list[Key] = []\n    current_pop = current.pop\n    current_append = current.append\n\n    for key in result:\n        for parent in dependents[key]:\n            num_needed[parent] -= 1\n            if not num_needed[parent]:\n                current_append(parent)\n    while current:\n        key = current_pop()\n        result[key] = 1 + sum(result[child] for child in dependencies[key])\n        for parent in dependents[key]:\n            num_needed[parent] -= 1\n            if not num_needed[parent]:\n                current_append(parent)\n    return num_dependencies, result\n\n\nOrderInfo = namedtuple(\n    \"OrderInfo\",\n    (\n        \"order\",\n        \"age\",\n        \"num_data_when_run\",\n        \"num_data_when_released\",\n        \"num_dependencies_freed\",\n    ),\n)\n\n\ndef diagnostics(\n    dsk: MutableMapping[Key, Any],\n    o: Mapping[Key, int] | None = None,\n    dependencies: MutableMapping[Key, set[Key]] | None = None,\n) -> tuple[dict[Key, OrderInfo], list[int]]:\n    \"\"\"Simulate runtime metrics as though running tasks one at a time in order.\n\n    These diagnostics can help reveal behaviors of and issues with ``order``.\n\n    Returns a dict of `namedtuple(\"OrderInfo\")` and a list of the number of outputs held over time.\n\n    OrderInfo fields:\n    - order : the order in which the node is run.\n    - age : how long the output of a node is held.\n    - num_data_when_run : the number of outputs held in memory when a node is run.\n    - num_data_when_released : the number of outputs held in memory when the output is released.\n    - num_dependencies_freed : the number of dependencies freed by running the node.\n    \"\"\"\n    if dependencies is None:\n        dependencies, dependents = get_deps(dsk)\n    else:\n        dependents = reverse_dict(dependencies)\n    assert dependencies is not None\n    if o is None:\n        o = order(dsk, dependencies=dependencies, return_stats=False)\n\n    pressure = []\n    num_in_memory = 0\n    age = {}\n    runpressure = {}\n    releasepressure = {}\n    freed = {}\n    num_needed = {key: len(val) for key, val in dependents.items()}\n    for i, key in enumerate(sorted(dsk, key=o.__getitem__)):\n        pressure.append(num_in_memory)\n        runpressure[key] = num_in_memory\n        released = 0\n        for dep in dependencies[key]:\n            num_needed[dep] -= 1\n            if num_needed[dep] == 0:\n                age[dep] = i - o[dep]\n                releasepressure[dep] = num_in_memory\n                released += 1\n        freed[key] = released\n        if dependents[key]:\n            num_in_memory -= released - 1\n        else:\n            age[key] = 0\n            releasepressure[key] = num_in_memory\n            num_in_memory -= released\n\n    rv = {\n        key: OrderInfo(\n            val, age[key], runpressure[key], releasepressure[key], freed[key]\n        )\n        for key, val in o.items()\n    }\n    return rv, pressure\n\n\ndef _f() -> None: ...\n\n\ndef sanitize_dsk(dsk: MutableMapping[Key, Any]) -> dict:\n    \"\"\"Take a dask graph and replace callables with a dummy function and remove\n    payload data like numpy arrays, dataframes, etc.\n    \"\"\"\n    from dask._task_spec import Task, TaskRef\n\n    new = {}\n    deps = DependenciesMapping(dsk)\n    for key, values in deps.items():\n        new[key] = Task(key, _f, *(TaskRef(k) for k in values))\n    return new\n\n\n### Dependency File: typing.py\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Callable, Hashable, Mapping, Sequence\nfrom enum import Enum\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Literal,\n    Protocol,\n    TypeVar,\n    Union,\n    runtime_checkable,\n)\n\nif TYPE_CHECKING:\n    # IPython import is relatively slow. Avoid if not necessary\n    # TODO import from typing (requires Python >=3.10)\n    from typing import TypeAlias\n\n    from IPython.display import DisplayObject\n\nCollType = TypeVar(\"CollType\", bound=\"DaskCollection\")\nCollType_co = TypeVar(\"CollType_co\", bound=\"DaskCollection\", covariant=True)\nPostComputeCallable = Callable\n\n\nKey: TypeAlias = Union[str, bytes, int, float, tuple[\"Key\", ...]]\n# FIXME: This type is a little misleading. Low level graphs are often\n# MutableMappings but HLGs are not\nGraph: TypeAlias = Mapping[Key, Any]\n# Potentially nested list of Dask keys\nNestedKeys: TypeAlias = list[Union[Key, \"NestedKeys\"]]\n\n\nclass SchedulerGetCallable(Protocol):\n    \"\"\"Protocol defining the signature of a ``__dask_scheduler__`` callable.\"\"\"\n\n    def __call__(\n        self,\n        dsk: Graph,\n        keys: Sequence[Key] | Key,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Method called as the default scheduler for a collection.\n\n        Parameters\n        ----------\n        dsk :\n            The task graph.\n        keys :\n            Key(s) corresponding to the desired data.\n        **kwargs :\n            Additional arguments.\n\n        Returns\n        -------\n        Any\n            Result(s) associated with `keys`\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\nclass PostPersistCallable(Protocol[CollType_co]):\n    \"\"\"Protocol defining the signature of a ``__dask_postpersist__`` callable.\"\"\"\n\n    def __call__(\n        self,\n        dsk: Graph,\n        *args: Any,\n        rename: Mapping[str, str] | None = None,\n    ) -> CollType_co:\n        \"\"\"Method called to rebuild a persisted collection.\n\n        Parameters\n        ----------\n        dsk: Mapping\n            A mapping which contains at least the output keys returned\n            by __dask_keys__().\n        *args : Any\n            Additional optional arguments If no extra arguments are\n            necessary, it must be an empty tuple.\n        rename : Mapping[str, str], optional\n            If defined, it indicates that output keys may be changing\n            too; e.g. if the previous output of :meth:`__dask_keys__`\n            was ``[(\"a\", 0), (\"a\", 1)]``, after calling\n            ``rebuild(dsk, *extra_args, rename={\"a\": \"b\"})``\n            it must become ``[(\"b\", 0), (\"b\", 1)]``.\n            The ``rename`` mapping may not contain the collection\n            name(s); in such case the associated keys do not change.\n            It may contain replacements for unexpected names, which\n            must be ignored.\n\n        Returns\n        -------\n        Collection\n            An equivalent Dask collection with the same keys as\n            computed through a different graph.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\n@runtime_checkable\nclass DaskCollection(Protocol):\n    \"\"\"Protocol defining the interface of a Dask collection.\"\"\"\n\n    @abc.abstractmethod\n    def __dask_graph__(self) -> Graph:\n        \"\"\"The Dask task graph.\n\n        The core Dask collections (Array, DataFrame, Bag, and Delayed)\n        use a :py:class:`~dask.highlevelgraph.HighLevelGraph` to\n        represent the collection task graph. It is also possible to\n        represent the task graph as a low level graph using a Python\n        dictionary.\n\n        Returns\n        -------\n        Mapping\n            The Dask task graph. If the instance returns a\n            :py:class:`dask.highlevelgraph.HighLevelGraph` then the\n            :py:func:`__dask_layers__` method must be implemented, as\n            defined by the :py:class:`~dask.typing.HLGDaskCollection`\n            protocol.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_keys__(self) -> NestedKeys:\n        \"\"\"The output keys of the task graph.\n\n        Note that there are additional constraints on keys for a Dask\n        collection than those described in the :doc:`task graph\n        specification documentation <spec>`. These additional\n        constraints are described below.\n\n        All keys must either be non-empty strings or tuples where the first element is a\n        non-empty string, followed by zero or more arbitrary str, bytes, int, float, or\n        tuples thereof. The non-empty string is commonly known as the *collection name*.\n        All collections embedded in the dask package have exactly one name, but this is\n        not a requirement.\n\n        These are all valid outputs:\n\n        - ``[]``\n        - ``[\"x\", \"y\"]``\n        - ``[[(\"y\", \"a\", 0), (\"y\", \"a\", 1)], [(\"y\", \"b\", 0), (\"y\", \"b\", 1)]``\n\n        Returns\n        -------\n        list\n            A possibly nested list of keys that represent the outputs\n            of the graph. After computation, the results will be\n            returned in the same layout, with the keys replaced with\n            their corresponding outputs.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_postcompute__(self) -> tuple[PostComputeCallable, tuple]:\n        \"\"\"Finalizer function and optional arguments to construct final result.\n\n        Upon computation each key in the collection will have an in\n        memory result, the postcompute function combines each key's\n        result into a final in memory representation. For example,\n        dask.array.Array concatenates the arrays at each chunk into a\n        final in-memory array.\n\n        Returns\n        -------\n        PostComputeCallable\n            Callable that receives the sequence of the results of each\n            final key along with optional arguments. An example signature\n            would be ``finalize(results: Sequence[Any], *args)``.\n        tuple[Any, ...]\n            Optional arguments passed to the function following the\n            key results (the `*args` part of the\n            ``PostComputeCallable``. If no additional arguments are to\n            be passed then this must be an empty tuple.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_postpersist__(self) -> tuple[PostPersistCallable, tuple]:\n        \"\"\"Rebuilder function and optional arguments to construct a persisted collection.\n\n        See also the documentation for :py:class:`dask.typing.PostPersistCallable`.\n\n        Returns\n        -------\n        PostPersistCallable\n            Callable that rebuilds the collection. The signature\n            should be\n            ``rebuild(dsk: Mapping, *args: Any, rename: Mapping[str, str] | None)``\n            (as defined by the\n            :py:class:`~dask.typing.PostPersistCallable` protocol).\n            The callable should return an equivalent Dask collection\n            with the same keys as `self`, but with results that are\n            computed through a different graph. In the case of\n            :py:func:`dask.persist`, the new graph will have just the\n            output keys and the values already computed.\n        tuple[Any, ...]\n            Optional arguments passed to the rebuild callable. If no\n            additional arguments are to be passed then this must be an\n            empty tuple.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_tokenize__(self) -> Hashable:\n        \"\"\"Value that must fully represent the object.\"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    __dask_optimize__: Any\n    \"\"\"Given a graph and keys, return a new optimized graph.\n\n    This method can be either a ``staticmethod`` or a ``classmethod``,\n    but not an ``instancemethod``. For example implementations see the\n    definitions of ``__dask_optimize__`` in the core Dask collections:\n    ``dask.array.Array``, ``dask.dataframe.DataFrame``, etc.\n\n    Note that graphs and keys are merged before calling\n    ``__dask_optimize__``; as such, the graph and keys passed to\n    this method may represent more than one collection sharing the\n    same optimize method.\n\n    Parameters\n    ----------\n    dsk : Graph\n        The merged graphs from all collections sharing the same\n        ``__dask_optimize__`` method.\n    keys : Sequence[Key]\n        A list of the outputs from ``__dask_keys__`` from all\n        collections sharing the same ``__dask_optimize__`` method.\n    **kwargs : Any\n        Extra keyword arguments forwarded from the call to\n        ``compute`` or ``persist``. Can be used or ignored as\n        needed.\n\n    Returns\n    -------\n    MutableMapping\n        The optimized Dask graph.\n\n    \"\"\"\n\n    # FIXME: It is currently not possible to define a staticmethod from a callback protocol\n    # Also, the definition in `is_dask_collection` cannot be satisfied by a\n    # protocol / typing check\n    # staticmethod[SchedulerGetCallable]\n    __dask_scheduler__: staticmethod\n    \"\"\"The default scheduler ``get`` to use for this object.\n\n    Usually attached to the class as a staticmethod, e.g.:\n\n    >>> import dask.threaded\n    >>> class MyCollection:\n    ...     # Use the threaded scheduler by default\n    ...     __dask_scheduler__ = staticmethod(dask.threaded.get)\n\n    \"\"\"\n\n    @abc.abstractmethod\n    def compute(self, **kwargs: Any) -> Any:\n        \"\"\"Compute this dask collection.\n\n        This turns a lazy Dask collection into its in-memory\n        equivalent. For example a Dask array turns into a NumPy array\n        and a Dask dataframe turns into a Pandas dataframe. The entire\n        dataset must fit into memory before calling this operation.\n\n        Parameters\n        ----------\n        scheduler : string, optional\n            Which scheduler to use like \"threads\", \"synchronous\" or\n            \"processes\". If not provided, the default is to check the\n            global settings first, and then fall back to the\n            collection defaults.\n        optimize_graph : bool, optional\n            If True [default], the graph is optimized before\n            computation. Otherwise the graph is run as is. This can be\n            useful for debugging.\n        kwargs :\n            Extra keywords to forward to the scheduler function.\n\n        Returns\n        -------\n        The collection's computed result.\n\n        See Also\n        --------\n        dask.compute\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def persist(self: CollType, **kwargs: Any) -> CollType:\n        \"\"\"Persist this dask collection into memory\n\n        This turns a lazy Dask collection into a Dask collection with\n        the same metadata, but now with the results fully computed or\n        actively computing in the background.\n\n        The action of function differs significantly depending on the\n        active task scheduler. If the task scheduler supports\n        asynchronous computing, such as is the case of the\n        dask.distributed scheduler, then persist will return\n        *immediately* and the return value's task graph will contain\n        Dask Future objects. However if the task scheduler only\n        supports blocking computation then the call to persist will\n        *block* and the return value's task graph will contain\n        concrete Python results.\n\n        This function is particularly useful when using distributed\n        systems, because the results will be kept in distributed\n        memory, rather than returned to the local process as with\n        compute.\n\n        Parameters\n        ----------\n        scheduler : string, optional\n            Which scheduler to use like \"threads\", \"synchronous\" or\n            \"processes\". If not provided, the default is to check the\n            global settings first, and then fall back to the\n            collection defaults.\n        optimize_graph : bool, optional\n            If True [default], the graph is optimized before\n            computation. Otherwise the graph is run as is. This can be\n            useful for debugging.\n        **kwargs\n            Extra keywords to forward to the scheduler function.\n\n        Returns\n        -------\n        New dask collections backed by in-memory data\n\n        See Also\n        --------\n        dask.persist\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def visualize(\n        self,\n        filename: str = \"mydask\",\n        format: str | None = None,\n        optimize_graph: bool = False,\n        **kwargs: Any,\n    ) -> DisplayObject | None:\n        \"\"\"Render the computation of this object's task graph using graphviz.\n\n        Requires ``graphviz`` to be installed.\n\n        Parameters\n        ----------\n        filename : str or None, optional\n            The name of the file to write to disk. If the provided\n            `filename` doesn't include an extension, '.png' will be\n            used by default. If `filename` is None, no file will be\n            written, and we communicate with dot using only pipes.\n        format : {'png', 'pdf', 'dot', 'svg', 'jpeg', 'jpg'}, optional\n            Format in which to write output file. Default is 'png'.\n        optimize_graph : bool, optional\n            If True, the graph is optimized before rendering.\n            Otherwise, the graph is displayed as is. Default is False.\n        color: {None, 'order'}, optional\n            Options to color nodes. Provide ``cmap=`` keyword for\n            additional colormap\n        **kwargs\n           Additional keyword arguments to forward to ``to_graphviz``.\n\n        Examples\n        --------\n        >>> x.visualize(filename='dask.pdf')  # doctest: +SKIP\n        >>> x.visualize(filename='dask.pdf', color='order')  # doctest: +SKIP\n\n        Returns\n        -------\n        result : IPython.display.Image, IPython.display.SVG, or None\n            See dask.dot.dot_graph for more information.\n\n        See Also\n        --------\n        dask.visualize\n        dask.dot.dot_graph\n\n        Notes\n        -----\n        For more information on optimization see here:\n\n        https://docs.dask.org/en/latest/optimize.html\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\n@runtime_checkable\nclass HLGDaskCollection(DaskCollection, Protocol):\n    \"\"\"Protocol defining a Dask collection that uses HighLevelGraphs.\n\n    This protocol is nearly identical to\n    :py:class:`~dask.typing.DaskCollection`, with the addition of the\n    ``__dask_layers__`` method (required for collections backed by\n    high level graphs).\n\n    \"\"\"\n\n    @abc.abstractmethod\n    def __dask_layers__(self) -> Sequence[str]:\n        \"\"\"Names of the HighLevelGraph layers.\"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\nclass _NoDefault(Enum):\n    \"\"\"typing-aware constant to detect when the user omits a parameter and you can't use\n    None.\n\n    Copied from pandas._libs.lib._NoDefault.\n\n    Usage\n    -----\n    from dask.typing import NoDefault, no_default\n\n    def f(x: int | None | NoDefault = no_default) -> int:\n        if x is no_default:\n            ...\n    \"\"\"\n\n    no_default = \"NO_DEFAULT\"\n\n    def __repr__(self) -> str:\n        return \"<no_default>\"\n\n\nno_default = _NoDefault.no_default\nNoDefault = Literal[_NoDefault.no_default]\n\n\n### Dependency File: _task_spec.py\nfrom __future__ import annotations\n\n\"\"\" Task specification for dask\n\nThis module contains the task specification for dask. It is used to represent\nrunnable (task) and non-runnable (data) nodes in a dask graph.\n\nSimple examples of how to express tasks in dask\n-----------------------------------------------\n\n.. code-block:: python\n\n    func(\"a\", \"b\") ~ Task(\"key\", func, \"a\", \"b\")\n\n    [func(\"a\"), func(\"b\")] ~ [Task(\"key-1\", func, \"a\"), Task(\"key-2\", func, \"b\")]\n\n    {\"a\": func(\"b\")} ~ {\"a\": Task(\"a\", func, \"b\")}\n\n    \"literal-string\" ~ DataNode(\"key\", \"literal-string\")\n\n\nKeys, Aliases and TaskRefs\n-------------------------\n\nKeys are used to identify tasks in a dask graph. Every `GraphNode` instance has a\nkey attribute that _should_ reference the key in the dask graph.\n\n.. code-block:: python\n\n    {\"key\": Task(\"key\", func, \"a\")}\n\nReferencing other tasks is possible by using either one of `Alias` or a\n`TaskRef`.\n\n.. code-block:: python\n\n    # TaskRef can be used to provide the name of the reference explicitly\n    t = Task(\"key\", func, TaskRef(\"key-1\"))\n\n    # If a task is still in scope, the method `ref` can be used for convenience\n    t2 = Task(\"key2\", func2, t.ref())\n\n\nExecuting a task\n----------------\n\nA task can be executed by calling it with a dictionary of values. The values\nshould contain the dependencies of the task.\n\n.. code-block:: python\n\n    t = Task(\"key\", add, TaskRef(\"a\"), TaskRef(\"b\"))\n    assert t.dependencies == {\"a\", \"b\"}\n    t({\"a\": 1, \"b\": 2}) == 3\n\n\"\"\"\nimport itertools\nimport sys\nfrom collections import defaultdict\nfrom collections.abc import Callable, Container, Iterable, Mapping, MutableMapping\nfrom functools import partial\nfrom typing import Any, TypeVar, cast\n\nfrom dask.sizeof import sizeof\nfrom dask.typing import Key as KeyType\nfrom dask.utils import funcname, is_namedtuple_instance\n\n_T = TypeVar(\"_T\")\n_T_GraphNode = TypeVar(\"_T_GraphNode\", bound=\"GraphNode\")\n_T_Iterable = TypeVar(\"_T_Iterable\", bound=Iterable)\n\n\n# Ported from more-itertools\n# https://github.com/more-itertools/more-itertools/blob/c8153e2801ade2527f3a6c8b623afae93f5a1ce1/more_itertools/recipes.py#L944-L973\ndef _batched(iterable, n, *, strict=False):\n    \"\"\"Batch data into tuples of length *n*. If the number of items in\n    *iterable* is not divisible by *n*:\n    * The last batch will be shorter if *strict* is ``False``.\n    * :exc:`ValueError` will be raised if *strict* is ``True``.\n\n    >>> list(batched('ABCDEFG', 3))\n    [('A', 'B', 'C'), ('D', 'E', 'F'), ('G',)]\n\n    On Python 3.13 and above, this is an alias for :func:`itertools.batched`.\n    \"\"\"\n    if n < 1:\n        raise ValueError(\"n must be at least one\")\n    it = iter(iterable)\n    while batch := tuple(itertools.islice(it, n)):\n        if strict and len(batch) != n:\n            raise ValueError(\"batched(): incomplete batch\")\n        yield batch\n\n\nif sys.hexversion >= 0x30D00A2:\n\n    def batched(iterable, n, *, strict=False):\n        return itertools.batched(iterable, n, strict=strict)\n\nelse:\n    batched = _batched\n\n    batched.__doc__ = _batched.__doc__\n# End port\n\n\ndef identity(*args):\n    return args\n\n\ndef _identity_cast(*args, typ):\n    return typ(args)\n\n\n_anom_count = itertools.count()\n\n\ndef parse_input(obj: Any) -> object:\n    \"\"\"Tokenize user input into GraphNode objects\n\n    Note: This is similar to `convert_legacy_task` but does not\n    - compare any values to a global set of known keys to infer references/futures\n    - parse tuples and interprets them as runnable tasks\n    - Deal with SubgraphCallables\n\n    Parameters\n    ----------\n    obj : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    if isinstance(obj, GraphNode):\n        return obj\n\n    if isinstance(obj, TaskRef):\n        return Alias(obj.key)\n\n    if isinstance(obj, dict):\n        parsed_dict = {k: parse_input(v) for k, v in obj.items()}\n        if any(isinstance(v, GraphNode) for v in parsed_dict.values()):\n            return Dict(parsed_dict)\n\n    if isinstance(obj, (list, set, tuple)):\n        parsed_collection = tuple(parse_input(o) for o in obj)\n        if any(isinstance(o, GraphNode) for o in parsed_collection):\n            if isinstance(obj, list):\n                return List(*parsed_collection)\n            if isinstance(obj, set):\n                return Set(*parsed_collection)\n            if isinstance(obj, tuple):\n                if is_namedtuple_instance(obj):\n                    return _wrap_namedtuple_task(None, obj, parse_input)\n                return Tuple(*parsed_collection)\n\n    return obj\n\n\ndef _wrap_namedtuple_task(k, obj, parser):\n    if hasattr(obj, \"__getnewargs_ex__\"):\n        new_args, kwargs = obj.__getnewargs_ex__()\n        kwargs = {k: parser(v) for k, v in kwargs.items()}\n    elif hasattr(obj, \"__getnewargs__\"):\n        new_args = obj.__getnewargs__()\n        kwargs = {}\n\n    args_converted = parse_input(type(new_args)(map(parser, new_args)))\n\n    return Task(\n        k, partial(_instantiate_named_tuple, type(obj)), args_converted, Dict(kwargs)\n    )\n\n\ndef _instantiate_named_tuple(typ, args, kwargs):\n    return typ(*args, **kwargs)\n\n\nclass _MultiContainer(Container):\n    container: tuple\n    __slots__ = (\"container\",)\n\n    def __init__(self, *container):\n        self.container = container\n\n    def __contains__(self, o: object) -> bool:\n        for c in self.container:\n            if o in c:\n                return True\n        return False\n\n\nSubgraphType = None\n\n\ndef _execute_subgraph(inner_dsk, outkey, inkeys, *dependencies):\n    final = {}\n    final.update(inner_dsk)\n    for k, v in zip(inkeys, dependencies):\n        final[k] = DataNode(None, v)\n    res = execute_graph(final, keys=[outkey])\n    return res[outkey]\n\n\ndef convert_legacy_task(\n    key: KeyType | None,\n    task: _T,\n    all_keys: Container,\n) -> GraphNode | _T:\n    if isinstance(task, GraphNode):\n        return task\n\n    if type(task) is tuple and task and callable(task[0]):\n        func, args = task[0], task[1:]\n        new_args = []\n        new: object\n        for a in args:\n            if isinstance(a, dict):\n                new = Dict(a)\n            else:\n                new = convert_legacy_task(None, a, all_keys)\n            new_args.append(new)\n        return Task(key, func, *new_args)\n    try:\n        if isinstance(task, (bytes, int, float, str, tuple)):\n            if task in all_keys:\n                if key is None:\n                    return Alias(task)\n                else:\n                    return Alias(key, target=task)\n    except TypeError:\n        # Unhashable\n        pass\n\n    if isinstance(task, (list, tuple, set, frozenset)):\n        if is_namedtuple_instance(task):\n            return _wrap_namedtuple_task(\n                key,\n                task,\n                partial(\n                    convert_legacy_task,\n                    None,\n                    all_keys=all_keys,\n                ),\n            )\n        else:\n            parsed_args = tuple(convert_legacy_task(None, t, all_keys) for t in task)\n            if any(isinstance(a, GraphNode) for a in parsed_args):\n                return Task(key, _identity_cast, *parsed_args, typ=type(task))\n            else:\n                return cast(_T, type(task)(parsed_args))\n    elif isinstance(task, TaskRef):\n        if key is None:\n            return Alias(task.key)\n        else:\n            return Alias(key, target=task.key)\n    else:\n        return task\n\n\ndef convert_legacy_graph(\n    dsk: Mapping,\n    all_keys: Container | None = None,\n):\n    if all_keys is None:\n        all_keys = set(dsk)\n    new_dsk = {}\n    for k, arg in dsk.items():\n        t = convert_legacy_task(k, arg, all_keys)\n        if isinstance(t, Alias) and t.target == k:\n            continue\n        elif not isinstance(t, GraphNode):\n            t = DataNode(k, t)\n        new_dsk[k] = t\n    return new_dsk\n\n\ndef resolve_aliases(dsk: dict, keys: set, dependents: dict) -> dict:\n    \"\"\"Remove trivial sequential alias chains\n\n    Example:\n\n        dsk = {'x': 1, 'y': Alias('x'), 'z': Alias('y')}\n\n        resolve_aliases(dsk, {'z'}, {'x': {'y'}, 'y': {'z'}}) == {'z': 1}\n\n    \"\"\"\n    if not keys:\n        raise ValueError(\"No keys provided\")\n    dsk = dict(dsk)\n    work = list(keys)\n    seen = set()\n    while work:\n        k = work.pop()\n        if k in seen or k not in dsk:\n            continue\n        seen.add(k)\n        t = dsk[k]\n        if isinstance(t, Alias):\n            target_key = t.target\n            # Rules for when we allow to collapse an alias\n            # 1. The target key is not in the keys set. The keys set is what the\n            #    user is requesting and by collapsing we'd no longer be able to\n            #    return that result.\n            # 2. The target key is in fact part of dsk. If it isnt' this could\n            #    point to a persisted dependency and we cannot collapse it.\n            # 3. The target key has only one dependent which is the key we're\n            #    currently looking at. This means that there is a one to one\n            #    relation between this and the target key in which case we can\n            #    collapse them.\n            #    Note: If target was an alias as well, we could continue with\n            #    more advanced optimizations but this isn't implemented, yet\n            if (\n                target_key not in keys\n                and target_key in dsk\n                # Note: whenever we're performing a collapse, we're not updating\n                # the dependents. The length == 1 should still be sufficient for\n                # chains of these aliases\n                and len(dependents[target_key]) == 1\n            ):\n                tnew = dsk.pop(target_key).copy()\n\n                dsk[k] = tnew\n                tnew.key = k\n                if isinstance(tnew, Alias):\n                    work.append(k)\n                    seen.discard(k)\n                else:\n                    work.extend(tnew.dependencies)\n\n        work.extend(t.dependencies)\n    return dsk\n\n\nclass TaskRef:\n    val: KeyType\n    __slots__ = (\"key\",)\n\n    def __init__(self, key: KeyType):\n        self.key = key\n\n    def __str__(self):\n        return str(self.key)\n\n    def __repr__(self):\n        return f\"{type(self).__name__}({self.key!r})\"\n\n    def __hash__(self) -> int:\n        return hash(self.key)\n\n    def __eq__(self, value: object) -> bool:\n        if not isinstance(value, TaskRef):\n            return False\n        return self.key == value.key\n\n\nclass GraphNode:\n    key: KeyType\n    _dependencies: frozenset\n\n    __slots__ = tuple(__annotations__)\n\n    def ref(self):\n        return Alias(self.key)\n\n    def copy(self):\n        raise NotImplementedError\n\n    @property\n    def data_producer(self) -> bool:\n        return False\n\n    @property\n    def dependencies(self) -> frozenset:\n        return self._dependencies\n\n    @property\n    def block_fusion(self) -> bool:\n        return False\n\n    def _verify_values(self, values: tuple | dict) -> None:\n        if not self.dependencies:\n            return\n        if missing := set(self.dependencies) - set(values):\n            raise RuntimeError(f\"Not enough arguments provided: missing keys {missing}\")\n\n    def __call__(self, values) -> Any:\n        raise NotImplementedError(\"Not implemented\")\n\n    def __eq__(self, value: object) -> bool:\n        if type(value) is not type(self):\n            return False\n\n        from dask.tokenize import tokenize\n\n        return tokenize(self) == tokenize(value)\n\n    @property\n    def is_coro(self) -> bool:\n        return False\n\n    def __sizeof__(self) -> int:\n        return sum(\n            sizeof(getattr(self, sl)) for sl in type(self).__slots__\n        ) + sys.getsizeof(type(self))\n\n    def substitute(\n        self, subs: dict[KeyType, KeyType | GraphNode], key: KeyType | None = None\n    ) -> GraphNode:\n        \"\"\"Substitute a dependency with a new value. The new value either has to\n        be a new valid key or a GraphNode to replace the dependency entirely.\n\n        The GraphNode will not be mutated but instead a shallow copy will be\n        returned. The substitution will be performed eagerly.\n\n        Parameters\n        ----------\n        subs : dict[KeyType, KeyType  |  GraphNode]\n            The mapping describing the substitutions to be made.\n        key : KeyType | None, optional\n            The key of the new GraphNode object. If None provided, the key of\n            the old one will be reused.\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def fuse(*tasks: GraphNode, key: KeyType | None = None) -> GraphNode:\n        \"\"\"Fuse a set of tasks into a single task.\n\n        The tasks are fused into a single task that will execute the tasks in a\n        subgraph. The internal tasks are no longer accessible from the outside.\n\n        All provided tasks must form a valid subgraph that will reduce to a\n        single key. If multiple outputs are possible with the provided tasks, an\n        exception will be raised.\n\n        The tasks will not be rewritten but instead a new Task will be created\n        that will merely reference the old task objects. This way, Task objects\n        may be reused in multiple fused tasks.\n\n        Parameters\n        ----------\n        key : KeyType | None, optional\n            The key of the new Task object. If None provided, the key of the\n            final task will be used.\n\n        See also\n        --------\n        GraphNode.substitute : Easer substitution of dependencies\n        \"\"\"\n        if any(t.key is None for t in tasks):\n            raise ValueError(\"Cannot fuse tasks with missing keys\")\n        if len(tasks) == 1:\n            return tasks[0].substitute({}, key=key)\n        all_keys = set()\n        all_deps: set[KeyType] = set()\n        for t in tasks:\n            all_deps.update(t.dependencies)\n            all_keys.add(t.key)\n        external_deps = tuple(sorted(all_deps - all_keys, key=hash))\n        leafs = all_keys - all_deps\n        if len(leafs) > 1:\n            raise ValueError(f\"Cannot fuse tasks with multiple outputs {leafs}\")\n\n        outkey = leafs.pop()\n        return Task(\n            key or outkey,\n            _execute_subgraph,\n            {t.key: t for t in tasks},\n            outkey,\n            external_deps,\n            *(TaskRef(k) for k in external_deps),\n            _data_producer=any(t.data_producer for t in tasks),\n        )\n\n\n_no_deps: frozenset = frozenset()\n\n\nclass Alias(GraphNode):\n    target: KeyType\n    __slots__ = tuple(__annotations__)\n\n    def __init__(\n        self, key: KeyType | TaskRef, target: Alias | TaskRef | KeyType | None = None\n    ):\n        if isinstance(key, TaskRef):\n            key = key.key\n        self.key = key\n        if target is None:\n            target = key\n        if isinstance(target, Alias):\n            target = target.target\n        if isinstance(target, TaskRef):\n            target = target.key\n        self.target = target\n        self._dependencies = frozenset((self.target,))\n\n    def copy(self):\n        return Alias(self.key, self.target)\n\n    def substitute(\n        self, subs: dict[KeyType, KeyType | GraphNode], key: KeyType | None = None\n    ) -> GraphNode:\n        if self.key in subs or self.target in subs:\n            sub_key = subs.get(self.key, self.key)\n            val = subs.get(self.target, self.target)\n            if sub_key == self.key and val == self.target:\n                return self\n            if isinstance(val, GraphNode):\n                return val.substitute({}, key=key)\n            if key is None and isinstance(sub_key, GraphNode):\n                raise RuntimeError(\n                    f\"Invalid substitution encountered {self.key!r} -> {sub_key}\"\n                )\n            return Alias(key or sub_key, val)  # type: ignore\n        return self\n\n    def __dask_tokenize__(self):\n        return (type(self).__name__, self.key, self.target)\n\n    def __call__(self, values=()):\n        self._verify_values(values)\n        return values[self.target]\n\n    def __repr__(self):\n        if self.key != self.target:\n            return f\"Alias({self.key!r}->{self.target!r})\"\n        else:\n            return f\"Alias({self.key!r})\"\n\n    def __eq__(self, value: object) -> bool:\n        if not isinstance(value, Alias):\n            return False\n        if self.key != value.key:\n            return False\n        if self.target != value.target:\n            return False\n        return True\n\n\nclass DataNode(GraphNode):\n    value: Any\n    typ: type\n    __slots__ = tuple(__annotations__)\n\n    def __init__(self, key: Any, value: Any):\n        if key is None:\n            key = (type(value).__name__, next(_anom_count))\n        self.key = key\n        self.value = value\n        self.typ = type(value)\n        self._dependencies = _no_deps\n\n    @property\n    def data_producer(self) -> bool:\n        return True\n\n    def copy(self):\n        return DataNode(self.key, self.value)\n\n    def __call__(self, values=()):\n        return self.value\n\n    def __repr__(self):\n        return f\"DataNode({self.value!r})\"\n\n    def __reduce__(self):\n        return (DataNode, (self.key, self.value))\n\n    def __dask_tokenize__(self):\n        from dask.base import tokenize\n\n        return (type(self).__name__, tokenize(self.value))\n\n    def substitute(\n        self, subs: dict[KeyType, KeyType | GraphNode], key: KeyType | None = None\n    ) -> DataNode:\n        if key is not None and key != self.key:\n            return DataNode(key, self.value)\n        return self\n\n    def __iter__(self):\n        return iter(self.value)\n\n\ndef _call_recursively(obj, values):\n    if isinstance(obj, GraphNode):\n        return obj(values)\n    elif isinstance(obj, dict):\n        return {k: _call_recursively(v, values) for k, v in obj.items()}\n    elif isinstance(obj, (list, tuple, frozenset, set)):\n        return type(obj)(_call_recursively(v, values) for v in obj)\n    return obj\n\n\ndef _get_dependencies(obj: object) -> set | frozenset:\n    if isinstance(obj, GraphNode):\n        return obj.dependencies\n    elif isinstance(obj, dict):\n        if not obj:\n            return _no_deps\n        return set().union(*map(_get_dependencies, obj.values()))\n    elif isinstance(obj, (list, tuple, frozenset, set)):\n        if not obj:\n            return _no_deps\n        return set().union(*map(_get_dependencies, obj))\n    return _no_deps\n\n\nclass Task(GraphNode):\n    func: Callable\n    args: tuple\n    kwargs: dict\n    _data_producer: bool\n    _token: str | None\n    _is_coro: bool | None\n    _repr: str | None\n\n    __slots__ = tuple(__annotations__)\n\n    def __init__(\n        self,\n        key: Any,\n        func: Callable,\n        /,\n        *args: Any,\n        _data_producer: bool = False,\n        _dependencies: set | frozenset | None = None,\n        **kwargs: Any,\n    ):\n        self.key = key\n        self.func = func\n        if isinstance(func, Task):\n            raise TypeError(\"Cannot nest tasks\")\n        self.args = tuple(\n            Alias(obj.key) if isinstance(obj, TaskRef) else obj for obj in args\n        )\n        self.kwargs = {\n            k: Alias(v.key) if isinstance(v, TaskRef) else v for k, v in kwargs.items()\n        }\n        if _dependencies is None:\n            _dependencies = set()\n            for a in itertools.chain(self.args, self.kwargs.values()):\n                if isinstance(a, GraphNode):\n                    _dependencies.update(a.dependencies)\n        if _dependencies:\n            self._dependencies = frozenset(_dependencies)\n        else:\n            self._dependencies = _no_deps\n        self._is_coro = None\n        self._token = None\n        self._repr = None\n        self._data_producer = _data_producer\n\n    @property\n    def data_producer(self) -> bool:\n        return self._data_producer\n\n    def copy(self):\n        return type(self)(\n            self.key,\n            self.func,\n            *self.args,\n            _dependencies=self._dependencies,\n            **self.kwargs,\n        )\n\n    def __hash__(self):\n        return hash(self._get_token())\n\n    def _get_token(self) -> str:\n        if self._token:\n            return self._token\n        from dask.base import tokenize\n\n        self._token = tokenize(\n            (\n                type(self).__name__,\n                self.func,\n                self.args,\n                self.kwargs,\n            )\n        )\n        return self._token\n\n    def __dask_tokenize__(self):\n        return self._get_token()\n\n    def __repr__(self) -> str:\n        # When `Task` is deserialized the constructor will not run and\n        # `self._repr` is thus undefined.\n        if not hasattr(self, \"_repr\") or not self._repr:\n            head = funcname(self.func)\n            tail = \")\"\n            label_size = 40\n            args = self.args\n            kwargs = self.kwargs\n            if args or kwargs:\n                label_size2 = int(\n                    (label_size - len(head) - len(tail) - len(str(self.key)))\n                    // (len(args) + len(kwargs))\n                )\n            if args:\n                if label_size2 > 5:\n                    args_repr = \", \".join(repr(t) for t in args)\n                else:\n                    args_repr = \"...\"\n            else:\n                args_repr = \"\"\n            if kwargs:\n                if label_size2 > 5:\n                    kwargs_repr = \", \" + \", \".join(\n                        f\"{k}={repr(v)}\" for k, v in sorted(kwargs.items())\n                    )\n                else:\n                    kwargs_repr = \", ...\"\n            else:\n                kwargs_repr = \"\"\n            self._repr = f\"<Task {self.key!r} {head}({args_repr}{kwargs_repr}{tail}>\"\n        return self._repr\n\n    def __call__(self, values=()):\n        self._verify_values(values)\n        new_argspec = tuple(\n            a({k: values[k] for k in a.dependencies}) if isinstance(a, GraphNode) else a\n            for a in self.args\n        )\n        if self.kwargs:\n            kwargs = {\n                k: (\n                    kw({k: values[k] for k in kw.dependencies})\n                    if isinstance(kw, GraphNode)\n                    else kw\n                )\n                for k, kw in self.kwargs.items()\n            }\n            return self.func(*new_argspec, **kwargs)\n        return self.func(*new_argspec)\n\n    @property\n    def is_coro(self):\n        if self._is_coro is None:\n            # Note: Can't use cached_property on objects without __dict__\n            try:\n                from distributed.utils import iscoroutinefunction\n\n                self._is_coro = iscoroutinefunction(self.func)\n            except Exception:\n                self._is_coro = False\n        return self._is_coro\n\n    def substitute(\n        self, subs: dict[KeyType, KeyType | GraphNode], key: KeyType | None = None\n    ) -> Task:\n        subs_filtered = {\n            k: v for k, v in subs.items() if k in self.dependencies and k != v\n        }\n        if subs_filtered:\n            new_args = tuple(\n                a.substitute(subs_filtered) if isinstance(a, GraphNode) else a\n                for a in self.args\n            )\n            new_kwargs = {\n                k: v.substitute(subs_filtered) if isinstance(v, GraphNode) else v\n                for k, v in self.kwargs.items()\n            }\n            return type(self)(\n                key or self.key,\n                self.func,\n                *new_args,\n                _data_producer=self.data_producer,\n                **new_kwargs,\n            )\n        elif key is None or key == self.key:\n            return self\n        else:\n            # Rename\n            return type(self)(\n                key,\n                self.func,\n                *self.args,\n                _data_producer=self.data_producer,\n                **self.kwargs,\n            )\n\n\nclass NestedContainer(Task):\n    constructor: Callable\n    klass: type\n    __slots__ = tuple(__annotations__)\n\n    def __init__(\n        self,\n        /,\n        *args: Any,\n        _dependencies: set | frozenset | None = None,\n        **kwargs: Any,\n    ):\n        if len(args) == 1 and isinstance(args[0], self.klass):\n            args = args[0]  # type: ignore\n        super().__init__(\n            None,\n            self.to_container,\n            *args,\n            constructor=self.constructor,\n            _dependencies=_dependencies,\n            **kwargs,\n        )\n\n    def __repr__(self):\n        return f\"{type(self).__name__}({self.args})\"\n\n    def substitute(\n        self, subs: dict[KeyType, KeyType | GraphNode], key: KeyType | None = None\n    ) -> NestedContainer:\n        subs_filtered = {\n            k: v for k, v in subs.items() if k in self.dependencies and k != v\n        }\n        if not subs_filtered:\n            return self\n        return type(self)(\n            *(\n                a.substitute(subs_filtered) if isinstance(a, GraphNode) else a\n                for a in self.args\n            )\n        )\n\n    def __dask_tokenize__(self):\n        from dask.tokenize import tokenize\n\n        return (\n            type(self).__name__,\n            self.klass,\n            sorted(tokenize(a) for a in self.args),\n        )\n\n        return super().__dask_tokenize__()\n\n    @staticmethod\n    def to_container(*args, constructor):\n        return constructor(args)\n\n\nclass List(NestedContainer):\n    constructor = klass = list\n\n\nclass Tuple(NestedContainer):\n    constructor = klass = tuple\n\n\nclass Set(NestedContainer):\n    constructor = klass = set\n\n\nclass Dict(NestedContainer):\n    klass = dict\n\n    def __init__(\n        self, /, *args: Any, _dependencies: set | frozenset | None = None, **kwargs: Any\n    ):\n        if args:\n            if len(args) > 1:\n                raise ValueError(\"Dict can only take one positional argument\")\n            kwargs = args[0]\n        super().__init__(\n            *(itertools.chain(*kwargs.items())), _dependencies=_dependencies\n        )\n\n    def __repr__(self):\n        values = \", \".join(f\"{k}: {v}\" for k, v in batched(self.args, 2, strict=True))\n        return f\"Dict({values})\"\n\n    def substitute(\n        self, subs: dict[KeyType, KeyType | GraphNode], key: KeyType | None = None\n    ) -> Dict:\n        subs_filtered = {\n            k: v for k, v in subs.items() if k in self.dependencies and k != v\n        }\n        if not subs_filtered:\n            return self\n\n        new = {}\n        for k, v in batched(self.args, 2, strict=True):\n            v = v.substitute(subs_filtered) if isinstance(v, GraphNode) else v\n            new[k] = v\n        return type(self)(new)\n\n    @staticmethod\n    def constructor(args):\n        return dict(batched(args, 2, strict=True))\n\n\nclass DependenciesMapping(MutableMapping):\n    def __init__(self, dsk):\n        self.dsk = dsk\n        self._removed = set()\n        # Set a copy of dsk to avoid dct resizing\n        self._cache = dsk.copy()\n        self._cache.clear()\n\n    def __getitem__(self, key):\n        if (val := self._cache.get(key)) is not None:\n            return val\n        else:\n            v = self.dsk[key]\n            try:\n                deps = v.dependencies\n            except AttributeError:\n                from dask.core import get_dependencies\n\n                deps = get_dependencies(self.dsk, task=v)\n\n            if self._removed:\n                # deps is a frozenset but for good measure, let's not use -= since\n                # that _may_ perform an inplace mutation\n                deps = deps - self._removed\n            self._cache[key] = deps\n            return deps\n\n    def __iter__(self):\n        return iter(self.dsk)\n\n    def __delitem__(self, key: Any) -> None:\n        self._cache.clear()\n        self._removed.add(key)\n\n    def __setitem__(self, key: Any, value: Any) -> None:\n        raise NotImplementedError\n\n    def __len__(self) -> int:\n        return len(self.dsk)\n\n\nclass _DevNullMapping(MutableMapping):\n    def __getitem__(self, key):\n        raise KeyError(key)\n\n    def __setitem__(self, key, value):\n        pass\n\n    def __delitem__(self, key):\n        pass\n\n    def __len__(self):\n        return 0\n\n    def __iter__(self):\n        return iter(())\n\n\ndef execute_graph(\n    dsk: Iterable[GraphNode] | Mapping[KeyType, GraphNode],\n    cache: MutableMapping[KeyType, object] | None = None,\n    keys: Container[KeyType] | None = None,\n) -> MutableMapping[KeyType, object]:\n    \"\"\"Execute a given graph.\n\n    The graph is exceuted in topological order as defined by dask.order until\n    all leaf nodes, i.e. nodes without any dependents, are reached. The returned\n    dictionary contains the results of the leaf nodes.\n\n    If keys are required that are not part of the graph, they can be provided in the `cache` argument.\n\n    If `keys` is provided, the result will contain only values that are part of the `keys` set.\n\n    \"\"\"\n    if isinstance(dsk, (list, tuple, set, frozenset)):\n        dsk = {t.key: t for t in dsk}\n    else:\n        assert isinstance(dsk, dict)\n\n    refcount: defaultdict[KeyType, int] = defaultdict(int)\n    for vals in DependenciesMapping(dsk).values():\n        for val in vals:\n            refcount[val] += 1\n\n    cache = cache or {}\n    from dask.order import order\n\n    priorities = order(dsk)\n\n    for key, node in sorted(dsk.items(), key=lambda it: priorities[it[0]]):\n        cache[key] = node(cache)\n        for dep in node.dependencies:\n            refcount[dep] -= 1\n            if refcount[dep] == 0 and keys and dep not in keys:\n                del cache[dep]\n\n    return cache\n\n\ndef fuse_linear_task_spec(dsk, keys):\n    \"\"\"\n    keys are the keys from the graph that are requested by a computation. We\n    can't fuse those together.\n    \"\"\"\n    from dask.core import reverse_dict\n    from dask.optimization import default_fused_keys_renamer\n\n    keys = set(keys)\n    dependencies = DependenciesMapping(dsk)\n    dependents = reverse_dict(dependencies)\n\n    seen = set()\n    result = {}\n\n    for key in dsk:\n        if key in seen:\n            continue\n\n        seen.add(key)\n\n        deps = dependencies[key]\n        dependents_key = dependents[key]\n\n        if len(deps) != 1 and len(dependents_key) != 1 or dsk[key].block_fusion:\n            result[key] = dsk[key]\n            continue\n\n        linear_chain = [dsk[key]]\n        top_key = key\n\n        # Walk towards the leafs as long as the nodes have a single dependency\n        # and a single dependent, we can't fuse two nodes of an intermediate node\n        # is the source for 2 dependents\n        while len(deps) == 1:\n            (new_key,) = deps\n            if new_key in seen:\n                break\n            seen.add(new_key)\n            if new_key not in dsk:\n                # This can happen if a future is in the graph, the dependency mapping\n                # adds the key that is referenced by the future as a dependency\n                # see test_futures_to_delayed_array\n                break\n            if (\n                len(dependents[new_key]) != 1\n                or dsk[new_key].block_fusion\n                or new_key in keys\n            ):\n                result[new_key] = dsk[new_key]\n                break\n            # backwards comp for new names, temporary until is_rootish is removed\n            linear_chain.insert(0, dsk[new_key])\n            deps = dependencies[new_key]\n\n        # Walk the tree towards the root as long as the nodes have a single dependent\n        # and a single dependency, we can't fuse two nodes if node has multiple\n        # dependencies\n        while len(dependents_key) == 1 and top_key not in keys:\n            new_key = dependents_key.pop()\n            if new_key in seen:\n                break\n            seen.add(new_key)\n            if len(dependencies[new_key]) != 1 or dsk[new_key].block_fusion:\n                # Exit if the dependent has multiple dependencies, triangle\n                result[new_key] = dsk[new_key]\n                break\n            linear_chain.append(dsk[new_key])\n            top_key = new_key\n            dependents_key = dependents[new_key]\n\n        if len(linear_chain) == 1:\n            result[top_key] = linear_chain[0]\n        else:\n            # Renaming the keys is necessary to preserve the rootish detection for now\n            renamed_key = default_fused_keys_renamer([tsk.key for tsk in linear_chain])\n            result[renamed_key] = Task.fuse(*linear_chain, key=renamed_key)\n            if renamed_key != top_key:\n                # Having the same prefixes can result in the same key, i.e. getitem-hash -> getitem-hash\n                result[top_key] = Alias(top_key, target=renamed_key)\n    return result\n\n\ndef cull(\n    dsk: dict[KeyType, GraphNode], keys: Iterable[KeyType]\n) -> dict[KeyType, GraphNode]:\n    work = set(keys)\n    seen: set[KeyType] = set()\n    dsk2 = {}\n    wpop = work.pop\n    wupdate = work.update\n    sadd = seen.add\n    while work:\n        k = wpop()\n        if k in seen or k not in dsk:\n            continue\n        sadd(k)\n        dsk2[k] = v = dsk[k]\n        wupdate(v.dependencies)\n    return dsk2\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:36:31"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport pytest\n\nimport dask\nfrom dask.local import finish_task, get_sync, sortkey, start_state_from_dask\nfrom dask.order import order\nfrom dask.utils_test import GetFunctionTestMixin, add, inc\n\nfib_dask = {\"f0\": 0, \"f1\": 1, \"f2\": 1, \"f3\": 2, \"f4\": 3, \"f5\": 5, \"f6\": 8}\n\n\ndef test_start_state():\n    dsk = {\"x\": 1, \"y\": 2, \"z\": (inc, \"x\"), \"w\": (add, \"z\", \"y\")}\n    result = start_state_from_dask(dsk)\n\n    expected = {\n        \"cache\": {\"x\": 1, \"y\": 2},\n        \"dependencies\": {\n            \"w\": {\"y\", \"z\"},\n            \"x\": set(),\n            \"y\": set(),\n            \"z\": {\"x\"},\n        },\n        \"dependents\": {\"w\": set(), \"x\": {\"z\"}, \"y\": {\"w\"}, \"z\": {\"w\"}},\n        \"finished\": set(),\n        \"released\": set(),\n        \"running\": set(),\n        \"ready\": [\"z\"],\n        \"waiting\": {\"w\": {\"z\"}},\n        \"waiting_data\": {\"x\": {\"z\"}, \"y\": {\"w\"}, \"z\": {\"w\"}},\n    }\n    assert result == expected\n\n\ndef test_start_state_looks_at_cache():\n    dsk = {\"b\": (inc, \"a\")}\n    cache = {\"a\": 1}\n    result = start_state_from_dask(dsk, cache)\n    assert result[\"dependencies\"][\"b\"] == {\"a\"}\n    assert result[\"ready\"] == [\"b\"]\n\n\ndef test_start_state_with_redirects():\n    dsk = {\"x\": 1, \"y\": \"x\", \"z\": (inc, \"y\")}\n    result = start_state_from_dask(dsk)\n    assert result[\"cache\"] == {\"x\": 1}\n\n\ndef test_start_state_with_independent_but_runnable_tasks():\n    assert start_state_from_dask({\"x\": (inc, 1)})[\"ready\"] == [\"x\"]\n\n\ndef test_start_state_with_tasks_no_deps():\n    dsk = {\"a\": [1, (inc, 2)], \"b\": [1, 2, 3, 4], \"c\": (inc, 3)}\n    state = start_state_from_dask(dsk)\n    assert list(state[\"cache\"].keys()) == [\"b\"]\n    assert \"a\" in state[\"ready\"] and \"c\" in state[\"ready\"]\n    deps = {k: set() for k in \"abc\"}\n    assert state[\"dependencies\"] == deps\n    assert state[\"dependents\"] == deps\n\n\ndef test_finish_task():\n    dsk = {\"x\": 1, \"y\": 2, \"z\": (inc, \"x\"), \"w\": (add, \"z\", \"y\")}\n    sortkey = order(dsk).get\n    state = start_state_from_dask(dsk)\n    state[\"ready\"].remove(\"z\")\n    state[\"running\"] = {\"z\", \"other-task\"}\n    task = \"z\"\n    result = 2\n\n    state[\"cache\"][\"z\"] = result\n    finish_task(dsk, task, state, set(), sortkey)\n\n    assert state == {\n        \"cache\": {\"y\": 2, \"z\": 2},\n        \"dependencies\": {\n            \"w\": {\"y\", \"z\"},\n            \"x\": set(),\n            \"y\": set(),\n            \"z\": {\"x\"},\n        },\n        \"finished\": {\"z\"},\n        \"released\": {\"x\"},\n        \"running\": {\"other-task\"},\n        \"dependents\": {\"w\": set(), \"x\": {\"z\"}, \"y\": {\"w\"}, \"z\": {\"w\"}},\n        \"ready\": [\"w\"],\n        \"waiting\": {},\n        \"waiting_data\": {\"y\": {\"w\"}, \"z\": {\"w\"}},\n    }\n\n\nclass TestGetAsync(GetFunctionTestMixin):\n    get = staticmethod(get_sync)\n\n    def test_get_sync_num_workers(self):\n        self.get({\"x\": (inc, \"y\"), \"y\": 1}, \"x\", num_workers=2)\n\n\ndef test_cache_options():\n    cache = {}\n\n    def inc2(x):\n        assert \"y\" in cache\n        return x + 1\n\n    with dask.config.set(cache=cache):\n        get_sync({\"x\": (inc2, \"y\"), \"y\": 1}, \"x\")\n\n\ndef test_sort_key():\n    L = [\"x\", (\"x\", 1), (\"z\", 0), (\"x\", 0)]\n    assert sorted(L, key=sortkey) == [\"x\", (\"x\", 0), (\"x\", 1), (\"z\", 0)]\n\n\ndef test_callback():\n    f = lambda x: x + 1\n    dsk = {\"a\": (f, 1)}\n    from dask.threaded import get\n\n    def start_callback(key, d, state):\n        assert key == \"a\" or key is None\n        assert d == dsk\n        assert isinstance(state, dict)\n\n    def end_callback(key, value, d, state, worker_id):\n        assert key == \"a\" or key is None\n        assert value == 2 or value is None\n        assert d == dsk\n        assert isinstance(state, dict)\n\n    get(dsk, \"a\", start_callback=start_callback, end_callback=end_callback)\n\n\ndef test_exceptions_propagate():\n    class MyException(Exception):\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n\n        def __str__(self):\n            return \"My Exception!\"\n\n    def f():\n        raise MyException(1, 2)\n\n    from dask.threaded import get\n\n    try:\n        get({\"x\": (f,)}, \"x\")\n        assert False\n    except MyException as e:\n        assert \"My Exception!\" in str(e)\n        assert \"a\" in dir(e)\n        assert e.a == 1\n        assert e.b == 2\n\n\ndef test_ordering():\n    L = []\n\n    def append(i):\n        L.append(i)\n\n    dsk = {(\"x\", i): (append, i) for i in range(10)}\n    x_keys = sorted(dsk)\n    dsk[\"y\"] = (lambda *args: None, list(x_keys))\n\n    get_sync(dsk, \"y\")\n\n    assert L == sorted(L, reverse=True)\n\n\ndef test_complex_ordering():\n    pytest.importorskip(\"numpy\")\n    da = pytest.importorskip(\"dask.array\")\n    from dask.diagnostics import Callback\n\n    actual_order = []\n\n    def track_order(key, dask, state):\n        actual_order.append(key)\n\n    x = da.random.normal(size=(20, 20), chunks=(-1, -1))\n    res = (x.dot(x.T) - x.mean(axis=0)).std()\n    dsk = dict(res.__dask_graph__())\n    exp_order_dict = order(dsk)\n    exp_order = sorted(exp_order_dict.keys(), key=exp_order_dict.get)\n    with Callback(pretask=track_order):\n        get_sync(dsk, exp_order[-1])\n    assert actual_order == exp_order\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_multiprocessing\\test_multiprocessing.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport copyreg\nimport multiprocessing\nimport multiprocessing.pool\nimport os\nimport pickle\nimport sys\nimport traceback\nfrom collections.abc import Mapping, Sequence\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nfrom warnings import warn\n\nimport cloudpickle\n\nfrom dask import config\nfrom dask.local import MultiprocessingPoolExecutor, get_async, reraise\nfrom dask.optimization import cull, fuse\nfrom dask.system import CPU_COUNT\nfrom dask.typing import Key\nfrom dask.utils import ensure_dict\n\n\ndef _reduce_method_descriptor(m):\n    return getattr, (m.__objclass__, m.__name__)\n\n\n# type(set.union) is used as a proxy to <class 'method_descriptor'>\ncopyreg.pickle(type(set.union), _reduce_method_descriptor)\n\n_dumps = partial(cloudpickle.dumps, protocol=pickle.HIGHEST_PROTOCOL)\n_loads = cloudpickle.loads\n\n\ndef _process_get_id():\n    return multiprocessing.current_process().ident\n\n\n# -- Remote Exception Handling --\n# By default, tracebacks can't be serialized using pickle. However, the\n# `tblib` library can enable support for this. Since we don't mandate\n# that tblib is installed, we do the following:\n#\n# - If tblib is installed, use it to serialize the traceback and reraise\n#   in the scheduler process\n# - Otherwise, use a ``RemoteException`` class to contain a serialized\n#   version of the formatted traceback, which will then print in the\n#   scheduler process.\n#\n# To enable testing of the ``RemoteException`` class even when tblib is\n# installed, we don't wrap the class in the try block below\nclass RemoteException(Exception):\n    \"\"\"Remote Exception\n\n    Contains the exception and traceback from a remotely run task\n    \"\"\"\n\n    def __init__(self, exception, traceback):\n        self.exception = exception\n        self.traceback = traceback\n\n    def __str__(self):\n        return str(self.exception) + \"\\n\\nTraceback\\n---------\\n\" + self.traceback\n\n    def __dir__(self):\n        return sorted(set(dir(type(self)) + list(self.__dict__) + dir(self.exception)))\n\n    def __getattr__(self, key):\n        try:\n            return object.__getattribute__(self, key)\n        except AttributeError:\n            return getattr(self.exception, key)\n\n\nexceptions: dict[type[Exception], type[Exception]] = {}\n\n\ndef remote_exception(exc: Exception, tb) -> Exception:\n    \"\"\"Metaclass that wraps exception type in RemoteException\"\"\"\n    if type(exc) in exceptions:\n        typ = exceptions[type(exc)]\n        return typ(exc, tb)\n    else:\n        try:\n            typ = type(\n                exc.__class__.__name__,\n                (RemoteException, type(exc)),\n                {\"exception_type\": type(exc)},\n            )\n            exceptions[type(exc)] = typ\n            return typ(exc, tb)\n        except TypeError:\n            return exc\n\n\ntry:\n    import tblib.pickling_support\n\n    tblib.pickling_support.install()\n\n    def _pack_traceback(tb):\n        return tb\n\nexcept ImportError:\n\n    def _pack_traceback(tb):\n        return \"\".join(traceback.format_tb(tb))\n\n    def reraise(exc, tb=None):\n        exc = remote_exception(exc, tb)\n        raise exc\n\n\ndef pack_exception(e, dumps):\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    tb = _pack_traceback(exc_traceback)\n    try:\n        result = dumps((e, tb))\n    except Exception as e:\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        tb = _pack_traceback(exc_traceback)\n        result = dumps((e, tb))\n    return result\n\n\n_CONTEXT_UNSUPPORTED = \"\"\"\\\nThe 'multiprocessing.context' configuration option will be ignored on Python 2\nand on Windows, because they each only support a single context.\n\"\"\"\n\n\ndef get_context():\n    \"\"\"Return the current multiprocessing context.\"\"\"\n    # fork context does fork()-without-exec(), which can lead to deadlocks,\n    # so default to \"spawn\".\n    context_name = config.get(\"multiprocessing.context\", \"spawn\")\n    if sys.platform == \"win32\":\n        if context_name != \"spawn\":\n            # Only spawn is supported on Win32, can't change it:\n            warn(_CONTEXT_UNSUPPORTED, UserWarning)\n        return multiprocessing\n    else:\n        return multiprocessing.get_context(context_name)\n\n\ndef get(\n    dsk: Mapping,\n    keys: Sequence[Key] | Key,\n    num_workers=None,\n    func_loads=None,\n    func_dumps=None,\n    optimize_graph=True,\n    pool=None,\n    initializer=None,\n    chunksize=None,\n    **kwargs,\n):\n    \"\"\"Multiprocessed get function appropriate for Bags\n\n    Parameters\n    ----------\n    dsk : dict\n        dask graph\n    keys : object or list\n        Desired results from graph\n    num_workers : int\n        Number of worker processes (defaults to number of cores)\n    func_dumps : function\n        Function to use for function serialization (defaults to cloudpickle.dumps)\n    func_loads : function\n        Function to use for function deserialization (defaults to cloudpickle.loads)\n    optimize_graph : bool\n        If True [default], `fuse` is applied to the graph before computation.\n    pool : Executor or Pool\n        Some sort of `Executor` or `Pool` to use\n    initializer: function\n        Ignored if ``pool`` has been set.\n        Function to initialize a worker process before running any tasks in it.\n    chunksize: int, optional\n        Size of chunks to use when dispatching work.\n        Defaults to 6 as some batching is helpful.\n        If -1, will be computed to evenly divide ready work across workers.\n    \"\"\"\n    chunksize = chunksize or config.get(\"chunksize\", 6)\n    pool = pool or config.get(\"pool\", None)\n    initializer = initializer or config.get(\"multiprocessing.initializer\", None)\n    num_workers = num_workers or config.get(\"num_workers\", None) or CPU_COUNT\n    if pool is None:\n        # In order to get consistent hashing in subprocesses, we need to set a\n        # consistent seed for the Python hash algorithm. Unfortunately, there\n        # is no way to specify environment variables only for the Pool\n        # processes, so we have to rely on environment variables being\n        # inherited.\n        if os.environ.get(\"PYTHONHASHSEED\") in (None, \"0\"):\n            # This number is arbitrary; it was chosen to commemorate\n            # https://github.com/dask/dask/issues/6640.\n            os.environ[\"PYTHONHASHSEED\"] = \"6640\"\n        context = get_context()\n        initializer = partial(initialize_worker_process, user_initializer=initializer)\n        pool = ProcessPoolExecutor(\n            num_workers, mp_context=context, initializer=initializer\n        )\n        cleanup = True\n    else:\n        if initializer is not None:\n            warn(\n                \"The ``initializer`` argument is ignored when ``pool`` is provided. \"\n                \"The user should configure ``pool`` with the needed ``initializer`` \"\n                \"on creation.\"\n            )\n        if isinstance(pool, multiprocessing.pool.Pool):\n            pool = MultiprocessingPoolExecutor(pool)\n        cleanup = False\n\n    # Optimize Dask\n    dsk = ensure_dict(dsk)\n    dsk2, dependencies = cull(dsk, keys)\n    if optimize_graph:\n        dsk3, dependencies = fuse(dsk2, keys, dependencies)\n    else:\n        dsk3 = dsk2\n\n    # We specify marshalling functions in order to catch serialization\n    # errors and report them to the user.\n    loads = func_loads or config.get(\"func_loads\", None) or _loads\n    dumps = func_dumps or config.get(\"func_dumps\", None) or _dumps\n\n    # Note former versions used a multiprocessing Manager to share\n    # a Queue between parent and workers, but this is fragile on Windows\n    # (issue #1652).\n    try:\n        # Run\n        result = get_async(\n            pool.submit,\n            pool._max_workers,\n            dsk3,\n            keys,\n            get_id=_process_get_id,\n            dumps=dumps,\n            loads=loads,\n            pack_exception=pack_exception,\n            raise_exception=reraise,\n            chunksize=chunksize,\n            **kwargs,\n        )\n    finally:\n        if cleanup:\n            pool.shutdown()\n    return result\n\n\ndef default_initializer():\n    # If Numpy is already imported, presumably its random state was\n    # inherited from the parent => re-seed it.\n    np = sys.modules.get(\"numpy\")\n    if np is not None:\n        np.random.seed()\n\n\ndef initialize_worker_process(user_initializer=None):\n    \"\"\"\n    Initialize a worker process before running any tasks in it.\n    \"\"\"\n    default_initializer()\n\n    if user_initializer is not None:\n        user_initializer()\n\n### Source File Dependency Files Content\n### Dependency File: local.py\n\"\"\"\nAsynchronous Shared-Memory Scheduler for Dask Graphs.\n\nThis scheduler coordinates several workers to execute tasks in a dask graph in\nparallel.  It depends on a ``concurrent.futures.Executor``\nand a corresponding Queue for worker-to-scheduler communication.\n\nIt tries to execute tasks in an order which maintains a small memory footprint\nthroughout execution.  It does this by running tasks that allow us to release\ndata resources.\n\n\nTask Selection Policy\n=====================\n\nWhen we complete a task we add more data in to our set of available data; this\nnew data makes new tasks available.  We preferentially choose tasks that were\njust made available in a last-in-first-out fashion.  We implement this as a\nsimple stack.  This results in more depth-first rather than breadth first\nbehavior which encourages us to process batches of data to completion before\nstarting in on new data when possible.\n\nWhen the addition of new data readies multiple tasks simultaneously we add\ntasks to the stack in sorted order so that tasks with greater keynames are run\nfirst.  This can be handy to break ties in a predictable fashion.\n\n\nState\n=====\n\nMany functions pass around a ``state`` variable that holds the current state of\nthe computation.  This variable consists of several other dictionaries and\nsets, explained below.\n\nConstant state\n--------------\n\n1.  dependencies: {x: [a, b ,c]} a,b,c, must be run before x\n2.  dependents: {a: [x, y]} a must run before x or y\n\nChanging state\n--------------\n\n### Data\n\n1.  cache: available concrete data.  {key: actual-data}\n2.  released: data that we've seen, used, and released because it is no longer\n    needed\n\n### Jobs\n\n1.  ready: A fifo stack of ready-to-run tasks\n2.  running: A set of tasks currently in execution\n3.  finished: A set of finished tasks\n4.  waiting: which tasks are still waiting on others :: {key: {keys}}\n    Real-time equivalent of dependencies\n5.  waiting_data: available data to yet-to-be-run-tasks :: {key: {keys}}\n    Real-time equivalent of dependents\n\n\nExamples\n--------\n\n>>> import pprint  # doctest: +SKIP\n>>> inc = lambda x: x + 1\n>>> add = lambda x, y: x + y\n>>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n>>> pprint.pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n{'cache': {'x': 1, 'y': 2},\n 'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n 'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n 'finished': set(),\n 'ready': ['z'],\n 'released': set(),\n 'running': set(),\n 'waiting': {'w': {'z'}},\n 'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n\nOptimizations\n=============\n\nWe build this scheduler with out-of-core array operations in mind.  To this end\nwe have encoded some particular optimizations.\n\nCompute to release data\n-----------------------\n\nWhen we choose a new task to execute we often have many options.  Policies at\nthis stage are cheap and can significantly impact performance.  One could\nimagine policies that expose parallelism, drive towards a particular output,\netc..\n\nOur current policy is to run tasks that were most recently made available.\n\n\nInlining computations\n---------------------\n\nWe hold on to intermediate computations either in memory or on disk.\n\nFor very cheap computations that may emit new copies of the data, like\n``np.transpose`` or possibly even ``x + 1`` we choose not to store these as\nseparate pieces of data / tasks.  Instead we combine them with the computations\nthat require them.  This may result in repeated computation but saves\nsignificantly on space and computation complexity.\n\nSee the function ``inline_functions`` for more information.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections.abc import Mapping, Sequence\nfrom concurrent.futures import Executor, Future\nfrom functools import partial\nfrom queue import Empty, Queue\n\nfrom dask import config\nfrom dask._task_spec import DataNode, DependenciesMapping, convert_legacy_graph\nfrom dask.callbacks import local_callbacks, unpack_callbacks\nfrom dask.core import flatten, get_dependencies, reverse_dict\nfrom dask.order import order\nfrom dask.typing import Key\n\nif os.name == \"nt\":\n    # Python 3 windows Queue.get doesn't handle interrupts properly. To\n    # workaround this we poll at a sufficiently large interval that it\n    # shouldn't affect performance, but small enough that users trying to kill\n    # an application shouldn't care.\n    def queue_get(q):\n        while True:\n            try:\n                return q.get(block=True, timeout=0.1)\n            except Empty:\n                pass\n\nelse:\n\n    def queue_get(q):\n        return q.get()\n\n\ndef start_state_from_dask(dsk, cache=None, sortkey=None):\n    \"\"\"Start state from a dask\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n    >>> from pprint import pprint  # doctest: +SKIP\n    >>> pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n    {'cache': {'x': 1, 'y': 2},\n     'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n     'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n     'finished': set(),\n     'ready': ['z'],\n     'released': set(),\n     'running': set(),\n     'waiting': {'w': {'z'}},\n     'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n    \"\"\"\n    if sortkey is None:\n        sortkey = order(dsk).get\n    if cache is None:\n        cache = config.get(\"cache\", None)\n    if cache is None:\n        cache = dict()\n\n    dsk = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    data_keys = set()\n    for k, v in dsk.items():\n        if isinstance(v, DataNode):\n            cache[k] = v()\n            data_keys.add(k)\n\n    dsk2 = dsk.copy()\n    dsk2.update(cache)\n\n    dependencies = DependenciesMapping(dsk)\n    waiting = {k: set(v) for k, v in dependencies.items() if k not in data_keys}\n\n    dependents = reverse_dict(dependencies)\n    for a in cache:\n        for b in dependents.get(a, ()):\n            waiting[b].remove(a)\n    waiting_data = {k: v.copy() for k, v in dependents.items() if v}\n\n    ready_set = {k for k, v in waiting.items() if not v}\n    ready = sorted(ready_set, key=sortkey, reverse=True)\n    waiting = {k: v for k, v in waiting.items() if v}\n\n    state = {\n        \"dependencies\": dependencies,\n        \"dependents\": dependents,\n        \"waiting\": waiting,\n        \"waiting_data\": waiting_data,\n        \"cache\": cache,\n        \"ready\": ready,\n        \"running\": set(),\n        \"finished\": set(),\n        \"released\": set(),\n    }\n\n    return state\n\n\n\"\"\"\nRunning tasks\n-------------\n\nWhen we execute tasks we both\n\n1.  Perform the actual work of collecting the appropriate data and calling the function\n2.  Manage administrative state to coordinate with the scheduler\n\"\"\"\n\n\ndef execute_task(key, task_info, dumps, loads, get_id, pack_exception):\n    \"\"\"\n    Compute task and handle all administration\n\n    See Also\n    --------\n    _execute_task : actually execute task\n    \"\"\"\n    try:\n        task, data = loads(task_info)\n        result = task(data)\n        id = get_id()\n        result = dumps((result, id))\n        failed = False\n    except BaseException as e:  # noqa: B036\n        result = pack_exception(e, dumps)\n        failed = True\n    return key, result, failed\n\n\ndef batch_execute_tasks(it):\n    \"\"\"\n    Batch computing of multiple tasks with `execute_task`\n    \"\"\"\n    return [execute_task(*a) for a in it]\n\n\ndef release_data(key, state, delete=True):\n    \"\"\"Remove data from temporary storage\n\n    See Also\n    --------\n    finish_task\n    \"\"\"\n    if key in state[\"waiting_data\"]:\n        assert not state[\"waiting_data\"][key]\n        del state[\"waiting_data\"][key]\n\n    state[\"released\"].add(key)\n\n    if delete:\n        del state[\"cache\"][key]\n\n\ndef finish_task(\n    dsk, key, state, results, sortkey, delete=True, release_data=release_data\n):\n    \"\"\"\n    Update execution state after a task finishes\n\n    Mutates.  This should run atomically (with a lock).\n    \"\"\"\n    for dep in sorted(state[\"dependents\"][key], key=sortkey, reverse=True):\n        s = state[\"waiting\"][dep]\n        s.remove(key)\n        if not s:\n            del state[\"waiting\"][dep]\n            state[\"ready\"].append(dep)\n\n    for dep in state[\"dependencies\"][key]:\n        if dep in state[\"waiting_data\"]:\n            s = state[\"waiting_data\"][dep]\n            s.remove(key)\n            if not s and dep not in results:\n                release_data(dep, state, delete=delete)\n        elif delete and dep not in results:\n            release_data(dep, state, delete=delete)\n\n    state[\"finished\"].add(key)\n    state[\"running\"].remove(key)\n\n    return state\n\n\ndef nested_get(ind, coll):\n    \"\"\"Get nested index from collection\n\n    Examples\n    --------\n\n    >>> nested_get(1, 'abc')\n    'b'\n    >>> nested_get([1, 0], 'abc')\n    ('b', 'a')\n    >>> nested_get([[1, 0], [0, 1]], 'abc')\n    (('b', 'a'), ('a', 'b'))\n    \"\"\"\n    if isinstance(ind, list):\n        return tuple(nested_get(i, coll) for i in ind)\n    else:\n        return coll[ind]\n\n\ndef default_get_id():\n    \"\"\"Default get_id\"\"\"\n    return None\n\n\ndef default_pack_exception(e, dumps):\n    raise\n\n\ndef reraise(exc, tb=None):\n    if exc.__traceback__ is not tb:\n        raise exc.with_traceback(tb)\n    raise exc\n\n\ndef identity(x):\n    \"\"\"Identity function. Returns x.\n\n    >>> identity(3)\n    3\n    \"\"\"\n    return x\n\n\n\"\"\"\nTask Selection\n--------------\n\nWe often have a choice among many tasks to run next.  This choice is both\ncheap and can significantly impact performance.\n\nWe currently select tasks that have recently been made ready.  We hope that\nthis first-in-first-out policy reduces memory footprint\n\"\"\"\n\n\"\"\"\n`get`\n-----\n\nThe main function of the scheduler.  Get is the main entry point.\n\"\"\"\n\n\ndef get_async(\n    submit,\n    num_workers,\n    dsk,\n    result,\n    cache=None,\n    get_id=default_get_id,\n    rerun_exceptions_locally=None,\n    pack_exception=default_pack_exception,\n    raise_exception=reraise,\n    callbacks=None,\n    dumps=identity,\n    loads=identity,\n    chunksize=None,\n    **kwargs,\n):\n    \"\"\"Asynchronous get function\n\n    This is a general version of various asynchronous schedulers for dask.  It\n    takes a ``concurrent.futures.Executor.submit`` function to form a more\n    specific ``get`` method that walks through the dask array with parallel\n    workers, avoiding repeat computation and minimizing memory use.\n\n    Parameters\n    ----------\n    submit : function\n        A ``concurrent.futures.Executor.submit`` function\n    num_workers : int\n        The number of workers that task submissions can be spread over\n    dsk : dict\n        A dask dictionary specifying a workflow\n    result : key or list of keys\n        Keys corresponding to desired data\n    cache : dict-like, optional\n        Temporary storage of results\n    get_id : callable, optional\n        Function to return the worker id, takes no arguments. Examples are\n        `threading.current_thread` and `multiprocessing.current_process`.\n    rerun_exceptions_locally : bool, optional\n        Whether to rerun failing tasks in local process to enable debugging\n        (False by default)\n    pack_exception : callable, optional\n        Function to take an exception and ``dumps`` method, and return a\n        serialized tuple of ``(exception, traceback)`` to send back to the\n        scheduler. Default is to just raise the exception.\n    raise_exception : callable, optional\n        Function that takes an exception and a traceback, and raises an error.\n    callbacks : tuple or list of tuples, optional\n        Callbacks are passed in as tuples of length 5. Multiple sets of\n        callbacks may be passed in as a list of tuples. For more information,\n        see the dask.diagnostics documentation.\n    dumps: callable, optional\n        Function to serialize task data and results to communicate between\n        worker and parent.  Defaults to identity.\n    loads: callable, optional\n        Inverse function of `dumps`.  Defaults to identity.\n    chunksize: int, optional\n        Size of chunks to use when dispatching work. Defaults to 1.\n        If -1, will be computed to evenly divide ready work across workers.\n\n    See Also\n    --------\n    threaded.get\n    \"\"\"\n    chunksize = chunksize or config.get(\"chunksize\", 1)\n\n    queue = Queue()\n\n    if isinstance(result, list):\n        result_flat = set(flatten(result))\n    else:\n        result_flat = {result}\n    results = set(result_flat)\n\n    dsk = dict(convert_legacy_graph(dsk))\n    with local_callbacks(callbacks) as callbacks:\n        _, _, pretask_cbs, posttask_cbs, _ = unpack_callbacks(callbacks)\n        started_cbs = []\n        succeeded = False\n        # if start_state_from_dask fails, we will have something\n        # to pass to the final block.\n        state = {}\n        try:\n            for cb in callbacks:\n                if cb[0]:\n                    cb[0](dsk)\n                started_cbs.append(cb)\n\n            keyorder = order(dsk)\n\n            state = start_state_from_dask(dsk, cache=cache, sortkey=keyorder.get)\n\n            for _, start_state, _, _, _ in callbacks:\n                if start_state:\n                    start_state(dsk, state)\n\n            if rerun_exceptions_locally is None:\n                rerun_exceptions_locally = config.get(\"rerun_exceptions_locally\", False)\n\n            if state[\"waiting\"] and not state[\"ready\"]:\n                raise ValueError(\"Found no accessible jobs in dask\")\n\n            def fire_tasks(chunksize):\n                \"\"\"Fire off a task to the thread pool\"\"\"\n                # Determine chunksize and/or number of tasks to submit\n                nready = len(state[\"ready\"])\n                if chunksize == -1:\n                    ntasks = nready\n                    chunksize = -(ntasks // -num_workers)\n                else:\n                    used_workers = -(len(state[\"running\"]) // -chunksize)\n                    avail_workers = max(num_workers - used_workers, 0)\n                    ntasks = min(nready, chunksize * avail_workers)\n\n                # Prep all ready tasks for submission\n                args = []\n                for _ in range(ntasks):\n                    # Get the next task to compute (most recently added)\n                    key = state[\"ready\"].pop()\n                    # Notify task is running\n                    state[\"running\"].add(key)\n                    for f in pretask_cbs:\n                        f(key, dsk, state)\n\n                    # Prep args to send\n                    data = {\n                        dep: state[\"cache\"][dep] for dep in get_dependencies(dsk, key)\n                    }\n                    args.append(\n                        (\n                            key,\n                            dumps((dsk[key], data)),\n                            dumps,\n                            loads,\n                            get_id,\n                            pack_exception,\n                        )\n                    )\n\n                # Batch submit\n                for i in range(-(len(args) // -chunksize)):\n                    each_args = args[i * chunksize : (i + 1) * chunksize]\n                    if not each_args:\n                        break\n                    fut = submit(batch_execute_tasks, each_args)\n                    fut.add_done_callback(queue.put)\n\n            # Main loop, wait on tasks to finish, insert new ones\n            while state[\"waiting\"] or state[\"ready\"] or state[\"running\"]:\n                fire_tasks(chunksize)\n                for key, res_info, failed in queue_get(queue).result():\n                    if failed:\n                        exc, tb = loads(res_info)\n                        if rerun_exceptions_locally:\n                            data = {\n                                dep: state[\"cache\"][dep]\n                                for dep in get_dependencies(dsk, key)\n                            }\n                            task = dsk[key]\n                            task(data)  # Re-execute locally\n                        else:\n                            raise_exception(exc, tb)\n                    res, worker_id = loads(res_info)\n                    state[\"cache\"][key] = res\n                    finish_task(dsk, key, state, results, keyorder.get)\n                    for f in posttask_cbs:\n                        f(key, res, dsk, state, worker_id)\n\n            succeeded = True\n\n        finally:\n            for _, _, _, _, finish in started_cbs:\n                if finish:\n                    finish(dsk, state, not succeeded)\n\n    return nested_get(result, state[\"cache\"])\n\n\n\"\"\" Synchronous concrete version of get_async\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a\nsequential one.  This is useful for debugging and for code dominated by the\nGIL\n\"\"\"\n\n\nclass SynchronousExecutor(Executor):\n    _max_workers = 1\n\n    def submit(self, fn, *args, **kwargs):\n        fut = Future()\n        try:\n            fut.set_result(fn(*args, **kwargs))\n        except BaseException as e:  # noqa: B036\n            fut.set_exception(e)\n        return fut\n\n\nsynchronous_executor = SynchronousExecutor()\n\n\ndef get_sync(dsk: Mapping, keys: Sequence[Key] | Key, **kwargs):\n    \"\"\"A naive synchronous version of get_async\n\n    Can be useful for debugging.\n    \"\"\"\n    kwargs.pop(\"num_workers\", None)  # if num_workers present, remove it\n    return get_async(\n        synchronous_executor.submit,\n        synchronous_executor._max_workers,\n        dsk,\n        keys,\n        **kwargs,\n    )\n\n\n\"\"\" Adaptor for ``multiprocessing.Pool`` instances\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a wrapper\nclass for ``multiprocessing.Pool`` instances so we can treat them like\n``concurrent.futures.Executor`` instances instead.\n\nThis is mainly useful for legacy use cases or users that prefer\n``multiprocessing.Pool``.\n\"\"\"\n\n\nclass MultiprocessingPoolExecutor(Executor):\n    def __init__(self, pool):\n        self.pool = pool\n        self._max_workers = len(pool._pool)\n\n    def submit(self, fn, *args, **kwargs):\n        return submit_apply_async(self.pool.apply_async, fn, *args, **kwargs)\n\n\ndef submit_apply_async(apply_async, fn, *args, **kwargs):\n    fut = Future()\n    apply_async(fn, args, kwargs, fut.set_result, fut.set_exception)\n    return fut\n\n\ndef get_apply_async(apply_async, num_workers, *args, **kwargs):\n    return get_async(\n        partial(submit_apply_async, apply_async), num_workers, *args, **kwargs\n    )\n\n\ndef sortkey(item):\n    \"\"\"Sorting key function that is robust to different types\n\n    Both strings and tuples are common key types in dask graphs.\n    However In Python 3 one can not compare strings with tuples directly.\n    This function maps many types to a form where they can be compared\n\n    Examples\n    --------\n    >>> sortkey('Hello')\n    ('str', 'Hello')\n\n    >>> sortkey(('x', 1))\n    ('tuple', ('x', 1))\n    \"\"\"\n    return (type(item).__name__, item)\n\n\n### Dependency File: optimization.py\nfrom __future__ import annotations\n\nimport math\nimport numbers\nfrom enum import Enum\n\nfrom dask import config, utils\nfrom dask._task_spec import GraphNode\nfrom dask.core import (\n    flatten,\n    get_dependencies,\n    ishashable,\n    istask,\n    reverse_dict,\n    subs,\n    toposort,\n)\n\n\ndef cull(dsk, keys):\n    \"\"\"Return new dask with only the tasks required to calculate keys.\n\n    In other words, remove unnecessary tasks from dask.\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> def add(x, y):\n    ...     return x + y\n\n    >>> d = {'x': 1, 'y': (inc, 'x'), 'out': (add, 'x', 10)}\n    >>> dsk, dependencies = cull(d, 'out')\n    >>> dsk                                                     # doctest: +ELLIPSIS\n    {'out': (<function add at ...>, 'x', 10), 'x': 1}\n    >>> dependencies                                            # doctest: +ELLIPSIS\n    {'out': ['x'], 'x': []}\n\n    Returns\n    -------\n    dsk: culled dask graph\n    dependencies: Dict mapping {key: [deps]}.  Useful side effect to accelerate\n        other optimizations, notably fuse.\n    \"\"\"\n    if not isinstance(keys, (list, set)):\n        keys = [keys]\n\n    seen = set()\n    dependencies = dict()\n    out = {}\n    work = list(set(flatten(keys)))\n\n    while work:\n        new_work = []\n        for k in work:\n            dependencies_k = get_dependencies(dsk, k, as_list=True)  # fuse needs lists\n            out[k] = dsk[k]\n            dependencies[k] = dependencies_k\n            for d in dependencies_k:\n                if d not in seen:\n                    seen.add(d)\n                    new_work.append(d)\n\n        work = new_work\n\n    return out, dependencies\n\n\ndef default_fused_linear_keys_renamer(keys):\n    \"\"\"Create new keys for fused tasks\"\"\"\n    typ = type(keys[0])\n    if typ is str:\n        names = [utils.key_split(x) for x in keys[:0:-1]]\n        names.append(keys[0])\n        return \"-\".join(names)\n    elif typ is tuple and len(keys[0]) > 0 and isinstance(keys[0][0], str):\n        names = [utils.key_split(x) for x in keys[:0:-1]]\n        names.append(keys[0][0])\n        return (\"-\".join(names),) + keys[0][1:]\n    else:\n        return None\n\n\ndef fuse_linear(dsk, keys=None, dependencies=None, rename_keys=True):\n    \"\"\"Return new dask graph with linear sequence of tasks fused together.\n\n    If specified, the keys in ``keys`` keyword argument are *not* fused.\n    Supply ``dependencies`` from output of ``cull`` if available to avoid\n    recomputing dependencies.\n\n    **This function is mostly superseded by ``fuse``**\n\n    Parameters\n    ----------\n    dsk: dict\n    keys: list\n    dependencies: dict, optional\n        {key: [list-of-keys]}.  Must be a list to provide count of each key\n        This optional input often comes from ``cull``\n    rename_keys: bool or func, optional\n        Whether to rename fused keys with ``default_fused_linear_keys_renamer``\n        or not.  Renaming fused keys can keep the graph more understandable\n        and comprehensive, but it comes at the cost of additional processing.\n        If False, then the top-most key will be used.  For advanced usage, a\n        func is also accepted, ``new_key = rename_keys(fused_key_list)``.\n\n    Examples\n    --------\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> def add(x, y):\n    ...     return x + y\n\n    >>> d = {'a': 1, 'b': (inc, 'a'), 'c': (inc, 'b')}\n    >>> dsk, dependencies = fuse(d)\n    >>> dsk # doctest: +SKIP\n    {'a-b-c': (inc, (inc, 1)), 'c': 'a-b-c'}\n    >>> dsk, dependencies = fuse(d, rename_keys=False)\n    >>> dsk # doctest: +ELLIPSIS\n    {'c': (<function inc at ...>, (<function inc at ...>, 1))}\n    >>> dsk, dependencies = fuse(d, keys=['b'], rename_keys=False)\n    >>> dsk  # doctest: +ELLIPSIS\n    {'b': (<function inc at ...>, 1), 'c': (<function inc at ...>, 'b')}\n\n    Returns\n    -------\n    dsk: output graph with keys fused\n    dependencies: dict mapping dependencies after fusion.  Useful side effect\n        to accelerate other downstream optimizations.\n    \"\"\"\n    if keys is not None and not isinstance(keys, set):\n        if not isinstance(keys, list):\n            keys = [keys]\n        keys = set(flatten(keys))\n\n    if dependencies is None:\n        dependencies = {k: get_dependencies(dsk, k, as_list=True) for k in dsk}\n\n    # locate all members of linear chains\n    child2parent = {}\n    unfusible = set()\n    for parent in dsk:\n        deps = dependencies[parent]\n        has_many_children = len(deps) > 1\n        for child in deps:\n            if keys is not None and child in keys:\n                unfusible.add(child)\n            elif child in child2parent:\n                del child2parent[child]\n                unfusible.add(child)\n            elif has_many_children:\n                unfusible.add(child)\n            elif child not in unfusible:\n                child2parent[child] = parent\n\n    # construct the chains from ancestor to descendant\n    chains = []\n    parent2child = dict(map(reversed, child2parent.items()))\n    while child2parent:\n        child, parent = child2parent.popitem()\n        chain = [child, parent]\n        while parent in child2parent:\n            parent = child2parent.pop(parent)\n            del parent2child[parent]\n            chain.append(parent)\n        chain.reverse()\n        while child in parent2child:\n            child = parent2child.pop(child)\n            del child2parent[child]\n            chain.append(child)\n        chains.append(chain)\n\n    dependencies = {k: set(v) for k, v in dependencies.items()}\n\n    if rename_keys is True:\n        key_renamer = default_fused_linear_keys_renamer\n    elif rename_keys is False:\n        key_renamer = None\n    else:\n        key_renamer = rename_keys\n\n    # create a new dask with fused chains\n    rv = {}\n    fused = set()\n    aliases = set()\n    is_renamed = False\n    for chain in chains:\n        if key_renamer is not None:\n            new_key = key_renamer(chain)\n            is_renamed = (\n                new_key is not None and new_key not in dsk and new_key not in rv\n            )\n        child = chain.pop()\n        val = dsk[child]\n        while chain:\n            parent = chain.pop()\n            dependencies[parent].update(dependencies.pop(child))\n            dependencies[parent].remove(child)\n            val = subs(dsk[parent], child, val)\n            fused.add(child)\n            child = parent\n        fused.add(child)\n        if is_renamed:\n            rv[new_key] = val\n            rv[child] = new_key\n            dependencies[new_key] = dependencies[child]\n            dependencies[child] = {new_key}\n            aliases.add(child)\n        else:\n            rv[child] = val\n    for key, val in dsk.items():\n        if key not in fused:\n            rv[key] = val\n    if aliases:\n        for key, deps in dependencies.items():\n            for old_key in deps & aliases:\n                new_key = rv[old_key]\n                deps.remove(old_key)\n                deps.add(new_key)\n                rv[key] = subs(rv[key], old_key, new_key)\n        if keys is not None:\n            for key in aliases - keys:\n                del rv[key]\n                del dependencies[key]\n    return rv, dependencies\n\n\ndef _flat_set(x):\n    if x is None:\n        return set()\n    elif isinstance(x, set):\n        return x\n    elif not isinstance(x, (list, set)):\n        x = [x]\n    return set(x)\n\n\ndef inline(dsk, keys=None, inline_constants=True, dependencies=None):\n    \"\"\"Return new dask with the given keys inlined with their values.\n\n    Inlines all constants if ``inline_constants`` keyword is True. Note that\n    the constant keys will remain in the graph, to remove them follow\n    ``inline`` with ``cull``.\n\n    Examples\n    --------\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> def add(x, y):\n    ...     return x + y\n\n    >>> d = {'x': 1, 'y': (inc, 'x'), 'z': (add, 'x', 'y')}\n    >>> inline(d)       # doctest: +ELLIPSIS\n    {'x': 1, 'y': (<function inc at ...>, 1), 'z': (<function add at ...>, 1, 'y')}\n\n    >>> inline(d, keys='y') # doctest: +ELLIPSIS\n    {'x': 1, 'y': (<function inc at ...>, 1), 'z': (<function add at ...>, 1, (<function inc at ...>, 1))}\n\n    >>> inline(d, keys='y', inline_constants=False) # doctest: +ELLIPSIS\n    {'x': 1, 'y': (<function inc at ...>, 'x'), 'z': (<function add at ...>, 'x', (<function inc at ...>, 'x'))}\n    \"\"\"\n    if dependencies and isinstance(next(iter(dependencies.values())), list):\n        dependencies = {k: set(v) for k, v in dependencies.items()}\n\n    keys = _flat_set(keys)\n\n    if dependencies is None:\n        dependencies = {k: get_dependencies(dsk, k) for k in dsk}\n\n    if inline_constants:\n        keys.update(\n            k\n            for k, v in dsk.items()\n            if (ishashable(v) and v in dsk) or (not dependencies[k] and not istask(v))\n        )\n\n    # Keys may depend on other keys, so determine replace order with toposort.\n    # The values stored in `keysubs` do not include other keys.\n    replaceorder = toposort(\n        {k: dsk[k] for k in keys if k in dsk}, dependencies=dependencies\n    )\n    keysubs = {}\n    for key in replaceorder:\n        val = dsk[key]\n        for dep in keys & dependencies[key]:\n            if dep in keysubs:\n                replace = keysubs[dep]\n            else:\n                replace = dsk[dep]\n            val = subs(val, dep, replace)\n        keysubs[key] = val\n\n    # Make new dask with substitutions\n    dsk2 = keysubs.copy()\n    for key, val in dsk.items():\n        if key not in dsk2:\n            for item in keys & dependencies[key]:\n                val = subs(val, item, keysubs[item])\n            dsk2[key] = val\n    return dsk2\n\n\ndef inline_functions(\n    dsk, output, fast_functions=None, inline_constants=False, dependencies=None\n):\n    \"\"\"Inline cheap functions into larger operations\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> double = lambda x: x * 2\n    >>> dsk = {'out': (add, 'i', 'd'),  # doctest: +SKIP\n    ...        'i': (inc, 'x'),\n    ...        'd': (double, 'y'),\n    ...        'x': 1, 'y': 1}\n    >>> inline_functions(dsk, [], [inc])  # doctest: +SKIP\n    {'out': (add, (inc, 'x'), 'd'),\n     'd': (double, 'y'),\n     'x': 1, 'y': 1}\n\n    Protect output keys.  In the example below ``i`` is not inlined because it\n    is marked as an output key.\n\n    >>> inline_functions(dsk, ['i', 'out'], [inc, double])  # doctest: +SKIP\n    {'out': (add, 'i', (double, 'y')),\n     'i': (inc, 'x'),\n     'x': 1, 'y': 1}\n    \"\"\"\n    if not fast_functions:\n        return dsk\n\n    output = set(output)\n\n    fast_functions = set(fast_functions)\n\n    if dependencies is None:\n        dependencies = {k: get_dependencies(dsk, k) for k in dsk}\n    dependents = reverse_dict(dependencies)\n\n    def inlinable(key, task):\n        if (\n            not isinstance(task, GraphNode)\n            and istask(task)\n            and key not in output\n            and dependents[key]\n        ):\n            try:\n                if functions_of(task).issubset(fast_functions) and not any(\n                    isinstance(dsk[d], GraphNode) for d in dependents[key]\n                ):\n                    return True\n            except TypeError:\n                pass\n        return False\n\n    keys = [k for k, v in dsk.items() if inlinable(k, v)]\n\n    if keys:\n        dsk = inline(\n            dsk, keys, inline_constants=inline_constants, dependencies=dependencies\n        )\n        for k in keys:\n            del dsk[k]\n    return dsk\n\n\ndef unwrap_partial(func):\n    while hasattr(func, \"func\"):\n        func = func.func\n    return func\n\n\ndef functions_of(task):\n    \"\"\"Set of functions contained within nested task\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> mul = lambda x, y: x * y\n    >>> task = (add, (mul, 1, 2), (inc, 3))  # doctest: +SKIP\n    >>> functions_of(task)  # doctest: +SKIP\n    set([add, mul, inc])\n    \"\"\"\n    funcs = set()\n\n    work = [task]\n    sequence_types = {list, tuple}\n\n    while work:\n        new_work = []\n        for task in work:\n            if type(task) in sequence_types:\n                if istask(task):\n                    funcs.add(unwrap_partial(task[0]))\n                    new_work.extend(task[1:])\n                else:\n                    new_work.extend(task)\n        work = new_work\n\n    return funcs\n\n\ndef default_fused_keys_renamer(keys, max_fused_key_length=120):\n    \"\"\"Create new keys for ``fuse`` tasks.\n\n    The optional parameter `max_fused_key_length` is used to limit the maximum string length for each renamed key.\n    If this parameter is set to `None`, there is no limit.\n    \"\"\"\n    it = reversed(keys)\n    first_key = next(it)\n    typ = type(first_key)\n\n    if max_fused_key_length:  # Take into account size of hash suffix\n        max_fused_key_length -= 5\n\n    def _enforce_max_key_limit(key_name):\n        if max_fused_key_length and len(key_name) > max_fused_key_length:\n            name_hash = f\"{hash(key_name):x}\"[:4]\n            key_name = f\"{key_name[:max_fused_key_length]}-{name_hash}\"\n        return key_name\n\n    if typ is str:\n        first_name = utils.key_split(first_key)\n        names = {utils.key_split(k) for k in it}\n        names.discard(first_name)\n        names = sorted(names)\n        names.append(first_key)\n        concatenated_name = \"-\".join(names)\n        return _enforce_max_key_limit(concatenated_name)\n    elif typ is tuple and len(first_key) > 0 and isinstance(first_key[0], str):\n        first_name = utils.key_split(first_key)\n        names = {utils.key_split(k) for k in it}\n        names.discard(first_name)\n        names = sorted(names)\n        names.append(first_key[0])\n        concatenated_name = \"-\".join(names)\n        return (_enforce_max_key_limit(concatenated_name),) + first_key[1:]\n\n\n# PEP-484 compliant singleton constant\n# https://www.python.org/dev/peps/pep-0484/#support-for-singleton-types-in-unions\nclass Default(Enum):\n    token = 0\n\n    def __repr__(self) -> str:\n        return \"<default>\"\n\n\n_default = Default.token\n\n\ndef fuse(\n    dsk,\n    keys=None,\n    dependencies=None,\n    ave_width=_default,\n    max_width=_default,\n    max_height=_default,\n    max_depth_new_edges=_default,\n    rename_keys=_default,\n):\n    \"\"\"Fuse tasks that form reductions; more advanced than ``fuse_linear``\n\n    This trades parallelism opportunities for faster scheduling by making tasks\n    less granular.  It can replace ``fuse_linear`` in optimization passes.\n\n    This optimization applies to all reductions--tasks that have at most one\n    dependent--so it may be viewed as fusing \"multiple input, single output\"\n    groups of tasks into a single task.  There are many parameters to fine\n    tune the behavior, which are described below.  ``ave_width`` is the\n    natural parameter with which to compare parallelism to granularity, so\n    it should always be specified.  Reasonable values for other parameters\n    will be determined using ``ave_width`` if necessary.\n\n    Parameters\n    ----------\n    dsk: dict\n        dask graph\n    keys: list or set, optional\n        Keys that must remain in the returned dask graph\n    dependencies: dict, optional\n        {key: [list-of-keys]}.  Must be a list to provide count of each key\n        This optional input often comes from ``cull``\n    ave_width: float (default 1)\n        Upper limit for ``width = num_nodes / height``, a good measure of\n        parallelizability.\n        dask.config key: ``optimization.fuse.ave-width``\n    max_width: int (default infinite)\n        Don't fuse if total width is greater than this.\n        dask.config key: ``optimization.fuse.max-width``\n    max_height: int or None (default None)\n        Don't fuse more than this many levels. Set to None to dynamically\n        adjust to ``1.5 + ave_width * log(ave_width + 1)``.\n        dask.config key: ``optimization.fuse.max-height``\n    max_depth_new_edges: int or None (default None)\n        Don't fuse if new dependencies are added after this many levels.\n        Set to None to dynamically adjust to ave_width * 1.5.\n        dask.config key: ``optimization.fuse.max-depth-new-edges``\n    rename_keys: bool or func, optional (default True)\n        Whether to rename the fused keys with ``default_fused_keys_renamer``\n        or not.  Renaming fused keys can keep the graph more understandable\n        and comprehensive, but it comes at the cost of additional processing.\n        If False, then the top-most key will be used.  For advanced usage, a\n        function to create the new name is also accepted.\n        dask.config key: ``optimization.fuse.rename-keys``\n\n    Returns\n    -------\n    dsk\n        output graph with keys fused\n    dependencies\n        dict mapping dependencies after fusion.  Useful side effect to accelerate other\n        downstream optimizations.\n    \"\"\"\n\n    # Perform low-level fusion unless the user has\n    # specified False explicitly.\n    if config.get(\"optimization.fuse.active\") is False:\n        return dsk, dependencies\n\n    if keys is not None and not isinstance(keys, set):\n        if not isinstance(keys, list):\n            keys = [keys]\n        keys = set(flatten(keys))\n\n    # Read defaults from dask.yaml and/or user-defined config file\n    if ave_width is _default:\n        ave_width = config.get(\"optimization.fuse.ave-width\")\n        assert ave_width is not _default\n    if max_height is _default:\n        max_height = config.get(\"optimization.fuse.max-height\")\n        assert max_height is not _default\n    if max_depth_new_edges is _default:\n        max_depth_new_edges = config.get(\"optimization.fuse.max-depth-new-edges\")\n        assert max_depth_new_edges is not _default\n    if max_depth_new_edges is None:\n        max_depth_new_edges = ave_width * 1.5\n    if max_width is _default:\n        max_width = config.get(\"optimization.fuse.max-width\")\n        assert max_width is not _default\n    if max_width is None:\n        max_width = 1.5 + ave_width * math.log(ave_width + 1)\n\n    if not ave_width or not max_height:\n        return dsk, dependencies\n\n    if rename_keys is _default:\n        rename_keys = config.get(\"optimization.fuse.rename-keys\")\n        assert rename_keys is not _default\n    if rename_keys is True:\n        key_renamer = default_fused_keys_renamer\n    elif rename_keys is False:\n        key_renamer = None\n    elif not callable(rename_keys):\n        raise TypeError(\"rename_keys must be a boolean or callable\")\n    else:\n        key_renamer = rename_keys\n    rename_keys = key_renamer is not None\n\n    if dependencies is None:\n        deps = {k: get_dependencies(dsk, k, as_list=True) for k in dsk}\n    else:\n        deps = {\n            k: v if isinstance(v, list) else get_dependencies(dsk, k, as_list=True)\n            for k, v in dependencies.items()\n        }\n\n    rdeps = {}\n    for k, vals in deps.items():\n        for v in vals:\n            if v not in rdeps:\n                rdeps[v] = [k]\n            else:\n                rdeps[v].append(k)\n        deps[k] = set(vals)\n\n    reducible = set()\n    for k, vals in rdeps.items():\n        if (\n            len(vals) == 1\n            and k not in (keys or ())\n            and k in dsk\n            and not isinstance(dsk[k], GraphNode)\n            and (type(dsk[k]) is tuple or isinstance(dsk[k], (numbers.Number, str)))\n            and not any(isinstance(dsk[v], GraphNode) for v in vals)\n        ):\n            reducible.add(k)\n\n    if not reducible and (all(len(set(v)) != 1 for v in rdeps.values())):\n        # Quick return if there's nothing to do. Only progress if there's tasks\n        # fusible by the main `fuse`\n        return dsk, deps\n\n    rv = dsk.copy()\n    fused_trees = {}\n    # These are the stacks we use to store data as we traverse the graph\n    info_stack = []\n    children_stack = []\n    # For speed\n    deps_pop = deps.pop\n    reducible_add = reducible.add\n    reducible_pop = reducible.pop\n    reducible_remove = reducible.remove\n    fused_trees_pop = fused_trees.pop\n    info_stack_append = info_stack.append\n    info_stack_pop = info_stack.pop\n    children_stack_append = children_stack.append\n    children_stack_extend = children_stack.extend\n    children_stack_pop = children_stack.pop\n    while reducible:\n        parent = reducible_pop()\n        reducible_add(parent)\n        while parent in reducible:\n            # Go to the top\n            parent = rdeps[parent][0]\n        children_stack_append(parent)\n        children_stack_extend(reducible & deps[parent])\n        while True:\n            child = children_stack[-1]\n            if child != parent:\n                children = reducible & deps[child]\n                while children:\n                    # Depth-first search\n                    children_stack_extend(children)\n                    parent = child\n                    child = children_stack[-1]\n                    children = reducible & deps[child]\n                children_stack_pop()\n                # This is a leaf node in the reduction region\n                # key, task, fused_keys, height, width, number of nodes, fudge, set of edges\n                info_stack_append(\n                    (\n                        child,\n                        rv[child],\n                        [child] if rename_keys else None,\n                        1,\n                        1,\n                        1,\n                        0,\n                        deps[child] - reducible,\n                    )\n                )\n            else:\n                children_stack_pop()\n                # Calculate metrics and fuse as appropriate\n                deps_parent = deps[parent]\n                edges = deps_parent - reducible\n                children = deps_parent - edges\n                num_children = len(children)\n\n                if num_children == 1:\n                    (\n                        child_key,\n                        child_task,\n                        child_keys,\n                        height,\n                        width,\n                        num_nodes,\n                        fudge,\n                        children_edges,\n                    ) = info_stack_pop()\n                    num_children_edges = len(children_edges)\n\n                    if fudge > num_children_edges - 1 >= 0:\n                        fudge = num_children_edges - 1\n                    edges |= children_edges\n                    no_new_edges = len(edges) == num_children_edges\n                    if not no_new_edges:\n                        fudge += 1\n                    if (\n                        (num_nodes + fudge) / height <= ave_width\n                        and\n                        # Sanity check; don't go too deep if new levels introduce new edge dependencies\n                        (no_new_edges or height < max_depth_new_edges)\n                        and (\n                            not isinstance(dsk[parent], GraphNode)\n                            # TODO: substitute can be implemented with GraphNode.inline\n                            # or isinstance(dsk[child_key], GraphNode)\n                        )\n                    ):\n                        # Perform substitutions as we go\n                        val = subs(dsk[parent], child_key, child_task)\n                        deps_parent.remove(child_key)\n                        deps_parent |= deps_pop(child_key)\n                        del rv[child_key]\n                        reducible_remove(child_key)\n                        if rename_keys:\n                            child_keys.append(parent)\n                            fused_trees[parent] = child_keys\n                            fused_trees_pop(child_key, None)\n\n                        if children_stack:\n                            if no_new_edges:\n                                # Linear fuse\n                                info_stack_append(\n                                    (\n                                        parent,\n                                        val,\n                                        child_keys,\n                                        height,\n                                        width,\n                                        num_nodes,\n                                        fudge,\n                                        edges,\n                                    )\n                                )\n                            else:\n                                info_stack_append(\n                                    (\n                                        parent,\n                                        val,\n                                        child_keys,\n                                        height + 1,\n                                        width,\n                                        num_nodes + 1,\n                                        fudge,\n                                        edges,\n                                    )\n                                )\n                        else:\n                            rv[parent] = val\n                            break\n                    else:\n                        rv[child_key] = child_task\n                        reducible_remove(child_key)\n                        if children_stack:\n                            # Allow the parent to be fused, but only under strict circumstances.\n                            # Ensure that linear chains may still be fused.\n                            if fudge > int(ave_width - 1):\n                                fudge = int(ave_width - 1)\n                            # This task *implicitly* depends on `edges`\n                            info_stack_append(\n                                (\n                                    parent,\n                                    rv[parent],\n                                    [parent] if rename_keys else None,\n                                    1,\n                                    width,\n                                    1,\n                                    fudge,\n                                    edges,\n                                )\n                            )\n                        else:\n                            break\n                else:\n                    child_keys = []\n                    height = 1\n                    width = 0\n                    num_single_nodes = 0\n                    num_nodes = 0\n                    fudge = 0\n                    children_edges = set()\n                    max_num_edges = 0\n                    children_info = info_stack[-num_children:]\n                    del info_stack[-num_children:]\n                    for (\n                        _,\n                        _,\n                        _,\n                        cur_height,\n                        cur_width,\n                        cur_num_nodes,\n                        cur_fudge,\n                        cur_edges,\n                    ) in children_info:\n                        if cur_height == 1:\n                            num_single_nodes += 1\n                        elif cur_height > height:\n                            height = cur_height\n                        width += cur_width\n                        num_nodes += cur_num_nodes\n                        fudge += cur_fudge\n                        if len(cur_edges) > max_num_edges:\n                            max_num_edges = len(cur_edges)\n                        children_edges |= cur_edges\n                    # Fudge factor to account for possible parallelism with the boundaries\n                    num_children_edges = len(children_edges)\n                    fudge += min(\n                        num_children - 1, max(0, num_children_edges - max_num_edges)\n                    )\n\n                    if fudge > num_children_edges - 1 >= 0:\n                        fudge = num_children_edges - 1\n                    edges |= children_edges\n                    no_new_edges = len(edges) == num_children_edges\n                    if not no_new_edges:\n                        fudge += 1\n                    if (\n                        (num_nodes + fudge) / height <= ave_width\n                        and num_single_nodes <= ave_width\n                        and width <= max_width\n                        and height <= max_height\n                        and\n                        # Sanity check; don't go too deep if new levels introduce new edge dependencies\n                        (no_new_edges or height < max_depth_new_edges)\n                        and (\n                            not isinstance(dsk[parent], GraphNode)\n                            and not any(\n                                isinstance(dsk[child_key], GraphNode)\n                                for child_key in children\n                            )\n                            # TODO: substitute can be implemented with GraphNode.inline\n                            # or all(\n                            #     isintance(dsk[child], GraphNode) for child in children\n                        )\n                    ):\n                        # Perform substitutions as we go\n                        val = dsk[parent]\n                        children_deps = set()\n                        for child_info in children_info:\n                            cur_child = child_info[0]\n                            val = subs(val, cur_child, child_info[1])\n                            del rv[cur_child]\n                            children_deps |= deps_pop(cur_child)\n                            reducible_remove(cur_child)\n                            if rename_keys:\n                                fused_trees_pop(cur_child, None)\n                                child_keys.extend(child_info[2])\n                        deps_parent -= children\n                        deps_parent |= children_deps\n\n                        if rename_keys:\n                            child_keys.append(parent)\n                            fused_trees[parent] = child_keys\n\n                        if children_stack:\n                            info_stack_append(\n                                (\n                                    parent,\n                                    val,\n                                    child_keys,\n                                    height + 1,\n                                    width,\n                                    num_nodes + 1,\n                                    fudge,\n                                    edges,\n                                )\n                            )\n                        else:\n                            rv[parent] = val\n                            break\n                    else:\n                        for child_info in children_info:\n                            rv[child_info[0]] = child_info[1]\n                            reducible_remove(child_info[0])\n                        if children_stack:\n                            # Allow the parent to be fused, but only under strict circumstances.\n                            # Ensure that linear chains may still be fused.\n                            if width > max_width:\n                                width = max_width\n                            if fudge > int(ave_width - 1):\n                                fudge = int(ave_width - 1)\n                            # key, task, height, width, number of nodes, fudge, set of edges\n                            # This task *implicitly* depends on `edges`\n                            info_stack_append(\n                                (\n                                    parent,\n                                    rv[parent],\n                                    [parent] if rename_keys else None,\n                                    1,\n                                    width,\n                                    1,\n                                    fudge,\n                                    edges,\n                                )\n                            )\n                        else:\n                            break\n                # Traverse upwards\n                parent = rdeps[parent][0]\n\n    if key_renamer:\n        for root_key, fused_keys in fused_trees.items():\n            alias = key_renamer(fused_keys)\n            if alias is not None and alias not in rv:\n                rv[alias] = rv[root_key]\n                rv[root_key] = alias\n                deps[alias] = deps[root_key]\n                deps[root_key] = {alias}\n\n    return rv, deps\n\n\n### Dependency File: system.py\nfrom __future__ import annotations\n\nimport math\nimport os\nimport sys\n\ntry:\n    import psutil\nexcept ImportError:\n    psutil = None  # type: ignore\n\n__all__ = (\"cpu_count\", \"CPU_COUNT\")\n\n\ndef _try_extract_cgroup_cpu_quota():\n    # cgroup v1\n    # The directory name isn't standardized across linux distros, check both\n    for dirname in [\"cpuacct,cpu\", \"cpu,cpuacct\"]:\n        try:\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_quota_us\" % dirname) as f:\n                quota = int(f.read())\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_period_us\" % dirname) as f:\n                period = int(f.read())\n            return quota, period\n        except Exception:\n            pass\n\n    # cgroup v2\n    try:\n        with open(\"/proc/self/cgroup\") as f:\n            group_path = f.read().strip().split(\":\")[-1]\n        if not group_path.endswith(\"/\"):\n            group_path = f\"{group_path}/\"\n        with open(\"/sys/fs/cgroup%scpu.max\" % group_path) as f:\n            quota, period = map(int, f.read().split(\" \"))\n            return quota, period\n    except Exception:\n        pass\n\n    # No cgroup CPU quota found\n    return None, None\n\n\ndef cpu_count():\n    \"\"\"Get the available CPU count for this system.\n\n    Takes the minimum value from the following locations:\n\n    - Total system cpus available on the host.\n    - CPU Affinity (if set)\n    - Cgroups limit (if set)\n    \"\"\"\n    count = os.cpu_count()\n\n    # Check CPU affinity if available\n    if psutil is not None:\n        try:\n            affinity_count = len(psutil.Process().cpu_affinity())\n            if affinity_count > 0:\n                count = min(count, affinity_count)\n        except Exception:\n            pass\n\n    # Check cgroups if available\n    if sys.platform == \"linux\":\n        quota, period = _try_extract_cgroup_cpu_quota()\n        if quota is not None and period is not None:\n            # We round up on fractional CPUs\n            cgroups_count = math.ceil(quota / period)\n            if cgroups_count > 0:\n                count = min(count, cgroups_count)\n\n    return count\n\n\nCPU_COUNT = cpu_count()\n\n\n### Dependency File: typing.py\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Callable, Hashable, Mapping, Sequence\nfrom enum import Enum\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Literal,\n    Protocol,\n    TypeVar,\n    Union,\n    runtime_checkable,\n)\n\nif TYPE_CHECKING:\n    # IPython import is relatively slow. Avoid if not necessary\n    # TODO import from typing (requires Python >=3.10)\n    from typing import TypeAlias\n\n    from IPython.display import DisplayObject\n\nCollType = TypeVar(\"CollType\", bound=\"DaskCollection\")\nCollType_co = TypeVar(\"CollType_co\", bound=\"DaskCollection\", covariant=True)\nPostComputeCallable = Callable\n\n\nKey: TypeAlias = Union[str, bytes, int, float, tuple[\"Key\", ...]]\n# FIXME: This type is a little misleading. Low level graphs are often\n# MutableMappings but HLGs are not\nGraph: TypeAlias = Mapping[Key, Any]\n# Potentially nested list of Dask keys\nNestedKeys: TypeAlias = list[Union[Key, \"NestedKeys\"]]\n\n\nclass SchedulerGetCallable(Protocol):\n    \"\"\"Protocol defining the signature of a ``__dask_scheduler__`` callable.\"\"\"\n\n    def __call__(\n        self,\n        dsk: Graph,\n        keys: Sequence[Key] | Key,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Method called as the default scheduler for a collection.\n\n        Parameters\n        ----------\n        dsk :\n            The task graph.\n        keys :\n            Key(s) corresponding to the desired data.\n        **kwargs :\n            Additional arguments.\n\n        Returns\n        -------\n        Any\n            Result(s) associated with `keys`\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\nclass PostPersistCallable(Protocol[CollType_co]):\n    \"\"\"Protocol defining the signature of a ``__dask_postpersist__`` callable.\"\"\"\n\n    def __call__(\n        self,\n        dsk: Graph,\n        *args: Any,\n        rename: Mapping[str, str] | None = None,\n    ) -> CollType_co:\n        \"\"\"Method called to rebuild a persisted collection.\n\n        Parameters\n        ----------\n        dsk: Mapping\n            A mapping which contains at least the output keys returned\n            by __dask_keys__().\n        *args : Any\n            Additional optional arguments If no extra arguments are\n            necessary, it must be an empty tuple.\n        rename : Mapping[str, str], optional\n            If defined, it indicates that output keys may be changing\n            too; e.g. if the previous output of :meth:`__dask_keys__`\n            was ``[(\"a\", 0), (\"a\", 1)]``, after calling\n            ``rebuild(dsk, *extra_args, rename={\"a\": \"b\"})``\n            it must become ``[(\"b\", 0), (\"b\", 1)]``.\n            The ``rename`` mapping may not contain the collection\n            name(s); in such case the associated keys do not change.\n            It may contain replacements for unexpected names, which\n            must be ignored.\n\n        Returns\n        -------\n        Collection\n            An equivalent Dask collection with the same keys as\n            computed through a different graph.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\n@runtime_checkable\nclass DaskCollection(Protocol):\n    \"\"\"Protocol defining the interface of a Dask collection.\"\"\"\n\n    @abc.abstractmethod\n    def __dask_graph__(self) -> Graph:\n        \"\"\"The Dask task graph.\n\n        The core Dask collections (Array, DataFrame, Bag, and Delayed)\n        use a :py:class:`~dask.highlevelgraph.HighLevelGraph` to\n        represent the collection task graph. It is also possible to\n        represent the task graph as a low level graph using a Python\n        dictionary.\n\n        Returns\n        -------\n        Mapping\n            The Dask task graph. If the instance returns a\n            :py:class:`dask.highlevelgraph.HighLevelGraph` then the\n            :py:func:`__dask_layers__` method must be implemented, as\n            defined by the :py:class:`~dask.typing.HLGDaskCollection`\n            protocol.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_keys__(self) -> NestedKeys:\n        \"\"\"The output keys of the task graph.\n\n        Note that there are additional constraints on keys for a Dask\n        collection than those described in the :doc:`task graph\n        specification documentation <spec>`. These additional\n        constraints are described below.\n\n        All keys must either be non-empty strings or tuples where the first element is a\n        non-empty string, followed by zero or more arbitrary str, bytes, int, float, or\n        tuples thereof. The non-empty string is commonly known as the *collection name*.\n        All collections embedded in the dask package have exactly one name, but this is\n        not a requirement.\n\n        These are all valid outputs:\n\n        - ``[]``\n        - ``[\"x\", \"y\"]``\n        - ``[[(\"y\", \"a\", 0), (\"y\", \"a\", 1)], [(\"y\", \"b\", 0), (\"y\", \"b\", 1)]``\n\n        Returns\n        -------\n        list\n            A possibly nested list of keys that represent the outputs\n            of the graph. After computation, the results will be\n            returned in the same layout, with the keys replaced with\n            their corresponding outputs.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_postcompute__(self) -> tuple[PostComputeCallable, tuple]:\n        \"\"\"Finalizer function and optional arguments to construct final result.\n\n        Upon computation each key in the collection will have an in\n        memory result, the postcompute function combines each key's\n        result into a final in memory representation. For example,\n        dask.array.Array concatenates the arrays at each chunk into a\n        final in-memory array.\n\n        Returns\n        -------\n        PostComputeCallable\n            Callable that receives the sequence of the results of each\n            final key along with optional arguments. An example signature\n            would be ``finalize(results: Sequence[Any], *args)``.\n        tuple[Any, ...]\n            Optional arguments passed to the function following the\n            key results (the `*args` part of the\n            ``PostComputeCallable``. If no additional arguments are to\n            be passed then this must be an empty tuple.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_postpersist__(self) -> tuple[PostPersistCallable, tuple]:\n        \"\"\"Rebuilder function and optional arguments to construct a persisted collection.\n\n        See also the documentation for :py:class:`dask.typing.PostPersistCallable`.\n\n        Returns\n        -------\n        PostPersistCallable\n            Callable that rebuilds the collection. The signature\n            should be\n            ``rebuild(dsk: Mapping, *args: Any, rename: Mapping[str, str] | None)``\n            (as defined by the\n            :py:class:`~dask.typing.PostPersistCallable` protocol).\n            The callable should return an equivalent Dask collection\n            with the same keys as `self`, but with results that are\n            computed through a different graph. In the case of\n            :py:func:`dask.persist`, the new graph will have just the\n            output keys and the values already computed.\n        tuple[Any, ...]\n            Optional arguments passed to the rebuild callable. If no\n            additional arguments are to be passed then this must be an\n            empty tuple.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_tokenize__(self) -> Hashable:\n        \"\"\"Value that must fully represent the object.\"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    __dask_optimize__: Any\n    \"\"\"Given a graph and keys, return a new optimized graph.\n\n    This method can be either a ``staticmethod`` or a ``classmethod``,\n    but not an ``instancemethod``. For example implementations see the\n    definitions of ``__dask_optimize__`` in the core Dask collections:\n    ``dask.array.Array``, ``dask.dataframe.DataFrame``, etc.\n\n    Note that graphs and keys are merged before calling\n    ``__dask_optimize__``; as such, the graph and keys passed to\n    this method may represent more than one collection sharing the\n    same optimize method.\n\n    Parameters\n    ----------\n    dsk : Graph\n        The merged graphs from all collections sharing the same\n        ``__dask_optimize__`` method.\n    keys : Sequence[Key]\n        A list of the outputs from ``__dask_keys__`` from all\n        collections sharing the same ``__dask_optimize__`` method.\n    **kwargs : Any\n        Extra keyword arguments forwarded from the call to\n        ``compute`` or ``persist``. Can be used or ignored as\n        needed.\n\n    Returns\n    -------\n    MutableMapping\n        The optimized Dask graph.\n\n    \"\"\"\n\n    # FIXME: It is currently not possible to define a staticmethod from a callback protocol\n    # Also, the definition in `is_dask_collection` cannot be satisfied by a\n    # protocol / typing check\n    # staticmethod[SchedulerGetCallable]\n    __dask_scheduler__: staticmethod\n    \"\"\"The default scheduler ``get`` to use for this object.\n\n    Usually attached to the class as a staticmethod, e.g.:\n\n    >>> import dask.threaded\n    >>> class MyCollection:\n    ...     # Use the threaded scheduler by default\n    ...     __dask_scheduler__ = staticmethod(dask.threaded.get)\n\n    \"\"\"\n\n    @abc.abstractmethod\n    def compute(self, **kwargs: Any) -> Any:\n        \"\"\"Compute this dask collection.\n\n        This turns a lazy Dask collection into its in-memory\n        equivalent. For example a Dask array turns into a NumPy array\n        and a Dask dataframe turns into a Pandas dataframe. The entire\n        dataset must fit into memory before calling this operation.\n\n        Parameters\n        ----------\n        scheduler : string, optional\n            Which scheduler to use like \"threads\", \"synchronous\" or\n            \"processes\". If not provided, the default is to check the\n            global settings first, and then fall back to the\n            collection defaults.\n        optimize_graph : bool, optional\n            If True [default], the graph is optimized before\n            computation. Otherwise the graph is run as is. This can be\n            useful for debugging.\n        kwargs :\n            Extra keywords to forward to the scheduler function.\n\n        Returns\n        -------\n        The collection's computed result.\n\n        See Also\n        --------\n        dask.compute\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def persist(self: CollType, **kwargs: Any) -> CollType:\n        \"\"\"Persist this dask collection into memory\n\n        This turns a lazy Dask collection into a Dask collection with\n        the same metadata, but now with the results fully computed or\n        actively computing in the background.\n\n        The action of function differs significantly depending on the\n        active task scheduler. If the task scheduler supports\n        asynchronous computing, such as is the case of the\n        dask.distributed scheduler, then persist will return\n        *immediately* and the return value's task graph will contain\n        Dask Future objects. However if the task scheduler only\n        supports blocking computation then the call to persist will\n        *block* and the return value's task graph will contain\n        concrete Python results.\n\n        This function is particularly useful when using distributed\n        systems, because the results will be kept in distributed\n        memory, rather than returned to the local process as with\n        compute.\n\n        Parameters\n        ----------\n        scheduler : string, optional\n            Which scheduler to use like \"threads\", \"synchronous\" or\n            \"processes\". If not provided, the default is to check the\n            global settings first, and then fall back to the\n            collection defaults.\n        optimize_graph : bool, optional\n            If True [default], the graph is optimized before\n            computation. Otherwise the graph is run as is. This can be\n            useful for debugging.\n        **kwargs\n            Extra keywords to forward to the scheduler function.\n\n        Returns\n        -------\n        New dask collections backed by in-memory data\n\n        See Also\n        --------\n        dask.persist\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def visualize(\n        self,\n        filename: str = \"mydask\",\n        format: str | None = None,\n        optimize_graph: bool = False,\n        **kwargs: Any,\n    ) -> DisplayObject | None:\n        \"\"\"Render the computation of this object's task graph using graphviz.\n\n        Requires ``graphviz`` to be installed.\n\n        Parameters\n        ----------\n        filename : str or None, optional\n            The name of the file to write to disk. If the provided\n            `filename` doesn't include an extension, '.png' will be\n            used by default. If `filename` is None, no file will be\n            written, and we communicate with dot using only pipes.\n        format : {'png', 'pdf', 'dot', 'svg', 'jpeg', 'jpg'}, optional\n            Format in which to write output file. Default is 'png'.\n        optimize_graph : bool, optional\n            If True, the graph is optimized before rendering.\n            Otherwise, the graph is displayed as is. Default is False.\n        color: {None, 'order'}, optional\n            Options to color nodes. Provide ``cmap=`` keyword for\n            additional colormap\n        **kwargs\n           Additional keyword arguments to forward to ``to_graphviz``.\n\n        Examples\n        --------\n        >>> x.visualize(filename='dask.pdf')  # doctest: +SKIP\n        >>> x.visualize(filename='dask.pdf', color='order')  # doctest: +SKIP\n\n        Returns\n        -------\n        result : IPython.display.Image, IPython.display.SVG, or None\n            See dask.dot.dot_graph for more information.\n\n        See Also\n        --------\n        dask.visualize\n        dask.dot.dot_graph\n\n        Notes\n        -----\n        For more information on optimization see here:\n\n        https://docs.dask.org/en/latest/optimize.html\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\n@runtime_checkable\nclass HLGDaskCollection(DaskCollection, Protocol):\n    \"\"\"Protocol defining a Dask collection that uses HighLevelGraphs.\n\n    This protocol is nearly identical to\n    :py:class:`~dask.typing.DaskCollection`, with the addition of the\n    ``__dask_layers__`` method (required for collections backed by\n    high level graphs).\n\n    \"\"\"\n\n    @abc.abstractmethod\n    def __dask_layers__(self) -> Sequence[str]:\n        \"\"\"Names of the HighLevelGraph layers.\"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\nclass _NoDefault(Enum):\n    \"\"\"typing-aware constant to detect when the user omits a parameter and you can't use\n    None.\n\n    Copied from pandas._libs.lib._NoDefault.\n\n    Usage\n    -----\n    from dask.typing import NoDefault, no_default\n\n    def f(x: int | None | NoDefault = no_default) -> int:\n        if x is no_default:\n            ...\n    \"\"\"\n\n    no_default = \"NO_DEFAULT\"\n\n    def __repr__(self) -> str:\n        return \"<no_default>\"\n\n\nno_default = _NoDefault.no_default\nNoDefault = Literal[_NoDefault.no_default]\n\n\n### Dependency File: utils.py\nfrom __future__ import annotations\n\nimport codecs\nimport functools\nimport inspect\nimport os\nimport re\nimport shutil\nimport sys\nimport tempfile\nimport types\nimport uuid\nimport warnings\nfrom collections.abc import Callable, Hashable, Iterable, Iterator, Mapping, Set\nfrom contextlib import contextmanager, nullcontext, suppress\nfrom datetime import datetime, timedelta\nfrom errno import ENOENT\nfrom functools import wraps\nfrom importlib import import_module\nfrom numbers import Integral, Number\nfrom operator import add\nfrom threading import Lock\nfrom typing import Any, ClassVar, Literal, TypeVar, cast, overload\nfrom weakref import WeakValueDictionary\n\nimport tlz as toolz\n\nfrom dask import config\nfrom dask.typing import no_default\n\nK = TypeVar(\"K\")\nV = TypeVar(\"V\")\nT = TypeVar(\"T\")\n\n# used in decorators to preserve the signature of the function it decorates\n# see https://mypy.readthedocs.io/en/stable/generics.html#declaring-decorators\nFuncType = Callable[..., Any]\nF = TypeVar(\"F\", bound=FuncType)\n\nsystem_encoding = sys.getdefaultencoding()\nif system_encoding == \"ascii\":\n    system_encoding = \"utf-8\"\n\n\ndef apply(func, args, kwargs=None):\n    \"\"\"Apply a function given its positional and keyword arguments.\n\n    Equivalent to ``func(*args, **kwargs)``\n    Most Dask users will never need to use the ``apply`` function.\n    It is typically only used by people who need to inject\n    keyword argument values into a low level Dask task graph.\n\n    Parameters\n    ----------\n    func : callable\n        The function you want to apply.\n    args : tuple\n        A tuple containing all the positional arguments needed for ``func``\n        (eg: ``(arg_1, arg_2, arg_3)``)\n    kwargs : dict, optional\n        A dictionary mapping the keyword arguments\n        (eg: ``{\"kwarg_1\": value, \"kwarg_2\": value}``\n\n    Examples\n    --------\n    >>> from dask.utils import apply\n    >>> def add(number, second_number=5):\n    ...     return number + second_number\n    ...\n    >>> apply(add, (10,), {\"second_number\": 2})  # equivalent to add(*args, **kwargs)\n    12\n\n    >>> task = apply(add, (10,), {\"second_number\": 2})\n    >>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\n    \"\"\"\n    if kwargs:\n        return func(*args, **kwargs)\n    else:\n        return func(*args)\n\n\ndef _deprecated(\n    *,\n    version: str | None = None,\n    after_version: str | None = None,\n    message: str | None = None,\n    use_instead: str | None = None,\n    category: type[Warning] = FutureWarning,\n):\n    \"\"\"Decorator to mark a function as deprecated\n\n    Parameters\n    ----------\n    version : str, optional\n        Version of Dask in which the function was deprecated. If specified, the version\n        will be included in the default warning message. This should no longer be used\n        after the introduction of automated versioning system.\n    after_version : str, optional\n        Version of Dask after which the function was deprecated. If specified, the\n        version will be included in the default warning message.\n    message : str, optional\n        Custom warning message to raise.\n    use_instead : str, optional\n        Name of function to use in place of the deprecated function.\n        If specified, this will be included in the default warning\n        message.\n    category : type[Warning], optional\n        Type of warning to raise. Defaults to ``FutureWarning``.\n\n    Examples\n    --------\n\n    >>> from dask.utils import _deprecated\n    >>> @_deprecated(after_version=\"X.Y.Z\", use_instead=\"bar\")\n    ... def foo():\n    ...     return \"baz\"\n    \"\"\"\n\n    def decorator(func):\n        if message is None:\n            msg = f\"{func.__name__} \"\n            if after_version is not None:\n                msg += f\"was deprecated after version {after_version} \"\n            elif version is not None:\n                msg += f\"was deprecated in version {version} \"\n            else:\n                msg += \"is deprecated \"\n            msg += \"and will be removed in a future release.\"\n\n            if use_instead is not None:\n                msg += f\" Please use {use_instead} instead.\"\n        else:\n            msg = message\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(msg, category=category, stacklevel=2)\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\ndef _deprecated_kwarg(\n    old_arg_name: str,\n    new_arg_name: str | None = None,\n    mapping: Mapping[Any, Any] | Callable[[Any], Any] | None = None,\n    stacklevel: int = 2,\n    comment: str | None = None,\n) -> Callable[[F], F]:\n    \"\"\"\n    Decorator to deprecate a keyword argument of a function.\n\n    Parameters\n    ----------\n    old_arg_name : str\n        Name of argument in function to deprecate\n    new_arg_name : str, optional\n        Name of preferred argument in function. Omit to warn that\n        ``old_arg_name`` keyword is deprecated.\n    mapping : dict or callable, optional\n        If mapping is present, use it to translate old arguments to\n        new arguments. A callable must do its own value checking;\n        values not found in a dict will be forwarded unchanged.\n    comment :  str, optional\n        Additional message to deprecation message. Useful to pass\n        on suggestions with the deprecation warning.\n\n    Examples\n    --------\n    The following deprecates 'cols', using 'columns' instead\n\n    >>> @_deprecated_kwarg(old_arg_name='cols', new_arg_name='columns')\n    ... def f(columns=''):\n    ...     print(columns)\n    ...\n    >>> f(columns='should work ok')\n    should work ok\n\n    >>> f(cols='should raise warning')  # doctest: +SKIP\n    FutureWarning: cols is deprecated, use columns instead\n      warnings.warn(msg, FutureWarning)\n    should raise warning\n\n    >>> f(cols='should error', columns=\"can\\'t pass do both\")  # doctest: +SKIP\n    TypeError: Can only specify 'cols' or 'columns', not both\n\n    >>> @_deprecated_kwarg('old', 'new', {'yes': True, 'no': False})\n    ... def f(new=False):\n    ...     print('yes!' if new else 'no!')\n    ...\n    >>> f(old='yes')  # doctest: +SKIP\n    FutureWarning: old='yes' is deprecated, use new=True instead\n      warnings.warn(msg, FutureWarning)\n    yes!\n\n    To raise a warning that a keyword will be removed entirely in the future\n\n    >>> @_deprecated_kwarg(old_arg_name='cols', new_arg_name=None)\n    ... def f(cols='', another_param=''):\n    ...     print(cols)\n    ...\n    >>> f(cols='should raise warning')  # doctest: +SKIP\n    FutureWarning: the 'cols' keyword is deprecated and will be removed in a\n    future version please takes steps to stop use of 'cols'\n    should raise warning\n    >>> f(another_param='should not raise warning')  # doctest: +SKIP\n    should not raise warning\n\n    >>> f(cols='should raise warning', another_param='')  # doctest: +SKIP\n    FutureWarning: the 'cols' keyword is deprecated and will be removed in a\n    future version please takes steps to stop use of 'cols'\n    should raise warning\n    \"\"\"\n    if mapping is not None and not hasattr(mapping, \"get\") and not callable(mapping):\n        raise TypeError(\n            \"mapping from old to new argument values must be dict or callable!\"\n        )\n\n    comment_ = f\"\\n{comment}\" or \"\"\n\n    def _deprecated_kwarg(func: F) -> F:\n        @wraps(func)\n        def wrapper(*args, **kwargs) -> Callable[..., Any]:\n            old_arg_value = kwargs.pop(old_arg_name, no_default)\n\n            if old_arg_value is not no_default:\n                if new_arg_name is None:\n                    msg = (\n                        f\"the {repr(old_arg_name)} keyword is deprecated and \"\n                        \"will be removed in a future version. Please take \"\n                        f\"steps to stop the use of {repr(old_arg_name)}\"\n                    ) + comment_\n                    warnings.warn(msg, FutureWarning, stacklevel=stacklevel)\n                    kwargs[old_arg_name] = old_arg_value\n                    return func(*args, **kwargs)\n\n                elif mapping is not None:\n                    if callable(mapping):\n                        new_arg_value = mapping(old_arg_value)\n                    else:\n                        new_arg_value = mapping.get(old_arg_value, old_arg_value)\n                    msg = (\n                        f\"the {old_arg_name}={repr(old_arg_value)} keyword is \"\n                        \"deprecated, use \"\n                        f\"{new_arg_name}={repr(new_arg_value)} instead.\"\n                    )\n                else:\n                    new_arg_value = old_arg_value\n                    msg = (\n                        f\"the {repr(old_arg_name)} keyword is deprecated, \"\n                        f\"use {repr(new_arg_name)} instead.\"\n                    )\n\n                warnings.warn(msg + comment_, FutureWarning, stacklevel=stacklevel)\n                if kwargs.get(new_arg_name) is not None:\n                    msg = (\n                        f\"Can only specify {repr(old_arg_name)} \"\n                        f\"or {repr(new_arg_name)}, not both.\"\n                    )\n                    raise TypeError(msg)\n                kwargs[new_arg_name] = new_arg_value\n            return func(*args, **kwargs)\n\n        return cast(F, wrapper)\n\n    return _deprecated_kwarg\n\n\ndef deepmap(func, *seqs):\n    \"\"\"Apply function inside nested lists\n\n    >>> inc = lambda x: x + 1\n    >>> deepmap(inc, [[1, 2], [3, 4]])\n    [[2, 3], [4, 5]]\n\n    >>> add = lambda x, y: x + y\n    >>> deepmap(add, [[1, 2], [3, 4]], [[10, 20], [30, 40]])\n    [[11, 22], [33, 44]]\n    \"\"\"\n    if isinstance(seqs[0], (list, Iterator)):\n        return [deepmap(func, *items) for items in zip(*seqs)]\n    else:\n        return func(*seqs)\n\n\n@_deprecated()\ndef homogeneous_deepmap(func, seq):\n    if not seq:\n        return seq\n    n = 0\n    tmp = seq\n    while isinstance(tmp, list):\n        n += 1\n        tmp = tmp[0]\n\n    return ndeepmap(n, func, seq)\n\n\ndef ndeepmap(n, func, seq):\n    \"\"\"Call a function on every element within a nested container\n\n    >>> def inc(x):\n    ...     return x + 1\n    >>> L = [[1, 2], [3, 4, 5]]\n    >>> ndeepmap(2, inc, L)\n    [[2, 3], [4, 5, 6]]\n    \"\"\"\n    if n == 1:\n        return [func(item) for item in seq]\n    elif n > 1:\n        return [ndeepmap(n - 1, func, item) for item in seq]\n    elif isinstance(seq, list):\n        return func(seq[0])\n    else:\n        return func(seq)\n\n\ndef import_required(mod_name, error_msg):\n    \"\"\"Attempt to import a required dependency.\n\n    Raises a RuntimeError if the requested module is not available.\n    \"\"\"\n    try:\n        return import_module(mod_name)\n    except ImportError as e:\n        raise RuntimeError(error_msg) from e\n\n\n@contextmanager\ndef tmpfile(extension=\"\", dir=None):\n    \"\"\"\n    Function to create and return a unique temporary file with the given extension, if provided.\n\n    Parameters\n    ----------\n    extension : str\n        The extension of the temporary file to be created\n    dir : str\n        If ``dir`` is not None, the file will be created in that directory; otherwise,\n        Python's default temporary directory is used.\n\n    Returns\n    -------\n    out : str\n        Path to the temporary file\n\n    See Also\n    --------\n    NamedTemporaryFile : Built-in alternative for creating temporary files\n    tmp_path : pytest fixture for creating a temporary directory unique to the test invocation\n\n    Notes\n    -----\n    This context manager is particularly useful on Windows for opening temporary files multiple times.\n    \"\"\"\n    extension = extension.lstrip(\".\")\n    if extension:\n        extension = \".\" + extension\n    handle, filename = tempfile.mkstemp(extension, dir=dir)\n    os.close(handle)\n    os.remove(filename)\n\n    try:\n        yield filename\n    finally:\n        if os.path.exists(filename):\n            with suppress(OSError):  # sometimes we can't remove a generated temp file\n                if os.path.isdir(filename):\n                    shutil.rmtree(filename)\n                else:\n                    os.remove(filename)\n\n\n@contextmanager\ndef tmpdir(dir=None):\n    \"\"\"\n    Function to create and return a unique temporary directory.\n\n    Parameters\n    ----------\n    dir : str\n        If ``dir`` is not None, the directory will be created in that directory; otherwise,\n        Python's default temporary directory is used.\n\n    Returns\n    -------\n    out : str\n        Path to the temporary directory\n\n    Notes\n    -----\n    This context manager is particularly useful on Windows for opening temporary directories multiple times.\n    \"\"\"\n    dirname = tempfile.mkdtemp(dir=dir)\n\n    try:\n        yield dirname\n    finally:\n        if os.path.exists(dirname):\n            if os.path.isdir(dirname):\n                with suppress(OSError):\n                    shutil.rmtree(dirname)\n            else:\n                with suppress(OSError):\n                    os.remove(dirname)\n\n\n@contextmanager\ndef filetext(text, extension=\"\", open=open, mode=\"w\"):\n    with tmpfile(extension=extension) as filename:\n        f = open(filename, mode=mode)\n        try:\n            f.write(text)\n        finally:\n            try:\n                f.close()\n            except AttributeError:\n                pass\n\n        yield filename\n\n\n@contextmanager\ndef changed_cwd(new_cwd):\n    old_cwd = os.getcwd()\n    os.chdir(new_cwd)\n    try:\n        yield\n    finally:\n        os.chdir(old_cwd)\n\n\n@contextmanager\ndef tmp_cwd(dir=None):\n    with tmpdir(dir) as dirname:\n        with changed_cwd(dirname):\n            yield dirname\n\n\nclass IndexCallable:\n    \"\"\"Provide getitem syntax for functions\n\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> I = IndexCallable(inc)\n    >>> I[3]\n    4\n    \"\"\"\n\n    __slots__ = (\"fn\",)\n\n    def __init__(self, fn):\n        self.fn = fn\n\n    def __getitem__(self, key):\n        return self.fn(key)\n\n\n@contextmanager\ndef filetexts(d, open=open, mode=\"t\", use_tmpdir=True):\n    \"\"\"Dumps a number of textfiles to disk\n\n    Parameters\n    ----------\n    d : dict\n        a mapping from filename to text like {'a.csv': '1,1\\n2,2'}\n\n    Since this is meant for use in tests, this context manager will\n    automatically switch to a temporary current directory, to avoid\n    race conditions when running tests in parallel.\n    \"\"\"\n    with tmp_cwd() if use_tmpdir else nullcontext():\n        for filename, text in d.items():\n            try:\n                os.makedirs(os.path.dirname(filename))\n            except OSError:\n                pass\n            f = open(filename, \"w\" + mode)\n            try:\n                f.write(text)\n            finally:\n                try:\n                    f.close()\n                except AttributeError:\n                    pass\n\n        yield list(d)\n\n        for filename in d:\n            if os.path.exists(filename):\n                with suppress(OSError):\n                    os.remove(filename)\n\n\ndef concrete(seq):\n    \"\"\"Make nested iterators concrete lists\n\n    >>> data = [[1, 2], [3, 4]]\n    >>> seq = iter(map(iter, data))\n    >>> concrete(seq)\n    [[1, 2], [3, 4]]\n    \"\"\"\n    if isinstance(seq, Iterator):\n        seq = list(seq)\n    if isinstance(seq, (tuple, list)):\n        seq = list(map(concrete, seq))\n    return seq\n\n\ndef pseudorandom(n: int, p, random_state=None):\n    \"\"\"Pseudorandom array of integer indexes\n\n    >>> pseudorandom(5, [0.5, 0.5], random_state=123)\n    array([1, 0, 0, 1, 1], dtype=int8)\n\n    >>> pseudorandom(10, [0.5, 0.2, 0.2, 0.1], random_state=5)\n    array([0, 2, 0, 3, 0, 1, 2, 1, 0, 0], dtype=int8)\n    \"\"\"\n    import numpy as np\n\n    p = list(p)\n    cp = np.cumsum([0] + p)\n    assert np.allclose(1, cp[-1])\n    assert len(p) < 256\n\n    if not isinstance(random_state, np.random.RandomState):\n        random_state = np.random.RandomState(random_state)\n\n    x = random_state.random_sample(n)\n    out = np.empty(n, dtype=\"i1\")\n\n    for i, (low, high) in enumerate(zip(cp[:-1], cp[1:])):\n        out[(x >= low) & (x < high)] = i\n    return out\n\n\ndef random_state_data(n: int, random_state=None) -> list:\n    \"\"\"Return a list of arrays that can initialize\n    ``np.random.RandomState``.\n\n    Parameters\n    ----------\n    n : int\n        Number of arrays to return.\n    random_state : int or np.random.RandomState, optional\n        If an int, is used to seed a new ``RandomState``.\n    \"\"\"\n    import numpy as np\n\n    if not all(\n        hasattr(random_state, attr) for attr in [\"normal\", \"beta\", \"bytes\", \"uniform\"]\n    ):\n        random_state = np.random.RandomState(random_state)\n\n    random_data = random_state.bytes(624 * n * 4)  # `n * 624` 32-bit integers\n    l = list(np.frombuffer(random_data, dtype=\"<u4\").reshape((n, -1)))\n    assert len(l) == n\n    return l\n\n\ndef is_integer(i) -> bool:\n    \"\"\"\n    >>> is_integer(6)\n    True\n    >>> is_integer(42.0)\n    True\n    >>> is_integer('abc')\n    False\n    \"\"\"\n    return isinstance(i, Integral) or (isinstance(i, float) and i.is_integer())\n\n\nONE_ARITY_BUILTINS = {\n    abs,\n    all,\n    any,\n    ascii,\n    bool,\n    bytearray,\n    bytes,\n    callable,\n    chr,\n    classmethod,\n    complex,\n    dict,\n    dir,\n    enumerate,\n    eval,\n    float,\n    format,\n    frozenset,\n    hash,\n    hex,\n    id,\n    int,\n    iter,\n    len,\n    list,\n    max,\n    min,\n    next,\n    oct,\n    open,\n    ord,\n    range,\n    repr,\n    reversed,\n    round,\n    set,\n    slice,\n    sorted,\n    staticmethod,\n    str,\n    sum,\n    tuple,\n    type,\n    vars,\n    zip,\n    memoryview,\n}\nMULTI_ARITY_BUILTINS = {\n    compile,\n    delattr,\n    divmod,\n    filter,\n    getattr,\n    hasattr,\n    isinstance,\n    issubclass,\n    map,\n    pow,\n    setattr,\n}\n\n\ndef getargspec(func):\n    \"\"\"Version of inspect.getargspec that works with partial and warps.\"\"\"\n    if isinstance(func, functools.partial):\n        return getargspec(func.func)\n\n    func = getattr(func, \"__wrapped__\", func)\n    if isinstance(func, type):\n        return inspect.getfullargspec(func.__init__)\n    else:\n        return inspect.getfullargspec(func)\n\n\ndef takes_multiple_arguments(func, varargs=True):\n    \"\"\"Does this function take multiple arguments?\n\n    >>> def f(x, y): pass\n    >>> takes_multiple_arguments(f)\n    True\n\n    >>> def f(x): pass\n    >>> takes_multiple_arguments(f)\n    False\n\n    >>> def f(x, y=None): pass\n    >>> takes_multiple_arguments(f)\n    False\n\n    >>> def f(*args): pass\n    >>> takes_multiple_arguments(f)\n    True\n\n    >>> class Thing:\n    ...     def __init__(self, a): pass\n    >>> takes_multiple_arguments(Thing)\n    False\n\n    \"\"\"\n    if func in ONE_ARITY_BUILTINS:\n        return False\n    elif func in MULTI_ARITY_BUILTINS:\n        return True\n\n    try:\n        spec = getargspec(func)\n    except Exception:\n        return False\n\n    try:\n        is_constructor = spec.args[0] == \"self\" and isinstance(func, type)\n    except Exception:\n        is_constructor = False\n\n    if varargs and spec.varargs:\n        return True\n\n    ndefaults = 0 if spec.defaults is None else len(spec.defaults)\n    return len(spec.args) - ndefaults - is_constructor > 1\n\n\ndef get_named_args(func) -> list[str]:\n    \"\"\"Get all non ``*args/**kwargs`` arguments for a function\"\"\"\n    s = inspect.signature(func)\n    return [\n        n\n        for n, p in s.parameters.items()\n        if p.kind in [p.POSITIONAL_OR_KEYWORD, p.POSITIONAL_ONLY, p.KEYWORD_ONLY]\n    ]\n\n\nclass Dispatch:\n    \"\"\"Simple single dispatch.\"\"\"\n\n    def __init__(self, name=None):\n        self._lookup = {}\n        self._lazy = {}\n        if name:\n            self.__name__ = name\n\n    def register(self, type, func=None):\n        \"\"\"Register dispatch of `func` on arguments of type `type`\"\"\"\n\n        def wrapper(func):\n            if isinstance(type, tuple):\n                for t in type:\n                    self.register(t, func)\n            else:\n                self._lookup[type] = func\n            return func\n\n        return wrapper(func) if func is not None else wrapper\n\n    def register_lazy(self, toplevel, func=None):\n        \"\"\"\n        Register a registration function which will be called if the\n        *toplevel* module (e.g. 'pandas') is ever loaded.\n        \"\"\"\n\n        def wrapper(func):\n            self._lazy[toplevel] = func\n            return func\n\n        return wrapper(func) if func is not None else wrapper\n\n    def dispatch(self, cls):\n        \"\"\"Return the function implementation for the given ``cls``\"\"\"\n        lk = self._lookup\n        for cls2 in cls.__mro__:\n            # Is a lazy registration function present?\n            toplevel, _, _ = cls2.__module__.partition(\".\")\n            try:\n                register = self._lazy[toplevel]\n            except KeyError:\n                pass\n            else:\n                register()\n                self._lazy.pop(toplevel, None)\n                return self.dispatch(cls)  # recurse\n            try:\n                impl = lk[cls2]\n            except KeyError:\n                pass\n            else:\n                if cls is not cls2:\n                    # Cache lookup\n                    lk[cls] = impl\n                return impl\n        raise TypeError(f\"No dispatch for {cls}\")\n\n    def __call__(self, arg, *args, **kwargs):\n        \"\"\"\n        Call the corresponding method based on type of argument.\n        \"\"\"\n        meth = self.dispatch(type(arg))\n        return meth(arg, *args, **kwargs)\n\n    @property\n    def __doc__(self):\n        try:\n            func = self.dispatch(object)\n            return func.__doc__\n        except TypeError:\n            return \"Single Dispatch for %s\" % self.__name__\n\n\ndef ensure_not_exists(filename) -> None:\n    \"\"\"\n    Ensure that a file does not exist.\n    \"\"\"\n    try:\n        os.unlink(filename)\n    except OSError as e:\n        if e.errno != ENOENT:\n            raise\n\n\ndef _skip_doctest(line):\n    # NumPy docstring contains cursor and comment only example\n    stripped = line.strip()\n    if stripped == \">>>\" or stripped.startswith(\">>> #\"):\n        return line\n    elif \">>>\" in stripped and \"+SKIP\" not in stripped:\n        if \"# doctest:\" in line:\n            return line + \", +SKIP\"\n        else:\n            return line + \"  # doctest: +SKIP\"\n    else:\n        return line\n\n\ndef skip_doctest(doc):\n    if doc is None:\n        return \"\"\n    return \"\\n\".join([_skip_doctest(line) for line in doc.split(\"\\n\")])\n\n\ndef extra_titles(doc):\n    lines = doc.split(\"\\n\")\n    titles = {\n        i: lines[i].strip()\n        for i in range(len(lines) - 1)\n        if lines[i + 1].strip() and all(c == \"-\" for c in lines[i + 1].strip())\n    }\n\n    seen = set()\n    for i, title in sorted(titles.items()):\n        if title in seen:\n            new_title = \"Extra \" + title\n            lines[i] = lines[i].replace(title, new_title)\n            lines[i + 1] = lines[i + 1].replace(\"-\" * len(title), \"-\" * len(new_title))\n        else:\n            seen.add(title)\n\n    return \"\\n\".join(lines)\n\n\ndef ignore_warning(doc, cls, name, extra=\"\", skipblocks=0, inconsistencies=None):\n    \"\"\"Expand docstring by adding disclaimer and extra text\"\"\"\n    import inspect\n\n    if inspect.isclass(cls):\n        l1 = \"This docstring was copied from {}.{}.{}.\\n\\n\".format(\n            cls.__module__,\n            cls.__name__,\n            name,\n        )\n    else:\n        l1 = f\"This docstring was copied from {cls.__name__}.{name}.\\n\\n\"\n    l2 = \"Some inconsistencies with the Dask version may exist.\"\n\n    i = doc.find(\"\\n\\n\")\n    if i != -1:\n        # Insert our warning\n        head = doc[: i + 2]\n        tail = doc[i + 2 :]\n        while skipblocks > 0:\n            i = tail.find(\"\\n\\n\")\n            head = tail[: i + 2]\n            tail = tail[i + 2 :]\n            skipblocks -= 1\n        # Indentation of next line\n        indent = re.match(r\"\\s*\", tail).group(0)\n        # Insert the warning, indented, with a blank line before and after\n        if extra:\n            more = [indent, extra.rstrip(\"\\n\") + \"\\n\\n\"]\n        else:\n            more = []\n        if inconsistencies is not None:\n            l3 = f\"Known inconsistencies: \\n {inconsistencies}\"\n            bits = [head, indent, l1, l2, \"\\n\\n\", l3, \"\\n\\n\"] + more + [tail]\n        else:\n            bits = [head, indent, l1, indent, l2, \"\\n\\n\"] + more + [tail]\n        doc = \"\".join(bits)\n\n    return doc\n\n\ndef unsupported_arguments(doc, args):\n    \"\"\"Mark unsupported arguments with a disclaimer\"\"\"\n    lines = doc.split(\"\\n\")\n    for arg in args:\n        subset = [\n            (i, line)\n            for i, line in enumerate(lines)\n            if re.match(r\"^\\s*\" + arg + \" ?:\", line)\n        ]\n        if len(subset) == 1:\n            [(i, line)] = subset\n            lines[i] = line + \"  (Not supported in Dask)\"\n    return \"\\n\".join(lines)\n\n\ndef _derived_from(\n    cls, method, ua_args=None, extra=\"\", skipblocks=0, inconsistencies=None\n):\n    \"\"\"Helper function for derived_from to ease testing\"\"\"\n    ua_args = ua_args or []\n\n    # do not use wraps here, as it hides keyword arguments displayed\n    # in the doc\n    original_method = getattr(cls, method.__name__)\n\n    doc = getattr(original_method, \"__doc__\", None)\n\n    if isinstance(original_method, property):\n        # some things like SeriesGroupBy.unique are generated.\n        original_method = original_method.fget\n        if not doc:\n            doc = getattr(original_method, \"__doc__\", None)\n\n    if isinstance(original_method, functools.cached_property):\n        original_method = original_method.func\n        if not doc:\n            doc = getattr(original_method, \"__doc__\", None)\n\n    if doc is None:\n        doc = \"\"\n\n    # pandas DataFrame/Series sometimes override methods without setting __doc__\n    if not doc and cls.__name__ in {\"DataFrame\", \"Series\"}:\n        for obj in cls.mro():\n            obj_method = getattr(obj, method.__name__, None)\n            if obj_method is not None and obj_method.__doc__:\n                doc = obj_method.__doc__\n                break\n\n    # Insert disclaimer that this is a copied docstring\n    if doc:\n        doc = ignore_warning(\n            doc,\n            cls,\n            method.__name__,\n            extra=extra,\n            skipblocks=skipblocks,\n            inconsistencies=inconsistencies,\n        )\n    elif extra:\n        doc += extra.rstrip(\"\\n\") + \"\\n\\n\"\n\n    # Mark unsupported arguments\n    try:\n        method_args = get_named_args(method)\n        original_args = get_named_args(original_method)\n        not_supported = [m for m in original_args if m not in method_args]\n    except ValueError:\n        not_supported = []\n    if len(ua_args) > 0:\n        not_supported.extend(ua_args)\n    if len(not_supported) > 0:\n        doc = unsupported_arguments(doc, not_supported)\n\n    doc = skip_doctest(doc)\n    doc = extra_titles(doc)\n\n    return doc\n\n\ndef derived_from(\n    original_klass, version=None, ua_args=None, skipblocks=0, inconsistencies=None\n):\n    \"\"\"Decorator to attach original class's docstring to the wrapped method.\n\n    The output structure will be: top line of docstring, disclaimer about this\n    being auto-derived, any extra text associated with the method being patched,\n    the body of the docstring and finally, the list of keywords that exist in\n    the original method but not in the dask version.\n\n    Parameters\n    ----------\n    original_klass: type\n        Original class which the method is derived from\n    version : str\n        Original package version which supports the wrapped method\n    ua_args : list\n        List of keywords which Dask doesn't support. Keywords existing in\n        original but not in Dask will automatically be added.\n    skipblocks : int\n        How many text blocks (paragraphs) to skip from the start of the\n        docstring. Useful for cases where the target has extra front-matter.\n    inconsistencies: list\n        List of known inconsistencies with method whose docstrings are being\n        copied.\n    \"\"\"\n    ua_args = ua_args or []\n\n    def wrapper(method):\n        try:\n            extra = getattr(method, \"__doc__\", None) or \"\"\n            method.__doc__ = _derived_from(\n                original_klass,\n                method,\n                ua_args=ua_args,\n                extra=extra,\n                skipblocks=skipblocks,\n                inconsistencies=inconsistencies,\n            )\n            return method\n\n        except AttributeError:\n            module_name = original_klass.__module__.split(\".\")[0]\n\n            @functools.wraps(method)\n            def wrapped(*args, **kwargs):\n                msg = f\"Base package doesn't support '{method.__name__}'.\"\n                if version is not None:\n                    msg2 = \" Use {0} {1} or later to use this method.\"\n                    msg += msg2.format(module_name, version)\n                raise NotImplementedError(msg)\n\n            return wrapped\n\n    return wrapper\n\n\ndef funcname(func) -> str:\n    \"\"\"Get the name of a function.\"\"\"\n    # functools.partial\n    if isinstance(func, functools.partial):\n        return funcname(func.func)\n    # methodcaller\n    if isinstance(func, methodcaller):\n        return func.method[:50]\n\n    module_name = getattr(func, \"__module__\", None) or \"\"\n    type_name = getattr(type(func), \"__name__\", None) or \"\"\n\n    # toolz.curry\n    if \"toolz\" in module_name and \"curry\" == type_name:\n        return func.func_name[:50]\n    # multipledispatch objects\n    if \"multipledispatch\" in module_name and \"Dispatcher\" == type_name:\n        return func.name[:50]\n    # numpy.vectorize objects\n    if \"numpy\" in module_name and \"vectorize\" == type_name:\n        return (\"vectorize_\" + funcname(func.pyfunc))[:50]\n\n    # All other callables\n    try:\n        name = func.__name__\n        if name == \"<lambda>\":\n            return \"lambda\"\n        return name[:50]\n    except AttributeError:\n        return str(func)[:50]\n\n\ndef typename(typ: Any, short: bool = False) -> str:\n    \"\"\"\n    Return the name of a type\n\n    Examples\n    --------\n    >>> typename(int)\n    'int'\n\n    >>> from dask.core import literal\n    >>> typename(literal)\n    'dask.core.literal'\n    >>> typename(literal, short=True)\n    'dask.literal'\n    \"\"\"\n    if not isinstance(typ, type):\n        return typename(type(typ))\n    try:\n        if not typ.__module__ or typ.__module__ == \"builtins\":\n            return typ.__name__\n        else:\n            if short:\n                module, *_ = typ.__module__.split(\".\")\n            else:\n                module = typ.__module__\n            return module + \".\" + typ.__name__\n    except AttributeError:\n        return str(typ)\n\n\ndef ensure_bytes(s) -> bytes:\n    \"\"\"Attempt to turn `s` into bytes.\n\n    Parameters\n    ----------\n    s : Any\n        The object to be converted. Will correctly handled\n        * str\n        * bytes\n        * objects implementing the buffer protocol (memoryview, ndarray, etc.)\n\n    Returns\n    -------\n    b : bytes\n\n    Raises\n    ------\n    TypeError\n        When `s` cannot be converted\n\n    Examples\n    --------\n    >>> ensure_bytes('123')\n    b'123'\n    >>> ensure_bytes(b'123')\n    b'123'\n    >>> ensure_bytes(bytearray(b'123'))\n    b'123'\n    \"\"\"\n    if isinstance(s, bytes):\n        return s\n    elif hasattr(s, \"encode\"):\n        return s.encode()\n    else:\n        try:\n            return bytes(s)\n        except Exception as e:\n            raise TypeError(\n                f\"Object {s} is neither a bytes object nor can be encoded to bytes\"\n            ) from e\n\n\ndef ensure_unicode(s) -> str:\n    \"\"\"Turn string or bytes to string\n\n    >>> ensure_unicode('123')\n    '123'\n    >>> ensure_unicode(b'123')\n    '123'\n    \"\"\"\n    if isinstance(s, str):\n        return s\n    elif hasattr(s, \"decode\"):\n        return s.decode()\n    else:\n        try:\n            return codecs.decode(s)\n        except Exception as e:\n            raise TypeError(\n                f\"Object {s} is neither a str object nor can be decoded to str\"\n            ) from e\n\n\ndef digit(n, k, base):\n    \"\"\"\n\n    >>> digit(1234, 0, 10)\n    4\n    >>> digit(1234, 1, 10)\n    3\n    >>> digit(1234, 2, 10)\n    2\n    >>> digit(1234, 3, 10)\n    1\n    \"\"\"\n    return n // base**k % base\n\n\ndef insert(tup, loc, val):\n    \"\"\"\n\n    >>> insert(('a', 'b', 'c'), 0, 'x')\n    ('x', 'b', 'c')\n    \"\"\"\n    L = list(tup)\n    L[loc] = val\n    return tuple(L)\n\n\ndef memory_repr(num):\n    for x in [\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n        if num < 1024.0:\n            return f\"{num:3.1f} {x}\"\n        num /= 1024.0\n\n\ndef asciitable(columns, rows):\n    \"\"\"Formats an ascii table for given columns and rows.\n\n    Parameters\n    ----------\n    columns : list\n        The column names\n    rows : list of tuples\n        The rows in the table. Each tuple must be the same length as\n        ``columns``.\n    \"\"\"\n    rows = [tuple(str(i) for i in r) for r in rows]\n    columns = tuple(str(i) for i in columns)\n    widths = tuple(max(max(map(len, x)), len(c)) for x, c in zip(zip(*rows), columns))\n    row_template = (\"|\" + (\" %%-%ds |\" * len(columns))) % widths\n    header = row_template % tuple(columns)\n    bar = \"+%s+\" % \"+\".join(\"-\" * (w + 2) for w in widths)\n    data = \"\\n\".join(row_template % r for r in rows)\n    return \"\\n\".join([bar, header, bar, data, bar])\n\n\ndef put_lines(buf, lines):\n    if any(not isinstance(x, str) for x in lines):\n        lines = [str(x) for x in lines]\n    buf.write(\"\\n\".join(lines))\n\n\n_method_cache: dict[str, methodcaller] = {}\n\n\nclass methodcaller:\n    \"\"\"\n    Return a callable object that calls the given method on its operand.\n\n    Unlike the builtin `operator.methodcaller`, instances of this class are\n    cached and arguments are passed at call time instead of build time.\n    \"\"\"\n\n    __slots__ = (\"method\",)\n    method: str\n\n    @property\n    def func(self) -> str:\n        # For `funcname` to work\n        return self.method\n\n    def __new__(cls, method: str):\n        try:\n            return _method_cache[method]\n        except KeyError:\n            self = object.__new__(cls)\n            self.method = method\n            _method_cache[method] = self\n            return self\n\n    def __call__(self, __obj, *args, **kwargs):\n        return getattr(__obj, self.method)(*args, **kwargs)\n\n    def __reduce__(self):\n        return (methodcaller, (self.method,))\n\n    def __str__(self):\n        return f\"<{self.__class__.__name__}: {self.method}>\"\n\n    __repr__ = __str__\n\n\nclass itemgetter:\n    \"\"\"Variant of operator.itemgetter that supports equality tests\"\"\"\n\n    __slots__ = (\"index\",)\n\n    def __init__(self, index):\n        self.index = index\n\n    def __call__(self, x):\n        return x[self.index]\n\n    def __reduce__(self):\n        return (itemgetter, (self.index,))\n\n    def __eq__(self, other):\n        return type(self) is type(other) and self.index == other.index\n\n\nclass MethodCache:\n    \"\"\"Attribute access on this object returns a methodcaller for that\n    attribute.\n\n    Examples\n    --------\n    >>> a = [1, 3, 3]\n    >>> M.count(a, 3) == a.count(3)\n    True\n    \"\"\"\n\n    def __getattr__(self, item):\n        return methodcaller(item)\n\n    def __dir__(self):\n        return list(_method_cache)\n\n\nM = MethodCache()\n\n\nclass SerializableLock:\n    \"\"\"A Serializable per-process Lock\n\n    This wraps a normal ``threading.Lock`` object and satisfies the same\n    interface.  However, this lock can also be serialized and sent to different\n    processes.  It will not block concurrent operations between processes (for\n    this you should look at ``multiprocessing.Lock`` or ``locket.lock_file``\n    but will consistently deserialize into the same lock.\n\n    So if we make a lock in one process::\n\n        lock = SerializableLock()\n\n    And then send it over to another process multiple times::\n\n        bytes = pickle.dumps(lock)\n        a = pickle.loads(bytes)\n        b = pickle.loads(bytes)\n\n    Then the deserialized objects will operate as though they were the same\n    lock, and collide as appropriate.\n\n    This is useful for consistently protecting resources on a per-process\n    level.\n\n    The creation of locks is itself not threadsafe.\n    \"\"\"\n\n    _locks: ClassVar[WeakValueDictionary[Hashable, Lock]] = WeakValueDictionary()\n    token: Hashable\n    lock: Lock\n\n    def __init__(self, token: Hashable | None = None):\n        self.token = token or str(uuid.uuid4())\n        if self.token in SerializableLock._locks:\n            self.lock = SerializableLock._locks[self.token]\n        else:\n            self.lock = Lock()\n            SerializableLock._locks[self.token] = self.lock\n\n    def acquire(self, *args, **kwargs):\n        return self.lock.acquire(*args, **kwargs)\n\n    def release(self, *args, **kwargs):\n        return self.lock.release(*args, **kwargs)\n\n    def __enter__(self):\n        self.lock.__enter__()\n\n    def __exit__(self, *args):\n        self.lock.__exit__(*args)\n\n    def locked(self):\n        return self.lock.locked()\n\n    def __getstate__(self):\n        return self.token\n\n    def __setstate__(self, token):\n        self.__init__(token)\n\n    def __str__(self):\n        return f\"<{self.__class__.__name__}: {self.token}>\"\n\n    __repr__ = __str__\n\n\ndef get_scheduler_lock(collection=None, scheduler=None):\n    \"\"\"Get an instance of the appropriate lock for a certain situation based on\n    scheduler used.\"\"\"\n    from dask import multiprocessing\n    from dask.base import get_scheduler\n\n    actual_get = get_scheduler(collections=[collection], scheduler=scheduler)\n\n    if actual_get == multiprocessing.get:\n        return multiprocessing.get_context().Manager().Lock()\n    else:\n        # if this is a distributed client, we need to lock on\n        # the level between processes, SerializableLock won't work\n        try:\n            import distributed.lock\n            from distributed.worker import get_client\n\n            client = get_client()\n        except (ImportError, ValueError):\n            pass\n        else:\n            if actual_get == client.get:\n                return distributed.lock.Lock()\n\n    return SerializableLock()\n\n\ndef ensure_dict(d: Mapping[K, V], *, copy: bool = False) -> dict[K, V]:\n    \"\"\"Convert a generic Mapping into a dict.\n    Optimize use case of :class:`~dask.highlevelgraph.HighLevelGraph`.\n\n    Parameters\n    ----------\n    d : Mapping\n    copy : bool\n        If True, guarantee that the return value is always a shallow copy of d;\n        otherwise it may be the input itself.\n    \"\"\"\n    if type(d) is dict:\n        return d.copy() if copy else d\n    try:\n        layers = d.layers  # type: ignore\n    except AttributeError:\n        return dict(d)\n\n    result = {}\n    for layer in toolz.unique(layers.values(), key=id):\n        result.update(layer)\n    return result\n\n\ndef ensure_set(s: Set[T], *, copy: bool = False) -> set[T]:\n    \"\"\"Convert a generic Set into a set.\n\n    Parameters\n    ----------\n    s : Set\n    copy : bool\n        If True, guarantee that the return value is always a shallow copy of s;\n        otherwise it may be the input itself.\n    \"\"\"\n    if type(s) is set:\n        return s.copy() if copy else s\n    return set(s)\n\n\nclass OperatorMethodMixin:\n    \"\"\"A mixin for dynamically implementing operators\"\"\"\n\n    __slots__ = ()\n\n    @classmethod\n    def _bind_operator(cls, op):\n        \"\"\"bind operator to this class\"\"\"\n        name = op.__name__\n\n        if name.endswith(\"_\"):\n            # for and_ and or_\n            name = name[:-1]\n        elif name == \"inv\":\n            name = \"invert\"\n\n        meth = f\"__{name}__\"\n\n        if name in (\"abs\", \"invert\", \"neg\", \"pos\"):\n            setattr(cls, meth, cls._get_unary_operator(op))\n        else:\n            setattr(cls, meth, cls._get_binary_operator(op))\n\n            if name in (\"eq\", \"gt\", \"ge\", \"lt\", \"le\", \"ne\", \"getitem\"):\n                return\n\n            rmeth = f\"__r{name}__\"\n            setattr(cls, rmeth, cls._get_binary_operator(op, inv=True))\n\n    @classmethod\n    def _get_unary_operator(cls, op):\n        \"\"\"Must return a method used by unary operator\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def _get_binary_operator(cls, op, inv=False):\n        \"\"\"Must return a method used by binary operator\"\"\"\n        raise NotImplementedError\n\n\ndef partial_by_order(*args, **kwargs):\n    \"\"\"\n\n    >>> from operator import add\n    >>> partial_by_order(5, function=add, other=[(1, 10)])\n    15\n    \"\"\"\n    function = kwargs.pop(\"function\")\n    other = kwargs.pop(\"other\")\n    args2 = list(args)\n    for i, arg in other:\n        args2.insert(i, arg)\n    return function(*args2, **kwargs)\n\n\ndef is_arraylike(x) -> bool:\n    \"\"\"Is this object a numpy array or something similar?\n\n    This function tests specifically for an object that already has\n    array attributes (e.g. np.ndarray, dask.array.Array, cupy.ndarray,\n    sparse.COO), **NOT** for something that can be coerced into an\n    array object (e.g. Python lists and tuples). It is meant for dask\n    developers and developers of downstream libraries.\n\n    Note that this function does not correspond with NumPy's\n    definition of array_like, which includes any object that can be\n    coerced into an array (see definition in the NumPy glossary):\n    https://numpy.org/doc/stable/glossary.html\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> is_arraylike(np.ones(5))\n    True\n    >>> is_arraylike(np.ones(()))\n    True\n    >>> is_arraylike(5)\n    False\n    >>> is_arraylike('cat')\n    False\n    \"\"\"\n    from dask.base import is_dask_collection\n\n    is_duck_array = hasattr(x, \"__array_function__\") or hasattr(x, \"__array_ufunc__\")\n\n    return bool(\n        hasattr(x, \"shape\")\n        and isinstance(x.shape, tuple)\n        and hasattr(x, \"dtype\")\n        and not any(is_dask_collection(n) for n in x.shape)\n        # We special case scipy.sparse and cupyx.scipy.sparse arrays as having partial\n        # support for them is useful in scenarios where we mostly call `map_partitions`\n        # or `map_blocks` with scikit-learn functions on dask arrays and dask dataframes.\n        # https://github.com/dask/dask/pull/3738\n        and (is_duck_array or \"scipy.sparse\" in typename(type(x)))\n    )\n\n\ndef is_dataframe_like(df) -> bool:\n    \"\"\"Looks like a Pandas DataFrame\"\"\"\n    if (df.__class__.__module__, df.__class__.__name__) == (\n        \"pandas.core.frame\",\n        \"DataFrame\",\n    ):\n        # fast exec for most likely input\n        return True\n    typ = df.__class__\n    return (\n        all(hasattr(typ, name) for name in (\"groupby\", \"head\", \"merge\", \"mean\"))\n        and all(hasattr(df, name) for name in (\"dtypes\", \"columns\"))\n        and not any(hasattr(typ, name) for name in (\"name\", \"dtype\"))\n    )\n\n\ndef is_series_like(s) -> bool:\n    \"\"\"Looks like a Pandas Series\"\"\"\n    typ = s.__class__\n    return (\n        all(hasattr(typ, name) for name in (\"groupby\", \"head\", \"mean\"))\n        and all(hasattr(s, name) for name in (\"dtype\", \"name\"))\n        and \"index\" not in typ.__name__.lower()\n    )\n\n\ndef is_index_like(s) -> bool:\n    \"\"\"Looks like a Pandas Index\"\"\"\n    typ = s.__class__\n    return (\n        all(hasattr(s, name) for name in (\"name\", \"dtype\"))\n        and \"index\" in typ.__name__.lower()\n    )\n\n\ndef is_cupy_type(x) -> bool:\n    # TODO: avoid explicit reference to CuPy\n    return \"cupy\" in str(type(x))\n\n\ndef natural_sort_key(s: str) -> list[str | int]:\n    \"\"\"\n    Sorting `key` function for performing a natural sort on a collection of\n    strings\n\n    See https://en.wikipedia.org/wiki/Natural_sort_order\n\n    Parameters\n    ----------\n    s : str\n        A string that is an element of the collection being sorted\n\n    Returns\n    -------\n    tuple[str or int]\n        Tuple of the parts of the input string where each part is either a\n        string or an integer\n\n    Examples\n    --------\n    >>> a = ['f0', 'f1', 'f2', 'f8', 'f9', 'f10', 'f11', 'f19', 'f20', 'f21']\n    >>> sorted(a)\n    ['f0', 'f1', 'f10', 'f11', 'f19', 'f2', 'f20', 'f21', 'f8', 'f9']\n    >>> sorted(a, key=natural_sort_key)\n    ['f0', 'f1', 'f2', 'f8', 'f9', 'f10', 'f11', 'f19', 'f20', 'f21']\n    \"\"\"\n    return [int(part) if part.isdigit() else part for part in re.split(r\"(\\d+)\", s)]\n\n\ndef parse_bytes(s: float | str) -> int:\n    \"\"\"Parse byte string to numbers\n\n    >>> from dask.utils import parse_bytes\n    >>> parse_bytes('100')\n    100\n    >>> parse_bytes('100 MB')\n    100000000\n    >>> parse_bytes('100M')\n    100000000\n    >>> parse_bytes('5kB')\n    5000\n    >>> parse_bytes('5.4 kB')\n    5400\n    >>> parse_bytes('1kiB')\n    1024\n    >>> parse_bytes('1e6')\n    1000000\n    >>> parse_bytes('1e6 kB')\n    1000000000\n    >>> parse_bytes('MB')\n    1000000\n    >>> parse_bytes(123)\n    123\n    >>> parse_bytes('5 foos')\n    Traceback (most recent call last):\n        ...\n    ValueError: Could not interpret 'foos' as a byte unit\n    \"\"\"\n    if isinstance(s, (int, float)):\n        return int(s)\n    s = s.replace(\" \", \"\")\n    if not any(char.isdigit() for char in s):\n        s = \"1\" + s\n\n    for i in range(len(s) - 1, -1, -1):\n        if not s[i].isalpha():\n            break\n    index = i + 1\n\n    prefix = s[:index]\n    suffix = s[index:]\n\n    try:\n        n = float(prefix)\n    except ValueError as e:\n        raise ValueError(\"Could not interpret '%s' as a number\" % prefix) from e\n\n    try:\n        multiplier = byte_sizes[suffix.lower()]\n    except KeyError as e:\n        raise ValueError(\"Could not interpret '%s' as a byte unit\" % suffix) from e\n\n    result = n * multiplier\n    return int(result)\n\n\nbyte_sizes = {\n    \"kB\": 10**3,\n    \"MB\": 10**6,\n    \"GB\": 10**9,\n    \"TB\": 10**12,\n    \"PB\": 10**15,\n    \"KiB\": 2**10,\n    \"MiB\": 2**20,\n    \"GiB\": 2**30,\n    \"TiB\": 2**40,\n    \"PiB\": 2**50,\n    \"B\": 1,\n    \"\": 1,\n}\nbyte_sizes = {k.lower(): v for k, v in byte_sizes.items()}\nbyte_sizes.update({k[0]: v for k, v in byte_sizes.items() if k and \"i\" not in k})\nbyte_sizes.update({k[:-1]: v for k, v in byte_sizes.items() if k and \"i\" in k})\n\n\ndef format_time(n: float) -> str:\n    \"\"\"format integers as time\n\n    >>> from dask.utils import format_time\n    >>> format_time(1)\n    '1.00 s'\n    >>> format_time(0.001234)\n    '1.23 ms'\n    >>> format_time(0.00012345)\n    '123.45 us'\n    >>> format_time(123.456)\n    '123.46 s'\n    >>> format_time(1234.567)\n    '20m 34s'\n    >>> format_time(12345.67)\n    '3hr 25m'\n    >>> format_time(123456.78)\n    '34hr 17m'\n    >>> format_time(1234567.89)\n    '14d 6hr'\n    \"\"\"\n    if n > 24 * 60 * 60 * 2:\n        d = int(n / 3600 / 24)\n        h = int((n - d * 3600 * 24) / 3600)\n        return f\"{d}d {h}hr\"\n    if n > 60 * 60 * 2:\n        h = int(n / 3600)\n        m = int((n - h * 3600) / 60)\n        return f\"{h}hr {m}m\"\n    if n > 60 * 10:\n        m = int(n / 60)\n        s = int(n - m * 60)\n        return f\"{m}m {s}s\"\n    if n >= 1:\n        return \"%.2f s\" % n\n    if n >= 1e-3:\n        return \"%.2f ms\" % (n * 1e3)\n    return \"%.2f us\" % (n * 1e6)\n\n\ndef format_time_ago(n: datetime) -> str:\n    \"\"\"Calculate a '3 hours ago' type string from a Python datetime.\n\n    Examples\n    --------\n    >>> from datetime import datetime, timedelta\n\n    >>> now = datetime.now()\n    >>> format_time_ago(now)\n    'Just now'\n\n    >>> past = datetime.now() - timedelta(minutes=1)\n    >>> format_time_ago(past)\n    '1 minute ago'\n\n    >>> past = datetime.now() - timedelta(minutes=2)\n    >>> format_time_ago(past)\n    '2 minutes ago'\n\n    >>> past = datetime.now() - timedelta(hours=1)\n    >>> format_time_ago(past)\n    '1 hour ago'\n\n    >>> past = datetime.now() - timedelta(hours=6)\n    >>> format_time_ago(past)\n    '6 hours ago'\n\n    >>> past = datetime.now() - timedelta(days=1)\n    >>> format_time_ago(past)\n    '1 day ago'\n\n    >>> past = datetime.now() - timedelta(days=5)\n    >>> format_time_ago(past)\n    '5 days ago'\n\n    >>> past = datetime.now() - timedelta(days=8)\n    >>> format_time_ago(past)\n    '1 week ago'\n\n    >>> past = datetime.now() - timedelta(days=16)\n    >>> format_time_ago(past)\n    '2 weeks ago'\n\n    >>> past = datetime.now() - timedelta(days=190)\n    >>> format_time_ago(past)\n    '6 months ago'\n\n    >>> past = datetime.now() - timedelta(days=800)\n    >>> format_time_ago(past)\n    '2 years ago'\n\n    \"\"\"\n    units = {\n        \"years\": lambda diff: diff.days / 365,\n        \"months\": lambda diff: diff.days / 30.436875,  # Average days per month\n        \"weeks\": lambda diff: diff.days / 7,\n        \"days\": lambda diff: diff.days,\n        \"hours\": lambda diff: diff.seconds / 3600,\n        \"minutes\": lambda diff: diff.seconds % 3600 / 60,\n    }\n    diff = datetime.now() - n\n    for unit in units:\n        dur = int(units[unit](diff))\n        if dur > 0:\n            if dur == 1:  # De-pluralize\n                unit = unit[:-1]\n            return f\"{dur} {unit} ago\"\n    return \"Just now\"\n\n\ndef format_bytes(n: int) -> str:\n    \"\"\"Format bytes as text\n\n    >>> from dask.utils import format_bytes\n    >>> format_bytes(1)\n    '1 B'\n    >>> format_bytes(1234)\n    '1.21 kiB'\n    >>> format_bytes(12345678)\n    '11.77 MiB'\n    >>> format_bytes(1234567890)\n    '1.15 GiB'\n    >>> format_bytes(1234567890000)\n    '1.12 TiB'\n    >>> format_bytes(1234567890000000)\n    '1.10 PiB'\n\n    For all values < 2**60, the output is always <= 10 characters.\n    \"\"\"\n    for prefix, k in (\n        (\"Pi\", 2**50),\n        (\"Ti\", 2**40),\n        (\"Gi\", 2**30),\n        (\"Mi\", 2**20),\n        (\"ki\", 2**10),\n    ):\n        if n >= k * 0.9:\n            return f\"{n / k:.2f} {prefix}B\"\n    return f\"{n} B\"\n\n\ntimedelta_sizes = {\n    \"s\": 1,\n    \"ms\": 1e-3,\n    \"us\": 1e-6,\n    \"ns\": 1e-9,\n    \"m\": 60,\n    \"h\": 3600,\n    \"d\": 3600 * 24,\n    \"w\": 7 * 3600 * 24,\n}\n\ntds2 = {\n    \"second\": 1,\n    \"minute\": 60,\n    \"hour\": 60 * 60,\n    \"day\": 60 * 60 * 24,\n    \"week\": 7 * 60 * 60 * 24,\n    \"millisecond\": 1e-3,\n    \"microsecond\": 1e-6,\n    \"nanosecond\": 1e-9,\n}\ntds2.update({k + \"s\": v for k, v in tds2.items()})\ntimedelta_sizes.update(tds2)\ntimedelta_sizes.update({k.upper(): v for k, v in timedelta_sizes.items()})\n\n\n@overload\ndef parse_timedelta(s: None, default: str | Literal[False] = \"seconds\") -> None: ...\n\n\n@overload\ndef parse_timedelta(\n    s: str | float | timedelta, default: str | Literal[False] = \"seconds\"\n) -> float: ...\n\n\ndef parse_timedelta(s, default=\"seconds\"):\n    \"\"\"Parse timedelta string to number of seconds\n\n    Parameters\n    ----------\n    s : str, float, timedelta, or None\n    default: str or False, optional\n        Unit of measure if s  does not specify one. Defaults to seconds.\n        Set to False to require s to explicitly specify its own unit.\n\n    Examples\n    --------\n    >>> from datetime import timedelta\n    >>> from dask.utils import parse_timedelta\n    >>> parse_timedelta('3s')\n    3\n    >>> parse_timedelta('3.5 seconds')\n    3.5\n    >>> parse_timedelta('300ms')\n    0.3\n    >>> parse_timedelta(timedelta(seconds=3))  # also supports timedeltas\n    3\n    \"\"\"\n    if s is None:\n        return None\n    if isinstance(s, timedelta):\n        s = s.total_seconds()\n        return int(s) if int(s) == s else s\n    if isinstance(s, Number):\n        s = str(s)\n    s = s.replace(\" \", \"\")\n    if not s[0].isdigit():\n        s = \"1\" + s\n\n    for i in range(len(s) - 1, -1, -1):\n        if not s[i].isalpha():\n            break\n    index = i + 1\n\n    prefix = s[:index]\n    suffix = s[index:] or default\n    if suffix is False:\n        raise ValueError(f\"Missing time unit: {s}\")\n    if not isinstance(suffix, str):\n        raise TypeError(f\"default must be str or False, got {default!r}\")\n\n    n = float(prefix)\n\n    try:\n        multiplier = timedelta_sizes[suffix.lower()]\n    except KeyError:\n        valid_units = \", \".join(timedelta_sizes.keys())\n        raise KeyError(\n            f\"Invalid time unit: {suffix}. Valid units are: {valid_units}\"\n        ) from None\n\n    result = n * multiplier\n    if int(result) == result:\n        result = int(result)\n    return result\n\n\ndef has_keyword(func, keyword):\n    try:\n        return keyword in inspect.signature(func).parameters\n    except Exception:\n        return False\n\n\ndef ndimlist(seq):\n    if not isinstance(seq, (list, tuple)):\n        return 0\n    elif not seq:\n        return 1\n    else:\n        return 1 + ndimlist(seq[0])\n\n\ndef iter_chunks(sizes, max_size):\n    \"\"\"Split sizes into chunks of total max_size each\n\n    Parameters\n    ----------\n    sizes : iterable of numbers\n        The sizes to be chunked\n    max_size : number\n        Maximum total size per chunk.\n        It must be greater or equal than each size in sizes\n    \"\"\"\n    chunk, chunk_sum = [], 0\n    iter_sizes = iter(sizes)\n    size = next(iter_sizes, None)\n    while size is not None:\n        assert size <= max_size\n        if chunk_sum + size <= max_size:\n            chunk.append(size)\n            chunk_sum += size\n            size = next(iter_sizes, None)\n        else:\n            assert chunk\n            yield chunk\n            chunk, chunk_sum = [], 0\n    if chunk:\n        yield chunk\n\n\nhex_pattern = re.compile(\"[a-f]+\")\n\n\n@functools.lru_cache(100000)\ndef key_split(s):\n    \"\"\"\n    >>> key_split('x')\n    'x'\n    >>> key_split('x-1')\n    'x'\n    >>> key_split('x-1-2-3')\n    'x'\n    >>> key_split(('x-2', 1))\n    'x'\n    >>> key_split(\"('x-2', 1)\")\n    'x'\n    >>> key_split(\"('x', 1)\")\n    'x'\n    >>> key_split('hello-world-1')\n    'hello-world'\n    >>> key_split(b'hello-world-1')\n    'hello-world'\n    >>> key_split('ae05086432ca935f6eba409a8ecd4896')\n    'data'\n    >>> key_split('<module.submodule.myclass object at 0xdaf372')\n    'myclass'\n    >>> key_split(None)\n    'Other'\n    >>> key_split('x-abcdefab')  # ignores hex\n    'x'\n    >>> key_split('_(x)')  # strips unpleasant characters\n    'x'\n    \"\"\"\n    # If we convert the key, recurse to utilize LRU cache better\n    if type(s) is bytes:\n        return key_split(s.decode())\n    if type(s) is tuple:\n        return key_split(s[0])\n    try:\n        words = s.split(\"-\")\n        if not words[0][0].isalpha():\n            result = words[0].split(\",\")[0].strip(\"_'()\\\"\")\n        else:\n            result = words[0]\n        for word in words[1:]:\n            if word.isalpha() and not (\n                len(word) == 8 and hex_pattern.match(word) is not None\n            ):\n                result += \"-\" + word\n            else:\n                break\n        if len(result) == 32 and re.match(r\"[a-f0-9]{32}\", result):\n            return \"data\"\n        else:\n            if result[0] == \"<\":\n                result = result.strip(\"<>\").split()[0].split(\".\")[-1]\n            return sys.intern(result)\n    except Exception:\n        return \"Other\"\n\n\ndef stringify(obj, exclusive: Iterable | None = None):\n    \"\"\"Convert an object to a string\n\n    If ``exclusive`` is specified, search through `obj` and convert\n    values that are in ``exclusive``.\n\n    Note that when searching through dictionaries, only values are\n    converted, not the keys.\n\n    Parameters\n    ----------\n    obj : Any\n        Object (or values within) to convert to string\n    exclusive: Iterable, optional\n        Set of values to search for when converting values to strings\n\n    Returns\n    -------\n    result : type(obj)\n        Stringified copy of ``obj`` or ``obj`` itself if it is already a\n        string or bytes.\n\n    Examples\n    --------\n    >>> stringify(b'x')\n    b'x'\n    >>> stringify('x')\n    'x'\n    >>> stringify({('a',0):('a',0), ('a',1): ('a',1)})\n    \"{('a', 0): ('a', 0), ('a', 1): ('a', 1)}\"\n    >>> stringify({('a',0):('a',0), ('a',1): ('a',1)}, exclusive={('a',0)})\n    {('a', 0): \"('a', 0)\", ('a', 1): ('a', 1)}\n    \"\"\"\n\n    typ = type(obj)\n    if typ is str or typ is bytes:\n        return obj\n    elif exclusive is None:\n        return str(obj)\n\n    if typ is list:\n        return [stringify(v, exclusive) for v in obj]\n    if typ is dict:\n        return {k: stringify(v, exclusive) for k, v in obj.items()}\n    try:\n        if obj in exclusive:\n            return stringify(obj)\n    except TypeError:  # `obj` not hashable\n        pass\n    if typ is tuple:  # If the tuple itself isn't a key, check its elements\n        return tuple(stringify(v, exclusive) for v in obj)\n    return obj\n\n\nclass cached_property(functools.cached_property):\n    \"\"\"Read only version of functools.cached_property.\"\"\"\n\n    def __set__(self, instance, val):\n        \"\"\"Raise an error when attempting to set a cached property.\"\"\"\n        raise AttributeError(\"Can't set attribute\")\n\n\nclass _HashIdWrapper:\n    \"\"\"Hash and compare a wrapped object by identity instead of value\"\"\"\n\n    def __init__(self, wrapped):\n        self.wrapped = wrapped\n\n    def __eq__(self, other):\n        if not isinstance(other, _HashIdWrapper):\n            return NotImplemented\n        return self.wrapped is other.wrapped\n\n    def __ne__(self, other):\n        if not isinstance(other, _HashIdWrapper):\n            return NotImplemented\n        return self.wrapped is not other.wrapped\n\n    def __hash__(self):\n        return id(self.wrapped)\n\n\n@functools.lru_cache\ndef _cumsum(seq, initial_zero):\n    if isinstance(seq, _HashIdWrapper):\n        seq = seq.wrapped\n    if initial_zero:\n        return tuple(toolz.accumulate(add, seq, 0))\n    else:\n        return tuple(toolz.accumulate(add, seq))\n\n\n@functools.lru_cache\ndef _max(seq):\n    if isinstance(seq, _HashIdWrapper):\n        seq = seq.wrapped\n    return max(seq)\n\n\ndef cached_max(seq):\n    \"\"\"Compute max with caching.\n\n    Caching is by the identity of `seq` rather than the value. It is thus\n    important that `seq` is a tuple of immutable objects, and this function\n    is intended for use where `seq` is a value that will persist (generally\n    block sizes).\n\n    Parameters\n    ----------\n    seq : tuple\n        Values to reduce\n\n    Returns\n    -------\n    tuple\n    \"\"\"\n    assert isinstance(seq, tuple)\n    # Look up by identity first, to avoid a linear-time __hash__\n    # if we've seen this tuple object before.\n    result = _max(_HashIdWrapper(seq))\n    return result\n\n\ndef cached_cumsum(seq, initial_zero=False):\n    \"\"\"Compute :meth:`toolz.accumulate` with caching.\n\n    Caching is by the identify of `seq` rather than the value. It is thus\n    important that `seq` is a tuple of immutable objects, and this function\n    is intended for use where `seq` is a value that will persist (generally\n    block sizes).\n\n    Parameters\n    ----------\n    seq : tuple\n        Values to cumulatively sum.\n    initial_zero : bool, optional\n        If true, the return value is prefixed with a zero.\n\n    Returns\n    -------\n    tuple\n    \"\"\"\n    if isinstance(seq, tuple):\n        # Look up by identity first, to avoid a linear-time __hash__\n        # if we've seen this tuple object before.\n        result = _cumsum(_HashIdWrapper(seq), initial_zero)\n    else:\n        # Construct a temporary tuple, and look up by value.\n        result = _cumsum(tuple(seq), initial_zero)\n    return result\n\n\ndef show_versions() -> None:\n    \"\"\"Provide version information for bug reports.\"\"\"\n\n    from json import dumps\n    from platform import uname\n    from sys import stdout, version_info\n\n    from dask._compatibility import importlib_metadata\n\n    try:\n        from distributed import __version__ as distributed_version\n    except ImportError:\n        distributed_version = None\n\n    from dask import __version__ as dask_version\n\n    deps = [\n        \"numpy\",\n        \"pandas\",\n        \"cloudpickle\",\n        \"fsspec\",\n        \"bokeh\",\n        \"pyarrow\",\n        \"zarr\",\n    ]\n\n    result: dict[str, str | None] = {\n        # note: only major, minor, micro are extracted\n        \"Python\": \".\".join([str(i) for i in version_info[:3]]),\n        \"Platform\": uname().system,\n        \"dask\": dask_version,\n        \"distributed\": distributed_version,\n    }\n\n    for modname in deps:\n        try:\n            result[modname] = importlib_metadata.version(modname)\n        except importlib_metadata.PackageNotFoundError:\n            result[modname] = None\n\n    stdout.writelines(dumps(result, indent=2))\n\n    return\n\n\ndef maybe_pluralize(count, noun, plural_form=None):\n    \"\"\"Pluralize a count-noun string pattern when necessary\"\"\"\n    if count == 1:\n        return f\"{count} {noun}\"\n    else:\n        return f\"{count} {plural_form or noun + 's'}\"\n\n\ndef is_namedtuple_instance(obj: Any) -> bool:\n    \"\"\"Returns True if obj is an instance of a namedtuple.\n\n    Note: This function checks for the existence of the methods and\n    attributes that make up the namedtuple API, so it will return True\n    IFF obj's type implements that API.\n    \"\"\"\n    return (\n        isinstance(obj, tuple)\n        and hasattr(obj, \"_make\")\n        and hasattr(obj, \"_asdict\")\n        and hasattr(obj, \"_replace\")\n        and hasattr(obj, \"_fields\")\n        and hasattr(obj, \"_field_defaults\")\n    )\n\n\ndef get_default_shuffle_method() -> str:\n    if d := config.get(\"dataframe.shuffle.method\", None):\n        return d\n    try:\n        from distributed import default_client\n\n        default_client()\n    except (ImportError, ValueError):\n        return \"disk\"\n\n    try:\n        from distributed.shuffle import check_minimal_arrow_version\n\n        check_minimal_arrow_version()\n    except ModuleNotFoundError:\n        return \"tasks\"\n    return \"p2p\"\n\n\ndef get_meta_library(like):\n    if hasattr(like, \"_meta\"):\n        like = like._meta\n\n    return import_module(typename(like).partition(\".\")[0])\n\n\nclass shorten_traceback:\n    \"\"\"Context manager that removes irrelevant stack elements from traceback.\n\n    * omits frames from modules that match `admin.traceback.shorten`\n    * always keeps the first and last frame.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: types.TracebackType | None,\n    ) -> None:\n        if exc_val and exc_tb:\n            exc_val.__traceback__ = self.shorten(exc_tb)\n\n    @staticmethod\n    def shorten(exc_tb: types.TracebackType) -> types.TracebackType:\n        paths = config.get(\"admin.traceback.shorten\")\n        if not paths:\n            return exc_tb\n\n        exp = re.compile(\".*(\" + \"|\".join(paths) + \")\")\n        curr: types.TracebackType | None = exc_tb\n        prev: types.TracebackType | None = None\n\n        while curr:\n            if prev is None:\n                prev = curr  # first frame\n            elif not curr.tb_next:\n                # always keep last frame\n                prev.tb_next = curr\n                prev = prev.tb_next\n            elif not exp.match(curr.tb_frame.f_code.co_filename):\n                # keep if module is not listed in config\n                prev.tb_next = curr\n                prev = curr\n            curr = curr.tb_next\n\n        # Uncomment to remove the first frame, which is something you don't want to keep\n        # if it matches the regexes. Requires Python >=3.11.\n        # if exc_tb.tb_next and exp.match(exc_tb.tb_frame.f_code.co_filename):\n        #     return exc_tb.tb_next\n\n        return exc_tb\n\n\ndef unzip(ls, nout):\n    \"\"\"Unzip a list of lists into ``nout`` outputs.\"\"\"\n    out = list(zip(*ls))\n    if not out:\n        out = [()] * nout\n    return out\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:36:31"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport multiprocessing\nimport pickle\nimport sys\nfrom concurrent.futures import ProcessPoolExecutor\nfrom operator import add\n\nimport pytest\n\nimport dask\nfrom dask import compute, delayed\nfrom dask.multiprocessing import _dumps, _loads, get, get_context, remote_exception\nfrom dask.system import CPU_COUNT\nfrom dask.utils_test import inc\n\n\ndef unrelated_function_global(a):\n    np = pytest.importorskip(\"numpy\")\n    return np.array([a])\n\n\ndef my_small_function_global(a, b):\n    return a + b\n\n\ndef test_pickle_globals():\n    \"\"\"Unrelated globals should not be included in serialized bytes\"\"\"\n    b = _dumps(my_small_function_global)\n    assert b\"my_small_function_global\" in b\n    assert b\"unrelated_function_global\" not in b\n    assert b\"numpy\" not in b\n\n\ndef test_pickle_locals():\n    \"\"\"Unrelated locals should not be included in serialized bytes\"\"\"\n    np = pytest.importorskip(\"numpy\")\n\n    def unrelated_function_local(a):\n        return np.array([a])\n\n    def my_small_function_local(a, b):\n        return a + b\n\n    b = _dumps(my_small_function_local)\n    assert b\"my_small_function_global\" not in b\n    assert b\"my_small_function_local\" in b\n    assert b\"unrelated_function_local\" not in b\n\n\n@pytest.mark.skipif(pickle.HIGHEST_PROTOCOL < 5, reason=\"requires pickle protocol 5\")\ndef test_out_of_band_pickling():\n    \"\"\"Test that out-of-band pickling works\"\"\"\n    np = pytest.importorskip(\"numpy\")\n    pytest.importorskip(\"cloudpickle\", minversion=\"1.3.0\")\n\n    a = np.arange(5)\n\n    l = []\n    b = _dumps(a, buffer_callback=l.append)\n    assert len(l) == 1\n    assert isinstance(l[0], pickle.PickleBuffer)\n    assert memoryview(l[0]) == memoryview(a)\n\n    a2 = _loads(b, buffers=l)\n    assert np.all(a == a2)\n\n\ndef bad():\n    raise ValueError(\"12345\")\n\n\ndef test_errors_propagate():\n    dsk = {\"x\": (bad,)}\n\n    with pytest.raises(ValueError) as e:\n        get(dsk, \"x\")\n    assert \"12345\" in str(e.value)\n\n\ndef test_remote_exception():\n    e = TypeError(\"hello\")\n    a = remote_exception(e, \"traceback-body\")\n    b = remote_exception(e, \"traceback-body\")\n\n    assert type(a) == type(b)\n    assert isinstance(a, TypeError)\n    assert \"hello\" in str(a)\n    assert \"Traceback\" in str(a)\n    assert \"traceback-body\" in str(a)\n\n\ndef test_lambda_with_cloudpickle():\n    dsk = {\"x\": 2, \"y\": (lambda x: x + 1, \"x\")}\n    assert get(dsk, \"y\") == 3\n\n\ndef lambda_result():\n    return lambda x: x + 1\n\n\ndef test_lambda_results_with_cloudpickle():\n    dsk = {\"x\": (lambda_result,)}\n    f = get(dsk, \"x\")\n    assert f(2) == 3\n\n\nclass NotUnpickleable:\n    def __getstate__(self):\n        return ()\n\n    def __setstate__(self, state):\n        raise ValueError(\"Can't unpickle me\")\n\n\ndef test_unpicklable_args_generate_errors():\n    a = NotUnpickleable()\n\n    dsk = {\"x\": (bool, a)}\n\n    with pytest.raises(ValueError):\n        get(dsk, \"x\")\n\n    dsk = {\"x\": (bool, \"a\"), \"a\": a}\n\n    with pytest.raises(ValueError):\n        get(dsk, \"x\")\n\n\n@pytest.mark.parametrize(\"pool_typ\", [multiprocessing.Pool, ProcessPoolExecutor])\ndef test_reuse_pool(pool_typ):\n    with pool_typ(CPU_COUNT) as pool:\n        with dask.config.set(pool=pool):\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n\n\ndef test_dumps_loads():\n    with dask.config.set(func_dumps=pickle.dumps, func_loads=pickle.loads):\n        assert get({\"x\": 1, \"y\": (add, \"x\", 2)}, \"y\") == 3\n\n\ndef test_fuse_doesnt_clobber_intermediates():\n    d = {\"x\": 1, \"y\": (inc, \"x\"), \"z\": (add, 10, \"y\")}\n    assert get(d, [\"y\", \"z\"]) == (2, 12)\n\n\ndef test_optimize_graph_false():\n    from dask.callbacks import Callback\n\n    d = {\"x\": 1, \"y\": (inc, \"x\"), \"z\": (add, 10, \"y\")}\n    keys = []\n    with Callback(pretask=lambda key, *args: keys.append(key)):\n        get(d, \"z\", optimize_graph=False)\n    assert len(keys) == 2\n\n\ndef test_works_with_highlevel_graph():\n    \"\"\"Previously `dask.multiprocessing.get` would accidentally forward\n    `HighLevelGraph` graphs through the dask optimization/scheduling routines,\n    resulting in odd errors. One way to trigger this was to have a\n    non-indexable object in a task. This is just a smoketest to ensure that\n    things work properly even if `HighLevelGraph` objects get passed to\n    `dask.multiprocessing.get`. See https://github.com/dask/dask/issues/7190.\n    \"\"\"\n\n    class NoIndex:\n        def __init__(self, x):\n            self.x = x\n\n        def __getitem__(self, key):\n            raise Exception(\"Oh no!\")\n\n    x = delayed(lambda x: x)(NoIndex(1))\n    (res,) = get(x.dask, x.__dask_keys__())\n    assert isinstance(res, NoIndex)\n    assert res.x == 1\n\n\n@pytest.mark.parametrize(\"random\", [\"numpy\", \"random\"])\ndef test_random_seeds(random):\n    if random == \"numpy\":\n        np = pytest.importorskip(\"numpy\")\n        random = np.random\n    else:\n        import random\n\n    @delayed(pure=False)\n    def f():\n        return tuple(random.randint(0, 10000) for i in range(5))\n\n    N = 10\n    with dask.config.set(scheduler=\"processes\"):\n        (results,) = compute([f() for _ in range(N)])\n\n    assert len(set(results)) == N\n\n\nclass global_:\n    value = 0\n\n\ndef proc_init():\n    global_.value = 1\n\n\n@pytest.mark.parametrize(\n    \"scheduler, initializer, expected_results\",\n    [\n        (\"threading\", None, [1] * 10),\n        (\"processes\", None, [0] * 10),\n        (\"processes\", proc_init, [1] * 10),\n    ],\n)\ndef test_process_initializer(scheduler, initializer, expected_results):\n    @delayed(pure=False)\n    def f():\n        return global_.value\n\n    global_.value = 1\n\n    with dask.config.set(\n        {\"scheduler\": scheduler, \"multiprocessing.initializer\": initializer}\n    ):\n        (results,) = compute([f() for _ in range(10)])\n    assert results == expected_results\n\n    (results2,) = compute(\n        [f() for _ in range(10)],\n        scheduler=scheduler,\n        initializer=initializer,\n    )\n    assert results2 == expected_results\n\n\ndef check_for_pytest():\n    \"\"\"We check for spawn by ensuring subprocess doesn't have modules only\n    parent process should have:\n    \"\"\"\n    import sys\n\n    return \"FAKE_MODULE_FOR_TEST\" in sys.modules\n\n\n@pytest.mark.skipif(\n    sys.platform == \"win32\", reason=\"Windows doesn't support different contexts\"\n)\ndef test_custom_context_used_python3_posix():\n    \"\"\"The 'multiprocessing.context' config is used to create the pool.\n\n    We assume default is 'spawn', and therefore test for 'fork'.\n    \"\"\"\n    # We check for 'fork' by ensuring subprocess doesn't have modules only\n    # parent process should have:\n\n    def check_for_pytest():\n        import sys\n\n        return \"FAKE_MODULE_FOR_TEST\" in sys.modules\n\n    import sys\n\n    sys.modules[\"FAKE_MODULE_FOR_TEST\"] = 1\n    try:\n        with dask.config.set({\"multiprocessing.context\": \"fork\"}):\n            result = get({\"x\": (check_for_pytest,)}, \"x\")\n        assert result\n    finally:\n        del sys.modules[\"FAKE_MODULE_FOR_TEST\"]\n\n\n@pytest.mark.skipif(\n    sys.platform == \"win32\", reason=\"Windows doesn't support different contexts\"\n)\ndef test_get_context_using_python3_posix():\n    \"\"\"get_context() respects configuration.\n\n    If default context is changed this test will need to change too.\n    \"\"\"\n    assert get_context() is multiprocessing.get_context(\"spawn\")\n    with dask.config.set({\"multiprocessing.context\": \"forkserver\"}):\n        assert get_context() is multiprocessing.get_context(\"forkserver\")\n    with dask.config.set({\"multiprocessing.context\": \"fork\"}):\n        assert get_context() is multiprocessing.get_context(\"fork\")\n\n\n@pytest.mark.skipif(sys.platform != \"win32\", reason=\"POSIX supports different contexts\")\ndef test_custom_context_ignored_elsewhere():\n    \"\"\"On Windows, setting 'multiprocessing.context' doesn't explode.\n\n    Presumption is it's not used since it's unsupported, but mostly we care about\n    not breaking anything.\n    \"\"\"\n    assert get({\"x\": (inc, 1)}, \"x\") == 2\n    with pytest.warns(UserWarning):\n        with dask.config.set({\"multiprocessing.context\": \"forkserver\"}):\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n\n\n@pytest.mark.skipif(sys.platform != \"win32\", reason=\"POSIX supports different contexts\")\ndef test_get_context_always_default():\n    \"\"\"On Python 2/Windows, get_context() always returns same context.\"\"\"\n    assert get_context() is multiprocessing\n    with pytest.warns(UserWarning):\n        with dask.config.set({\"multiprocessing.context\": \"forkserver\"}):\n            assert get_context() is multiprocessing\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_rewrite\\test_rewrite.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections import deque\n\nfrom dask.core import istask, subs\n\n\ndef head(task):\n    \"\"\"Return the top level node of a task\"\"\"\n\n    if istask(task):\n        return task[0]\n    elif isinstance(task, list):\n        return list\n    else:\n        return task\n\n\ndef args(task):\n    \"\"\"Get the arguments for the current task\"\"\"\n\n    if istask(task):\n        return task[1:]\n    elif isinstance(task, list):\n        return task\n    else:\n        return ()\n\n\nclass Traverser:\n    \"\"\"Traverser interface for tasks.\n\n    Class for storing the state while performing a preorder-traversal of a\n    task.\n\n    Parameters\n    ----------\n    term : task\n        The task to be traversed\n\n    Attributes\n    ----------\n    term\n        The current element in the traversal\n    current\n        The head of the current element in the traversal. This is simply `head`\n        applied to the attribute `term`.\n    \"\"\"\n\n    def __init__(self, term, stack=None):\n        self.term = term\n        if not stack:\n            self._stack = deque([END])\n        else:\n            self._stack = stack\n\n    def __iter__(self):\n        while self.current is not END:\n            yield self.current\n            self.next()\n\n    def copy(self):\n        \"\"\"Copy the traverser in its current state.\n\n        This allows the traversal to be pushed onto a stack, for easy\n        backtracking.\"\"\"\n\n        return Traverser(self.term, deque(self._stack))\n\n    def next(self):\n        \"\"\"Proceed to the next term in the preorder traversal.\"\"\"\n\n        subterms = args(self.term)\n        if not subterms:\n            # No subterms, pop off stack\n            self.term = self._stack.pop()\n        else:\n            self.term = subterms[0]\n            self._stack.extend(reversed(subterms[1:]))\n\n    @property\n    def current(self):\n        return head(self.term)\n\n    def skip(self):\n        \"\"\"Skip over all subterms of the current level in the traversal\"\"\"\n        self.term = self._stack.pop()\n\n\nclass Token:\n    \"\"\"A token object.\n\n    Used to express certain objects in the traversal of a task or pattern.\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n\n    def __repr__(self):\n        return self.name\n\n\n# A variable to represent *all* variables in a discrimination net\nVAR = Token(\"?\")\n# Represents the end of the traversal of an expression. We can't use `None`,\n# 'False', etc... here, as anything may be an argument to a function.\nEND = Token(\"end\")\n\n\nclass Node(tuple):\n    \"\"\"A Discrimination Net node.\"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, edges=None, patterns=None):\n        edges = edges if edges else {}\n        patterns = patterns if patterns else []\n        return tuple.__new__(cls, (edges, patterns))\n\n    @property\n    def edges(self):\n        \"\"\"A dictionary, where the keys are edges, and the values are nodes\"\"\"\n        return self[0]\n\n    @property\n    def patterns(self):\n        \"\"\"A list of all patterns that currently match at this node\"\"\"\n        return self[1]\n\n\nclass RewriteRule:\n    \"\"\"A rewrite rule.\n\n    Expresses `lhs` -> `rhs`, for variables `vars`.\n\n    Parameters\n    ----------\n    lhs : task\n        The left-hand-side of the rewrite rule.\n    rhs : task or function\n        The right-hand-side of the rewrite rule. If it's a task, variables in\n        `rhs` will be replaced by terms in the subject that match the variables\n        in `lhs`. If it's a function, the function will be called with a dict\n        of such matches.\n    vars: tuple, optional\n        Tuple of variables found in the lhs. Variables can be represented as\n        any hashable object; a good convention is to use strings. If there are\n        no variables, this can be omitted.\n\n    Examples\n    --------\n    Here's a `RewriteRule` to replace all nested calls to `list`, so that\n    `(list, (list, 'x'))` is replaced with `(list, 'x')`, where `'x'` is a\n    variable.\n\n    >>> import dask.rewrite as dr\n    >>> lhs = (list, (list, 'x'))\n    >>> rhs = (list, 'x')\n    >>> variables = ('x',)\n    >>> rule = dr.RewriteRule(lhs, rhs, variables)\n\n    Here's a more complicated rule that uses a callable right-hand-side. A\n    callable `rhs` takes in a dictionary mapping variables to their matching\n    values. This rule replaces all occurrences of `(list, 'x')` with `'x'` if\n    `'x'` is a list itself.\n\n    >>> lhs = (list, 'x')\n    >>> def repl_list(sd):\n    ...     x = sd['x']\n    ...     if isinstance(x, list):\n    ...         return x\n    ...     else:\n    ...         return (list, x)\n    >>> rule = dr.RewriteRule(lhs, repl_list, variables)\n    \"\"\"\n\n    def __init__(self, lhs, rhs, vars=()):\n        if not isinstance(vars, tuple):\n            raise TypeError(\"vars must be a tuple of variables\")\n        self.lhs = lhs\n        if callable(rhs):\n            self.subs = rhs\n        else:\n            self.subs = self._apply\n        self.rhs = rhs\n        self._varlist = [t for t in Traverser(lhs) if t in vars]\n        # Reduce vars down to just variables found in lhs\n        self.vars = tuple(sorted(set(self._varlist)))\n\n    def _apply(self, sub_dict):\n        term = self.rhs\n        for key, val in sub_dict.items():\n            term = subs(term, key, val)\n        return term\n\n    def __str__(self):\n        return f\"RewriteRule({self.lhs}, {self.rhs}, {self.vars})\"\n\n    def __repr__(self):\n        return str(self)\n\n\nclass RuleSet:\n    \"\"\"A set of rewrite rules.\n\n    Forms a structure for fast rewriting over a set of rewrite rules. This\n    allows for syntactic matching of terms to patterns for many patterns at\n    the same time.\n\n    Examples\n    --------\n\n    >>> import dask.rewrite as dr\n    >>> def f(*args): pass\n    >>> def g(*args): pass\n    >>> def h(*args): pass\n    >>> from operator import add\n\n    >>> rs = dr.RuleSet(\n    ...         dr.RewriteRule((add, 'x', 0), 'x', ('x',)),\n    ...         dr.RewriteRule((f, (g, 'x'), 'y'),\n    ...                        (h, 'x', 'y'),\n    ...                        ('x', 'y')))\n\n    >>> rs.rewrite((add, 2, 0))\n    2\n\n    >>> rs.rewrite((f, (g, 'a', 3)))    # doctest: +ELLIPSIS\n    (<function h at ...>, 'a', 3)\n\n    >>> dsk = {'a': (add, 2, 0),\n    ...        'b': (f, (g, 'a', 3))}\n\n    >>> from toolz import valmap\n    >>> valmap(rs.rewrite, dsk)         # doctest: +ELLIPSIS\n    {'a': 2, 'b': (<function h at ...>, 'a', 3)}\n\n    Attributes\n    ----------\n    rules : list\n        A list of `RewriteRule`s included in the `RuleSet`.\n    \"\"\"\n\n    def __init__(self, *rules):\n        \"\"\"Create a `RuleSet` for a number of rules\n\n        Parameters\n        ----------\n        rules\n            One or more instances of RewriteRule\n        \"\"\"\n        self._net = Node()\n        self.rules = []\n        for p in rules:\n            self.add(p)\n\n    def add(self, rule):\n        \"\"\"Add a rule to the RuleSet.\n\n        Parameters\n        ----------\n        rule : RewriteRule\n        \"\"\"\n\n        if not isinstance(rule, RewriteRule):\n            raise TypeError(\"rule must be instance of RewriteRule\")\n        vars = rule.vars\n        curr_node = self._net\n        ind = len(self.rules)\n        # List of variables, in order they appear in the POT of the term\n        for t in Traverser(rule.lhs):\n            prev_node = curr_node\n            if t in vars:\n                t = VAR\n            if t in curr_node.edges:\n                curr_node = curr_node.edges[t]\n            else:\n                curr_node.edges[t] = Node()\n                curr_node = curr_node.edges[t]\n        # We've reached a leaf node. Add the term index to this leaf.\n        prev_node.edges[t].patterns.append(ind)\n        self.rules.append(rule)\n\n    def iter_matches(self, term):\n        \"\"\"A generator that lazily finds matchings for term from the RuleSet.\n\n        Parameters\n        ----------\n        term : task\n\n        Yields\n        ------\n        Tuples of `(rule, subs)`, where `rule` is the rewrite rule being\n        matched, and `subs` is a dictionary mapping the variables in the lhs\n        of the rule to their matching values in the term.\"\"\"\n\n        S = Traverser(term)\n        for m, syms in _match(S, self._net):\n            for i in m:\n                rule = self.rules[i]\n                subs = _process_match(rule, syms)\n                if subs is not None:\n                    yield rule, subs\n\n    def _rewrite(self, term):\n        \"\"\"Apply the rewrite rules in RuleSet to top level of term\"\"\"\n\n        for rule, sd in self.iter_matches(term):\n            # We use for (...) because it's fast in all cases for getting the\n            # first element from the match iterator. As we only want that\n            # element, we break here\n            term = rule.subs(sd)\n            break\n        return term\n\n    def rewrite(self, task, strategy=\"bottom_up\"):\n        \"\"\"Apply the `RuleSet` to `task`.\n\n        This applies the most specific matching rule in the RuleSet to the\n        task, using the provided strategy.\n\n        Parameters\n        ----------\n        term: a task\n            The task to be rewritten\n        strategy: str, optional\n            The rewriting strategy to use. Options are \"bottom_up\" (default),\n            or \"top_level\".\n\n        Examples\n        --------\n        Suppose there was a function `add` that returned the sum of 2 numbers,\n        and another function `double` that returned twice its input:\n\n        >>> add = lambda x, y: x + y\n        >>> double = lambda x: 2*x\n\n        Now suppose `double` was *significantly* faster than `add`, so\n        you'd like to replace all expressions `(add, x, x)` with `(double,\n        x)`, where `x` is a variable. This can be expressed as a rewrite rule:\n\n        >>> rule = RewriteRule((add, 'x', 'x'), (double, 'x'), ('x',))\n        >>> rs = RuleSet(rule)\n\n        This can then be applied to terms to perform the rewriting:\n\n        >>> term = (add, (add, 2, 2), (add, 2, 2))\n        >>> rs.rewrite(term)  # doctest: +SKIP\n        (double, (double, 2))\n\n        If we only wanted to apply this to the top level of the term, the\n        `strategy` kwarg can be set to \"top_level\".\n\n        >>> rs.rewrite(term)  # doctest: +SKIP\n        (double, (add, 2, 2))\n        \"\"\"\n        return strategies[strategy](self, task)\n\n\ndef _top_level(net, term):\n    return net._rewrite(term)\n\n\ndef _bottom_up(net, term):\n    if istask(term):\n        term = (head(term),) + tuple(_bottom_up(net, t) for t in args(term))\n    elif isinstance(term, list):\n        term = [_bottom_up(net, t) for t in args(term)]\n    return net._rewrite(term)\n\n\nstrategies = {\"top_level\": _top_level, \"bottom_up\": _bottom_up}\n\n\ndef _match(S, N):\n    \"\"\"Structural matching of term S to discrimination net node N.\"\"\"\n\n    stack = deque()\n    restore_state_flag = False\n    # matches are stored in a tuple, because all mutations result in a copy,\n    # preventing operations from changing matches stored on the stack.\n    matches = ()\n    while True:\n        if S.current is END:\n            yield N.patterns, matches\n        try:\n            # This try-except block is to catch hashing errors from un-hashable\n            # types. This allows for variables to be matched with un-hashable\n            # objects.\n            n = N.edges.get(S.current, None)\n            if n and not restore_state_flag:\n                stack.append((S.copy(), N, matches))\n                N = n\n                S.next()\n                continue\n        except TypeError:\n            pass\n        n = N.edges.get(VAR, None)\n        if n:\n            restore_state_flag = False\n            matches = matches + (S.term,)\n            S.skip()\n            N = n\n            continue\n        try:\n            # Backtrack here\n            (S, N, matches) = stack.pop()\n            restore_state_flag = True\n        except Exception:\n            return\n\n\ndef _process_match(rule, syms):\n    \"\"\"Process a match to determine if it is correct, and to find the correct\n    substitution that will convert the term into the pattern.\n\n    Parameters\n    ----------\n    rule : RewriteRule\n    syms : iterable\n        Iterable of subterms that match a corresponding variable.\n\n    Returns\n    -------\n    A dictionary of {vars : subterms} describing the substitution to make the\n    pattern equivalent with the term. Returns `None` if the match is\n    invalid.\"\"\"\n\n    subs = {}\n    varlist = rule._varlist\n    if not len(varlist) == len(syms):\n        raise RuntimeError(\"length of varlist doesn't match length of syms.\")\n    for v, s in zip(varlist, syms):\n        if v in subs and subs[v] != s:\n            return None\n        else:\n            subs[v] = s\n    return subs\n\n### Source File Dependency Files Content\n### Dependency File: core.py\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom collections.abc import Collection, Iterable, Mapping, MutableMapping\nfrom typing import Any, Literal, TypeVar, cast, overload\n\nimport toolz\n\nfrom dask._task_spec import DependenciesMapping, convert_legacy_graph, execute_graph\nfrom dask.typing import Graph, Key, NoDefault, no_default\n\n\ndef ishashable(x):\n    \"\"\"Is x hashable?\n\n    Examples\n    --------\n\n    >>> ishashable(1)\n    True\n    >>> ishashable([1])\n    False\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    try:\n        hash(x)\n        return True\n    except TypeError:\n        return False\n\n\ndef istask(x):\n    \"\"\"Is x a runnable task?\n\n    A task is a tuple with a callable first argument\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> istask((inc, 1))\n    True\n    >>> istask(1)\n    False\n    \"\"\"\n    from dask._task_spec import DataNode, GraphNode\n\n    if isinstance(x, GraphNode):\n        if isinstance(x, DataNode):\n            return False\n        return True\n    return type(x) is tuple and x and callable(x[0])\n\n\ndef preorder_traversal(task):\n    \"\"\"A generator to preorder-traverse a task.\"\"\"\n\n    for item in task:\n        if istask(item):\n            yield from preorder_traversal(item)\n        elif isinstance(item, list):\n            yield list\n            yield from preorder_traversal(item)\n        else:\n            yield item\n\n\ndef lists_to_tuples(res, keys):\n    if isinstance(keys, list):\n        return tuple(lists_to_tuples(r, k) for r, k in zip(res, keys))\n    return res\n\n\ndef _pack_result(result: Mapping, keys: list | Key) -> Any:\n    if isinstance(keys, list):\n        return tuple(_pack_result(result, k) for k in keys)\n    return result[keys]\n\n\ndef get(dsk: Mapping, out: list | Key, cache: MutableMapping | None = None) -> Any:\n    \"\"\"Get value from Dask\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': 1, 'y': (inc, 'x')}\n\n    >>> get(d, 'x')\n    1\n    >>> get(d, 'y')\n    2\n    \"\"\"\n    for k in flatten(out):\n        if k not in dsk:\n            raise KeyError(f\"{k} is not a key in the graph\")\n    if cache is None:\n        cache = {}\n\n    dsk2 = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    result = execute_graph(dsk2, cache, keys=set(flatten([out])))\n    return _pack_result(result, out)\n\n\ndef keys_in_tasks(keys: Collection[Key], tasks: Iterable[Any], as_list: bool = False):\n    \"\"\"Returns the keys in `keys` that are also in `tasks`\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> keys_in_tasks(dsk, ['x', 'y', 'j'])  # doctest: +SKIP\n    {'x', 'y'}\n    \"\"\"\n    from dask._task_spec import GraphNode\n\n    ret: list[Key] = []\n    while tasks:\n        work = []\n        for w in tasks:\n            typ = type(w)\n            if typ is tuple and w and callable(w[0]):  # istask(w)\n                work.extend(w[1:])\n            elif typ is list:\n                work.extend(w)\n            elif typ is dict:\n                work.extend(w.values())\n            elif isinstance(w, GraphNode):\n                work.extend(w.dependencies)\n            else:\n                try:\n                    if w in keys:\n                        ret.append(w)\n                except TypeError:  # not hashable\n                    pass\n        tasks = work\n    return ret if as_list else set(ret)\n\n\ndef iskey(key: object) -> bool:\n    \"\"\"Return True if the given object is a potential dask key; False otherwise.\n\n    The definition of a key in a Dask graph is any str, bytes, int, float, or tuple\n    thereof.\n\n    See Also\n    --------\n    ishashable\n    validate_key\n    dask.typing.Key\n    \"\"\"\n    typ = type(key)\n    if typ is tuple:\n        return all(iskey(i) for i in cast(tuple, key))\n    return typ in {bytes, int, float, str}\n\n\ndef validate_key(key: object) -> None:\n    \"\"\"Validate the format of a dask key.\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    if iskey(key):\n        return\n    typ = type(key)\n\n    if typ is tuple:\n        index = None\n        try:\n            for index, part in enumerate(cast(tuple, key)):  # noqa: B007\n                validate_key(part)\n        except TypeError as e:\n            raise TypeError(\n                f\"Composite key contains unexpected key type at {index=} ({key=!r})\"\n            ) from e\n    raise TypeError(f\"Unexpected key type {typ} ({key=!r})\")\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = ...,\n    task: Key | NoDefault = ...,\n    as_list: Literal[False] = ...,\n) -> set[Key]: ...\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None,\n    task: Key | NoDefault,\n    as_list: Literal[True],\n) -> list[Key]: ...\n\n\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = None,\n    task: Key | NoDefault = no_default,\n    as_list: bool = False,\n) -> set[Key] | list[Key]:\n    \"\"\"Get the immediate tasks on which this task depends\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> get_dependencies(dsk, 'x')\n    set()\n\n    >>> get_dependencies(dsk, 'y')\n    {'x'}\n\n    >>> get_dependencies(dsk, 'z')  # doctest: +SKIP\n    {'x', 'y'}\n\n    >>> get_dependencies(dsk, 'w')  # Only direct dependencies\n    {'z'}\n\n    >>> get_dependencies(dsk, 'a')  # Ignore non-keys\n    {'x'}\n\n    >>> get_dependencies(dsk, task=(inc, 'x'))  # provide tasks directly\n    {'x'}\n    \"\"\"\n    if key is not None:\n        arg = dsk[key]\n    elif task is not no_default:\n        arg = task\n    else:\n        raise ValueError(\"Provide either key or task\")\n\n    return keys_in_tasks(dsk, [arg], as_list=as_list)\n\n\ndef get_deps(dsk: Graph) -> tuple[dict[Key, set[Key]], dict[Key, set[Key]]]:\n    \"\"\"Get dependencies and dependents from dask dask graph\n\n    >>> inc = lambda x: x + 1\n    >>> dsk = {'a': 1, 'b': (inc, 'a'), 'c': (inc, 'b')}\n    >>> dependencies, dependents = get_deps(dsk)\n    >>> dependencies\n    {'a': set(), 'b': {'a'}, 'c': {'b'}}\n    >>> dependents  # doctest: +SKIP\n    {'a': {'b'}, 'b': {'c'}, 'c': set()}\n    \"\"\"\n    dependencies = {k: get_dependencies(dsk, task=v) for k, v in dsk.items()}\n    dependents = reverse_dict(dependencies)\n    return dependencies, dependents\n\n\ndef flatten(seq, container=list):\n    \"\"\"\n\n    >>> list(flatten([1]))\n    [1]\n\n    >>> list(flatten([[1, 2], [1, 2]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten([[[1], [2]], [[1], [2]]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten(((1, 2), (1, 2)))) # Don't flatten tuples\n    [(1, 2), (1, 2)]\n\n    >>> list(flatten((1, 2, [3, 4]))) # support heterogeneous\n    [1, 2, 3, 4]\n    \"\"\"\n    if isinstance(seq, str):\n        yield seq\n    else:\n        for item in seq:\n            if isinstance(item, container):\n                yield from flatten(item, container=container)\n            else:\n                yield item\n\n\nT_ = TypeVar(\"T_\")\n\n\ndef reverse_dict(d: Mapping[T_, Iterable[T_]]) -> dict[T_, set[T_]]:\n    \"\"\"\n\n    >>> a, b, c = 'abc'\n    >>> d = {a: [b, c], b: [c]}\n    >>> reverse_dict(d)  # doctest: +SKIP\n    {'a': set([]), 'b': set(['a']}, 'c': set(['a', 'b'])}\n    \"\"\"\n    result: defaultdict[T_, set[T_]] = defaultdict(set)\n    _add = set.add\n    for k, vals in d.items():\n        result[k]\n        for val in vals:\n            _add(result[val], k)\n    return dict(result)\n\n\ndef subs(task, key, val):\n    \"\"\"Perform a substitution on a task\n\n    Examples\n    --------\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> subs((inc, 'x'), 'x', 1)  # doctest: +ELLIPSIS\n    (<function inc at ...>, 1)\n    \"\"\"\n    type_task = type(task)\n    if not (type_task is tuple and task and callable(task[0])):  # istask(task):\n        try:\n            if type_task is type(key) and task == key:\n                return val\n        except Exception:\n            pass\n        if type_task is list:\n            return [subs(x, key, val) for x in task]\n        return task\n    newargs = []\n    hash_key = {key}\n    for arg in task[1:]:\n        type_arg = type(arg)\n        if type_arg is tuple and arg and callable(arg[0]):  # istask(task):\n            arg = subs(arg, key, val)\n        elif type_arg is list:\n            arg = [subs(x, key, val) for x in arg]\n        else:\n            try:\n                if arg in hash_key:  # Hash and equality match\n                    arg = val\n            except TypeError:  # not hashable\n                pass\n        newargs.append(arg)\n    return task[:1] + tuple(newargs)\n\n\ndef _toposort(dsk, keys=None, returncycle=False, dependencies=None):\n\n    # Stack-based depth-first search traversal.  This is based on Tarjan's\n    # method for topological sorting (see wikipedia for pseudocode)\n    if keys is None:\n        keys = dsk\n    elif not isinstance(keys, list):\n        keys = [keys]\n    if not returncycle:\n        ordered = []\n\n    # Nodes whose descendents have been completely explored.\n    # These nodes are guaranteed to not be part of a cycle.\n    completed = set()\n\n    # All nodes that have been visited in the current traversal.  Because\n    # we are doing depth-first search, going \"deeper\" should never result\n    # in visiting a node that has already been seen.  The `seen` and\n    # `completed` sets are mutually exclusive; it is okay to visit a node\n    # that has already been added to `completed`.\n    seen = set()\n\n    if dependencies is None:\n\n        dependencies = DependenciesMapping(dsk)\n\n    for key in keys:\n        if key in completed:\n            continue\n        nodes = [key]\n        while nodes:\n            # Keep current node on the stack until all descendants are visited\n            cur = nodes[-1]\n            if cur in completed:\n                # Already fully traversed descendants of cur\n                nodes.pop()\n                continue\n            seen.add(cur)\n\n            # Add direct descendants of cur to nodes stack\n            next_nodes = []\n            for nxt in dependencies[cur]:\n                if nxt not in completed:\n                    if nxt in seen:\n                        # Cycle detected!\n                        # Let's report only the nodes that directly participate in the cycle.\n                        # We use `priorities` below to greedily construct a short cycle.\n                        # Shorter cycles may exist.\n                        priorities = {}\n                        prev = nodes[-1]\n                        # Give priority to nodes that were seen earlier.\n                        while nodes[-1] != nxt:\n                            priorities[nodes.pop()] = -len(priorities)\n                        priorities[nxt] = -len(priorities)\n                        # We're going to get the cycle by walking backwards along dependents,\n                        # so calculate dependents only for the nodes in play.\n                        inplay = set(priorities)\n                        dependents = reverse_dict(\n                            {k: inplay.intersection(dependencies[k]) for k in inplay}\n                        )\n                        # Begin with the node that was seen twice and the node `prev` from\n                        # which we detected the cycle.\n                        cycle = [nodes.pop()]\n                        cycle.append(prev)\n                        while prev != cycle[0]:\n                            # Greedily take a step that takes us closest to completing the cycle.\n                            # This may not give us the shortest cycle, but we get *a* short cycle.\n                            deps = dependents[cycle[-1]]\n                            prev = min(deps, key=priorities.__getitem__)\n                            cycle.append(prev)\n                        cycle.reverse()\n\n                        if returncycle:\n                            return cycle\n                        else:\n                            cycle = \"->\".join(str(x) for x in cycle)\n                            raise RuntimeError(\"Cycle detected in Dask: %s\" % cycle)\n                    next_nodes.append(nxt)\n\n            if next_nodes:\n                nodes.extend(next_nodes)\n            else:\n                # cur has no more descendants to explore, so we're done with it\n                if not returncycle:\n                    ordered.append(cur)\n                completed.add(cur)\n                seen.remove(cur)\n                nodes.pop()\n    if returncycle:\n        return []\n    return ordered\n\n\ndef toposort(dsk, dependencies=None):\n    \"\"\"Return a list of keys of dask sorted in topological order.\"\"\"\n    return _toposort(dsk, dependencies=dependencies)\n\n\ndef getcycle(d, keys):\n    \"\"\"Return a list of nodes that form a cycle if Dask is not a DAG.\n\n    Returns an empty list if no cycle is found.\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': (inc, 'z'), 'y': (inc, 'x'), 'z': (inc, 'y')}\n    >>> getcycle(d, 'x')\n    ['x', 'z', 'y', 'x']\n\n    See Also\n    --------\n    isdag\n    \"\"\"\n    return _toposort(d, keys=keys, returncycle=True)\n\n\ndef isdag(d, keys):\n    \"\"\"Does Dask form a directed acyclic graph when calculating keys?\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> inc = lambda x: x + 1\n    >>> isdag({'x': 0, 'y': (inc, 'x')}, 'y')\n    True\n    >>> isdag({'x': (inc, 'y'), 'y': (inc, 'x')}, 'y')\n    False\n\n    See Also\n    --------\n    getcycle\n    \"\"\"\n    return not getcycle(d, keys)\n\n\nclass literal:\n    \"\"\"A small serializable object to wrap literal values without copying\"\"\"\n\n    __slots__ = (\"data\",)\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return \"literal<type=%s>\" % type(self.data).__name__\n\n    def __reduce__(self):\n        return (literal, (self.data,))\n\n    def __call__(self):\n        return self.data\n\n\ndef quote(x):\n    \"\"\"Ensure that this value remains this value in a dask graph\n\n    Some values in dask graph take on special meaning. Sometimes we want to\n    ensure that our data is not interpreted but remains literal.\n\n    >>> add = lambda x, y: x + y\n    >>> quote((add, 1, 2))\n    (literal<type=tuple>,)\n    \"\"\"\n    if istask(x) or type(x) is list or type(x) is dict:\n        return (literal(x),)\n    return x\n\n\ndef reshapelist(shape, seq):\n    \"\"\"Reshape iterator to nested shape\n\n    >>> reshapelist((2, 3), range(6))\n    [[0, 1, 2], [3, 4, 5]]\n    \"\"\"\n    if len(shape) == 1:\n        return list(seq)\n    else:\n        n = int(len(seq) / shape[0])\n        return [reshapelist(shape[1:], part) for part in toolz.partition(n, seq)]\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:36:31"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nfrom dask.rewrite import VAR, RewriteRule, RuleSet, Traverser, args, head\nfrom dask.utils_test import add, inc\n\n\ndef double(x):\n    return x * 2\n\n\ndef test_head():\n    assert head((inc, 1)) == inc\n    assert head((add, 1, 2)) == add\n    assert head((add, (inc, 1), (inc, 1))) == add\n    assert head([1, 2, 3]) == list\n\n\ndef test_args():\n    assert args((inc, 1)) == (1,)\n    assert args((add, 1, 2)) == (1, 2)\n    assert args(1) == ()\n    assert args([1, 2, 3]) == [1, 2, 3]\n\n\ndef test_traverser():\n    term = (add, (inc, 1), (double, (inc, 1), 2))\n    t = Traverser(term)\n    t2 = t.copy()\n    assert t.current == add\n    t.next()\n    assert t.current == inc\n    # Ensure copies aren't advanced when the original advances\n    assert t2.current == add\n    t.skip()\n    assert t.current == double\n    t.next()\n    assert t.current == inc\n    assert list(t2) == [add, inc, 1, double, inc, 1, 2]\n\n\nvars = (\"a\", \"b\", \"c\")\n# add(a, 1) -> inc(a)\nrule1 = RewriteRule((add, \"a\", 1), (inc, \"a\"), vars)\n# add(a, a) -> double(a)\nrule2 = RewriteRule((add, \"a\", \"a\"), (double, \"a\"), vars)\n# add(inc(a), inc(a)) -> add(double(a), 2)\nrule3 = RewriteRule((add, (inc, \"a\"), (inc, \"a\")), (add, (double, \"a\"), 2), vars)\n# add(inc(b), inc(a)) -> add(add(a, b), 2)\nrule4 = RewriteRule((add, (inc, \"b\"), (inc, \"a\")), (add, (add, \"a\", \"b\"), 2), vars)\n# sum([c, b, a]) -> add(add(a, b), c)\nrule5 = RewriteRule((sum, [\"c\", \"b\", \"a\"]), (add, (add, \"a\", \"b\"), \"c\"), vars)\n# list(x) -> x if x is a list\n\n\ndef repl_list(sd):\n    x = sd[\"x\"]\n    if isinstance(x, list):\n        return x\n    else:\n        return (list, x)\n\n\nrule6 = RewriteRule((list, \"x\"), repl_list, (\"x\",))\n\n\ndef test_RewriteRule():\n    # Test extraneous vars are removed, varlist is correct\n    assert rule1.vars == (\"a\",)\n    assert rule1._varlist == [\"a\"]\n    assert rule2.vars == (\"a\",)\n    assert rule2._varlist == [\"a\", \"a\"]\n    assert rule3.vars == (\"a\",)\n    assert rule3._varlist == [\"a\", \"a\"]\n    assert rule4.vars == (\"a\", \"b\")\n    assert rule4._varlist == [\"b\", \"a\"]\n    assert rule5.vars == (\"a\", \"b\", \"c\")\n    assert rule5._varlist == [\"c\", \"b\", \"a\"]\n\n\ndef test_RewriteRuleSubs():\n    # Test both rhs substitution and callable rhs\n    assert rule1.subs({\"a\": 1}) == (inc, 1)\n    assert rule6.subs({\"x\": [1, 2, 3]}) == [1, 2, 3]\n\n\nrules = [rule1, rule2, rule3, rule4, rule5, rule6]\nrs = RuleSet(*rules)\n\n\ndef test_RuleSet():\n    net = (\n        {\n            add: (\n                {\n                    VAR: ({VAR: ({}, [1]), 1: ({}, [0])}, []),\n                    inc: ({VAR: ({inc: ({VAR: ({}, [2, 3])}, [])}, [])}, []),\n                },\n                [],\n            ),\n            list: ({VAR: ({}, [5])}, []),\n            sum: ({list: ({VAR: ({VAR: ({VAR: ({}, [4])}, [])}, [])}, [])}, []),\n        },\n        [],\n    )\n    assert rs._net == net\n    assert rs.rules == rules\n\n\ndef test_matches():\n    term = (add, 2, 1)\n    matches = list(rs.iter_matches(term))\n    assert len(matches) == 1\n    assert matches[0] == (rule1, {\"a\": 2})\n    # Test matches specific before general\n    term = (add, 1, 1)\n    matches = list(rs.iter_matches(term))\n    assert len(matches) == 2\n    assert matches[0] == (rule1, {\"a\": 1})\n    assert matches[1] == (rule2, {\"a\": 1})\n    # Test matches unhashable. What it's getting rewritten to doesn't make\n    # sense, this is just to test that it works. :)\n    term = (add, [1], [1])\n    matches = list(rs.iter_matches(term))\n    assert len(matches) == 1\n    assert matches[0] == (rule2, {\"a\": [1]})\n    # Test match at depth\n    term = (add, (inc, 1), (inc, 1))\n    matches = list(rs.iter_matches(term))\n    assert len(matches) == 3\n    assert matches[0] == (rule3, {\"a\": 1})\n    assert matches[1] == (rule4, {\"a\": 1, \"b\": 1})\n    assert matches[2] == (rule2, {\"a\": (inc, 1)})\n    # Test non-linear pattern checking\n    term = (add, 2, 3)\n    matches = list(rs.iter_matches(term))\n    assert len(matches) == 0\n\n\ndef test_rewrite():\n    # Rewrite inside list\n    term = (sum, [(add, 1, 1), (add, 1, 1), (add, 1, 1)])\n    new_term = rs.rewrite(term)\n    assert new_term == (add, (add, (inc, 1), (inc, 1)), (inc, 1))\n    # Rules aren't applied to exhaustion, this can be further simplified\n    new_term = rs.rewrite(new_term)\n    assert new_term == (add, (add, (double, 1), 2), (inc, 1))\n    term = (\n        add,\n        (add, (add, (add, 1, 2), (add, 1, 2)), (add, (add, 1, 2), (add, 1, 2))),\n        1,\n    )\n    assert rs.rewrite(term) == (inc, (double, (double, (add, 1, 2))))\n    # Callable RewriteRule rhs\n    term = (list, [1, 2, 3])\n    assert rs.rewrite(term) == [1, 2, 3]\n    term = (list, (map, inc, [1, 2, 3]))\n    assert rs.rewrite(term) == term\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_sizeof\\test_sizeof.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport itertools\nimport logging\nimport random\nimport sys\nfrom array import array\n\nfrom packaging.version import Version\n\nfrom dask._compatibility import importlib_metadata\nfrom dask.utils import Dispatch\n\nsizeof = Dispatch(name=\"sizeof\")\n\nlogger = logging.getLogger(__name__)\n\n\n@sizeof.register(object)\ndef sizeof_default(o):\n    return sys.getsizeof(o)\n\n\n@sizeof.register(bytes)\n@sizeof.register(bytearray)\ndef sizeof_bytes(o):\n    return len(o)\n\n\n@sizeof.register(memoryview)\ndef sizeof_memoryview(o):\n    return o.nbytes\n\n\n@sizeof.register(array)\ndef sizeof_array(o):\n    return o.itemsize * len(o)\n\n\n@sizeof.register(list)\n@sizeof.register(tuple)\n@sizeof.register(set)\n@sizeof.register(frozenset)\ndef sizeof_python_collection(seq):\n    num_items = len(seq)\n    num_samples = 10\n    if num_items > num_samples:\n        if isinstance(seq, (set, frozenset)):\n            # As of Python v3.9, it is deprecated to call random.sample() on\n            # sets but since sets are unordered anyways we can simply pick\n            # the first `num_samples` items.\n            samples = itertools.islice(seq, num_samples)\n        else:\n            samples = random.sample(seq, num_samples)\n        return sys.getsizeof(seq) + int(\n            num_items / num_samples * sum(map(sizeof, samples))\n        )\n    else:\n        return sys.getsizeof(seq) + sum(map(sizeof, seq))\n\n\nclass SimpleSizeof:\n    \"\"\"Sentinel class to mark a class to be skipped by the dispatcher. This only\n    works if this sentinel mixin is first in the mro.\n\n    Examples\n    --------\n    >>> def _get_gc_overhead():\n    ...     class _CustomObject:\n    ...         def __sizeof__(self):\n    ...             return 0\n    ...\n    ...     return sys.getsizeof(_CustomObject())\n\n    >>> class TheAnswer(SimpleSizeof):\n    ...     def __sizeof__(self):\n    ...         # Sizeof always add overhead of an object for GC\n    ...         return 42 - _get_gc_overhead()\n\n    >>> sizeof(TheAnswer())\n    42\n\n    \"\"\"\n\n\n@sizeof.register(SimpleSizeof)\ndef sizeof_blocked(d):\n    return sys.getsizeof(d)\n\n\n@sizeof.register(dict)\ndef sizeof_python_dict(d):\n    return (\n        sys.getsizeof(d)\n        + sizeof(list(d.keys()))\n        + sizeof(list(d.values()))\n        - 2 * sizeof(list())\n    )\n\n\n@sizeof.register_lazy(\"cupy\")\ndef register_cupy():\n    import cupy\n\n    @sizeof.register(cupy.ndarray)\n    def sizeof_cupy_ndarray(x):\n        return int(x.nbytes)\n\n\n@sizeof.register_lazy(\"numba\")\ndef register_numba():\n    import numba.cuda\n\n    @sizeof.register(numba.cuda.cudadrv.devicearray.DeviceNDArray)\n    def sizeof_numba_devicendarray(x):\n        return int(x.nbytes)\n\n\n@sizeof.register_lazy(\"rmm\")\ndef register_rmm():\n    import rmm\n\n    # Only included in 0.11.0+\n    if hasattr(rmm, \"DeviceBuffer\"):\n\n        @sizeof.register(rmm.DeviceBuffer)\n        def sizeof_rmm_devicebuffer(x):\n            return int(x.nbytes)\n\n\n@sizeof.register_lazy(\"numpy\")\ndef register_numpy():\n    import numpy as np\n\n    @sizeof.register(np.ndarray)\n    def sizeof_numpy_ndarray(x):\n        if 0 in x.strides:\n            xs = x[tuple(slice(None) if s != 0 else slice(1) for s in x.strides)]\n            return xs.nbytes\n        return int(x.nbytes)\n\n\n@sizeof.register_lazy(\"pandas\")\ndef register_pandas():\n    import numpy as np\n    import pandas as pd\n\n    OBJECT_DTYPES = (object, pd.StringDtype(\"python\"))\n\n    def object_size(*xs):\n        if not xs:\n            return 0\n        ncells = sum(len(x) for x in xs)\n        if not ncells:\n            return 0\n\n        # Deduplicate Series of references to the same objects,\n        # e.g. as produced by read_parquet\n        unique_samples = {}\n        for x in xs:\n            sample = np.random.choice(x, size=100, replace=True)\n            for i in sample.tolist():\n                unique_samples[id(i)] = i\n\n        nsamples = 100 * len(xs)\n        sample_nbytes = sum(sizeof(i) for i in unique_samples.values())\n        if len(unique_samples) / nsamples > 0.5:\n            # Less than half of the references are duplicated.\n            # Assume that, if we were to analyze twice the amount of random references,\n            # we would get twice the amount of unique objects too.\n            return int(sample_nbytes * ncells / nsamples)\n        else:\n            # Assume we've already found all unique objects and that all references that\n            # we have not yet analyzed are going to point to the same data.\n            return sample_nbytes\n\n    @sizeof.register(pd.DataFrame)\n    def sizeof_pandas_dataframe(df):\n        p = sizeof(df.index) + sizeof(df.columns)\n        object_cols = []\n        prev_dtype = None\n\n        # Unlike df.items(), df._series will not duplicate multiple views of the same\n        # column e.g. df[[\"x\", \"x\", \"x\"]]\n        for col in df._series.values():\n            if prev_dtype is None or col.dtype != prev_dtype:\n                prev_dtype = col.dtype\n                # Contiguous columns of the same dtype share the same overhead\n                p += 1200\n            p += col.memory_usage(index=False, deep=False)\n            if col.dtype in OBJECT_DTYPES:\n                object_cols.append(col._values)\n\n        # Deduplicate references to the same objects appearing in different Series\n        p += object_size(*object_cols)\n\n        return max(1200, p)\n\n    @sizeof.register(pd.Series)\n    def sizeof_pandas_series(s):\n        # https://github.com/dask/dask/pull/9776#issuecomment-1359085962\n        p = 1200 + sizeof(s.index) + s.memory_usage(index=False, deep=False)\n        if s.dtype in OBJECT_DTYPES:\n            p += object_size(s._values)\n        return p\n\n    @sizeof.register(pd.Index)\n    def sizeof_pandas_index(i):\n        p = 400 + i.memory_usage(deep=False)\n        if i.dtype in OBJECT_DTYPES:\n            p += object_size(i)\n        return p\n\n    @sizeof.register(pd.MultiIndex)\n    def sizeof_pandas_multiindex(i):\n        return sum(sizeof(l) for l in i.levels) + sum(c.nbytes for c in i.codes)\n\n\n@sizeof.register_lazy(\"scipy\")\ndef register_spmatrix():\n    import scipy\n    from scipy import sparse\n\n    if Version(scipy.__version__) < Version(\"1.12.0.dev0\"):\n\n        @sizeof.register(sparse.dok_matrix)\n        def sizeof_spmatrix_dok(s):\n            return s.__sizeof__()\n\n    @sizeof.register(sparse.spmatrix)\n    def sizeof_spmatrix(s):\n        return sum(sizeof(v) for v in s.__dict__.values())\n\n\n@sizeof.register_lazy(\"pyarrow\")\ndef register_pyarrow():\n    import pyarrow as pa\n\n    def _get_col_size(data):\n        p = 0\n        if not isinstance(data, pa.ChunkedArray):\n            data = data.data  # pyarrow <0.15.0\n        for chunk in data.iterchunks():\n            for buffer in chunk.buffers():\n                if buffer:\n                    p += buffer.size\n        return p\n\n    @sizeof.register(pa.Table)\n    def sizeof_pyarrow_table(table):\n        p = sizeof(table.schema.metadata)\n        for col in table.itercolumns():\n            p += _get_col_size(col)\n        return int(p) + 1000\n\n    @sizeof.register(pa.ChunkedArray)\n    def sizeof_pyarrow_chunked_array(data):\n        return int(_get_col_size(data)) + 1000\n\n\n@sizeof.register_lazy(\"xarray\")\ndef register_xarray():\n    import sys\n\n    import xarray as xr\n\n    XARRAY_VERSION = Version(xr.__version__)\n    XARRAY_GE_2024_02 = XARRAY_VERSION >= Version(\"2024.02.0\")\n\n    @sizeof.register(xr.core.utils.Frozen)\n    def xarray_sizeof_frozen(obj):\n        return sys.getsizeof(obj) + sizeof(obj.mapping)\n\n    @sizeof.register(xr.DataArray)\n    @sizeof.register(xr.Variable)\n    def xarray_sizeof_da(obj):\n        return sys.getsizeof(obj) + sizeof(obj.data)\n\n    @sizeof.register(xr.Dataset)\n    def xarray_sizeof_ds(obj):\n        return sys.getsizeof(obj) + sizeof(obj.variables)\n\n    if XARRAY_GE_2024_02:\n        xarray_sizeof_da = sizeof.register(xr.NamedArray)(xarray_sizeof_da)\n\n    @sizeof.register(xr.core.indexes.Indexes)\n    def xarray_sizeof_indexes(obj):\n        return (\n            sys.getsizeof(obj)\n            + sizeof(obj._index_type)\n            + sizeof(obj._indexes)\n            + sizeof(obj._variables)\n            + sizeof(obj._dims)\n        )\n\n    @sizeof.register(xr.core.indexes.PandasIndex)\n    def xarray_sizeof_pd_index(obj):\n        return (\n            sys.getsizeof(obj)\n            + sizeof(obj.index)\n            + sizeof(obj.dim)\n            + sizeof(obj.coord_dtype)\n        )\n\n\ndef _register_entry_point_plugins():\n    \"\"\"Register sizeof implementations exposed by the entry_point mechanism.\"\"\"\n    for entry_point in importlib_metadata.entry_points(group=\"dask.sizeof\"):\n        registrar = entry_point.load()\n        try:\n            registrar(sizeof)\n        except Exception:\n            logger.exception(\n                f\"Failed to register sizeof entry point {entry_point.name}\"\n            )\n\n\n_register_entry_point_plugins()\n\n### Source File Dependency Files Content\n### Dependency File: utils.py\nfrom __future__ import annotations\n\nimport codecs\nimport functools\nimport inspect\nimport os\nimport re\nimport shutil\nimport sys\nimport tempfile\nimport types\nimport uuid\nimport warnings\nfrom collections.abc import Callable, Hashable, Iterable, Iterator, Mapping, Set\nfrom contextlib import contextmanager, nullcontext, suppress\nfrom datetime import datetime, timedelta\nfrom errno import ENOENT\nfrom functools import wraps\nfrom importlib import import_module\nfrom numbers import Integral, Number\nfrom operator import add\nfrom threading import Lock\nfrom typing import Any, ClassVar, Literal, TypeVar, cast, overload\nfrom weakref import WeakValueDictionary\n\nimport tlz as toolz\n\nfrom dask import config\nfrom dask.typing import no_default\n\nK = TypeVar(\"K\")\nV = TypeVar(\"V\")\nT = TypeVar(\"T\")\n\n# used in decorators to preserve the signature of the function it decorates\n# see https://mypy.readthedocs.io/en/stable/generics.html#declaring-decorators\nFuncType = Callable[..., Any]\nF = TypeVar(\"F\", bound=FuncType)\n\nsystem_encoding = sys.getdefaultencoding()\nif system_encoding == \"ascii\":\n    system_encoding = \"utf-8\"\n\n\ndef apply(func, args, kwargs=None):\n    \"\"\"Apply a function given its positional and keyword arguments.\n\n    Equivalent to ``func(*args, **kwargs)``\n    Most Dask users will never need to use the ``apply`` function.\n    It is typically only used by people who need to inject\n    keyword argument values into a low level Dask task graph.\n\n    Parameters\n    ----------\n    func : callable\n        The function you want to apply.\n    args : tuple\n        A tuple containing all the positional arguments needed for ``func``\n        (eg: ``(arg_1, arg_2, arg_3)``)\n    kwargs : dict, optional\n        A dictionary mapping the keyword arguments\n        (eg: ``{\"kwarg_1\": value, \"kwarg_2\": value}``\n\n    Examples\n    --------\n    >>> from dask.utils import apply\n    >>> def add(number, second_number=5):\n    ...     return number + second_number\n    ...\n    >>> apply(add, (10,), {\"second_number\": 2})  # equivalent to add(*args, **kwargs)\n    12\n\n    >>> task = apply(add, (10,), {\"second_number\": 2})\n    >>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\n    \"\"\"\n    if kwargs:\n        return func(*args, **kwargs)\n    else:\n        return func(*args)\n\n\ndef _deprecated(\n    *,\n    version: str | None = None,\n    after_version: str | None = None,\n    message: str | None = None,\n    use_instead: str | None = None,\n    category: type[Warning] = FutureWarning,\n):\n    \"\"\"Decorator to mark a function as deprecated\n\n    Parameters\n    ----------\n    version : str, optional\n        Version of Dask in which the function was deprecated. If specified, the version\n        will be included in the default warning message. This should no longer be used\n        after the introduction of automated versioning system.\n    after_version : str, optional\n        Version of Dask after which the function was deprecated. If specified, the\n        version will be included in the default warning message.\n    message : str, optional\n        Custom warning message to raise.\n    use_instead : str, optional\n        Name of function to use in place of the deprecated function.\n        If specified, this will be included in the default warning\n        message.\n    category : type[Warning], optional\n        Type of warning to raise. Defaults to ``FutureWarning``.\n\n    Examples\n    --------\n\n    >>> from dask.utils import _deprecated\n    >>> @_deprecated(after_version=\"X.Y.Z\", use_instead=\"bar\")\n    ... def foo():\n    ...     return \"baz\"\n    \"\"\"\n\n    def decorator(func):\n        if message is None:\n            msg = f\"{func.__name__} \"\n            if after_version is not None:\n                msg += f\"was deprecated after version {after_version} \"\n            elif version is not None:\n                msg += f\"was deprecated in version {version} \"\n            else:\n                msg += \"is deprecated \"\n            msg += \"and will be removed in a future release.\"\n\n            if use_instead is not None:\n                msg += f\" Please use {use_instead} instead.\"\n        else:\n            msg = message\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(msg, category=category, stacklevel=2)\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\ndef _deprecated_kwarg(\n    old_arg_name: str,\n    new_arg_name: str | None = None,\n    mapping: Mapping[Any, Any] | Callable[[Any], Any] | None = None,\n    stacklevel: int = 2,\n    comment: str | None = None,\n) -> Callable[[F], F]:\n    \"\"\"\n    Decorator to deprecate a keyword argument of a function.\n\n    Parameters\n    ----------\n    old_arg_name : str\n        Name of argument in function to deprecate\n    new_arg_name : str, optional\n        Name of preferred argument in function. Omit to warn that\n        ``old_arg_name`` keyword is deprecated.\n    mapping : dict or callable, optional\n        If mapping is present, use it to translate old arguments to\n        new arguments. A callable must do its own value checking;\n        values not found in a dict will be forwarded unchanged.\n    comment :  str, optional\n        Additional message to deprecation message. Useful to pass\n        on suggestions with the deprecation warning.\n\n    Examples\n    --------\n    The following deprecates 'cols', using 'columns' instead\n\n    >>> @_deprecated_kwarg(old_arg_name='cols', new_arg_name='columns')\n    ... def f(columns=''):\n    ...     print(columns)\n    ...\n    >>> f(columns='should work ok')\n    should work ok\n\n    >>> f(cols='should raise warning')  # doctest: +SKIP\n    FutureWarning: cols is deprecated, use columns instead\n      warnings.warn(msg, FutureWarning)\n    should raise warning\n\n    >>> f(cols='should error', columns=\"can\\'t pass do both\")  # doctest: +SKIP\n    TypeError: Can only specify 'cols' or 'columns', not both\n\n    >>> @_deprecated_kwarg('old', 'new', {'yes': True, 'no': False})\n    ... def f(new=False):\n    ...     print('yes!' if new else 'no!')\n    ...\n    >>> f(old='yes')  # doctest: +SKIP\n    FutureWarning: old='yes' is deprecated, use new=True instead\n      warnings.warn(msg, FutureWarning)\n    yes!\n\n    To raise a warning that a keyword will be removed entirely in the future\n\n    >>> @_deprecated_kwarg(old_arg_name='cols', new_arg_name=None)\n    ... def f(cols='', another_param=''):\n    ...     print(cols)\n    ...\n    >>> f(cols='should raise warning')  # doctest: +SKIP\n    FutureWarning: the 'cols' keyword is deprecated and will be removed in a\n    future version please takes steps to stop use of 'cols'\n    should raise warning\n    >>> f(another_param='should not raise warning')  # doctest: +SKIP\n    should not raise warning\n\n    >>> f(cols='should raise warning', another_param='')  # doctest: +SKIP\n    FutureWarning: the 'cols' keyword is deprecated and will be removed in a\n    future version please takes steps to stop use of 'cols'\n    should raise warning\n    \"\"\"\n    if mapping is not None and not hasattr(mapping, \"get\") and not callable(mapping):\n        raise TypeError(\n            \"mapping from old to new argument values must be dict or callable!\"\n        )\n\n    comment_ = f\"\\n{comment}\" or \"\"\n\n    def _deprecated_kwarg(func: F) -> F:\n        @wraps(func)\n        def wrapper(*args, **kwargs) -> Callable[..., Any]:\n            old_arg_value = kwargs.pop(old_arg_name, no_default)\n\n            if old_arg_value is not no_default:\n                if new_arg_name is None:\n                    msg = (\n                        f\"the {repr(old_arg_name)} keyword is deprecated and \"\n                        \"will be removed in a future version. Please take \"\n                        f\"steps to stop the use of {repr(old_arg_name)}\"\n                    ) + comment_\n                    warnings.warn(msg, FutureWarning, stacklevel=stacklevel)\n                    kwargs[old_arg_name] = old_arg_value\n                    return func(*args, **kwargs)\n\n                elif mapping is not None:\n                    if callable(mapping):\n                        new_arg_value = mapping(old_arg_value)\n                    else:\n                        new_arg_value = mapping.get(old_arg_value, old_arg_value)\n                    msg = (\n                        f\"the {old_arg_name}={repr(old_arg_value)} keyword is \"\n                        \"deprecated, use \"\n                        f\"{new_arg_name}={repr(new_arg_value)} instead.\"\n                    )\n                else:\n                    new_arg_value = old_arg_value\n                    msg = (\n                        f\"the {repr(old_arg_name)} keyword is deprecated, \"\n                        f\"use {repr(new_arg_name)} instead.\"\n                    )\n\n                warnings.warn(msg + comment_, FutureWarning, stacklevel=stacklevel)\n                if kwargs.get(new_arg_name) is not None:\n                    msg = (\n                        f\"Can only specify {repr(old_arg_name)} \"\n                        f\"or {repr(new_arg_name)}, not both.\"\n                    )\n                    raise TypeError(msg)\n                kwargs[new_arg_name] = new_arg_value\n            return func(*args, **kwargs)\n\n        return cast(F, wrapper)\n\n    return _deprecated_kwarg\n\n\ndef deepmap(func, *seqs):\n    \"\"\"Apply function inside nested lists\n\n    >>> inc = lambda x: x + 1\n    >>> deepmap(inc, [[1, 2], [3, 4]])\n    [[2, 3], [4, 5]]\n\n    >>> add = lambda x, y: x + y\n    >>> deepmap(add, [[1, 2], [3, 4]], [[10, 20], [30, 40]])\n    [[11, 22], [33, 44]]\n    \"\"\"\n    if isinstance(seqs[0], (list, Iterator)):\n        return [deepmap(func, *items) for items in zip(*seqs)]\n    else:\n        return func(*seqs)\n\n\n@_deprecated()\ndef homogeneous_deepmap(func, seq):\n    if not seq:\n        return seq\n    n = 0\n    tmp = seq\n    while isinstance(tmp, list):\n        n += 1\n        tmp = tmp[0]\n\n    return ndeepmap(n, func, seq)\n\n\ndef ndeepmap(n, func, seq):\n    \"\"\"Call a function on every element within a nested container\n\n    >>> def inc(x):\n    ...     return x + 1\n    >>> L = [[1, 2], [3, 4, 5]]\n    >>> ndeepmap(2, inc, L)\n    [[2, 3], [4, 5, 6]]\n    \"\"\"\n    if n == 1:\n        return [func(item) for item in seq]\n    elif n > 1:\n        return [ndeepmap(n - 1, func, item) for item in seq]\n    elif isinstance(seq, list):\n        return func(seq[0])\n    else:\n        return func(seq)\n\n\ndef import_required(mod_name, error_msg):\n    \"\"\"Attempt to import a required dependency.\n\n    Raises a RuntimeError if the requested module is not available.\n    \"\"\"\n    try:\n        return import_module(mod_name)\n    except ImportError as e:\n        raise RuntimeError(error_msg) from e\n\n\n@contextmanager\ndef tmpfile(extension=\"\", dir=None):\n    \"\"\"\n    Function to create and return a unique temporary file with the given extension, if provided.\n\n    Parameters\n    ----------\n    extension : str\n        The extension of the temporary file to be created\n    dir : str\n        If ``dir`` is not None, the file will be created in that directory; otherwise,\n        Python's default temporary directory is used.\n\n    Returns\n    -------\n    out : str\n        Path to the temporary file\n\n    See Also\n    --------\n    NamedTemporaryFile : Built-in alternative for creating temporary files\n    tmp_path : pytest fixture for creating a temporary directory unique to the test invocation\n\n    Notes\n    -----\n    This context manager is particularly useful on Windows for opening temporary files multiple times.\n    \"\"\"\n    extension = extension.lstrip(\".\")\n    if extension:\n        extension = \".\" + extension\n    handle, filename = tempfile.mkstemp(extension, dir=dir)\n    os.close(handle)\n    os.remove(filename)\n\n    try:\n        yield filename\n    finally:\n        if os.path.exists(filename):\n            with suppress(OSError):  # sometimes we can't remove a generated temp file\n                if os.path.isdir(filename):\n                    shutil.rmtree(filename)\n                else:\n                    os.remove(filename)\n\n\n@contextmanager\ndef tmpdir(dir=None):\n    \"\"\"\n    Function to create and return a unique temporary directory.\n\n    Parameters\n    ----------\n    dir : str\n        If ``dir`` is not None, the directory will be created in that directory; otherwise,\n        Python's default temporary directory is used.\n\n    Returns\n    -------\n    out : str\n        Path to the temporary directory\n\n    Notes\n    -----\n    This context manager is particularly useful on Windows for opening temporary directories multiple times.\n    \"\"\"\n    dirname = tempfile.mkdtemp(dir=dir)\n\n    try:\n        yield dirname\n    finally:\n        if os.path.exists(dirname):\n            if os.path.isdir(dirname):\n                with suppress(OSError):\n                    shutil.rmtree(dirname)\n            else:\n                with suppress(OSError):\n                    os.remove(dirname)\n\n\n@contextmanager\ndef filetext(text, extension=\"\", open=open, mode=\"w\"):\n    with tmpfile(extension=extension) as filename:\n        f = open(filename, mode=mode)\n        try:\n            f.write(text)\n        finally:\n            try:\n                f.close()\n            except AttributeError:\n                pass\n\n        yield filename\n\n\n@contextmanager\ndef changed_cwd(new_cwd):\n    old_cwd = os.getcwd()\n    os.chdir(new_cwd)\n    try:\n        yield\n    finally:\n        os.chdir(old_cwd)\n\n\n@contextmanager\ndef tmp_cwd(dir=None):\n    with tmpdir(dir) as dirname:\n        with changed_cwd(dirname):\n            yield dirname\n\n\nclass IndexCallable:\n    \"\"\"Provide getitem syntax for functions\n\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> I = IndexCallable(inc)\n    >>> I[3]\n    4\n    \"\"\"\n\n    __slots__ = (\"fn\",)\n\n    def __init__(self, fn):\n        self.fn = fn\n\n    def __getitem__(self, key):\n        return self.fn(key)\n\n\n@contextmanager\ndef filetexts(d, open=open, mode=\"t\", use_tmpdir=True):\n    \"\"\"Dumps a number of textfiles to disk\n\n    Parameters\n    ----------\n    d : dict\n        a mapping from filename to text like {'a.csv': '1,1\\n2,2'}\n\n    Since this is meant for use in tests, this context manager will\n    automatically switch to a temporary current directory, to avoid\n    race conditions when running tests in parallel.\n    \"\"\"\n    with tmp_cwd() if use_tmpdir else nullcontext():\n        for filename, text in d.items():\n            try:\n                os.makedirs(os.path.dirname(filename))\n            except OSError:\n                pass\n            f = open(filename, \"w\" + mode)\n            try:\n                f.write(text)\n            finally:\n                try:\n                    f.close()\n                except AttributeError:\n                    pass\n\n        yield list(d)\n\n        for filename in d:\n            if os.path.exists(filename):\n                with suppress(OSError):\n                    os.remove(filename)\n\n\ndef concrete(seq):\n    \"\"\"Make nested iterators concrete lists\n\n    >>> data = [[1, 2], [3, 4]]\n    >>> seq = iter(map(iter, data))\n    >>> concrete(seq)\n    [[1, 2], [3, 4]]\n    \"\"\"\n    if isinstance(seq, Iterator):\n        seq = list(seq)\n    if isinstance(seq, (tuple, list)):\n        seq = list(map(concrete, seq))\n    return seq\n\n\ndef pseudorandom(n: int, p, random_state=None):\n    \"\"\"Pseudorandom array of integer indexes\n\n    >>> pseudorandom(5, [0.5, 0.5], random_state=123)\n    array([1, 0, 0, 1, 1], dtype=int8)\n\n    >>> pseudorandom(10, [0.5, 0.2, 0.2, 0.1], random_state=5)\n    array([0, 2, 0, 3, 0, 1, 2, 1, 0, 0], dtype=int8)\n    \"\"\"\n    import numpy as np\n\n    p = list(p)\n    cp = np.cumsum([0] + p)\n    assert np.allclose(1, cp[-1])\n    assert len(p) < 256\n\n    if not isinstance(random_state, np.random.RandomState):\n        random_state = np.random.RandomState(random_state)\n\n    x = random_state.random_sample(n)\n    out = np.empty(n, dtype=\"i1\")\n\n    for i, (low, high) in enumerate(zip(cp[:-1], cp[1:])):\n        out[(x >= low) & (x < high)] = i\n    return out\n\n\ndef random_state_data(n: int, random_state=None) -> list:\n    \"\"\"Return a list of arrays that can initialize\n    ``np.random.RandomState``.\n\n    Parameters\n    ----------\n    n : int\n        Number of arrays to return.\n    random_state : int or np.random.RandomState, optional\n        If an int, is used to seed a new ``RandomState``.\n    \"\"\"\n    import numpy as np\n\n    if not all(\n        hasattr(random_state, attr) for attr in [\"normal\", \"beta\", \"bytes\", \"uniform\"]\n    ):\n        random_state = np.random.RandomState(random_state)\n\n    random_data = random_state.bytes(624 * n * 4)  # `n * 624` 32-bit integers\n    l = list(np.frombuffer(random_data, dtype=\"<u4\").reshape((n, -1)))\n    assert len(l) == n\n    return l\n\n\ndef is_integer(i) -> bool:\n    \"\"\"\n    >>> is_integer(6)\n    True\n    >>> is_integer(42.0)\n    True\n    >>> is_integer('abc')\n    False\n    \"\"\"\n    return isinstance(i, Integral) or (isinstance(i, float) and i.is_integer())\n\n\nONE_ARITY_BUILTINS = {\n    abs,\n    all,\n    any,\n    ascii,\n    bool,\n    bytearray,\n    bytes,\n    callable,\n    chr,\n    classmethod,\n    complex,\n    dict,\n    dir,\n    enumerate,\n    eval,\n    float,\n    format,\n    frozenset,\n    hash,\n    hex,\n    id,\n    int,\n    iter,\n    len,\n    list,\n    max,\n    min,\n    next,\n    oct,\n    open,\n    ord,\n    range,\n    repr,\n    reversed,\n    round,\n    set,\n    slice,\n    sorted,\n    staticmethod,\n    str,\n    sum,\n    tuple,\n    type,\n    vars,\n    zip,\n    memoryview,\n}\nMULTI_ARITY_BUILTINS = {\n    compile,\n    delattr,\n    divmod,\n    filter,\n    getattr,\n    hasattr,\n    isinstance,\n    issubclass,\n    map,\n    pow,\n    setattr,\n}\n\n\ndef getargspec(func):\n    \"\"\"Version of inspect.getargspec that works with partial and warps.\"\"\"\n    if isinstance(func, functools.partial):\n        return getargspec(func.func)\n\n    func = getattr(func, \"__wrapped__\", func)\n    if isinstance(func, type):\n        return inspect.getfullargspec(func.__init__)\n    else:\n        return inspect.getfullargspec(func)\n\n\ndef takes_multiple_arguments(func, varargs=True):\n    \"\"\"Does this function take multiple arguments?\n\n    >>> def f(x, y): pass\n    >>> takes_multiple_arguments(f)\n    True\n\n    >>> def f(x): pass\n    >>> takes_multiple_arguments(f)\n    False\n\n    >>> def f(x, y=None): pass\n    >>> takes_multiple_arguments(f)\n    False\n\n    >>> def f(*args): pass\n    >>> takes_multiple_arguments(f)\n    True\n\n    >>> class Thing:\n    ...     def __init__(self, a): pass\n    >>> takes_multiple_arguments(Thing)\n    False\n\n    \"\"\"\n    if func in ONE_ARITY_BUILTINS:\n        return False\n    elif func in MULTI_ARITY_BUILTINS:\n        return True\n\n    try:\n        spec = getargspec(func)\n    except Exception:\n        return False\n\n    try:\n        is_constructor = spec.args[0] == \"self\" and isinstance(func, type)\n    except Exception:\n        is_constructor = False\n\n    if varargs and spec.varargs:\n        return True\n\n    ndefaults = 0 if spec.defaults is None else len(spec.defaults)\n    return len(spec.args) - ndefaults - is_constructor > 1\n\n\ndef get_named_args(func) -> list[str]:\n    \"\"\"Get all non ``*args/**kwargs`` arguments for a function\"\"\"\n    s = inspect.signature(func)\n    return [\n        n\n        for n, p in s.parameters.items()\n        if p.kind in [p.POSITIONAL_OR_KEYWORD, p.POSITIONAL_ONLY, p.KEYWORD_ONLY]\n    ]\n\n\nclass Dispatch:\n    \"\"\"Simple single dispatch.\"\"\"\n\n    def __init__(self, name=None):\n        self._lookup = {}\n        self._lazy = {}\n        if name:\n            self.__name__ = name\n\n    def register(self, type, func=None):\n        \"\"\"Register dispatch of `func` on arguments of type `type`\"\"\"\n\n        def wrapper(func):\n            if isinstance(type, tuple):\n                for t in type:\n                    self.register(t, func)\n            else:\n                self._lookup[type] = func\n            return func\n\n        return wrapper(func) if func is not None else wrapper\n\n    def register_lazy(self, toplevel, func=None):\n        \"\"\"\n        Register a registration function which will be called if the\n        *toplevel* module (e.g. 'pandas') is ever loaded.\n        \"\"\"\n\n        def wrapper(func):\n            self._lazy[toplevel] = func\n            return func\n\n        return wrapper(func) if func is not None else wrapper\n\n    def dispatch(self, cls):\n        \"\"\"Return the function implementation for the given ``cls``\"\"\"\n        lk = self._lookup\n        for cls2 in cls.__mro__:\n            # Is a lazy registration function present?\n            toplevel, _, _ = cls2.__module__.partition(\".\")\n            try:\n                register = self._lazy[toplevel]\n            except KeyError:\n                pass\n            else:\n                register()\n                self._lazy.pop(toplevel, None)\n                return self.dispatch(cls)  # recurse\n            try:\n                impl = lk[cls2]\n            except KeyError:\n                pass\n            else:\n                if cls is not cls2:\n                    # Cache lookup\n                    lk[cls] = impl\n                return impl\n        raise TypeError(f\"No dispatch for {cls}\")\n\n    def __call__(self, arg, *args, **kwargs):\n        \"\"\"\n        Call the corresponding method based on type of argument.\n        \"\"\"\n        meth = self.dispatch(type(arg))\n        return meth(arg, *args, **kwargs)\n\n    @property\n    def __doc__(self):\n        try:\n            func = self.dispatch(object)\n            return func.__doc__\n        except TypeError:\n            return \"Single Dispatch for %s\" % self.__name__\n\n\ndef ensure_not_exists(filename) -> None:\n    \"\"\"\n    Ensure that a file does not exist.\n    \"\"\"\n    try:\n        os.unlink(filename)\n    except OSError as e:\n        if e.errno != ENOENT:\n            raise\n\n\ndef _skip_doctest(line):\n    # NumPy docstring contains cursor and comment only example\n    stripped = line.strip()\n    if stripped == \">>>\" or stripped.startswith(\">>> #\"):\n        return line\n    elif \">>>\" in stripped and \"+SKIP\" not in stripped:\n        if \"# doctest:\" in line:\n            return line + \", +SKIP\"\n        else:\n            return line + \"  # doctest: +SKIP\"\n    else:\n        return line\n\n\ndef skip_doctest(doc):\n    if doc is None:\n        return \"\"\n    return \"\\n\".join([_skip_doctest(line) for line in doc.split(\"\\n\")])\n\n\ndef extra_titles(doc):\n    lines = doc.split(\"\\n\")\n    titles = {\n        i: lines[i].strip()\n        for i in range(len(lines) - 1)\n        if lines[i + 1].strip() and all(c == \"-\" for c in lines[i + 1].strip())\n    }\n\n    seen = set()\n    for i, title in sorted(titles.items()):\n        if title in seen:\n            new_title = \"Extra \" + title\n            lines[i] = lines[i].replace(title, new_title)\n            lines[i + 1] = lines[i + 1].replace(\"-\" * len(title), \"-\" * len(new_title))\n        else:\n            seen.add(title)\n\n    return \"\\n\".join(lines)\n\n\ndef ignore_warning(doc, cls, name, extra=\"\", skipblocks=0, inconsistencies=None):\n    \"\"\"Expand docstring by adding disclaimer and extra text\"\"\"\n    import inspect\n\n    if inspect.isclass(cls):\n        l1 = \"This docstring was copied from {}.{}.{}.\\n\\n\".format(\n            cls.__module__,\n            cls.__name__,\n            name,\n        )\n    else:\n        l1 = f\"This docstring was copied from {cls.__name__}.{name}.\\n\\n\"\n    l2 = \"Some inconsistencies with the Dask version may exist.\"\n\n    i = doc.find(\"\\n\\n\")\n    if i != -1:\n        # Insert our warning\n        head = doc[: i + 2]\n        tail = doc[i + 2 :]\n        while skipblocks > 0:\n            i = tail.find(\"\\n\\n\")\n            head = tail[: i + 2]\n            tail = tail[i + 2 :]\n            skipblocks -= 1\n        # Indentation of next line\n        indent = re.match(r\"\\s*\", tail).group(0)\n        # Insert the warning, indented, with a blank line before and after\n        if extra:\n            more = [indent, extra.rstrip(\"\\n\") + \"\\n\\n\"]\n        else:\n            more = []\n        if inconsistencies is not None:\n            l3 = f\"Known inconsistencies: \\n {inconsistencies}\"\n            bits = [head, indent, l1, l2, \"\\n\\n\", l3, \"\\n\\n\"] + more + [tail]\n        else:\n            bits = [head, indent, l1, indent, l2, \"\\n\\n\"] + more + [tail]\n        doc = \"\".join(bits)\n\n    return doc\n\n\ndef unsupported_arguments(doc, args):\n    \"\"\"Mark unsupported arguments with a disclaimer\"\"\"\n    lines = doc.split(\"\\n\")\n    for arg in args:\n        subset = [\n            (i, line)\n            for i, line in enumerate(lines)\n            if re.match(r\"^\\s*\" + arg + \" ?:\", line)\n        ]\n        if len(subset) == 1:\n            [(i, line)] = subset\n            lines[i] = line + \"  (Not supported in Dask)\"\n    return \"\\n\".join(lines)\n\n\ndef _derived_from(\n    cls, method, ua_args=None, extra=\"\", skipblocks=0, inconsistencies=None\n):\n    \"\"\"Helper function for derived_from to ease testing\"\"\"\n    ua_args = ua_args or []\n\n    # do not use wraps here, as it hides keyword arguments displayed\n    # in the doc\n    original_method = getattr(cls, method.__name__)\n\n    doc = getattr(original_method, \"__doc__\", None)\n\n    if isinstance(original_method, property):\n        # some things like SeriesGroupBy.unique are generated.\n        original_method = original_method.fget\n        if not doc:\n            doc = getattr(original_method, \"__doc__\", None)\n\n    if isinstance(original_method, functools.cached_property):\n        original_method = original_method.func\n        if not doc:\n            doc = getattr(original_method, \"__doc__\", None)\n\n    if doc is None:\n        doc = \"\"\n\n    # pandas DataFrame/Series sometimes override methods without setting __doc__\n    if not doc and cls.__name__ in {\"DataFrame\", \"Series\"}:\n        for obj in cls.mro():\n            obj_method = getattr(obj, method.__name__, None)\n            if obj_method is not None and obj_method.__doc__:\n                doc = obj_method.__doc__\n                break\n\n    # Insert disclaimer that this is a copied docstring\n    if doc:\n        doc = ignore_warning(\n            doc,\n            cls,\n            method.__name__,\n            extra=extra,\n            skipblocks=skipblocks,\n            inconsistencies=inconsistencies,\n        )\n    elif extra:\n        doc += extra.rstrip(\"\\n\") + \"\\n\\n\"\n\n    # Mark unsupported arguments\n    try:\n        method_args = get_named_args(method)\n        original_args = get_named_args(original_method)\n        not_supported = [m for m in original_args if m not in method_args]\n    except ValueError:\n        not_supported = []\n    if len(ua_args) > 0:\n        not_supported.extend(ua_args)\n    if len(not_supported) > 0:\n        doc = unsupported_arguments(doc, not_supported)\n\n    doc = skip_doctest(doc)\n    doc = extra_titles(doc)\n\n    return doc\n\n\ndef derived_from(\n    original_klass, version=None, ua_args=None, skipblocks=0, inconsistencies=None\n):\n    \"\"\"Decorator to attach original class's docstring to the wrapped method.\n\n    The output structure will be: top line of docstring, disclaimer about this\n    being auto-derived, any extra text associated with the method being patched,\n    the body of the docstring and finally, the list of keywords that exist in\n    the original method but not in the dask version.\n\n    Parameters\n    ----------\n    original_klass: type\n        Original class which the method is derived from\n    version : str\n        Original package version which supports the wrapped method\n    ua_args : list\n        List of keywords which Dask doesn't support. Keywords existing in\n        original but not in Dask will automatically be added.\n    skipblocks : int\n        How many text blocks (paragraphs) to skip from the start of the\n        docstring. Useful for cases where the target has extra front-matter.\n    inconsistencies: list\n        List of known inconsistencies with method whose docstrings are being\n        copied.\n    \"\"\"\n    ua_args = ua_args or []\n\n    def wrapper(method):\n        try:\n            extra = getattr(method, \"__doc__\", None) or \"\"\n            method.__doc__ = _derived_from(\n                original_klass,\n                method,\n                ua_args=ua_args,\n                extra=extra,\n                skipblocks=skipblocks,\n                inconsistencies=inconsistencies,\n            )\n            return method\n\n        except AttributeError:\n            module_name = original_klass.__module__.split(\".\")[0]\n\n            @functools.wraps(method)\n            def wrapped(*args, **kwargs):\n                msg = f\"Base package doesn't support '{method.__name__}'.\"\n                if version is not None:\n                    msg2 = \" Use {0} {1} or later to use this method.\"\n                    msg += msg2.format(module_name, version)\n                raise NotImplementedError(msg)\n\n            return wrapped\n\n    return wrapper\n\n\ndef funcname(func) -> str:\n    \"\"\"Get the name of a function.\"\"\"\n    # functools.partial\n    if isinstance(func, functools.partial):\n        return funcname(func.func)\n    # methodcaller\n    if isinstance(func, methodcaller):\n        return func.method[:50]\n\n    module_name = getattr(func, \"__module__\", None) or \"\"\n    type_name = getattr(type(func), \"__name__\", None) or \"\"\n\n    # toolz.curry\n    if \"toolz\" in module_name and \"curry\" == type_name:\n        return func.func_name[:50]\n    # multipledispatch objects\n    if \"multipledispatch\" in module_name and \"Dispatcher\" == type_name:\n        return func.name[:50]\n    # numpy.vectorize objects\n    if \"numpy\" in module_name and \"vectorize\" == type_name:\n        return (\"vectorize_\" + funcname(func.pyfunc))[:50]\n\n    # All other callables\n    try:\n        name = func.__name__\n        if name == \"<lambda>\":\n            return \"lambda\"\n        return name[:50]\n    except AttributeError:\n        return str(func)[:50]\n\n\ndef typename(typ: Any, short: bool = False) -> str:\n    \"\"\"\n    Return the name of a type\n\n    Examples\n    --------\n    >>> typename(int)\n    'int'\n\n    >>> from dask.core import literal\n    >>> typename(literal)\n    'dask.core.literal'\n    >>> typename(literal, short=True)\n    'dask.literal'\n    \"\"\"\n    if not isinstance(typ, type):\n        return typename(type(typ))\n    try:\n        if not typ.__module__ or typ.__module__ == \"builtins\":\n            return typ.__name__\n        else:\n            if short:\n                module, *_ = typ.__module__.split(\".\")\n            else:\n                module = typ.__module__\n            return module + \".\" + typ.__name__\n    except AttributeError:\n        return str(typ)\n\n\ndef ensure_bytes(s) -> bytes:\n    \"\"\"Attempt to turn `s` into bytes.\n\n    Parameters\n    ----------\n    s : Any\n        The object to be converted. Will correctly handled\n        * str\n        * bytes\n        * objects implementing the buffer protocol (memoryview, ndarray, etc.)\n\n    Returns\n    -------\n    b : bytes\n\n    Raises\n    ------\n    TypeError\n        When `s` cannot be converted\n\n    Examples\n    --------\n    >>> ensure_bytes('123')\n    b'123'\n    >>> ensure_bytes(b'123')\n    b'123'\n    >>> ensure_bytes(bytearray(b'123'))\n    b'123'\n    \"\"\"\n    if isinstance(s, bytes):\n        return s\n    elif hasattr(s, \"encode\"):\n        return s.encode()\n    else:\n        try:\n            return bytes(s)\n        except Exception as e:\n            raise TypeError(\n                f\"Object {s} is neither a bytes object nor can be encoded to bytes\"\n            ) from e\n\n\ndef ensure_unicode(s) -> str:\n    \"\"\"Turn string or bytes to string\n\n    >>> ensure_unicode('123')\n    '123'\n    >>> ensure_unicode(b'123')\n    '123'\n    \"\"\"\n    if isinstance(s, str):\n        return s\n    elif hasattr(s, \"decode\"):\n        return s.decode()\n    else:\n        try:\n            return codecs.decode(s)\n        except Exception as e:\n            raise TypeError(\n                f\"Object {s} is neither a str object nor can be decoded to str\"\n            ) from e\n\n\ndef digit(n, k, base):\n    \"\"\"\n\n    >>> digit(1234, 0, 10)\n    4\n    >>> digit(1234, 1, 10)\n    3\n    >>> digit(1234, 2, 10)\n    2\n    >>> digit(1234, 3, 10)\n    1\n    \"\"\"\n    return n // base**k % base\n\n\ndef insert(tup, loc, val):\n    \"\"\"\n\n    >>> insert(('a', 'b', 'c'), 0, 'x')\n    ('x', 'b', 'c')\n    \"\"\"\n    L = list(tup)\n    L[loc] = val\n    return tuple(L)\n\n\ndef memory_repr(num):\n    for x in [\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n        if num < 1024.0:\n            return f\"{num:3.1f} {x}\"\n        num /= 1024.0\n\n\ndef asciitable(columns, rows):\n    \"\"\"Formats an ascii table for given columns and rows.\n\n    Parameters\n    ----------\n    columns : list\n        The column names\n    rows : list of tuples\n        The rows in the table. Each tuple must be the same length as\n        ``columns``.\n    \"\"\"\n    rows = [tuple(str(i) for i in r) for r in rows]\n    columns = tuple(str(i) for i in columns)\n    widths = tuple(max(max(map(len, x)), len(c)) for x, c in zip(zip(*rows), columns))\n    row_template = (\"|\" + (\" %%-%ds |\" * len(columns))) % widths\n    header = row_template % tuple(columns)\n    bar = \"+%s+\" % \"+\".join(\"-\" * (w + 2) for w in widths)\n    data = \"\\n\".join(row_template % r for r in rows)\n    return \"\\n\".join([bar, header, bar, data, bar])\n\n\ndef put_lines(buf, lines):\n    if any(not isinstance(x, str) for x in lines):\n        lines = [str(x) for x in lines]\n    buf.write(\"\\n\".join(lines))\n\n\n_method_cache: dict[str, methodcaller] = {}\n\n\nclass methodcaller:\n    \"\"\"\n    Return a callable object that calls the given method on its operand.\n\n    Unlike the builtin `operator.methodcaller`, instances of this class are\n    cached and arguments are passed at call time instead of build time.\n    \"\"\"\n\n    __slots__ = (\"method\",)\n    method: str\n\n    @property\n    def func(self) -> str:\n        # For `funcname` to work\n        return self.method\n\n    def __new__(cls, method: str):\n        try:\n            return _method_cache[method]\n        except KeyError:\n            self = object.__new__(cls)\n            self.method = method\n            _method_cache[method] = self\n            return self\n\n    def __call__(self, __obj, *args, **kwargs):\n        return getattr(__obj, self.method)(*args, **kwargs)\n\n    def __reduce__(self):\n        return (methodcaller, (self.method,))\n\n    def __str__(self):\n        return f\"<{self.__class__.__name__}: {self.method}>\"\n\n    __repr__ = __str__\n\n\nclass itemgetter:\n    \"\"\"Variant of operator.itemgetter that supports equality tests\"\"\"\n\n    __slots__ = (\"index\",)\n\n    def __init__(self, index):\n        self.index = index\n\n    def __call__(self, x):\n        return x[self.index]\n\n    def __reduce__(self):\n        return (itemgetter, (self.index,))\n\n    def __eq__(self, other):\n        return type(self) is type(other) and self.index == other.index\n\n\nclass MethodCache:\n    \"\"\"Attribute access on this object returns a methodcaller for that\n    attribute.\n\n    Examples\n    --------\n    >>> a = [1, 3, 3]\n    >>> M.count(a, 3) == a.count(3)\n    True\n    \"\"\"\n\n    def __getattr__(self, item):\n        return methodcaller(item)\n\n    def __dir__(self):\n        return list(_method_cache)\n\n\nM = MethodCache()\n\n\nclass SerializableLock:\n    \"\"\"A Serializable per-process Lock\n\n    This wraps a normal ``threading.Lock`` object and satisfies the same\n    interface.  However, this lock can also be serialized and sent to different\n    processes.  It will not block concurrent operations between processes (for\n    this you should look at ``multiprocessing.Lock`` or ``locket.lock_file``\n    but will consistently deserialize into the same lock.\n\n    So if we make a lock in one process::\n\n        lock = SerializableLock()\n\n    And then send it over to another process multiple times::\n\n        bytes = pickle.dumps(lock)\n        a = pickle.loads(bytes)\n        b = pickle.loads(bytes)\n\n    Then the deserialized objects will operate as though they were the same\n    lock, and collide as appropriate.\n\n    This is useful for consistently protecting resources on a per-process\n    level.\n\n    The creation of locks is itself not threadsafe.\n    \"\"\"\n\n    _locks: ClassVar[WeakValueDictionary[Hashable, Lock]] = WeakValueDictionary()\n    token: Hashable\n    lock: Lock\n\n    def __init__(self, token: Hashable | None = None):\n        self.token = token or str(uuid.uuid4())\n        if self.token in SerializableLock._locks:\n            self.lock = SerializableLock._locks[self.token]\n        else:\n            self.lock = Lock()\n            SerializableLock._locks[self.token] = self.lock\n\n    def acquire(self, *args, **kwargs):\n        return self.lock.acquire(*args, **kwargs)\n\n    def release(self, *args, **kwargs):\n        return self.lock.release(*args, **kwargs)\n\n    def __enter__(self):\n        self.lock.__enter__()\n\n    def __exit__(self, *args):\n        self.lock.__exit__(*args)\n\n    def locked(self):\n        return self.lock.locked()\n\n    def __getstate__(self):\n        return self.token\n\n    def __setstate__(self, token):\n        self.__init__(token)\n\n    def __str__(self):\n        return f\"<{self.__class__.__name__}: {self.token}>\"\n\n    __repr__ = __str__\n\n\ndef get_scheduler_lock(collection=None, scheduler=None):\n    \"\"\"Get an instance of the appropriate lock for a certain situation based on\n    scheduler used.\"\"\"\n    from dask import multiprocessing\n    from dask.base import get_scheduler\n\n    actual_get = get_scheduler(collections=[collection], scheduler=scheduler)\n\n    if actual_get == multiprocessing.get:\n        return multiprocessing.get_context().Manager().Lock()\n    else:\n        # if this is a distributed client, we need to lock on\n        # the level between processes, SerializableLock won't work\n        try:\n            import distributed.lock\n            from distributed.worker import get_client\n\n            client = get_client()\n        except (ImportError, ValueError):\n            pass\n        else:\n            if actual_get == client.get:\n                return distributed.lock.Lock()\n\n    return SerializableLock()\n\n\ndef ensure_dict(d: Mapping[K, V], *, copy: bool = False) -> dict[K, V]:\n    \"\"\"Convert a generic Mapping into a dict.\n    Optimize use case of :class:`~dask.highlevelgraph.HighLevelGraph`.\n\n    Parameters\n    ----------\n    d : Mapping\n    copy : bool\n        If True, guarantee that the return value is always a shallow copy of d;\n        otherwise it may be the input itself.\n    \"\"\"\n    if type(d) is dict:\n        return d.copy() if copy else d\n    try:\n        layers = d.layers  # type: ignore\n    except AttributeError:\n        return dict(d)\n\n    result = {}\n    for layer in toolz.unique(layers.values(), key=id):\n        result.update(layer)\n    return result\n\n\ndef ensure_set(s: Set[T], *, copy: bool = False) -> set[T]:\n    \"\"\"Convert a generic Set into a set.\n\n    Parameters\n    ----------\n    s : Set\n    copy : bool\n        If True, guarantee that the return value is always a shallow copy of s;\n        otherwise it may be the input itself.\n    \"\"\"\n    if type(s) is set:\n        return s.copy() if copy else s\n    return set(s)\n\n\nclass OperatorMethodMixin:\n    \"\"\"A mixin for dynamically implementing operators\"\"\"\n\n    __slots__ = ()\n\n    @classmethod\n    def _bind_operator(cls, op):\n        \"\"\"bind operator to this class\"\"\"\n        name = op.__name__\n\n        if name.endswith(\"_\"):\n            # for and_ and or_\n            name = name[:-1]\n        elif name == \"inv\":\n            name = \"invert\"\n\n        meth = f\"__{name}__\"\n\n        if name in (\"abs\", \"invert\", \"neg\", \"pos\"):\n            setattr(cls, meth, cls._get_unary_operator(op))\n        else:\n            setattr(cls, meth, cls._get_binary_operator(op))\n\n            if name in (\"eq\", \"gt\", \"ge\", \"lt\", \"le\", \"ne\", \"getitem\"):\n                return\n\n            rmeth = f\"__r{name}__\"\n            setattr(cls, rmeth, cls._get_binary_operator(op, inv=True))\n\n    @classmethod\n    def _get_unary_operator(cls, op):\n        \"\"\"Must return a method used by unary operator\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def _get_binary_operator(cls, op, inv=False):\n        \"\"\"Must return a method used by binary operator\"\"\"\n        raise NotImplementedError\n\n\ndef partial_by_order(*args, **kwargs):\n    \"\"\"\n\n    >>> from operator import add\n    >>> partial_by_order(5, function=add, other=[(1, 10)])\n    15\n    \"\"\"\n    function = kwargs.pop(\"function\")\n    other = kwargs.pop(\"other\")\n    args2 = list(args)\n    for i, arg in other:\n        args2.insert(i, arg)\n    return function(*args2, **kwargs)\n\n\ndef is_arraylike(x) -> bool:\n    \"\"\"Is this object a numpy array or something similar?\n\n    This function tests specifically for an object that already has\n    array attributes (e.g. np.ndarray, dask.array.Array, cupy.ndarray,\n    sparse.COO), **NOT** for something that can be coerced into an\n    array object (e.g. Python lists and tuples). It is meant for dask\n    developers and developers of downstream libraries.\n\n    Note that this function does not correspond with NumPy's\n    definition of array_like, which includes any object that can be\n    coerced into an array (see definition in the NumPy glossary):\n    https://numpy.org/doc/stable/glossary.html\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> is_arraylike(np.ones(5))\n    True\n    >>> is_arraylike(np.ones(()))\n    True\n    >>> is_arraylike(5)\n    False\n    >>> is_arraylike('cat')\n    False\n    \"\"\"\n    from dask.base import is_dask_collection\n\n    is_duck_array = hasattr(x, \"__array_function__\") or hasattr(x, \"__array_ufunc__\")\n\n    return bool(\n        hasattr(x, \"shape\")\n        and isinstance(x.shape, tuple)\n        and hasattr(x, \"dtype\")\n        and not any(is_dask_collection(n) for n in x.shape)\n        # We special case scipy.sparse and cupyx.scipy.sparse arrays as having partial\n        # support for them is useful in scenarios where we mostly call `map_partitions`\n        # or `map_blocks` with scikit-learn functions on dask arrays and dask dataframes.\n        # https://github.com/dask/dask/pull/3738\n        and (is_duck_array or \"scipy.sparse\" in typename(type(x)))\n    )\n\n\ndef is_dataframe_like(df) -> bool:\n    \"\"\"Looks like a Pandas DataFrame\"\"\"\n    if (df.__class__.__module__, df.__class__.__name__) == (\n        \"pandas.core.frame\",\n        \"DataFrame\",\n    ):\n        # fast exec for most likely input\n        return True\n    typ = df.__class__\n    return (\n        all(hasattr(typ, name) for name in (\"groupby\", \"head\", \"merge\", \"mean\"))\n        and all(hasattr(df, name) for name in (\"dtypes\", \"columns\"))\n        and not any(hasattr(typ, name) for name in (\"name\", \"dtype\"))\n    )\n\n\ndef is_series_like(s) -> bool:\n    \"\"\"Looks like a Pandas Series\"\"\"\n    typ = s.__class__\n    return (\n        all(hasattr(typ, name) for name in (\"groupby\", \"head\", \"mean\"))\n        and all(hasattr(s, name) for name in (\"dtype\", \"name\"))\n        and \"index\" not in typ.__name__.lower()\n    )\n\n\ndef is_index_like(s) -> bool:\n    \"\"\"Looks like a Pandas Index\"\"\"\n    typ = s.__class__\n    return (\n        all(hasattr(s, name) for name in (\"name\", \"dtype\"))\n        and \"index\" in typ.__name__.lower()\n    )\n\n\ndef is_cupy_type(x) -> bool:\n    # TODO: avoid explicit reference to CuPy\n    return \"cupy\" in str(type(x))\n\n\ndef natural_sort_key(s: str) -> list[str | int]:\n    \"\"\"\n    Sorting `key` function for performing a natural sort on a collection of\n    strings\n\n    See https://en.wikipedia.org/wiki/Natural_sort_order\n\n    Parameters\n    ----------\n    s : str\n        A string that is an element of the collection being sorted\n\n    Returns\n    -------\n    tuple[str or int]\n        Tuple of the parts of the input string where each part is either a\n        string or an integer\n\n    Examples\n    --------\n    >>> a = ['f0', 'f1', 'f2', 'f8', 'f9', 'f10', 'f11', 'f19', 'f20', 'f21']\n    >>> sorted(a)\n    ['f0', 'f1', 'f10', 'f11', 'f19', 'f2', 'f20', 'f21', 'f8', 'f9']\n    >>> sorted(a, key=natural_sort_key)\n    ['f0', 'f1', 'f2', 'f8', 'f9', 'f10', 'f11', 'f19', 'f20', 'f21']\n    \"\"\"\n    return [int(part) if part.isdigit() else part for part in re.split(r\"(\\d+)\", s)]\n\n\ndef parse_bytes(s: float | str) -> int:\n    \"\"\"Parse byte string to numbers\n\n    >>> from dask.utils import parse_bytes\n    >>> parse_bytes('100')\n    100\n    >>> parse_bytes('100 MB')\n    100000000\n    >>> parse_bytes('100M')\n    100000000\n    >>> parse_bytes('5kB')\n    5000\n    >>> parse_bytes('5.4 kB')\n    5400\n    >>> parse_bytes('1kiB')\n    1024\n    >>> parse_bytes('1e6')\n    1000000\n    >>> parse_bytes('1e6 kB')\n    1000000000\n    >>> parse_bytes('MB')\n    1000000\n    >>> parse_bytes(123)\n    123\n    >>> parse_bytes('5 foos')\n    Traceback (most recent call last):\n        ...\n    ValueError: Could not interpret 'foos' as a byte unit\n    \"\"\"\n    if isinstance(s, (int, float)):\n        return int(s)\n    s = s.replace(\" \", \"\")\n    if not any(char.isdigit() for char in s):\n        s = \"1\" + s\n\n    for i in range(len(s) - 1, -1, -1):\n        if not s[i].isalpha():\n            break\n    index = i + 1\n\n    prefix = s[:index]\n    suffix = s[index:]\n\n    try:\n        n = float(prefix)\n    except ValueError as e:\n        raise ValueError(\"Could not interpret '%s' as a number\" % prefix) from e\n\n    try:\n        multiplier = byte_sizes[suffix.lower()]\n    except KeyError as e:\n        raise ValueError(\"Could not interpret '%s' as a byte unit\" % suffix) from e\n\n    result = n * multiplier\n    return int(result)\n\n\nbyte_sizes = {\n    \"kB\": 10**3,\n    \"MB\": 10**6,\n    \"GB\": 10**9,\n    \"TB\": 10**12,\n    \"PB\": 10**15,\n    \"KiB\": 2**10,\n    \"MiB\": 2**20,\n    \"GiB\": 2**30,\n    \"TiB\": 2**40,\n    \"PiB\": 2**50,\n    \"B\": 1,\n    \"\": 1,\n}\nbyte_sizes = {k.lower(): v for k, v in byte_sizes.items()}\nbyte_sizes.update({k[0]: v for k, v in byte_sizes.items() if k and \"i\" not in k})\nbyte_sizes.update({k[:-1]: v for k, v in byte_sizes.items() if k and \"i\" in k})\n\n\ndef format_time(n: float) -> str:\n    \"\"\"format integers as time\n\n    >>> from dask.utils import format_time\n    >>> format_time(1)\n    '1.00 s'\n    >>> format_time(0.001234)\n    '1.23 ms'\n    >>> format_time(0.00012345)\n    '123.45 us'\n    >>> format_time(123.456)\n    '123.46 s'\n    >>> format_time(1234.567)\n    '20m 34s'\n    >>> format_time(12345.67)\n    '3hr 25m'\n    >>> format_time(123456.78)\n    '34hr 17m'\n    >>> format_time(1234567.89)\n    '14d 6hr'\n    \"\"\"\n    if n > 24 * 60 * 60 * 2:\n        d = int(n / 3600 / 24)\n        h = int((n - d * 3600 * 24) / 3600)\n        return f\"{d}d {h}hr\"\n    if n > 60 * 60 * 2:\n        h = int(n / 3600)\n        m = int((n - h * 3600) / 60)\n        return f\"{h}hr {m}m\"\n    if n > 60 * 10:\n        m = int(n / 60)\n        s = int(n - m * 60)\n        return f\"{m}m {s}s\"\n    if n >= 1:\n        return \"%.2f s\" % n\n    if n >= 1e-3:\n        return \"%.2f ms\" % (n * 1e3)\n    return \"%.2f us\" % (n * 1e6)\n\n\ndef format_time_ago(n: datetime) -> str:\n    \"\"\"Calculate a '3 hours ago' type string from a Python datetime.\n\n    Examples\n    --------\n    >>> from datetime import datetime, timedelta\n\n    >>> now = datetime.now()\n    >>> format_time_ago(now)\n    'Just now'\n\n    >>> past = datetime.now() - timedelta(minutes=1)\n    >>> format_time_ago(past)\n    '1 minute ago'\n\n    >>> past = datetime.now() - timedelta(minutes=2)\n    >>> format_time_ago(past)\n    '2 minutes ago'\n\n    >>> past = datetime.now() - timedelta(hours=1)\n    >>> format_time_ago(past)\n    '1 hour ago'\n\n    >>> past = datetime.now() - timedelta(hours=6)\n    >>> format_time_ago(past)\n    '6 hours ago'\n\n    >>> past = datetime.now() - timedelta(days=1)\n    >>> format_time_ago(past)\n    '1 day ago'\n\n    >>> past = datetime.now() - timedelta(days=5)\n    >>> format_time_ago(past)\n    '5 days ago'\n\n    >>> past = datetime.now() - timedelta(days=8)\n    >>> format_time_ago(past)\n    '1 week ago'\n\n    >>> past = datetime.now() - timedelta(days=16)\n    >>> format_time_ago(past)\n    '2 weeks ago'\n\n    >>> past = datetime.now() - timedelta(days=190)\n    >>> format_time_ago(past)\n    '6 months ago'\n\n    >>> past = datetime.now() - timedelta(days=800)\n    >>> format_time_ago(past)\n    '2 years ago'\n\n    \"\"\"\n    units = {\n        \"years\": lambda diff: diff.days / 365,\n        \"months\": lambda diff: diff.days / 30.436875,  # Average days per month\n        \"weeks\": lambda diff: diff.days / 7,\n        \"days\": lambda diff: diff.days,\n        \"hours\": lambda diff: diff.seconds / 3600,\n        \"minutes\": lambda diff: diff.seconds % 3600 / 60,\n    }\n    diff = datetime.now() - n\n    for unit in units:\n        dur = int(units[unit](diff))\n        if dur > 0:\n            if dur == 1:  # De-pluralize\n                unit = unit[:-1]\n            return f\"{dur} {unit} ago\"\n    return \"Just now\"\n\n\ndef format_bytes(n: int) -> str:\n    \"\"\"Format bytes as text\n\n    >>> from dask.utils import format_bytes\n    >>> format_bytes(1)\n    '1 B'\n    >>> format_bytes(1234)\n    '1.21 kiB'\n    >>> format_bytes(12345678)\n    '11.77 MiB'\n    >>> format_bytes(1234567890)\n    '1.15 GiB'\n    >>> format_bytes(1234567890000)\n    '1.12 TiB'\n    >>> format_bytes(1234567890000000)\n    '1.10 PiB'\n\n    For all values < 2**60, the output is always <= 10 characters.\n    \"\"\"\n    for prefix, k in (\n        (\"Pi\", 2**50),\n        (\"Ti\", 2**40),\n        (\"Gi\", 2**30),\n        (\"Mi\", 2**20),\n        (\"ki\", 2**10),\n    ):\n        if n >= k * 0.9:\n            return f\"{n / k:.2f} {prefix}B\"\n    return f\"{n} B\"\n\n\ntimedelta_sizes = {\n    \"s\": 1,\n    \"ms\": 1e-3,\n    \"us\": 1e-6,\n    \"ns\": 1e-9,\n    \"m\": 60,\n    \"h\": 3600,\n    \"d\": 3600 * 24,\n    \"w\": 7 * 3600 * 24,\n}\n\ntds2 = {\n    \"second\": 1,\n    \"minute\": 60,\n    \"hour\": 60 * 60,\n    \"day\": 60 * 60 * 24,\n    \"week\": 7 * 60 * 60 * 24,\n    \"millisecond\": 1e-3,\n    \"microsecond\": 1e-6,\n    \"nanosecond\": 1e-9,\n}\ntds2.update({k + \"s\": v for k, v in tds2.items()})\ntimedelta_sizes.update(tds2)\ntimedelta_sizes.update({k.upper(): v for k, v in timedelta_sizes.items()})\n\n\n@overload\ndef parse_timedelta(s: None, default: str | Literal[False] = \"seconds\") -> None: ...\n\n\n@overload\ndef parse_timedelta(\n    s: str | float | timedelta, default: str | Literal[False] = \"seconds\"\n) -> float: ...\n\n\ndef parse_timedelta(s, default=\"seconds\"):\n    \"\"\"Parse timedelta string to number of seconds\n\n    Parameters\n    ----------\n    s : str, float, timedelta, or None\n    default: str or False, optional\n        Unit of measure if s  does not specify one. Defaults to seconds.\n        Set to False to require s to explicitly specify its own unit.\n\n    Examples\n    --------\n    >>> from datetime import timedelta\n    >>> from dask.utils import parse_timedelta\n    >>> parse_timedelta('3s')\n    3\n    >>> parse_timedelta('3.5 seconds')\n    3.5\n    >>> parse_timedelta('300ms')\n    0.3\n    >>> parse_timedelta(timedelta(seconds=3))  # also supports timedeltas\n    3\n    \"\"\"\n    if s is None:\n        return None\n    if isinstance(s, timedelta):\n        s = s.total_seconds()\n        return int(s) if int(s) == s else s\n    if isinstance(s, Number):\n        s = str(s)\n    s = s.replace(\" \", \"\")\n    if not s[0].isdigit():\n        s = \"1\" + s\n\n    for i in range(len(s) - 1, -1, -1):\n        if not s[i].isalpha():\n            break\n    index = i + 1\n\n    prefix = s[:index]\n    suffix = s[index:] or default\n    if suffix is False:\n        raise ValueError(f\"Missing time unit: {s}\")\n    if not isinstance(suffix, str):\n        raise TypeError(f\"default must be str or False, got {default!r}\")\n\n    n = float(prefix)\n\n    try:\n        multiplier = timedelta_sizes[suffix.lower()]\n    except KeyError:\n        valid_units = \", \".join(timedelta_sizes.keys())\n        raise KeyError(\n            f\"Invalid time unit: {suffix}. Valid units are: {valid_units}\"\n        ) from None\n\n    result = n * multiplier\n    if int(result) == result:\n        result = int(result)\n    return result\n\n\ndef has_keyword(func, keyword):\n    try:\n        return keyword in inspect.signature(func).parameters\n    except Exception:\n        return False\n\n\ndef ndimlist(seq):\n    if not isinstance(seq, (list, tuple)):\n        return 0\n    elif not seq:\n        return 1\n    else:\n        return 1 + ndimlist(seq[0])\n\n\ndef iter_chunks(sizes, max_size):\n    \"\"\"Split sizes into chunks of total max_size each\n\n    Parameters\n    ----------\n    sizes : iterable of numbers\n        The sizes to be chunked\n    max_size : number\n        Maximum total size per chunk.\n        It must be greater or equal than each size in sizes\n    \"\"\"\n    chunk, chunk_sum = [], 0\n    iter_sizes = iter(sizes)\n    size = next(iter_sizes, None)\n    while size is not None:\n        assert size <= max_size\n        if chunk_sum + size <= max_size:\n            chunk.append(size)\n            chunk_sum += size\n            size = next(iter_sizes, None)\n        else:\n            assert chunk\n            yield chunk\n            chunk, chunk_sum = [], 0\n    if chunk:\n        yield chunk\n\n\nhex_pattern = re.compile(\"[a-f]+\")\n\n\n@functools.lru_cache(100000)\ndef key_split(s):\n    \"\"\"\n    >>> key_split('x')\n    'x'\n    >>> key_split('x-1')\n    'x'\n    >>> key_split('x-1-2-3')\n    'x'\n    >>> key_split(('x-2', 1))\n    'x'\n    >>> key_split(\"('x-2', 1)\")\n    'x'\n    >>> key_split(\"('x', 1)\")\n    'x'\n    >>> key_split('hello-world-1')\n    'hello-world'\n    >>> key_split(b'hello-world-1')\n    'hello-world'\n    >>> key_split('ae05086432ca935f6eba409a8ecd4896')\n    'data'\n    >>> key_split('<module.submodule.myclass object at 0xdaf372')\n    'myclass'\n    >>> key_split(None)\n    'Other'\n    >>> key_split('x-abcdefab')  # ignores hex\n    'x'\n    >>> key_split('_(x)')  # strips unpleasant characters\n    'x'\n    \"\"\"\n    # If we convert the key, recurse to utilize LRU cache better\n    if type(s) is bytes:\n        return key_split(s.decode())\n    if type(s) is tuple:\n        return key_split(s[0])\n    try:\n        words = s.split(\"-\")\n        if not words[0][0].isalpha():\n            result = words[0].split(\",\")[0].strip(\"_'()\\\"\")\n        else:\n            result = words[0]\n        for word in words[1:]:\n            if word.isalpha() and not (\n                len(word) == 8 and hex_pattern.match(word) is not None\n            ):\n                result += \"-\" + word\n            else:\n                break\n        if len(result) == 32 and re.match(r\"[a-f0-9]{32}\", result):\n            return \"data\"\n        else:\n            if result[0] == \"<\":\n                result = result.strip(\"<>\").split()[0].split(\".\")[-1]\n            return sys.intern(result)\n    except Exception:\n        return \"Other\"\n\n\ndef stringify(obj, exclusive: Iterable | None = None):\n    \"\"\"Convert an object to a string\n\n    If ``exclusive`` is specified, search through `obj` and convert\n    values that are in ``exclusive``.\n\n    Note that when searching through dictionaries, only values are\n    converted, not the keys.\n\n    Parameters\n    ----------\n    obj : Any\n        Object (or values within) to convert to string\n    exclusive: Iterable, optional\n        Set of values to search for when converting values to strings\n\n    Returns\n    -------\n    result : type(obj)\n        Stringified copy of ``obj`` or ``obj`` itself if it is already a\n        string or bytes.\n\n    Examples\n    --------\n    >>> stringify(b'x')\n    b'x'\n    >>> stringify('x')\n    'x'\n    >>> stringify({('a',0):('a',0), ('a',1): ('a',1)})\n    \"{('a', 0): ('a', 0), ('a', 1): ('a', 1)}\"\n    >>> stringify({('a',0):('a',0), ('a',1): ('a',1)}, exclusive={('a',0)})\n    {('a', 0): \"('a', 0)\", ('a', 1): ('a', 1)}\n    \"\"\"\n\n    typ = type(obj)\n    if typ is str or typ is bytes:\n        return obj\n    elif exclusive is None:\n        return str(obj)\n\n    if typ is list:\n        return [stringify(v, exclusive) for v in obj]\n    if typ is dict:\n        return {k: stringify(v, exclusive) for k, v in obj.items()}\n    try:\n        if obj in exclusive:\n            return stringify(obj)\n    except TypeError:  # `obj` not hashable\n        pass\n    if typ is tuple:  # If the tuple itself isn't a key, check its elements\n        return tuple(stringify(v, exclusive) for v in obj)\n    return obj\n\n\nclass cached_property(functools.cached_property):\n    \"\"\"Read only version of functools.cached_property.\"\"\"\n\n    def __set__(self, instance, val):\n        \"\"\"Raise an error when attempting to set a cached property.\"\"\"\n        raise AttributeError(\"Can't set attribute\")\n\n\nclass _HashIdWrapper:\n    \"\"\"Hash and compare a wrapped object by identity instead of value\"\"\"\n\n    def __init__(self, wrapped):\n        self.wrapped = wrapped\n\n    def __eq__(self, other):\n        if not isinstance(other, _HashIdWrapper):\n            return NotImplemented\n        return self.wrapped is other.wrapped\n\n    def __ne__(self, other):\n        if not isinstance(other, _HashIdWrapper):\n            return NotImplemented\n        return self.wrapped is not other.wrapped\n\n    def __hash__(self):\n        return id(self.wrapped)\n\n\n@functools.lru_cache\ndef _cumsum(seq, initial_zero):\n    if isinstance(seq, _HashIdWrapper):\n        seq = seq.wrapped\n    if initial_zero:\n        return tuple(toolz.accumulate(add, seq, 0))\n    else:\n        return tuple(toolz.accumulate(add, seq))\n\n\n@functools.lru_cache\ndef _max(seq):\n    if isinstance(seq, _HashIdWrapper):\n        seq = seq.wrapped\n    return max(seq)\n\n\ndef cached_max(seq):\n    \"\"\"Compute max with caching.\n\n    Caching is by the identity of `seq` rather than the value. It is thus\n    important that `seq` is a tuple of immutable objects, and this function\n    is intended for use where `seq` is a value that will persist (generally\n    block sizes).\n\n    Parameters\n    ----------\n    seq : tuple\n        Values to reduce\n\n    Returns\n    -------\n    tuple\n    \"\"\"\n    assert isinstance(seq, tuple)\n    # Look up by identity first, to avoid a linear-time __hash__\n    # if we've seen this tuple object before.\n    result = _max(_HashIdWrapper(seq))\n    return result\n\n\ndef cached_cumsum(seq, initial_zero=False):\n    \"\"\"Compute :meth:`toolz.accumulate` with caching.\n\n    Caching is by the identify of `seq` rather than the value. It is thus\n    important that `seq` is a tuple of immutable objects, and this function\n    is intended for use where `seq` is a value that will persist (generally\n    block sizes).\n\n    Parameters\n    ----------\n    seq : tuple\n        Values to cumulatively sum.\n    initial_zero : bool, optional\n        If true, the return value is prefixed with a zero.\n\n    Returns\n    -------\n    tuple\n    \"\"\"\n    if isinstance(seq, tuple):\n        # Look up by identity first, to avoid a linear-time __hash__\n        # if we've seen this tuple object before.\n        result = _cumsum(_HashIdWrapper(seq), initial_zero)\n    else:\n        # Construct a temporary tuple, and look up by value.\n        result = _cumsum(tuple(seq), initial_zero)\n    return result\n\n\ndef show_versions() -> None:\n    \"\"\"Provide version information for bug reports.\"\"\"\n\n    from json import dumps\n    from platform import uname\n    from sys import stdout, version_info\n\n    from dask._compatibility import importlib_metadata\n\n    try:\n        from distributed import __version__ as distributed_version\n    except ImportError:\n        distributed_version = None\n\n    from dask import __version__ as dask_version\n\n    deps = [\n        \"numpy\",\n        \"pandas\",\n        \"cloudpickle\",\n        \"fsspec\",\n        \"bokeh\",\n        \"pyarrow\",\n        \"zarr\",\n    ]\n\n    result: dict[str, str | None] = {\n        # note: only major, minor, micro are extracted\n        \"Python\": \".\".join([str(i) for i in version_info[:3]]),\n        \"Platform\": uname().system,\n        \"dask\": dask_version,\n        \"distributed\": distributed_version,\n    }\n\n    for modname in deps:\n        try:\n            result[modname] = importlib_metadata.version(modname)\n        except importlib_metadata.PackageNotFoundError:\n            result[modname] = None\n\n    stdout.writelines(dumps(result, indent=2))\n\n    return\n\n\ndef maybe_pluralize(count, noun, plural_form=None):\n    \"\"\"Pluralize a count-noun string pattern when necessary\"\"\"\n    if count == 1:\n        return f\"{count} {noun}\"\n    else:\n        return f\"{count} {plural_form or noun + 's'}\"\n\n\ndef is_namedtuple_instance(obj: Any) -> bool:\n    \"\"\"Returns True if obj is an instance of a namedtuple.\n\n    Note: This function checks for the existence of the methods and\n    attributes that make up the namedtuple API, so it will return True\n    IFF obj's type implements that API.\n    \"\"\"\n    return (\n        isinstance(obj, tuple)\n        and hasattr(obj, \"_make\")\n        and hasattr(obj, \"_asdict\")\n        and hasattr(obj, \"_replace\")\n        and hasattr(obj, \"_fields\")\n        and hasattr(obj, \"_field_defaults\")\n    )\n\n\ndef get_default_shuffle_method() -> str:\n    if d := config.get(\"dataframe.shuffle.method\", None):\n        return d\n    try:\n        from distributed import default_client\n\n        default_client()\n    except (ImportError, ValueError):\n        return \"disk\"\n\n    try:\n        from distributed.shuffle import check_minimal_arrow_version\n\n        check_minimal_arrow_version()\n    except ModuleNotFoundError:\n        return \"tasks\"\n    return \"p2p\"\n\n\ndef get_meta_library(like):\n    if hasattr(like, \"_meta\"):\n        like = like._meta\n\n    return import_module(typename(like).partition(\".\")[0])\n\n\nclass shorten_traceback:\n    \"\"\"Context manager that removes irrelevant stack elements from traceback.\n\n    * omits frames from modules that match `admin.traceback.shorten`\n    * always keeps the first and last frame.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: types.TracebackType | None,\n    ) -> None:\n        if exc_val and exc_tb:\n            exc_val.__traceback__ = self.shorten(exc_tb)\n\n    @staticmethod\n    def shorten(exc_tb: types.TracebackType) -> types.TracebackType:\n        paths = config.get(\"admin.traceback.shorten\")\n        if not paths:\n            return exc_tb\n\n        exp = re.compile(\".*(\" + \"|\".join(paths) + \")\")\n        curr: types.TracebackType | None = exc_tb\n        prev: types.TracebackType | None = None\n\n        while curr:\n            if prev is None:\n                prev = curr  # first frame\n            elif not curr.tb_next:\n                # always keep last frame\n                prev.tb_next = curr\n                prev = prev.tb_next\n            elif not exp.match(curr.tb_frame.f_code.co_filename):\n                # keep if module is not listed in config\n                prev.tb_next = curr\n                prev = curr\n            curr = curr.tb_next\n\n        # Uncomment to remove the first frame, which is something you don't want to keep\n        # if it matches the regexes. Requires Python >=3.11.\n        # if exc_tb.tb_next and exp.match(exc_tb.tb_frame.f_code.co_filename):\n        #     return exc_tb.tb_next\n\n        return exc_tb\n\n\ndef unzip(ls, nout):\n    \"\"\"Unzip a list of lists into ``nout`` outputs.\"\"\"\n    out = list(zip(*ls))\n    if not out:\n        out = [()] * nout\n    return out\n\n\n### Dependency File: _compatibility.py\nfrom __future__ import annotations\n\nimport importlib\nimport sys\nimport types\nimport warnings\nfrom typing import Literal\n\nif sys.version_info >= (3, 12):\n    import importlib.metadata as importlib_metadata\nelse:\n    import importlib_metadata\nfrom packaging.version import Version\n\nPY_VERSION = Version(\".\".join(map(str, sys.version_info[:3])))\n\nEMSCRIPTEN = sys.platform == \"emscripten\"\n\nLINUX = sys.platform == \"linux\"\nMACOS = sys.platform == \"darwin\"\nWINDOWS = sys.platform == \"win32\"\n\n\ndef entry_points(group=None):\n    warnings.warn(\n        \"`dask._compatibility.entry_points` has been replaced by `importlib_metadata.entry_points` and will be removed \"\n        \"in a future version. Please use `importlib_metadata.entry_points` instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return importlib_metadata.entry_points(group=group)\n\n\nVERSIONS = {\n    \"numpy\": \"1.21.0\",\n    \"pandas\": \"2.0.0\",\n    \"bokeh\": \"3.1.0\",\n    \"jinja2\": \"2.10.3\",\n    \"pyarrow\": \"14.0.1\",\n    \"lz4\": \"4.3.2\",\n}\n\n# A mapping from import name to package name (on PyPI) for packages where\n# these two names are different.\n\nINSTALL_MAPPING = {\n    \"sqlalchemy\": \"SQLAlchemy\",\n    \"tables\": \"pytables\",\n}\n\n\ndef get_version(module: types.ModuleType) -> str:\n    version = getattr(module, \"__version__\", None)\n\n    if version is None:\n        raise ImportError(f\"Can't determine version for {module.__name__}\")\n    if module.__name__ == \"psycopg2\":\n        # psycopg2 appends \" (dt dec pq3 ext lo64)\" to it's version\n        version = version.split()[0]\n    return version\n\n\ndef import_optional_dependency(\n    name: str,\n    extra: str = \"\",\n    min_version: str | None = None,\n    *,\n    errors: Literal[\"raise\", \"warn\", \"ignore\"] = \"raise\",\n) -> types.ModuleType | None:\n    \"\"\"\n    Import an optional dependency.\n\n    By default, if a dependency is missing an ImportError with a nice\n    message will be raised. If a dependency is present, but too old,\n    we raise.\n\n    Parameters\n    ----------\n    name : str\n        The module name.\n    extra : str\n        Additional text to include in the ImportError message.\n    errors : str {'raise', 'warn', 'ignore'}\n        What to do when a dependency is not found or its version is too old.\n\n        * raise : Raise an ImportError\n        * warn : Only applicable when a module's version is to old.\n          Warns that the version is too old and returns None\n        * ignore: If the module is not installed, return None, otherwise,\n          return the module, even if the version is too old.\n          It's expected that users validate the version locally when\n          using ``errors=\"ignore\"`` (see. ``io/html.py``)\n    min_version : str, default None\n        Specify a minimum version that is different from the global pandas\n        minimum version required.\n    Returns\n    -------\n    maybe_module : Optional[ModuleType]\n        The imported module, when found and the version is correct.\n        None is returned when the package is not found and `errors`\n        is False, or when the package's version is too old and `errors`\n        is ``'warn'`` or ``'ignore'``.\n    \"\"\"\n    assert errors in {\"warn\", \"raise\", \"ignore\"}\n\n    package_name = INSTALL_MAPPING.get(name)\n    install_name = package_name if package_name is not None else name\n\n    msg = (\n        f\"Missing optional dependency '{install_name}'. {extra} \"\n        f\"Use pip or conda to install {install_name}.\"\n    )\n    try:\n        module = importlib.import_module(name)\n    except ImportError as err:\n        if errors == \"raise\":\n            raise ImportError(msg) from err\n        return None\n\n    # Handle submodules: if we have submodule, grab parent module from sys.modules\n    parent = name.split(\".\")[0]\n    if parent != name:\n        install_name = parent\n        module_to_get = sys.modules[install_name]\n    else:\n        module_to_get = module\n    minimum_version = min_version if min_version is not None else VERSIONS.get(parent)\n    if minimum_version:\n        version = get_version(module_to_get)\n        if version and Version(version) < Version(minimum_version):\n            msg = (\n                f\"Dask requires version '{minimum_version}' or newer of '{parent}' \"\n                f\"(version '{version}' currently installed).\"\n            )\n            if errors == \"warn\":\n                warnings.warn(msg, UserWarning)\n                return None\n            elif errors == \"raise\":\n                raise ImportError(msg)\n            else:\n                return None\n\n    return module\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:36:31"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport os\nimport sys\nfrom array import array\n\nimport pytest\nfrom packaging.version import Version\n\nfrom dask.multiprocessing import get_context\nfrom dask.sizeof import sizeof\nfrom dask.utils import funcname, tmpdir\n\ntry:\n    import pandas as pd\nexcept ImportError:\n    pd = None\n\nrequires_pandas = pytest.mark.skipif(pd is None, reason=\"requires pandas\")\n\n\ndef test_base():\n    assert sizeof(1) == sys.getsizeof(1)\n\n\ndef test_name():\n    assert funcname(sizeof) == \"sizeof\"\n\n\ndef test_containers():\n    assert sizeof([1, 2, [3]]) > (sys.getsizeof(3) * 3 + sys.getsizeof([]))\n\n\ndef test_bytes_like():\n    assert 1000 <= sizeof(bytes(1000)) <= 2000\n    assert 1000 <= sizeof(bytearray(1000)) <= 2000\n    assert 1000 <= sizeof(memoryview(bytes(1000))) <= 2000\n    assert 8000 <= sizeof(array(\"d\", range(1000))) <= 9000\n\n\ndef test_numpy():\n    np = pytest.importorskip(\"numpy\")\n    assert 8000 <= sizeof(np.empty(1000, dtype=\"f8\")) <= 9000\n    dt = np.dtype(\"f8\")\n    assert sizeof(dt) == sys.getsizeof(dt)\n\n\ndef test_numpy_0_strided():\n    np = pytest.importorskip(\"numpy\")\n    x = np.broadcast_to(1, (100, 100, 100))\n    assert sizeof(x) <= 8\n\n\n@requires_pandas\ndef test_pandas():\n    df = pd.DataFrame(\n        {\"x\": [1, 2, 3], \"y\": [\"a\" * 100, \"b\" * 100, \"c\" * 100]}, index=[10, 20, 30]\n    )\n\n    assert sizeof(df) >= sizeof(df.x) + sizeof(df.y) - sizeof(df.index)\n    assert sizeof(df.x) >= sizeof(df.index)\n    assert sizeof(df.y) >= 100 * 3\n    assert sizeof(df.index) >= 20\n\n    assert isinstance(sizeof(df), int)\n    assert isinstance(sizeof(df.x), int)\n    assert isinstance(sizeof(df.index), int)\n\n\n@requires_pandas\ndef test_pandas_contiguous_dtypes():\n    \"\"\"2+ contiguous columns of the same dtype in the same DataFrame share the same\n    surface thus have lower overhead\n    \"\"\"\n    df1 = pd.DataFrame([[1, 2.2], [3, 4.4]])\n    df2 = pd.DataFrame([[1.1, 2.2], [3.3, 4.4]])\n    assert sizeof(df2) < sizeof(df1)\n\n\n@requires_pandas\ndef test_pandas_multiindex():\n    index = pd.MultiIndex.from_product([range(50), list(\"abcdefghilmnopqrstuvwxyz\")])\n    actual_size = sys.getsizeof(index)\n\n    assert 0.5 * actual_size < sizeof(index) < 3 * actual_size\n    assert isinstance(sizeof(index), int)\n\n\n@requires_pandas\ndef test_pandas_repeated_column():\n    df = pd.DataFrame({\"x\": list(range(10_000))})\n    df2 = df[[\"x\", \"x\", \"x\"]]\n    df3 = pd.DataFrame({\"x\": list(range(10_000)), \"y\": list(range(10_000))})\n    assert 80_000 < sizeof(df) < 85_000\n    assert 80_000 < sizeof(df2) < 85_000\n    assert 160_000 < sizeof(df3) < 165_000\n\n\ndef test_sparse_matrix():\n    sparse = pytest.importorskip(\"scipy.sparse\")\n    sp = sparse.eye(10)\n    # These are the 32-bit Python 2.7 values.\n    assert sizeof(sp.todia()) >= 152\n    assert sizeof(sp.tobsr()) >= 232\n    assert sizeof(sp.tocoo()) >= 240\n    assert sizeof(sp.tocsc()) >= 232\n    assert sizeof(sp.tocsr()) >= 232\n    assert sizeof(sp.todok()) >= 184\n    assert sizeof(sp.tolil()) >= 204\n\n\n@requires_pandas\n@pytest.mark.parametrize(\"cls_name\", [\"Series\", \"DataFrame\", \"Index\"])\n@pytest.mark.parametrize(\"dtype\", [object, \"string[python]\"])\ndef test_pandas_object_dtype(dtype, cls_name):\n    cls = getattr(pd, cls_name)\n    s1 = cls([f\"x{i:3d}\" for i in range(1000)], dtype=dtype)\n    assert sizeof(\"x000\") * 1000 < sizeof(s1) < 2 * sizeof(\"x000\") * 1000\n\n    x = \"x\" * 100_000\n    y = \"y\" * 100_000\n    z = \"z\" * 100_000\n    w = \"w\" * 100_000\n\n    # High duplication of references to the same object\n    s2 = cls([x, y, z, w] * 1000, dtype=dtype)\n    assert 400_000 < sizeof(s2) < 500_000\n\n    # Low duplication of references to the same object\n    s3 = cls([x, y, z, w], dtype=dtype)\n    s4 = cls([x, y, z, x], dtype=dtype)\n    s5 = cls([x, x, x, x], dtype=dtype)\n    assert sizeof(s5) < sizeof(s4) < sizeof(s3)\n\n\n@requires_pandas\n@pytest.mark.parametrize(\"dtype\", [object, \"string[python]\"])\ndef test_dataframe_object_dtype(dtype):\n    x = \"x\" * 100_000\n    y = \"y\" * 100_000\n    z = \"z\" * 100_000\n    w = \"w\" * 100_000\n\n    # High duplication of references to the same object, across different columns\n    objs = [x, y, z, w]\n    df1 = pd.DataFrame([objs * 3] * 1000, dtype=dtype)\n    assert 400_000 < sizeof(df1) < 550_000\n\n    # Low duplication of references to the same object, across different columns\n    df2 = pd.DataFrame([[x, y], [z, w]], dtype=dtype)\n    df3 = pd.DataFrame([[x, y], [z, x]], dtype=dtype)\n    df4 = pd.DataFrame([[x, x], [x, x]], dtype=dtype)\n    assert sizeof(df4) < sizeof(df3) < sizeof(df2)\n\n\n@pytest.mark.parametrize(\"cls_name\", [\"Series\", \"DataFrame\", \"Index\"])\ndef test_pandas_string_arrow_dtype(cls_name):\n    pytest.importorskip(\"pyarrow\")\n    cls = getattr(pd, cls_name)\n\n    s = cls([\"x\" * 100_000, \"y\" * 50_000], dtype=\"string[pyarrow]\")\n    assert 150_000 < sizeof(s) < 155_000\n\n\n@requires_pandas\ndef test_pandas_empty():\n    df = pd.DataFrame(\n        {\"x\": [1, 2, 3], \"y\": [\"a\" * 100, \"b\" * 100, \"c\" * 100]}, index=[10, 20, 30]\n    )\n    empty = df.head(0)\n\n    assert sizeof(empty) > 0\n    assert sizeof(empty.x) > 0\n    assert sizeof(empty.y) > 0\n    assert sizeof(empty.index) > 0\n\n\n@requires_pandas\ndef test_pyarrow_table():\n    pa = pytest.importorskip(\"pyarrow\")\n    df = pd.DataFrame(\n        {\"x\": [1, 2, 3], \"y\": [\"a\" * 100, \"b\" * 100, \"c\" * 100]}, index=[10, 20, 30]\n    )\n    table = pa.Table.from_pandas(df)\n\n    assert sizeof(table) > sizeof(table.schema.metadata)\n    assert isinstance(sizeof(table), int)\n    assert isinstance(sizeof(table.columns[0]), int)\n    assert isinstance(sizeof(table.columns[1]), int)\n    assert isinstance(sizeof(table.columns[2]), int)\n\n    empty = pa.Table.from_pandas(df.head(0))\n\n    assert sizeof(empty) > sizeof(empty.schema.metadata)\n    assert sizeof(empty.columns[0]) > 0\n    assert sizeof(empty.columns[1]) > 0\n    assert sizeof(empty.columns[2]) > 0\n\n\ndef test_dict():\n    np = pytest.importorskip(\"numpy\")\n    x = np.ones(10000)\n    assert sizeof({\"x\": x}) > x.nbytes\n    assert sizeof({\"x\": [x]}) > x.nbytes\n    assert sizeof({\"x\": [{\"y\": x}]}) > x.nbytes\n\n    d = {i: x for i in range(100)}\n    assert sizeof(d) > x.nbytes * 100\n    assert isinstance(sizeof(d), int)\n\n\ndef _get_sizeof_on_path(path, size):\n    sys.path.append(os.fsdecode(path))\n\n    # Dask will have already called _register_entry_point_plugins\n    # before we can modify sys.path, so we re-register here.\n    import dask.sizeof\n\n    dask.sizeof._register_entry_point_plugins()\n\n    import class_impl\n\n    cls = class_impl.Impl(size)\n    return sizeof(cls)\n\n\ndef test_register_backend_entrypoint(tmp_path):\n    # Create special sizeof implementation for a dummy class\n    (tmp_path / \"impl_sizeof.py\").write_bytes(\n        b\"def sizeof_plugin(sizeof):\\n\"\n        b'    print(\"REG\")\\n'\n        b'    @sizeof.register_lazy(\"class_impl\")\\n'\n        b\"    def register_impl():\\n\"\n        b\"        import class_impl\\n\"\n        b\"        @sizeof.register(class_impl.Impl)\\n\"\n        b\"        def sizeof_impl(obj):\\n\"\n        b\"            return obj.size \\n\"\n    )\n    # Define dummy class that possesses a size attribute\n    (tmp_path / \"class_impl.py\").write_bytes(\n        b\"class Impl:\\n    def __init__(self, size):\\n        self.size = size\"\n    )\n    dist_info = tmp_path / \"impl_sizeof-0.0.0.dist-info\"\n    dist_info.mkdir()\n    (dist_info / \"entry_points.txt\").write_bytes(\n        b\"[dask.sizeof]\\nimpl = impl_sizeof:sizeof_plugin\\n\"\n    )\n\n    with get_context().Pool(1) as pool:\n        assert (\n            pool.apply(_get_sizeof_on_path, args=(tmp_path, 3_14159265)) == 3_14159265\n        )\n    pool.join()\n\n\ndef test_xarray():\n    xr = pytest.importorskip(\"xarray\")\n    np = pytest.importorskip(\"numpy\")\n\n    ind = np.arange(-66, 67, 1).astype(float)\n    arr = np.random.random((len(ind),))\n\n    dataset = (\n        xr.DataArray(\n            arr,\n            dims=[\"coord\"],\n            coords={\"coord\": ind},\n        )\n        .rename(\"foo\")\n        .to_dataset()\n    )\n    assert sizeof(dataset) > sizeof(arr)\n    assert sizeof(dataset.foo) >= sizeof(arr)\n    assert sizeof(dataset[\"coord\"]) >= sizeof(ind)\n    assert sizeof(dataset.indexes) >= sizeof(ind)\n\n\ndef test_xarray_not_in_memory():\n    xr = pytest.importorskip(\"xarray\")\n    zarr = pytest.importorskip(\"zarr\")\n    np = pytest.importorskip(\"numpy\")\n    pytest.importorskip(\"zarr\")\n\n    ind = np.arange(-66, 67, 1).astype(float)\n    arr = np.random.random((len(ind),))\n\n    # TODO: remove this conditional after consolidated metadata lands in v3\n    if Version(zarr.__version__) > Version(\"3.0.0.a0\") and Version(\n        zarr.__version__\n    ) < Version(\"3.0.0\"):\n        pytest.xfail(\"consolidated metadata and xarray support is not complete\")\n\n    with tmpdir() as path:\n        xr.DataArray(\n            arr,\n            dims=[\"coord\"],\n            coords={\"coord\": ind},\n        ).rename(\n            \"foo\"\n        ).to_dataset().to_zarr(path)\n        dataset = xr.open_zarr(path, chunks={\"foo\": 10})\n        assert not dataset.foo._in_memory\n        assert sizeof(ind) < sizeof(dataset) < sizeof(arr) + sizeof(ind)\n        assert sizeof(dataset.foo) < sizeof(arr)\n        assert sizeof(dataset[\"coord\"]) >= sizeof(ind)\n        assert sizeof(dataset.indexes) >= sizeof(ind)\n        assert not dataset.foo._in_memory\n\n        dataset.load()\n        assert dataset.foo._in_memory\n        assert sizeof(dataset) >= sizeof(arr) + sizeof(ind)\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_system\\test_system.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport math\nimport os\nimport sys\n\ntry:\n    import psutil\nexcept ImportError:\n    psutil = None  # type: ignore\n\n__all__ = (\"cpu_count\", \"CPU_COUNT\")\n\n\ndef _try_extract_cgroup_cpu_quota():\n    # cgroup v1\n    # The directory name isn't standardized across linux distros, check both\n    for dirname in [\"cpuacct,cpu\", \"cpu,cpuacct\"]:\n        try:\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_quota_us\" % dirname) as f:\n                quota = int(f.read())\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_period_us\" % dirname) as f:\n                period = int(f.read())\n            return quota, period\n        except Exception:\n            pass\n\n    # cgroup v2\n    try:\n        with open(\"/proc/self/cgroup\") as f:\n            group_path = f.read().strip().split(\":\")[-1]\n        if not group_path.endswith(\"/\"):\n            group_path = f\"{group_path}/\"\n        with open(\"/sys/fs/cgroup%scpu.max\" % group_path) as f:\n            quota, period = map(int, f.read().split(\" \"))\n            return quota, period\n    except Exception:\n        pass\n\n    # No cgroup CPU quota found\n    return None, None\n\n\ndef cpu_count():\n    \"\"\"Get the available CPU count for this system.\n\n    Takes the minimum value from the following locations:\n\n    - Total system cpus available on the host.\n    - CPU Affinity (if set)\n    - Cgroups limit (if set)\n    \"\"\"\n    count = os.cpu_count()\n\n    # Check CPU affinity if available\n    if psutil is not None:\n        try:\n            affinity_count = len(psutil.Process().cpu_affinity())\n            if affinity_count > 0:\n                count = min(count, affinity_count)\n        except Exception:\n            pass\n\n    # Check cgroups if available\n    if sys.platform == \"linux\":\n        quota, period = _try_extract_cgroup_cpu_quota()\n        if quota is not None and period is not None:\n            # We round up on fractional CPUs\n            cgroups_count = math.ceil(quota / period)\n            if cgroups_count > 0:\n                count = min(count, cgroups_count)\n\n    return count\n\n\nCPU_COUNT = cpu_count()\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:36:31"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport builtins\nimport io\nimport os\nimport sys\n\nimport pytest\n\nfrom dask.system import cpu_count\n\npsutil = pytest.importorskip(\"psutil\")\n\n\ndef test_cpu_count():\n    count = cpu_count()\n    assert isinstance(count, int)\n    assert count <= os.cpu_count()\n    assert count >= 1\n\n\n@pytest.mark.parametrize(\"dirname\", [\"cpuacct,cpu\", \"cpu,cpuacct\", None])\ndef test_cpu_count_cgroups(dirname, monkeypatch):\n    def mycpu_count():\n        # Absurdly high, unlikely to match real value\n        return 250\n\n    monkeypatch.setattr(os, \"cpu_count\", mycpu_count)\n\n    class MyProcess:\n        def cpu_affinity(self):\n            # No affinity set\n            return []\n\n    monkeypatch.setattr(psutil, \"Process\", MyProcess)\n\n    if dirname:\n        paths = {\n            \"/sys/fs/cgroup/%s/cpu.cfs_quota_us\" % dirname: io.StringIO(\"2005\"),\n            \"/sys/fs/cgroup/%s/cpu.cfs_period_us\" % dirname: io.StringIO(\"10\"),\n        }\n        builtin_open = builtins.open\n\n        def myopen(path, *args, **kwargs):\n            if path in paths:\n                return paths.get(path)\n            return builtin_open(path, *args, **kwargs)\n\n        monkeypatch.setattr(builtins, \"open\", myopen)\n        monkeypatch.setattr(sys, \"platform\", \"linux\")\n\n    count = cpu_count()\n    if dirname:\n        # Rounds up\n        assert count == 201\n    else:\n        assert count == 250\n\n\n@pytest.mark.parametrize(\"group_name\", [\"/\", \"/user.slice\", \"/user.slice/more.slice\"])\n@pytest.mark.parametrize(\"quota\", [\"max\", \"2005\"])\ndef test_cpu_count_cgroups_v2(quota, group_name, monkeypatch):\n    def mycpu_count():\n        # Absurdly high, unlikely to match real value\n        return 250\n\n    monkeypatch.setattr(os, \"cpu_count\", mycpu_count)\n\n    class MyProcess:\n        def cpu_affinity(self):\n            # No affinity set\n            return []\n\n    monkeypatch.setattr(psutil, \"Process\", MyProcess)\n\n    if not group_name.endswith(\"/\"):\n        group_name = f\"{group_name}/\"\n\n    paths = {\n        \"/proc/self/cgroup\": io.StringIO(\"0::%s\" % group_name),\n        \"/sys/fs/cgroup%scpu.max\" % group_name: io.StringIO(\"%s 10\" % quota),\n    }\n    builtin_open = builtins.open\n\n    def myopen(path, *args, **kwargs):\n        if path in paths:\n            return paths.get(path)\n        return builtin_open(path, *args, **kwargs)\n\n    monkeypatch.setattr(builtins, \"open\", myopen)\n    monkeypatch.setattr(sys, \"platform\", \"linux\")\n\n    count = cpu_count()\n    if quota == \"max\":\n        assert count == 250\n    else:\n        # Rounds up\n        assert count == 201\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_threaded\\test_threaded.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"\nA threaded shared-memory scheduler\n\nSee local.py\n\"\"\"\n\nfrom __future__ import annotations\n\nimport atexit\nimport multiprocessing.pool\nimport sys\nimport threading\nfrom collections import defaultdict\nfrom collections.abc import Mapping, Sequence\nfrom concurrent.futures import Executor, ThreadPoolExecutor\nfrom threading import Lock, current_thread\n\nfrom dask import config\nfrom dask.local import MultiprocessingPoolExecutor, get_async\nfrom dask.system import CPU_COUNT\nfrom dask.typing import Key\n\n\ndef _thread_get_id():\n    return current_thread().ident\n\n\nmain_thread = current_thread()\ndefault_pool: Executor | None = None\npools: defaultdict[threading.Thread, dict[int, Executor]] = defaultdict(dict)\npools_lock = Lock()\n\n\ndef pack_exception(e, dumps):\n    return e, sys.exc_info()[2]\n\n\ndef get(\n    dsk: Mapping,\n    keys: Sequence[Key] | Key,\n    cache=None,\n    num_workers=None,\n    pool=None,\n    **kwargs,\n):\n    \"\"\"Threaded cached implementation of dask.get\n\n    Parameters\n    ----------\n\n    dsk: dict\n        A dask dictionary specifying a workflow\n    keys: key or list of keys\n        Keys corresponding to desired data\n    num_workers: integer of thread count\n        The number of threads to use in the ThreadPool that will actually execute tasks\n    cache: dict-like (optional)\n        Temporary storage of results\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}\n    >>> get(dsk, 'w')\n    4\n    >>> get(dsk, ['w', 'y'])\n    (4, 2)\n    \"\"\"\n    global default_pool\n    pool = pool or config.get(\"pool\", None)\n    num_workers = num_workers or config.get(\"num_workers\", None)\n    thread = current_thread()\n\n    with pools_lock:\n        if pool is None:\n            if num_workers is None and thread is main_thread:\n                if default_pool is None:\n                    default_pool = ThreadPoolExecutor(CPU_COUNT)\n                    atexit.register(default_pool.shutdown)\n                pool = default_pool\n            elif thread in pools and num_workers in pools[thread]:\n                pool = pools[thread][num_workers]\n            else:\n                pool = ThreadPoolExecutor(num_workers)\n                atexit.register(pool.shutdown)\n                pools[thread][num_workers] = pool\n        elif isinstance(pool, multiprocessing.pool.Pool):\n            pool = MultiprocessingPoolExecutor(pool)\n\n    results = get_async(\n        pool.submit,\n        pool._max_workers,\n        dsk,\n        keys,\n        cache=cache,\n        get_id=_thread_get_id,\n        pack_exception=pack_exception,\n        **kwargs,\n    )\n\n    # Cleanup pools associated to dead threads\n    with pools_lock:\n        active_threads = set(threading.enumerate())\n        if thread is not main_thread:\n            for t in list(pools):\n                if t not in active_threads:\n                    for p in pools.pop(t).values():\n                        p.shutdown()\n\n    return results\n\n### Source File Dependency Files Content\n### Dependency File: config.py\nfrom __future__ import annotations\n\nimport ast\nimport base64\nimport builtins  # Explicitly use builtins.set as 'set' will be shadowed by a function\nimport json\nimport os\nimport pathlib\nimport site\nimport sys\nimport threading\nimport warnings\nfrom collections.abc import Iterator, Mapping, Sequence\nfrom typing import Any, Literal, overload\n\nimport yaml\n\nfrom dask.typing import no_default\n\n\ndef _get_paths():\n    \"\"\"Get locations to search for YAML configuration files.\n\n    This logic exists as a separate function for testing purposes.\n    \"\"\"\n\n    paths = [\n        os.getenv(\"DASK_ROOT_CONFIG\", \"/etc/dask\"),\n        os.path.join(sys.prefix, \"etc\", \"dask\"),\n        *[os.path.join(prefix, \"etc\", \"dask\") for prefix in site.PREFIXES],\n        os.path.join(os.path.expanduser(\"~\"), \".config\", \"dask\"),\n    ]\n    if \"DASK_CONFIG\" in os.environ:\n        paths.append(os.environ[\"DASK_CONFIG\"])\n\n    # Remove duplicate paths while preserving ordering\n    paths = list(reversed(list(dict.fromkeys(reversed(paths)))))\n\n    return paths\n\n\npaths = _get_paths()\n\nif \"DASK_CONFIG\" in os.environ:\n    PATH = os.environ[\"DASK_CONFIG\"]\nelse:\n    PATH = os.path.join(os.path.expanduser(\"~\"), \".config\", \"dask\")\n\n\nconfig: dict = {}\nglobal_config = config  # alias\n\n\nconfig_lock = threading.Lock()\n\n\ndefaults: list[Mapping] = []\n\n\ndef canonical_name(k: str, config: dict) -> str:\n    \"\"\"Return the canonical name for a key.\n\n    Handles user choice of '-' or '_' conventions by standardizing on whichever\n    version was set first. If a key already exists in either hyphen or\n    underscore form, the existing version is the canonical name. If neither\n    version exists the original key is used as is.\n    \"\"\"\n    try:\n        if k in config:\n            return k\n    except TypeError:\n        # config is not a mapping, return the same name as provided\n        return k\n\n    altk = k.replace(\"_\", \"-\") if \"_\" in k else k.replace(\"-\", \"_\")\n\n    if altk in config:\n        return altk\n\n    return k\n\n\ndef update(\n    old: dict,\n    new: Mapping,\n    priority: Literal[\"old\", \"new\", \"new-defaults\"] = \"new\",\n    defaults: Mapping | None = None,\n) -> dict:\n    \"\"\"Update a nested dictionary with values from another\n\n    This is like dict.update except that it smoothly merges nested values\n\n    This operates in-place and modifies old\n\n    Parameters\n    ----------\n    priority: string {'old', 'new', 'new-defaults'}\n        If new (default) then the new dictionary has preference.\n        Otherwise the old dictionary does.\n        If 'new-defaults', a mapping should be given of the current defaults.\n        Only if a value in ``old`` matches the current default, it will be\n        updated with ``new``.\n\n    Examples\n    --------\n    >>> a = {'x': 1, 'y': {'a': 2}}\n    >>> b = {'x': 2, 'y': {'b': 3}}\n    >>> update(a, b)  # doctest: +SKIP\n    {'x': 2, 'y': {'a': 2, 'b': 3}}\n\n    >>> a = {'x': 1, 'y': {'a': 2}}\n    >>> b = {'x': 2, 'y': {'b': 3}}\n    >>> update(a, b, priority='old')  # doctest: +SKIP\n    {'x': 1, 'y': {'a': 2, 'b': 3}}\n\n    >>> d = {'x': 0, 'y': {'a': 2}}\n    >>> a = {'x': 1, 'y': {'a': 2}}\n    >>> b = {'x': 2, 'y': {'a': 3, 'b': 3}}\n    >>> update(a, b, priority='new-defaults', defaults=d)  # doctest: +SKIP\n    {'x': 1, 'y': {'a': 3, 'b': 3}}\n\n    See Also\n    --------\n    dask.config.merge\n    \"\"\"\n    for k, v in new.items():\n        k = canonical_name(k, old)\n\n        if isinstance(v, Mapping):\n            if k not in old or old[k] is None or not isinstance(old[k], dict):\n                old[k] = {}\n            update(\n                old[k],\n                v,\n                priority=priority,\n                defaults=defaults.get(k) if defaults else None,\n            )\n        else:\n            if (\n                priority == \"new\"\n                or k not in old\n                or (\n                    priority == \"new-defaults\"\n                    and defaults\n                    and k in defaults\n                    and defaults[k] == old[k]\n                )\n            ):\n                old[k] = v\n\n    return old\n\n\ndef merge(*dicts: Mapping) -> dict:\n    \"\"\"Update a sequence of nested dictionaries\n\n    This prefers the values in the latter dictionaries to those in the former\n\n    Examples\n    --------\n    >>> a = {'x': 1, 'y': {'a': 2}}\n    >>> b = {'y': {'b': 3}}\n    >>> merge(a, b)  # doctest: +SKIP\n    {'x': 1, 'y': {'a': 2, 'b': 3}}\n\n    See Also\n    --------\n    dask.config.update\n    \"\"\"\n    result: dict = {}\n    for d in dicts:\n        update(result, d)\n    return result\n\n\ndef _load_config_file(path: str) -> dict | None:\n    \"\"\"A helper for loading a config file from a path, and erroring\n    appropriately if the file is malformed.\"\"\"\n    try:\n        with open(path) as f:\n            config = yaml.safe_load(f.read())\n    except OSError:\n        # Ignore permission errors\n        return None\n    except Exception as exc:\n        raise ValueError(\n            f\"A dask config file at {path!r} is malformed, original error \"\n            f\"message:\\n\\n{exc}\"\n        ) from None\n    if config is not None and not isinstance(config, dict):\n        raise ValueError(\n            f\"A dask config file at {path!r} is malformed - config files must have \"\n            f\"a dict as the top level object, got a {type(config).__name__} instead\"\n        )\n    return config\n\n\n@overload\ndef collect_yaml(\n    paths: Sequence[str], *, return_paths: Literal[False] = False\n) -> Iterator[dict]: ...\n\n\n@overload\ndef collect_yaml(\n    paths: Sequence[str], *, return_paths: Literal[True]\n) -> Iterator[tuple[pathlib.Path, dict]]: ...\n\n\ndef collect_yaml(\n    paths: Sequence[str], *, return_paths: bool = False\n) -> Iterator[dict | tuple[pathlib.Path, dict]]:\n    \"\"\"Collect configuration from yaml files\n\n    This searches through a list of paths, expands to find all yaml or json\n    files, and then parses each file.\n    \"\"\"\n    # Find all paths\n    file_paths = []\n    for path in paths:\n        if os.path.exists(path):\n            if os.path.isdir(path):\n                try:\n                    file_paths.extend(\n                        sorted(\n                            os.path.join(path, p)\n                            for p in os.listdir(path)\n                            if os.path.splitext(p)[1].lower()\n                            in (\".json\", \".yaml\", \".yml\")\n                        )\n                    )\n                except OSError:\n                    # Ignore permission errors\n                    pass\n            else:\n                file_paths.append(path)\n\n    # Parse yaml files\n    for path in file_paths:\n        config = _load_config_file(path)\n        if config is not None:\n            if return_paths:\n                yield pathlib.Path(path), config\n            else:\n                yield config\n\n\ndef collect_env(env: Mapping[str, str] | None = None) -> dict:\n    \"\"\"Collect config from environment variables\n\n    This grabs environment variables of the form \"DASK_FOO__BAR_BAZ=123\" and\n    turns these into config variables of the form ``{\"foo\": {\"bar-baz\": 123}}``\n    It transforms the key and value in the following way:\n\n    -  Lower-cases the key text\n    -  Treats ``__`` (double-underscore) as nested access\n    -  Calls ``ast.literal_eval`` on the value\n\n    Any serialized config passed via ``DASK_INTERNAL_INHERIT_CONFIG`` is also set here.\n\n    \"\"\"\n\n    if env is None:\n        env = os.environ\n\n    if \"DASK_INTERNAL_INHERIT_CONFIG\" in env:\n        d = deserialize(env[\"DASK_INTERNAL_INHERIT_CONFIG\"])\n    else:\n        d = {}\n\n    for name, value in env.items():\n        if name.startswith(\"DASK_\"):\n            varname = name[5:].lower().replace(\"__\", \".\")\n            d[varname] = interpret_value(value)\n\n    result: dict = {}\n    set(d, config=result)\n    return result\n\n\ndef interpret_value(value: str) -> Any:\n    try:\n        return ast.literal_eval(value)\n    except (SyntaxError, ValueError):\n        pass\n\n    # Avoid confusion of YAML vs. Python syntax\n    hardcoded_map = {\"none\": None, \"null\": None, \"false\": False, \"true\": True}\n    return hardcoded_map.get(value.lower(), value)\n\n\ndef paths_containing_key(\n    key: str,\n    paths: Sequence[str] = paths,\n) -> Iterator[pathlib.Path]:\n    \"\"\"\n    Generator yielding paths which contain the given key.\n    \"\"\"\n    # Check existing config files for any which contains this key.\n    for path_ in paths:\n        for path, config in collect_yaml([path_], return_paths=True):\n            try:\n                get(key, config=config)\n            except KeyError:\n                continue\n            else:\n                yield pathlib.Path(path)\n\n\ndef ensure_file(\n    source: str, destination: str | None = None, comment: bool = True\n) -> None:\n    \"\"\"\n    Copy file to default location if it does not already exist\n\n    This tries to move a default configuration file to a default location if\n    if does not already exist.  It also comments out that file by default.\n\n    This is to be used by downstream modules (like dask.distributed) that may\n    have default configuration files that they wish to include in the default\n    configuration path.\n\n    Parameters\n    ----------\n    source : string, filename\n        Source configuration file, typically within a source directory.\n    destination : string, directory\n        Destination directory. Configurable by ``DASK_CONFIG`` environment\n        variable, falling back to ~/.config/dask.\n    comment : bool, True by default\n        Whether or not to comment out the config file when copying.\n    \"\"\"\n    if destination is None:\n        destination = PATH\n\n    # destination is a file and already exists, never overwrite\n    if os.path.isfile(destination):\n        return\n\n    # If destination is not an existing file, interpret as a directory,\n    # use the source basename as the filename\n    directory = destination\n    destination = os.path.join(directory, os.path.basename(source))\n\n    try:\n        if not os.path.exists(destination):\n            os.makedirs(directory, exist_ok=True)\n\n            # Atomically create destination.  Parallel testing discovered\n            # a race condition where a process can be busy creating the\n            # destination while another process reads an empty config file.\n            tmp = \"%s.tmp.%d\" % (destination, os.getpid())\n            with open(source) as f:\n                lines = list(f)\n\n            if comment:\n                lines = [\n                    \"# \" + line if line.strip() and not line.startswith(\"#\") else line\n                    for line in lines\n                ]\n\n            with open(tmp, \"w\") as f:\n                f.write(\"\".join(lines))\n\n            try:\n                os.rename(tmp, destination)\n            except OSError:\n                os.remove(tmp)\n    except OSError:\n        pass\n\n\nclass set:\n    \"\"\"Temporarily set configuration values within a context manager\n\n    Parameters\n    ----------\n    arg : mapping or None, optional\n        A mapping of configuration key-value pairs to set.\n    **kwargs :\n        Additional key-value pairs to set. If ``arg`` is provided, values set\n        in ``arg`` will be applied before those in ``kwargs``.\n        Double-underscores (``__``) in keyword arguments will be replaced with\n        ``.``, allowing nested values to be easily set.\n\n    Examples\n    --------\n    >>> import dask\n\n    Set ``'foo.bar'`` in a context, by providing a mapping.\n\n    >>> with dask.config.set({'foo.bar': 123}):\n    ...     pass\n\n    Set ``'foo.bar'`` in a context, by providing a keyword argument.\n\n    >>> with dask.config.set(foo__bar=123):\n    ...     pass\n\n    Set ``'foo.bar'`` globally.\n\n    >>> dask.config.set(foo__bar=123)  # doctest: +SKIP\n\n    See Also\n    --------\n    dask.config.get\n    \"\"\"\n\n    config: dict\n    # [(op, path, value), ...]\n    _record: list[tuple[Literal[\"insert\", \"replace\"], tuple[str, ...], Any]]\n\n    def __init__(\n        self,\n        arg: Mapping | None = None,\n        config: dict | None = None,\n        lock: threading.Lock = config_lock,\n        **kwargs,\n    ):\n        if config is None:  # Keep Sphinx autofunction documentation clean\n            config = global_config\n\n        with lock:\n            self.config = config\n            self._record = []\n\n            if arg is not None:\n                for key, value in arg.items():\n                    key = check_deprecations(key)\n                    self._assign(key.split(\".\"), value, config)\n            if kwargs:\n                for key, value in kwargs.items():\n                    key = key.replace(\"__\", \".\")\n                    key = check_deprecations(key)\n                    self._assign(key.split(\".\"), value, config)\n\n    def __enter__(self):\n        return self.config\n\n    def __exit__(self, type, value, traceback):\n        for op, path, value in reversed(self._record):\n            d = self.config\n            if op == \"replace\":\n                for key in path[:-1]:\n                    d = d.setdefault(key, {})\n                d[path[-1]] = value\n            else:  # insert\n                for key in path[:-1]:\n                    try:\n                        d = d[key]\n                    except KeyError:\n                        break\n                else:\n                    d.pop(path[-1], None)\n\n    def _assign(\n        self,\n        keys: Sequence[str],\n        value: Any,\n        d: dict,\n        path: tuple[str, ...] = (),\n        record: bool = True,\n    ) -> None:\n        \"\"\"Assign value into a nested configuration dictionary\n\n        Parameters\n        ----------\n        keys : Sequence[str]\n            The nested path of keys to assign the value.\n        value : object\n        d : dict\n            The part of the nested dictionary into which we want to assign the\n            value\n        path : tuple[str], optional\n            The path history up to this point.\n        record : bool, optional\n            Whether this operation needs to be recorded to allow for rollback.\n        \"\"\"\n        key = canonical_name(keys[0], d)\n\n        path = path + (key,)\n\n        if len(keys) == 1:\n            if record:\n                if key in d:\n                    self._record.append((\"replace\", path, d[key]))\n                else:\n                    self._record.append((\"insert\", path, None))\n            d[key] = value\n        else:\n            if key not in d:\n                if record:\n                    self._record.append((\"insert\", path, None))\n                d[key] = {}\n                # No need to record subsequent operations after an insert\n                record = False\n            self._assign(keys[1:], value, d[key], path, record=record)\n\n\ndef collect(paths: list[str] = paths, env: Mapping[str, str] | None = None) -> dict:\n    \"\"\"\n    Collect configuration from paths and environment variables\n\n    Parameters\n    ----------\n    paths : list[str]\n        A list of paths to search for yaml config files\n\n    env : Mapping[str, str]\n        The system environment variables\n\n    Returns\n    -------\n    config: dict\n\n    See Also\n    --------\n    dask.config.refresh: collect configuration and update into primary config\n    \"\"\"\n    if env is None:\n        env = os.environ\n\n    configs = [*collect_yaml(paths=paths), collect_env(env=env)]\n    return merge(*configs)\n\n\ndef refresh(\n    config: dict | None = None, defaults: list[Mapping] = defaults, **kwargs\n) -> None:\n    \"\"\"\n    Update configuration by re-reading yaml files and env variables\n\n    This mutates the global dask.config.config, or the config parameter if\n    passed in.\n\n    This goes through the following stages:\n\n    1.  Clearing out all old configuration\n    2.  Updating from the stored defaults from downstream libraries\n        (see update_defaults)\n    3.  Updating from yaml files and environment variables\n    4.  Automatically renaming deprecated keys (with a warning)\n\n    Note that some functionality only checks configuration once at startup and\n    may not change behavior, even if configuration changes.  It is recommended\n    to restart your python process if convenient to ensure that new\n    configuration changes take place.\n\n    See Also\n    --------\n    dask.config.collect: for parameters\n    dask.config.update_defaults\n    \"\"\"\n    if config is None:  # Keep Sphinx autofunction documentation clean\n        config = global_config\n\n    config.clear()\n\n    for d in defaults:\n        update(config, d, priority=\"old\")\n\n    update(config, collect(**kwargs))\n    rename(deprecations, config)\n\n\ndef get(\n    key: str,\n    default: Any = no_default,\n    config: dict | None = None,\n    override_with: Any = None,\n) -> Any:\n    \"\"\"\n    Get elements from global config\n\n    If ``override_with`` is not None this value will be passed straight back.\n    Useful for getting kwarg defaults from Dask config.\n\n    Use '.' for nested access\n\n    Examples\n    --------\n    >>> from dask import config\n    >>> config.get('foo')  # doctest: +SKIP\n    {'x': 1, 'y': 2}\n\n    >>> config.get('foo.x')  # doctest: +SKIP\n    1\n\n    >>> config.get('foo.x.y', default=123)  # doctest: +SKIP\n    123\n\n    >>> config.get('foo.y', override_with=None)  # doctest: +SKIP\n    2\n\n    >>> config.get('foo.y', override_with=3)  # doctest: +SKIP\n    3\n\n    See Also\n    --------\n    dask.config.set\n    \"\"\"\n    if override_with is not None:\n        return override_with\n\n    if config is None:  # Keep Sphinx autofunction documentation clean\n        config = global_config\n\n    keys = key.split(\".\")\n    result = config\n    for k in keys:\n        k = canonical_name(k, result)\n        try:\n            result = result[k]\n        except (TypeError, IndexError, KeyError):\n            if default is no_default:\n                raise\n            return default\n\n    return result\n\n\ndef pop(key: str, default: Any = no_default, config: dict = config) -> Any:\n    \"\"\"Like ``get``, but remove the element if found\n\n    See Also\n    --------\n    dask.config.get\n    dask.config.set\n    \"\"\"\n    keys = key.split(\".\")\n    result = config\n    for i, k in enumerate(keys):\n        k = canonical_name(k, result)\n        try:\n            if i == len(keys) - 1:\n                return result.pop(k)\n            else:\n                result = result[k]\n        except (TypeError, IndexError, KeyError):\n            if default is no_default:\n                raise\n            return default\n\n\ndef update_defaults(\n    new: Mapping, config: dict = config, defaults: list[Mapping] = defaults\n) -> None:\n    \"\"\"Add a new set of defaults to the configuration\n\n    It does two things:\n\n    1.  Add the defaults to a global collection to be used by refresh later\n    2.  Updates the global config with the new configuration.\n        Old values are prioritized over new ones, unless the current value\n        is the old default, in which case it's updated to the new default.\n    \"\"\"\n    current_defaults = merge(*defaults)\n    defaults.append(new)\n    update(config, new, priority=\"new-defaults\", defaults=current_defaults)\n\n\ndef expand_environment_variables(config: Any) -> Any:\n    \"\"\"Expand environment variables in a nested config dictionary\n\n    This function will recursively search through any nested dictionaries\n    and/or lists.\n\n    Parameters\n    ----------\n    config : dict, iterable, or str\n        Input object to search for environment variables\n\n    Returns\n    -------\n    config : same type as input\n\n    Examples\n    --------\n    >>> expand_environment_variables({'x': [1, 2, '$USER']})  # doctest: +SKIP\n    {'x': [1, 2, 'my-username']}\n    \"\"\"\n    if isinstance(config, Mapping):\n        return {k: expand_environment_variables(v) for k, v in config.items()}\n    elif isinstance(config, str):\n        return os.path.expandvars(config)\n    elif isinstance(config, (list, tuple, builtins.set)):\n        return type(config)(expand_environment_variables(v) for v in config)\n    else:\n        return config\n\n\n#: Mapping of {deprecated key: new key} for renamed keys, or {deprecated key: None} for\n#: removed keys. All deprecated keys must use '-' instead of '_'.\n#: This is used in three places:\n#: 1. In refresh(), which calls rename() to rename and warn upon loading\n#:    from ~/.config/dask.yaml, DASK_ env variables, etc.\n#: 2. in distributed/config.py and equivalent modules, where we perform additional\n#:    distributed-specific renames for the yaml/env config and enrich this dict\n#: 3. from individual calls to dask.config.set(), which internally invoke\n#     check_deprecations()\ndeprecations: dict[str, str | None] = {\n    \"fuse-ave-width\": \"optimization.fuse.ave-width\",\n    \"fuse-max-height\": \"optimization.fuse.max-height\",\n    \"fuse-max-width\": \"optimization.fuse.max-width\",\n    \"fuse-rename-keys\": \"optimization.fuse.rename-keys\",\n    \"fuse-max-depth-new-edges\": \"optimization.fuse.max-depth-new-edges\",\n    # See https://github.com/dask/distributed/pull/4916\n    \"ucx.cuda-copy\": \"distributed.ucx.cuda_copy\",\n    \"ucx.tcp\": \"distributed.ucx.tcp\",\n    \"ucx.nvlink\": \"distributed.ucx.nvlink\",\n    \"ucx.infiniband\": \"distributed.ucx.infiniband\",\n    \"ucx.rdmacm\": \"distributed.ucx.rdmacm\",\n    \"ucx.net-devices\": \"distributed.ucx.net-devices\",\n    \"ucx.reuse-endpoints\": \"distributed.ucx.reuse-endpoints\",\n    \"rmm.pool-size\": \"distributed.rmm.pool-size\",\n    \"shuffle\": \"dataframe.shuffle.algorithm\",\n    \"array.rechunk-threshold\": \"array.rechunk.threshold\",\n    \"dataframe.shuffle.algorithm\": \"dataframe.shuffle.method\",\n    \"dataframe.shuffle-compression\": \"dataframe.shuffle.compression\",\n    \"admin.traceback.shorten.what\": \"admin.traceback.shorten\",  # changed in 2023.9.0\n    \"array.shuffle.chunksize-tolerance\": \"array.chunk-size-tolerance\",\n}\n\n\ndef rename(\n    deprecations: Mapping[str, str | None] = deprecations, config: dict = config\n) -> None:\n    \"\"\"Rename old keys to new keys\n\n    This helps migrate older configuration versions over time\n\n    See Also\n    --------\n    check_deprecations\n    \"\"\"\n    for key in deprecations:\n        try:\n            value = pop(key, config=config)\n        except (TypeError, IndexError, KeyError):\n            continue\n        key = canonical_name(key, config=config)\n        new = check_deprecations(key, deprecations)\n        if new:\n            set({new: value}, config=config)\n\n\ndef check_deprecations(\n    key: str, deprecations: Mapping[str, str | None] = deprecations\n) -> str:\n    \"\"\"Check if the provided value has been renamed or removed\n\n    Parameters\n    ----------\n    key : str\n        The configuration key to check\n    deprecations : Dict[str, str]\n        The mapping of aliases\n\n    Examples\n    --------\n    >>> deprecations = {\"old_key\": \"new_key\", \"invalid\": None}\n    >>> check_deprecations(\"old_key\", deprecations=deprecations)  # doctest: +SKIP\n    FutureWarning: Dask configuration key 'old_key' has been deprecated; please use \"new_key\" instead\n\n    >>> check_deprecations(\"invalid\", deprecations=deprecations)\n    Traceback (most recent call last):\n        ...\n    ValueError: Dask configuration key 'invalid' has been removed\n\n    >>> check_deprecations(\"another_key\", deprecations=deprecations)\n    'another_key'\n\n    Returns\n    -------\n    new: str\n        The proper key, whether the original (if no deprecation) or the aliased\n        value\n\n    See Also\n    --------\n    rename\n    \"\"\"\n    old = key.replace(\"_\", \"-\")\n    if old in deprecations:\n        new = deprecations[old]\n        if new:\n            warnings.warn(\n                f\"Dask configuration key {key!r} has been deprecated; \"\n                f\"please use {new!r} instead\",\n                FutureWarning,\n            )\n            return new\n        else:\n            raise ValueError(f\"Dask configuration key {key!r} has been removed\")\n    else:\n        return key\n\n\ndef serialize(data: Any) -> str:\n    \"\"\"Serialize config data into a string.\n\n    Typically used to pass config via the ``DASK_INTERNAL_INHERIT_CONFIG`` environment variable.\n\n    Parameters\n    ----------\n    data: json-serializable object\n        The data to serialize\n\n    Returns\n    -------\n    serialized_data: str\n        The serialized data as a string\n\n    \"\"\"\n    return base64.urlsafe_b64encode(json.dumps(data).encode()).decode()\n\n\ndef deserialize(data: str) -> Any:\n    \"\"\"De-serialize config data into the original object.\n\n    Typically when receiving config via the ``DASK_INTERNAL_INHERIT_CONFIG`` environment variable.\n\n    Parameters\n    ----------\n    data: str\n        String serialized by :func:`dask.config.serialize`\n\n    Returns\n    -------\n    deserialized_data: obj\n        The de-serialized data\n\n    \"\"\"\n    return json.loads(base64.urlsafe_b64decode(data.encode()).decode())\n\n\ndef _initialize() -> None:\n    fn = os.path.join(os.path.dirname(__file__), \"dask.yaml\")\n\n    with open(fn) as f:\n        _defaults = yaml.safe_load(f)\n\n    update_defaults(_defaults)\n\n\nrefresh()\n_initialize()\n\n\n### Dependency File: local.py\n\"\"\"\nAsynchronous Shared-Memory Scheduler for Dask Graphs.\n\nThis scheduler coordinates several workers to execute tasks in a dask graph in\nparallel.  It depends on a ``concurrent.futures.Executor``\nand a corresponding Queue for worker-to-scheduler communication.\n\nIt tries to execute tasks in an order which maintains a small memory footprint\nthroughout execution.  It does this by running tasks that allow us to release\ndata resources.\n\n\nTask Selection Policy\n=====================\n\nWhen we complete a task we add more data in to our set of available data; this\nnew data makes new tasks available.  We preferentially choose tasks that were\njust made available in a last-in-first-out fashion.  We implement this as a\nsimple stack.  This results in more depth-first rather than breadth first\nbehavior which encourages us to process batches of data to completion before\nstarting in on new data when possible.\n\nWhen the addition of new data readies multiple tasks simultaneously we add\ntasks to the stack in sorted order so that tasks with greater keynames are run\nfirst.  This can be handy to break ties in a predictable fashion.\n\n\nState\n=====\n\nMany functions pass around a ``state`` variable that holds the current state of\nthe computation.  This variable consists of several other dictionaries and\nsets, explained below.\n\nConstant state\n--------------\n\n1.  dependencies: {x: [a, b ,c]} a,b,c, must be run before x\n2.  dependents: {a: [x, y]} a must run before x or y\n\nChanging state\n--------------\n\n### Data\n\n1.  cache: available concrete data.  {key: actual-data}\n2.  released: data that we've seen, used, and released because it is no longer\n    needed\n\n### Jobs\n\n1.  ready: A fifo stack of ready-to-run tasks\n2.  running: A set of tasks currently in execution\n3.  finished: A set of finished tasks\n4.  waiting: which tasks are still waiting on others :: {key: {keys}}\n    Real-time equivalent of dependencies\n5.  waiting_data: available data to yet-to-be-run-tasks :: {key: {keys}}\n    Real-time equivalent of dependents\n\n\nExamples\n--------\n\n>>> import pprint  # doctest: +SKIP\n>>> inc = lambda x: x + 1\n>>> add = lambda x, y: x + y\n>>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n>>> pprint.pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n{'cache': {'x': 1, 'y': 2},\n 'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n 'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n 'finished': set(),\n 'ready': ['z'],\n 'released': set(),\n 'running': set(),\n 'waiting': {'w': {'z'}},\n 'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n\nOptimizations\n=============\n\nWe build this scheduler with out-of-core array operations in mind.  To this end\nwe have encoded some particular optimizations.\n\nCompute to release data\n-----------------------\n\nWhen we choose a new task to execute we often have many options.  Policies at\nthis stage are cheap and can significantly impact performance.  One could\nimagine policies that expose parallelism, drive towards a particular output,\netc..\n\nOur current policy is to run tasks that were most recently made available.\n\n\nInlining computations\n---------------------\n\nWe hold on to intermediate computations either in memory or on disk.\n\nFor very cheap computations that may emit new copies of the data, like\n``np.transpose`` or possibly even ``x + 1`` we choose not to store these as\nseparate pieces of data / tasks.  Instead we combine them with the computations\nthat require them.  This may result in repeated computation but saves\nsignificantly on space and computation complexity.\n\nSee the function ``inline_functions`` for more information.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections.abc import Mapping, Sequence\nfrom concurrent.futures import Executor, Future\nfrom functools import partial\nfrom queue import Empty, Queue\n\nfrom dask import config\nfrom dask._task_spec import DataNode, DependenciesMapping, convert_legacy_graph\nfrom dask.callbacks import local_callbacks, unpack_callbacks\nfrom dask.core import flatten, get_dependencies, reverse_dict\nfrom dask.order import order\nfrom dask.typing import Key\n\nif os.name == \"nt\":\n    # Python 3 windows Queue.get doesn't handle interrupts properly. To\n    # workaround this we poll at a sufficiently large interval that it\n    # shouldn't affect performance, but small enough that users trying to kill\n    # an application shouldn't care.\n    def queue_get(q):\n        while True:\n            try:\n                return q.get(block=True, timeout=0.1)\n            except Empty:\n                pass\n\nelse:\n\n    def queue_get(q):\n        return q.get()\n\n\ndef start_state_from_dask(dsk, cache=None, sortkey=None):\n    \"\"\"Start state from a dask\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n    >>> from pprint import pprint  # doctest: +SKIP\n    >>> pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n    {'cache': {'x': 1, 'y': 2},\n     'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n     'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n     'finished': set(),\n     'ready': ['z'],\n     'released': set(),\n     'running': set(),\n     'waiting': {'w': {'z'}},\n     'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n    \"\"\"\n    if sortkey is None:\n        sortkey = order(dsk).get\n    if cache is None:\n        cache = config.get(\"cache\", None)\n    if cache is None:\n        cache = dict()\n\n    dsk = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    data_keys = set()\n    for k, v in dsk.items():\n        if isinstance(v, DataNode):\n            cache[k] = v()\n            data_keys.add(k)\n\n    dsk2 = dsk.copy()\n    dsk2.update(cache)\n\n    dependencies = DependenciesMapping(dsk)\n    waiting = {k: set(v) for k, v in dependencies.items() if k not in data_keys}\n\n    dependents = reverse_dict(dependencies)\n    for a in cache:\n        for b in dependents.get(a, ()):\n            waiting[b].remove(a)\n    waiting_data = {k: v.copy() for k, v in dependents.items() if v}\n\n    ready_set = {k for k, v in waiting.items() if not v}\n    ready = sorted(ready_set, key=sortkey, reverse=True)\n    waiting = {k: v for k, v in waiting.items() if v}\n\n    state = {\n        \"dependencies\": dependencies,\n        \"dependents\": dependents,\n        \"waiting\": waiting,\n        \"waiting_data\": waiting_data,\n        \"cache\": cache,\n        \"ready\": ready,\n        \"running\": set(),\n        \"finished\": set(),\n        \"released\": set(),\n    }\n\n    return state\n\n\n\"\"\"\nRunning tasks\n-------------\n\nWhen we execute tasks we both\n\n1.  Perform the actual work of collecting the appropriate data and calling the function\n2.  Manage administrative state to coordinate with the scheduler\n\"\"\"\n\n\ndef execute_task(key, task_info, dumps, loads, get_id, pack_exception):\n    \"\"\"\n    Compute task and handle all administration\n\n    See Also\n    --------\n    _execute_task : actually execute task\n    \"\"\"\n    try:\n        task, data = loads(task_info)\n        result = task(data)\n        id = get_id()\n        result = dumps((result, id))\n        failed = False\n    except BaseException as e:  # noqa: B036\n        result = pack_exception(e, dumps)\n        failed = True\n    return key, result, failed\n\n\ndef batch_execute_tasks(it):\n    \"\"\"\n    Batch computing of multiple tasks with `execute_task`\n    \"\"\"\n    return [execute_task(*a) for a in it]\n\n\ndef release_data(key, state, delete=True):\n    \"\"\"Remove data from temporary storage\n\n    See Also\n    --------\n    finish_task\n    \"\"\"\n    if key in state[\"waiting_data\"]:\n        assert not state[\"waiting_data\"][key]\n        del state[\"waiting_data\"][key]\n\n    state[\"released\"].add(key)\n\n    if delete:\n        del state[\"cache\"][key]\n\n\ndef finish_task(\n    dsk, key, state, results, sortkey, delete=True, release_data=release_data\n):\n    \"\"\"\n    Update execution state after a task finishes\n\n    Mutates.  This should run atomically (with a lock).\n    \"\"\"\n    for dep in sorted(state[\"dependents\"][key], key=sortkey, reverse=True):\n        s = state[\"waiting\"][dep]\n        s.remove(key)\n        if not s:\n            del state[\"waiting\"][dep]\n            state[\"ready\"].append(dep)\n\n    for dep in state[\"dependencies\"][key]:\n        if dep in state[\"waiting_data\"]:\n            s = state[\"waiting_data\"][dep]\n            s.remove(key)\n            if not s and dep not in results:\n                release_data(dep, state, delete=delete)\n        elif delete and dep not in results:\n            release_data(dep, state, delete=delete)\n\n    state[\"finished\"].add(key)\n    state[\"running\"].remove(key)\n\n    return state\n\n\ndef nested_get(ind, coll):\n    \"\"\"Get nested index from collection\n\n    Examples\n    --------\n\n    >>> nested_get(1, 'abc')\n    'b'\n    >>> nested_get([1, 0], 'abc')\n    ('b', 'a')\n    >>> nested_get([[1, 0], [0, 1]], 'abc')\n    (('b', 'a'), ('a', 'b'))\n    \"\"\"\n    if isinstance(ind, list):\n        return tuple(nested_get(i, coll) for i in ind)\n    else:\n        return coll[ind]\n\n\ndef default_get_id():\n    \"\"\"Default get_id\"\"\"\n    return None\n\n\ndef default_pack_exception(e, dumps):\n    raise\n\n\ndef reraise(exc, tb=None):\n    if exc.__traceback__ is not tb:\n        raise exc.with_traceback(tb)\n    raise exc\n\n\ndef identity(x):\n    \"\"\"Identity function. Returns x.\n\n    >>> identity(3)\n    3\n    \"\"\"\n    return x\n\n\n\"\"\"\nTask Selection\n--------------\n\nWe often have a choice among many tasks to run next.  This choice is both\ncheap and can significantly impact performance.\n\nWe currently select tasks that have recently been made ready.  We hope that\nthis first-in-first-out policy reduces memory footprint\n\"\"\"\n\n\"\"\"\n`get`\n-----\n\nThe main function of the scheduler.  Get is the main entry point.\n\"\"\"\n\n\ndef get_async(\n    submit,\n    num_workers,\n    dsk,\n    result,\n    cache=None,\n    get_id=default_get_id,\n    rerun_exceptions_locally=None,\n    pack_exception=default_pack_exception,\n    raise_exception=reraise,\n    callbacks=None,\n    dumps=identity,\n    loads=identity,\n    chunksize=None,\n    **kwargs,\n):\n    \"\"\"Asynchronous get function\n\n    This is a general version of various asynchronous schedulers for dask.  It\n    takes a ``concurrent.futures.Executor.submit`` function to form a more\n    specific ``get`` method that walks through the dask array with parallel\n    workers, avoiding repeat computation and minimizing memory use.\n\n    Parameters\n    ----------\n    submit : function\n        A ``concurrent.futures.Executor.submit`` function\n    num_workers : int\n        The number of workers that task submissions can be spread over\n    dsk : dict\n        A dask dictionary specifying a workflow\n    result : key or list of keys\n        Keys corresponding to desired data\n    cache : dict-like, optional\n        Temporary storage of results\n    get_id : callable, optional\n        Function to return the worker id, takes no arguments. Examples are\n        `threading.current_thread` and `multiprocessing.current_process`.\n    rerun_exceptions_locally : bool, optional\n        Whether to rerun failing tasks in local process to enable debugging\n        (False by default)\n    pack_exception : callable, optional\n        Function to take an exception and ``dumps`` method, and return a\n        serialized tuple of ``(exception, traceback)`` to send back to the\n        scheduler. Default is to just raise the exception.\n    raise_exception : callable, optional\n        Function that takes an exception and a traceback, and raises an error.\n    callbacks : tuple or list of tuples, optional\n        Callbacks are passed in as tuples of length 5. Multiple sets of\n        callbacks may be passed in as a list of tuples. For more information,\n        see the dask.diagnostics documentation.\n    dumps: callable, optional\n        Function to serialize task data and results to communicate between\n        worker and parent.  Defaults to identity.\n    loads: callable, optional\n        Inverse function of `dumps`.  Defaults to identity.\n    chunksize: int, optional\n        Size of chunks to use when dispatching work. Defaults to 1.\n        If -1, will be computed to evenly divide ready work across workers.\n\n    See Also\n    --------\n    threaded.get\n    \"\"\"\n    chunksize = chunksize or config.get(\"chunksize\", 1)\n\n    queue = Queue()\n\n    if isinstance(result, list):\n        result_flat = set(flatten(result))\n    else:\n        result_flat = {result}\n    results = set(result_flat)\n\n    dsk = dict(convert_legacy_graph(dsk))\n    with local_callbacks(callbacks) as callbacks:\n        _, _, pretask_cbs, posttask_cbs, _ = unpack_callbacks(callbacks)\n        started_cbs = []\n        succeeded = False\n        # if start_state_from_dask fails, we will have something\n        # to pass to the final block.\n        state = {}\n        try:\n            for cb in callbacks:\n                if cb[0]:\n                    cb[0](dsk)\n                started_cbs.append(cb)\n\n            keyorder = order(dsk)\n\n            state = start_state_from_dask(dsk, cache=cache, sortkey=keyorder.get)\n\n            for _, start_state, _, _, _ in callbacks:\n                if start_state:\n                    start_state(dsk, state)\n\n            if rerun_exceptions_locally is None:\n                rerun_exceptions_locally = config.get(\"rerun_exceptions_locally\", False)\n\n            if state[\"waiting\"] and not state[\"ready\"]:\n                raise ValueError(\"Found no accessible jobs in dask\")\n\n            def fire_tasks(chunksize):\n                \"\"\"Fire off a task to the thread pool\"\"\"\n                # Determine chunksize and/or number of tasks to submit\n                nready = len(state[\"ready\"])\n                if chunksize == -1:\n                    ntasks = nready\n                    chunksize = -(ntasks // -num_workers)\n                else:\n                    used_workers = -(len(state[\"running\"]) // -chunksize)\n                    avail_workers = max(num_workers - used_workers, 0)\n                    ntasks = min(nready, chunksize * avail_workers)\n\n                # Prep all ready tasks for submission\n                args = []\n                for _ in range(ntasks):\n                    # Get the next task to compute (most recently added)\n                    key = state[\"ready\"].pop()\n                    # Notify task is running\n                    state[\"running\"].add(key)\n                    for f in pretask_cbs:\n                        f(key, dsk, state)\n\n                    # Prep args to send\n                    data = {\n                        dep: state[\"cache\"][dep] for dep in get_dependencies(dsk, key)\n                    }\n                    args.append(\n                        (\n                            key,\n                            dumps((dsk[key], data)),\n                            dumps,\n                            loads,\n                            get_id,\n                            pack_exception,\n                        )\n                    )\n\n                # Batch submit\n                for i in range(-(len(args) // -chunksize)):\n                    each_args = args[i * chunksize : (i + 1) * chunksize]\n                    if not each_args:\n                        break\n                    fut = submit(batch_execute_tasks, each_args)\n                    fut.add_done_callback(queue.put)\n\n            # Main loop, wait on tasks to finish, insert new ones\n            while state[\"waiting\"] or state[\"ready\"] or state[\"running\"]:\n                fire_tasks(chunksize)\n                for key, res_info, failed in queue_get(queue).result():\n                    if failed:\n                        exc, tb = loads(res_info)\n                        if rerun_exceptions_locally:\n                            data = {\n                                dep: state[\"cache\"][dep]\n                                for dep in get_dependencies(dsk, key)\n                            }\n                            task = dsk[key]\n                            task(data)  # Re-execute locally\n                        else:\n                            raise_exception(exc, tb)\n                    res, worker_id = loads(res_info)\n                    state[\"cache\"][key] = res\n                    finish_task(dsk, key, state, results, keyorder.get)\n                    for f in posttask_cbs:\n                        f(key, res, dsk, state, worker_id)\n\n            succeeded = True\n\n        finally:\n            for _, _, _, _, finish in started_cbs:\n                if finish:\n                    finish(dsk, state, not succeeded)\n\n    return nested_get(result, state[\"cache\"])\n\n\n\"\"\" Synchronous concrete version of get_async\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a\nsequential one.  This is useful for debugging and for code dominated by the\nGIL\n\"\"\"\n\n\nclass SynchronousExecutor(Executor):\n    _max_workers = 1\n\n    def submit(self, fn, *args, **kwargs):\n        fut = Future()\n        try:\n            fut.set_result(fn(*args, **kwargs))\n        except BaseException as e:  # noqa: B036\n            fut.set_exception(e)\n        return fut\n\n\nsynchronous_executor = SynchronousExecutor()\n\n\ndef get_sync(dsk: Mapping, keys: Sequence[Key] | Key, **kwargs):\n    \"\"\"A naive synchronous version of get_async\n\n    Can be useful for debugging.\n    \"\"\"\n    kwargs.pop(\"num_workers\", None)  # if num_workers present, remove it\n    return get_async(\n        synchronous_executor.submit,\n        synchronous_executor._max_workers,\n        dsk,\n        keys,\n        **kwargs,\n    )\n\n\n\"\"\" Adaptor for ``multiprocessing.Pool`` instances\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a wrapper\nclass for ``multiprocessing.Pool`` instances so we can treat them like\n``concurrent.futures.Executor`` instances instead.\n\nThis is mainly useful for legacy use cases or users that prefer\n``multiprocessing.Pool``.\n\"\"\"\n\n\nclass MultiprocessingPoolExecutor(Executor):\n    def __init__(self, pool):\n        self.pool = pool\n        self._max_workers = len(pool._pool)\n\n    def submit(self, fn, *args, **kwargs):\n        return submit_apply_async(self.pool.apply_async, fn, *args, **kwargs)\n\n\ndef submit_apply_async(apply_async, fn, *args, **kwargs):\n    fut = Future()\n    apply_async(fn, args, kwargs, fut.set_result, fut.set_exception)\n    return fut\n\n\ndef get_apply_async(apply_async, num_workers, *args, **kwargs):\n    return get_async(\n        partial(submit_apply_async, apply_async), num_workers, *args, **kwargs\n    )\n\n\ndef sortkey(item):\n    \"\"\"Sorting key function that is robust to different types\n\n    Both strings and tuples are common key types in dask graphs.\n    However In Python 3 one can not compare strings with tuples directly.\n    This function maps many types to a form where they can be compared\n\n    Examples\n    --------\n    >>> sortkey('Hello')\n    ('str', 'Hello')\n\n    >>> sortkey(('x', 1))\n    ('tuple', ('x', 1))\n    \"\"\"\n    return (type(item).__name__, item)\n\n\n### Dependency File: system.py\nfrom __future__ import annotations\n\nimport math\nimport os\nimport sys\n\ntry:\n    import psutil\nexcept ImportError:\n    psutil = None  # type: ignore\n\n__all__ = (\"cpu_count\", \"CPU_COUNT\")\n\n\ndef _try_extract_cgroup_cpu_quota():\n    # cgroup v1\n    # The directory name isn't standardized across linux distros, check both\n    for dirname in [\"cpuacct,cpu\", \"cpu,cpuacct\"]:\n        try:\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_quota_us\" % dirname) as f:\n                quota = int(f.read())\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_period_us\" % dirname) as f:\n                period = int(f.read())\n            return quota, period\n        except Exception:\n            pass\n\n    # cgroup v2\n    try:\n        with open(\"/proc/self/cgroup\") as f:\n            group_path = f.read().strip().split(\":\")[-1]\n        if not group_path.endswith(\"/\"):\n            group_path = f\"{group_path}/\"\n        with open(\"/sys/fs/cgroup%scpu.max\" % group_path) as f:\n            quota, period = map(int, f.read().split(\" \"))\n            return quota, period\n    except Exception:\n        pass\n\n    # No cgroup CPU quota found\n    return None, None\n\n\ndef cpu_count():\n    \"\"\"Get the available CPU count for this system.\n\n    Takes the minimum value from the following locations:\n\n    - Total system cpus available on the host.\n    - CPU Affinity (if set)\n    - Cgroups limit (if set)\n    \"\"\"\n    count = os.cpu_count()\n\n    # Check CPU affinity if available\n    if psutil is not None:\n        try:\n            affinity_count = len(psutil.Process().cpu_affinity())\n            if affinity_count > 0:\n                count = min(count, affinity_count)\n        except Exception:\n            pass\n\n    # Check cgroups if available\n    if sys.platform == \"linux\":\n        quota, period = _try_extract_cgroup_cpu_quota()\n        if quota is not None and period is not None:\n            # We round up on fractional CPUs\n            cgroups_count = math.ceil(quota / period)\n            if cgroups_count > 0:\n                count = min(count, cgroups_count)\n\n    return count\n\n\nCPU_COUNT = cpu_count()\n\n\n### Dependency File: typing.py\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Callable, Hashable, Mapping, Sequence\nfrom enum import Enum\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Literal,\n    Protocol,\n    TypeVar,\n    Union,\n    runtime_checkable,\n)\n\nif TYPE_CHECKING:\n    # IPython import is relatively slow. Avoid if not necessary\n    # TODO import from typing (requires Python >=3.10)\n    from typing import TypeAlias\n\n    from IPython.display import DisplayObject\n\nCollType = TypeVar(\"CollType\", bound=\"DaskCollection\")\nCollType_co = TypeVar(\"CollType_co\", bound=\"DaskCollection\", covariant=True)\nPostComputeCallable = Callable\n\n\nKey: TypeAlias = Union[str, bytes, int, float, tuple[\"Key\", ...]]\n# FIXME: This type is a little misleading. Low level graphs are often\n# MutableMappings but HLGs are not\nGraph: TypeAlias = Mapping[Key, Any]\n# Potentially nested list of Dask keys\nNestedKeys: TypeAlias = list[Union[Key, \"NestedKeys\"]]\n\n\nclass SchedulerGetCallable(Protocol):\n    \"\"\"Protocol defining the signature of a ``__dask_scheduler__`` callable.\"\"\"\n\n    def __call__(\n        self,\n        dsk: Graph,\n        keys: Sequence[Key] | Key,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Method called as the default scheduler for a collection.\n\n        Parameters\n        ----------\n        dsk :\n            The task graph.\n        keys :\n            Key(s) corresponding to the desired data.\n        **kwargs :\n            Additional arguments.\n\n        Returns\n        -------\n        Any\n            Result(s) associated with `keys`\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\nclass PostPersistCallable(Protocol[CollType_co]):\n    \"\"\"Protocol defining the signature of a ``__dask_postpersist__`` callable.\"\"\"\n\n    def __call__(\n        self,\n        dsk: Graph,\n        *args: Any,\n        rename: Mapping[str, str] | None = None,\n    ) -> CollType_co:\n        \"\"\"Method called to rebuild a persisted collection.\n\n        Parameters\n        ----------\n        dsk: Mapping\n            A mapping which contains at least the output keys returned\n            by __dask_keys__().\n        *args : Any\n            Additional optional arguments If no extra arguments are\n            necessary, it must be an empty tuple.\n        rename : Mapping[str, str], optional\n            If defined, it indicates that output keys may be changing\n            too; e.g. if the previous output of :meth:`__dask_keys__`\n            was ``[(\"a\", 0), (\"a\", 1)]``, after calling\n            ``rebuild(dsk, *extra_args, rename={\"a\": \"b\"})``\n            it must become ``[(\"b\", 0), (\"b\", 1)]``.\n            The ``rename`` mapping may not contain the collection\n            name(s); in such case the associated keys do not change.\n            It may contain replacements for unexpected names, which\n            must be ignored.\n\n        Returns\n        -------\n        Collection\n            An equivalent Dask collection with the same keys as\n            computed through a different graph.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\n@runtime_checkable\nclass DaskCollection(Protocol):\n    \"\"\"Protocol defining the interface of a Dask collection.\"\"\"\n\n    @abc.abstractmethod\n    def __dask_graph__(self) -> Graph:\n        \"\"\"The Dask task graph.\n\n        The core Dask collections (Array, DataFrame, Bag, and Delayed)\n        use a :py:class:`~dask.highlevelgraph.HighLevelGraph` to\n        represent the collection task graph. It is also possible to\n        represent the task graph as a low level graph using a Python\n        dictionary.\n\n        Returns\n        -------\n        Mapping\n            The Dask task graph. If the instance returns a\n            :py:class:`dask.highlevelgraph.HighLevelGraph` then the\n            :py:func:`__dask_layers__` method must be implemented, as\n            defined by the :py:class:`~dask.typing.HLGDaskCollection`\n            protocol.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_keys__(self) -> NestedKeys:\n        \"\"\"The output keys of the task graph.\n\n        Note that there are additional constraints on keys for a Dask\n        collection than those described in the :doc:`task graph\n        specification documentation <spec>`. These additional\n        constraints are described below.\n\n        All keys must either be non-empty strings or tuples where the first element is a\n        non-empty string, followed by zero or more arbitrary str, bytes, int, float, or\n        tuples thereof. The non-empty string is commonly known as the *collection name*.\n        All collections embedded in the dask package have exactly one name, but this is\n        not a requirement.\n\n        These are all valid outputs:\n\n        - ``[]``\n        - ``[\"x\", \"y\"]``\n        - ``[[(\"y\", \"a\", 0), (\"y\", \"a\", 1)], [(\"y\", \"b\", 0), (\"y\", \"b\", 1)]``\n\n        Returns\n        -------\n        list\n            A possibly nested list of keys that represent the outputs\n            of the graph. After computation, the results will be\n            returned in the same layout, with the keys replaced with\n            their corresponding outputs.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_postcompute__(self) -> tuple[PostComputeCallable, tuple]:\n        \"\"\"Finalizer function and optional arguments to construct final result.\n\n        Upon computation each key in the collection will have an in\n        memory result, the postcompute function combines each key's\n        result into a final in memory representation. For example,\n        dask.array.Array concatenates the arrays at each chunk into a\n        final in-memory array.\n\n        Returns\n        -------\n        PostComputeCallable\n            Callable that receives the sequence of the results of each\n            final key along with optional arguments. An example signature\n            would be ``finalize(results: Sequence[Any], *args)``.\n        tuple[Any, ...]\n            Optional arguments passed to the function following the\n            key results (the `*args` part of the\n            ``PostComputeCallable``. If no additional arguments are to\n            be passed then this must be an empty tuple.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_postpersist__(self) -> tuple[PostPersistCallable, tuple]:\n        \"\"\"Rebuilder function and optional arguments to construct a persisted collection.\n\n        See also the documentation for :py:class:`dask.typing.PostPersistCallable`.\n\n        Returns\n        -------\n        PostPersistCallable\n            Callable that rebuilds the collection. The signature\n            should be\n            ``rebuild(dsk: Mapping, *args: Any, rename: Mapping[str, str] | None)``\n            (as defined by the\n            :py:class:`~dask.typing.PostPersistCallable` protocol).\n            The callable should return an equivalent Dask collection\n            with the same keys as `self`, but with results that are\n            computed through a different graph. In the case of\n            :py:func:`dask.persist`, the new graph will have just the\n            output keys and the values already computed.\n        tuple[Any, ...]\n            Optional arguments passed to the rebuild callable. If no\n            additional arguments are to be passed then this must be an\n            empty tuple.\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def __dask_tokenize__(self) -> Hashable:\n        \"\"\"Value that must fully represent the object.\"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    __dask_optimize__: Any\n    \"\"\"Given a graph and keys, return a new optimized graph.\n\n    This method can be either a ``staticmethod`` or a ``classmethod``,\n    but not an ``instancemethod``. For example implementations see the\n    definitions of ``__dask_optimize__`` in the core Dask collections:\n    ``dask.array.Array``, ``dask.dataframe.DataFrame``, etc.\n\n    Note that graphs and keys are merged before calling\n    ``__dask_optimize__``; as such, the graph and keys passed to\n    this method may represent more than one collection sharing the\n    same optimize method.\n\n    Parameters\n    ----------\n    dsk : Graph\n        The merged graphs from all collections sharing the same\n        ``__dask_optimize__`` method.\n    keys : Sequence[Key]\n        A list of the outputs from ``__dask_keys__`` from all\n        collections sharing the same ``__dask_optimize__`` method.\n    **kwargs : Any\n        Extra keyword arguments forwarded from the call to\n        ``compute`` or ``persist``. Can be used or ignored as\n        needed.\n\n    Returns\n    -------\n    MutableMapping\n        The optimized Dask graph.\n\n    \"\"\"\n\n    # FIXME: It is currently not possible to define a staticmethod from a callback protocol\n    # Also, the definition in `is_dask_collection` cannot be satisfied by a\n    # protocol / typing check\n    # staticmethod[SchedulerGetCallable]\n    __dask_scheduler__: staticmethod\n    \"\"\"The default scheduler ``get`` to use for this object.\n\n    Usually attached to the class as a staticmethod, e.g.:\n\n    >>> import dask.threaded\n    >>> class MyCollection:\n    ...     # Use the threaded scheduler by default\n    ...     __dask_scheduler__ = staticmethod(dask.threaded.get)\n\n    \"\"\"\n\n    @abc.abstractmethod\n    def compute(self, **kwargs: Any) -> Any:\n        \"\"\"Compute this dask collection.\n\n        This turns a lazy Dask collection into its in-memory\n        equivalent. For example a Dask array turns into a NumPy array\n        and a Dask dataframe turns into a Pandas dataframe. The entire\n        dataset must fit into memory before calling this operation.\n\n        Parameters\n        ----------\n        scheduler : string, optional\n            Which scheduler to use like \"threads\", \"synchronous\" or\n            \"processes\". If not provided, the default is to check the\n            global settings first, and then fall back to the\n            collection defaults.\n        optimize_graph : bool, optional\n            If True [default], the graph is optimized before\n            computation. Otherwise the graph is run as is. This can be\n            useful for debugging.\n        kwargs :\n            Extra keywords to forward to the scheduler function.\n\n        Returns\n        -------\n        The collection's computed result.\n\n        See Also\n        --------\n        dask.compute\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def persist(self: CollType, **kwargs: Any) -> CollType:\n        \"\"\"Persist this dask collection into memory\n\n        This turns a lazy Dask collection into a Dask collection with\n        the same metadata, but now with the results fully computed or\n        actively computing in the background.\n\n        The action of function differs significantly depending on the\n        active task scheduler. If the task scheduler supports\n        asynchronous computing, such as is the case of the\n        dask.distributed scheduler, then persist will return\n        *immediately* and the return value's task graph will contain\n        Dask Future objects. However if the task scheduler only\n        supports blocking computation then the call to persist will\n        *block* and the return value's task graph will contain\n        concrete Python results.\n\n        This function is particularly useful when using distributed\n        systems, because the results will be kept in distributed\n        memory, rather than returned to the local process as with\n        compute.\n\n        Parameters\n        ----------\n        scheduler : string, optional\n            Which scheduler to use like \"threads\", \"synchronous\" or\n            \"processes\". If not provided, the default is to check the\n            global settings first, and then fall back to the\n            collection defaults.\n        optimize_graph : bool, optional\n            If True [default], the graph is optimized before\n            computation. Otherwise the graph is run as is. This can be\n            useful for debugging.\n        **kwargs\n            Extra keywords to forward to the scheduler function.\n\n        Returns\n        -------\n        New dask collections backed by in-memory data\n\n        See Also\n        --------\n        dask.persist\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n    @abc.abstractmethod\n    def visualize(\n        self,\n        filename: str = \"mydask\",\n        format: str | None = None,\n        optimize_graph: bool = False,\n        **kwargs: Any,\n    ) -> DisplayObject | None:\n        \"\"\"Render the computation of this object's task graph using graphviz.\n\n        Requires ``graphviz`` to be installed.\n\n        Parameters\n        ----------\n        filename : str or None, optional\n            The name of the file to write to disk. If the provided\n            `filename` doesn't include an extension, '.png' will be\n            used by default. If `filename` is None, no file will be\n            written, and we communicate with dot using only pipes.\n        format : {'png', 'pdf', 'dot', 'svg', 'jpeg', 'jpg'}, optional\n            Format in which to write output file. Default is 'png'.\n        optimize_graph : bool, optional\n            If True, the graph is optimized before rendering.\n            Otherwise, the graph is displayed as is. Default is False.\n        color: {None, 'order'}, optional\n            Options to color nodes. Provide ``cmap=`` keyword for\n            additional colormap\n        **kwargs\n           Additional keyword arguments to forward to ``to_graphviz``.\n\n        Examples\n        --------\n        >>> x.visualize(filename='dask.pdf')  # doctest: +SKIP\n        >>> x.visualize(filename='dask.pdf', color='order')  # doctest: +SKIP\n\n        Returns\n        -------\n        result : IPython.display.Image, IPython.display.SVG, or None\n            See dask.dot.dot_graph for more information.\n\n        See Also\n        --------\n        dask.visualize\n        dask.dot.dot_graph\n\n        Notes\n        -----\n        For more information on optimization see here:\n\n        https://docs.dask.org/en/latest/optimize.html\n\n        \"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\n@runtime_checkable\nclass HLGDaskCollection(DaskCollection, Protocol):\n    \"\"\"Protocol defining a Dask collection that uses HighLevelGraphs.\n\n    This protocol is nearly identical to\n    :py:class:`~dask.typing.DaskCollection`, with the addition of the\n    ``__dask_layers__`` method (required for collections backed by\n    high level graphs).\n\n    \"\"\"\n\n    @abc.abstractmethod\n    def __dask_layers__(self) -> Sequence[str]:\n        \"\"\"Names of the HighLevelGraph layers.\"\"\"\n        raise NotImplementedError(\"Inheriting class must implement this method.\")\n\n\nclass _NoDefault(Enum):\n    \"\"\"typing-aware constant to detect when the user omits a parameter and you can't use\n    None.\n\n    Copied from pandas._libs.lib._NoDefault.\n\n    Usage\n    -----\n    from dask.typing import NoDefault, no_default\n\n    def f(x: int | None | NoDefault = no_default) -> int:\n        if x is no_default:\n            ...\n    \"\"\"\n\n    no_default = \"NO_DEFAULT\"\n\n    def __repr__(self) -> str:\n        return \"<no_default>\"\n\n\nno_default = _NoDefault.no_default\nNoDefault = Literal[_NoDefault.no_default]\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:36:31"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport signal\nimport sys\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\nfrom multiprocessing.pool import ThreadPool\nfrom time import sleep, time\n\nimport pytest\n\nimport dask\nfrom dask.system import CPU_COUNT\nfrom dask.threaded import get\nfrom dask.utils_test import add, inc\n\n\ndef test_get():\n    dsk = {\"x\": 1, \"y\": 2, \"z\": (inc, \"x\"), \"w\": (add, \"z\", \"y\")}\n    assert get(dsk, \"w\") == 4\n    assert get(dsk, [\"w\", \"z\"]) == (4, 2)\n\n\ndef test_nested_get():\n    dsk = {\"x\": 1, \"y\": 2, \"a\": (add, \"x\", \"y\"), \"b\": (sum, [\"x\", \"y\"])}\n    assert get(dsk, [\"a\", \"b\"]) == (3, 3)\n\n\ndef test_get_without_computation():\n    dsk = {\"x\": 1}\n    assert get(dsk, \"x\") == 1\n\n\ndef test_broken_callback():\n    from dask.callbacks import Callback\n\n    def _f_ok(*args, **kwargs):\n        pass\n\n    def _f_broken(*args, **kwargs):\n        raise ValueError(\"my_exception\")\n\n    dsk = {\"x\": 1}\n\n    with Callback(start=_f_broken, finish=_f_ok):\n        with Callback(start=_f_ok, finish=_f_ok):\n            with pytest.raises(ValueError, match=\"my_exception\"):\n                get(dsk, \"x\")\n\n\ndef bad(x):\n    raise ValueError()\n\n\ndef test_exceptions_rise_to_top():\n    dsk = {\"x\": 1, \"y\": (bad, \"x\")}\n    pytest.raises(ValueError, lambda: get(dsk, \"y\"))\n\n\n@pytest.mark.parametrize(\"pool_typ\", [ThreadPool, ThreadPoolExecutor])\ndef test_reuse_pool(pool_typ):\n    with pool_typ(CPU_COUNT) as pool:\n        with dask.config.set(pool=pool):\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n\n\n@pytest.mark.parametrize(\"pool_typ\", [ThreadPool, ThreadPoolExecutor])\ndef test_pool_kwarg(pool_typ):\n    def f():\n        sleep(0.01)\n        return threading.get_ident()\n\n    dsk = {(\"x\", i): (f,) for i in range(30)}\n    dsk[\"x\"] = (len, (set, [(\"x\", i) for i in range(len(dsk))]))\n\n    with pool_typ(3) as pool:\n        assert get(dsk, \"x\", pool=pool) == 3\n\n\ndef test_threaded_within_thread():\n    L = []\n\n    def f(i):\n        result = get({\"x\": (lambda: i,)}, \"x\", num_workers=2)\n        L.append(result)\n\n    before = threading.active_count()\n\n    for _ in range(20):\n        t = threading.Thread(target=f, args=(1,))\n        t.daemon = True\n        t.start()\n        t.join()\n        assert L == [1]\n        del L[:]\n\n    start = time()  # wait for most threads to join\n    while threading.active_count() > before + 10:\n        sleep(0.01)\n        assert time() < start + 5\n\n\ndef test_dont_spawn_too_many_threads():\n    before = threading.active_count()\n\n    dsk = {(\"x\", i): (lambda i=i: i,) for i in range(10)}\n    dsk[\"x\"] = (sum, list(dsk))\n    for _ in range(20):\n        get(dsk, \"x\", num_workers=4)\n\n    after = threading.active_count()\n\n    assert after <= before + 8\n\n\ndef test_dont_spawn_too_many_threads_CPU_COUNT():\n    before = threading.active_count()\n\n    dsk = {(\"x\", i): (lambda i=i: i,) for i in range(10)}\n    dsk[\"x\"] = (sum, list(dsk))\n    for _ in range(20):\n        get(dsk, \"x\")\n\n    after = threading.active_count()\n\n    assert after <= before + CPU_COUNT * 2\n\n\ndef test_thread_safety():\n    def f(x):\n        return 1\n\n    dsk = {\"x\": (sleep, 0.05), \"y\": (f, \"x\")}\n\n    L = []\n\n    def test_f():\n        L.append(get(dsk, \"y\"))\n\n    threads = []\n    for _ in range(20):\n        t = threading.Thread(target=test_f)\n        t.daemon = True\n        t.start()\n        threads.append(t)\n\n    for thread in threads:\n        thread.join()\n\n    assert L == [1] * 20\n\n\ndef test_interrupt():\n    # Windows implements `queue.get` using polling,\n    # which means we can set an exception to interrupt the call to `get`.\n    # Python 3 on other platforms requires sending SIGINT to the main thread.\n    if sys.platform == \"win32\":\n        from _thread import interrupt_main\n    else:\n        main_thread = threading.get_ident()\n\n        def interrupt_main() -> None:\n            signal.pthread_kill(main_thread, signal.SIGINT)\n\n    in_clog_event = threading.Event()\n    clog_event = threading.Event()\n\n    def clog(in_clog_event: threading.Event, clog_event: threading.Event) -> None:\n        in_clog_event.set()\n        clog_event.wait()\n\n    def interrupt(in_clog_event: threading.Event) -> None:\n        in_clog_event.wait()\n        interrupt_main()\n\n    dsk = {(\"x\", i): (clog, in_clog_event, clog_event) for i in range(20)}\n    dsk[\"x\"] = (len, list(dsk.keys()))\n\n    interrupter = threading.Thread(target=interrupt, args=(in_clog_event,))\n    interrupter.start()\n\n    # Use explicitly created ThreadPoolExecutor to avoid leaking threads after\n    # the KeyboardInterrupt\n    with ThreadPoolExecutor(CPU_COUNT) as pool:\n        with pytest.raises(KeyboardInterrupt):\n            get(dsk, \"x\", pool=pool)\n        clog_event.set()\n    interrupter.join()\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: auto-sklearn\nTest File Path: auto-sklearn\\test_backend\\test_backend.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport glob\nimport os\nimport pickle\nimport re\nimport shutil\nimport tempfile\nimport time\nimport uuid\nimport warnings\nfrom typing import Dict, List, Optional, Tuple, TypeVar, Union, cast\nfrom pathlib import Path\n\nimport numpy as np\n\nimport pandas as pd\n\nimport scipy.sparse\n\nfrom sklearn.pipeline import Pipeline\n\n\nfrom .logging_ import PicklableClientLogger, get_named_client_logger\nfrom ..ensemble_building.abstract_ensemble import AbstractEnsemble\n\n\n__all__ = [\"Backend\"]\n\n\nDATAMANAGER_TYPE = TypeVar(\"DATAMANAGER_TYPE\")\nPIPELINE_IDENTIFIER_TYPE = Tuple[int, int, float]\n\n\ndef create(\n    temporary_directory: str,\n    output_directory: Optional[str],\n    prefix: str,\n    delete_tmp_folder_after_terminate: bool = True,\n    delete_output_folder_after_terminate: bool = True,\n) -> \"Backend\":\n    context = BackendContext(\n        temporary_directory,\n        output_directory,\n        delete_tmp_folder_after_terminate,\n        delete_output_folder_after_terminate,\n        prefix=prefix,\n    )\n    backend = Backend(context, prefix)\n\n    return backend\n\n\ndef get_randomized_directory_name(\n    prefix: str,\n    temporary_directory: Optional[str] = None,\n) -> str:\n    uuid_str = str(uuid.uuid1(clock_seq=os.getpid()))\n\n    temporary_directory = (\n        temporary_directory\n        if temporary_directory\n        else os.path.join(\n            tempfile.gettempdir(),\n            \"{}_tmp_{}\".format(\n                prefix,\n                uuid_str,\n            ),\n        )\n    )\n\n    return temporary_directory\n\n\nclass BackendContext(object):\n    def __init__(\n        self,\n        temporary_directory: str,\n        output_directory: Optional[str],\n        delete_tmp_folder_after_terminate: bool,\n        delete_output_folder_after_terminate: bool,\n        prefix: str,\n    ):\n\n        # Check that the names of tmp_dir and output_dir is not the same.\n        if temporary_directory == output_directory and temporary_directory is not None:\n            raise ValueError(\"The temporary and the output directory \" \"must be different.\")\n\n        self.delete_tmp_folder_after_terminate = delete_tmp_folder_after_terminate\n        self.delete_output_folder_after_terminate = delete_output_folder_after_terminate\n        # attributes to check that directories were properly created\n        self._tmp_dir_created = False\n        self._output_dir_created = False\n        self._prefix = prefix\n\n        self._temporary_directory = get_randomized_directory_name(\n            temporary_directory=temporary_directory,\n            prefix=self._prefix,\n        )\n        self._output_directory = output_directory\n        # Logging happens via PicklableClientLogger\n        # For this reason we need a port to communicate with the server\n        # When the backend is created, this port is not available\n        # When the port is available in the main process, we\n        # call the setup_logger with this port and update self.logger\n        self._logger = None  # type: Optional[PicklableClientLogger]\n        self.create_directories()\n\n    def setup_logger(self, port: int) -> None:\n        self._logger = get_named_client_logger(\n            name=__name__,\n            port=port,\n        )\n\n    @property\n    def output_directory(self) -> Optional[str]:\n        if self._output_directory is not None:\n            # make sure that tilde does not appear on the path.\n            return os.path.expanduser(os.path.expandvars(self._output_directory))\n        else:\n            return None\n\n    @property\n    def temporary_directory(self) -> str:\n        # make sure that tilde does not appear on the path.\n        return os.path.expanduser(os.path.expandvars(self._temporary_directory))\n\n    def create_directories(self, exist_ok: bool = False) -> None:\n        # No Exception is raised if self.temporary_directory already exists.\n        # This allows to continue the search, also, an error will be raised if\n        # save_start_time is called which ensures two searches are not performed\n        os.makedirs(self.temporary_directory, exist_ok=exist_ok)\n        self._tmp_dir_created = True\n\n        if self.output_directory is not None:\n            os.makedirs(self.output_directory, exist_ok=exist_ok)\n            self._output_dir_created = True\n\n    def delete_directories(self, force: bool = True) -> None:\n        if self.output_directory and (self.delete_output_folder_after_terminate or force):\n            if self._output_dir_created is False:\n                raise ValueError(\n                    \"Failed to delete output dir: %s\"\n                    \"Please make sure that the specified output dir did not \"\n                    \"previously exist.\" % self.output_directory\n                )\n            try:\n                shutil.rmtree(self.output_directory)\n            except Exception:\n                try:\n                    if self._logger is not None:\n                        self._logger.warning(\n                            \"Could not delete output dir: %s\" % self.output_directory\n                        )\n                    else:\n                        print(\"Could not delete output dir: %s\" % self.output_directory)\n                except Exception:\n                    print(\"Could not delete output dir: %s\" % self.output_directory)\n\n        if self.delete_tmp_folder_after_terminate or force:\n            if self._tmp_dir_created is False:\n                raise ValueError(\n                    f\"Failed to delete tmp dir {self.temporary_directory}.\"\n                    \"Please make sure that the specified tmp dir did not \"\n                    \"previously exist.\"\n                )\n            try:\n                shutil.rmtree(self.temporary_directory)\n            except Exception:\n                try:\n                    if self._logger is not None:\n                        self._logger.warning(\n                            \"Could not delete tmp dir: %s\" % self.temporary_directory\n                        )\n                    else:\n                        print(\"Could not delete tmp dir: %s\" % self.temporary_directory)\n                except Exception:\n                    print(\"Could not delete tmp dir: %s\" % self.temporary_directory)\n\n\nclass Backend(object):\n    \"\"\"Utility class to load and save all objects to be persisted.\n\n    These are:\n    * start time\n    * true targets of the ensemble\n    \"\"\"\n\n    def __init__(self, context: BackendContext, prefix: str):\n        # When the backend is created, this port is not available\n        # When the port is available in the main process, we\n        # call the setup_logger with this port and update self.logger\n        self.logger = None  # type: Optional[PicklableClientLogger]\n        self.context = context\n        self.prefix = prefix\n        # Track the number of configurations launched\n        # num_run == 1 means a dummy estimator run\n        self.active_num_run = 1\n\n        # Create the temporary directory if it does not yet exist\n        try:\n            os.makedirs(self.temporary_directory)\n        except Exception:\n            pass\n        # This does not have to exist or be specified\n        if self.output_directory is not None:\n            if not os.path.exists(self.output_directory):\n                raise ValueError(\"Output directory %s does not exist.\" % self.output_directory)\n\n        self.internals_directory = os.path.join(self.temporary_directory, f\".{self.prefix}\")\n        self._make_internals_directory()\n\n    def setup_logger(self, port: int) -> None:\n        self.logger = get_named_client_logger(\n            name=__name__,\n            port=port,\n        )\n        self.context.setup_logger(port)\n\n    @property\n    def output_directory(self) -> Optional[str]:\n        return self.context.output_directory\n\n    @property\n    def temporary_directory(self) -> str:\n        return self.context.temporary_directory\n\n    def _make_internals_directory(self) -> None:\n        try:\n            os.makedirs(self.internals_directory)\n        except Exception as e:\n            if self.logger is not None:\n                self.logger.debug(\"_make_internals_directory: %s\" % e)\n        try:\n            os.makedirs(self.get_runs_directory())\n        except Exception as e:\n            if self.logger is not None:\n                self.logger.debug(\"_make_internals_directory: %s\" % e)\n\n    def _get_start_time_filename(self, seed: Union[str, int]) -> str:\n        if isinstance(seed, str):\n            seed = int(seed)\n        return os.path.join(self.internals_directory, \"start_time_%d\" % seed)\n\n    def save_start_time(self, seed: str) -> str:\n        self._make_internals_directory()\n        start_time = time.time()\n\n        filepath = self._get_start_time_filename(seed)\n\n        if not isinstance(start_time, float):\n            raise ValueError(\"Start time must be a float, but is %s.\" % type(start_time))\n\n        if os.path.exists(filepath):\n            raise ValueError(\n                f\"{filepath} already exist. Different seeds should be provided for different jobs.\"\n            )\n\n        with tempfile.NamedTemporaryFile(\"w\", dir=os.path.dirname(filepath), delete=False) as fh:\n            fh.write(str(start_time))\n            tempname = fh.name\n        os.rename(tempname, filepath)\n\n        return filepath\n\n    def load_start_time(self, seed: int) -> float:\n        with open(self._get_start_time_filename(seed), \"r\") as fh:\n            start_time = float(fh.read())\n        return start_time\n\n    def get_smac_output_directory(self) -> str:\n        return os.path.join(self.temporary_directory, \"smac3-output\")\n\n    def get_smac_output_directory_for_run(self, seed: int) -> str:\n        return os.path.join(self.temporary_directory, \"smac3-output\", \"run_%d\" % seed)\n\n    def _get_targets_ensemble_filename(self, end: str | None = None) -> str:\n        dir = Path(self.internals_directory)\n        stem = \"true_targets_ensemble\"\n        existing = [p for p in dir.iterdir() if stem in p.name]\n\n        # Sanity check to make sure we have one or None\n        assert len(existing) in [0, 1]\n\n        if not any(existing):\n            end = \"npy\"\n        else:\n            end = existing[0].name.split('.')[-1]\n\n        return os.path.join(self.internals_directory, f\"{stem}.{end}\")\n\n    def _get_input_ensemble_filename(self, end: str | None = None) -> str:\n        dir = Path(self.internals_directory)\n        stem = \"true_input_ensemble\"\n        existing = [p for p in dir.iterdir() if stem in p.name]\n\n        # Sanity check to make sure we have one or None\n        assert len(existing) in [0, 1]\n\n        if not any(existing):\n            end = \"npy\"\n        else:\n            end = existing[0].name.split('.')[-1]\n\n        return os.path.join(self.internals_directory, f\"{stem}.{end}\")\n\n    def save_additional_data(\n        self,\n        data: Union[np.ndarray, pd.DataFrame, scipy.sparse.spmatrix],\n        what: str,\n        overwrite: bool = False,\n    ) -> str:\n        self._make_internals_directory()\n        if isinstance(data, np.ndarray):\n            end = \"npy\"\n        elif isinstance(data, scipy.sparse.spmatrix):\n            end = \"npz\"\n        elif isinstance(data, pd.DataFrame):\n            end = \"pd\"\n        else:\n            raise ValueError(\n                \"Targets must be of type np.ndarray, pd.Dataframe or\"\n                \" scipy.sparse.spmatrix but is %s\" % type(data)\n            )\n\n        if what == \"targets_ensemble\":\n            filepath = self._get_targets_ensemble_filename(end=end)\n        elif what == \"input_ensemble\":\n            filepath = self._get_input_ensemble_filename(end=end)\n        else:\n            raise ValueError(f\"Unknown data type {what}\")\n\n        # Only store data if it does not exist yet\n        if not overwrite and os.path.isfile(filepath):\n            return filepath\n\n        tempname = self._save_array(data=data, filepath=filepath)\n        os.rename(tempname, filepath)\n\n        return filepath\n\n    @staticmethod\n    def _save_array(\n        data: Union[np.ndarray, pd.DataFrame, scipy.sparse.spmatrix], filepath: str\n    ) -> str:\n        if isinstance(data, np.ndarray):\n            with tempfile.NamedTemporaryFile(\n                \"wb\", dir=os.path.dirname(filepath), delete=False\n            ) as fh_w:\n                np.save(fh_w, data.astype(np.float32))\n        elif isinstance(data, scipy.sparse.spmatrix):\n            with tempfile.NamedTemporaryFile(\n                \"wb\", dir=os.path.dirname(filepath), delete=False\n            ) as fh_w:\n                scipy.sparse.save_npz(fh_w, data)\n        elif isinstance(data, pd.DataFrame):\n            with tempfile.NamedTemporaryFile(\n                \"wb\", dir=os.path.dirname(filepath), delete=False\n            ) as fh_w:\n                data.to_pickle(fh_w)\n        return fh_w.name\n\n    @staticmethod\n    def _load_array(filepath: str) -> np.array:\n        end = filepath.split(\".\")[-1]\n        if end == \"npy\":\n            targets = np.load(filepath, allow_pickle=True)\n        elif end == \"npz\":\n            targets = scipy.sparse.load_npz(filepath)\n        elif end == \"pd\":\n            targets = pd.read_pickle(filepath)\n        else:\n            raise ValueError(f\"Unknown file type {end} in {filepath}\")\n\n        return targets\n\n    def load_targets_ensemble(self) -> np.ndarray:\n        return self._load_array(filepath=self._get_targets_ensemble_filename())\n\n    def load_input_ensemble(self) -> np.ndarray:\n        return self._load_array(filepath=self._get_input_ensemble_filename())\n\n    def _get_datamanager_pickle_filename(self) -> str:\n        return os.path.join(self.internals_directory, \"datamanager.pkl\")\n\n    def save_datamanager(self, datamanager: DATAMANAGER_TYPE) -> str:\n        self._make_internals_directory()\n        filepath = self._get_datamanager_pickle_filename()\n\n        with tempfile.NamedTemporaryFile(\"wb\", dir=os.path.dirname(filepath), delete=False) as fh:\n            pickle.dump(datamanager, fh, -1)\n            tempname = fh.name\n        os.rename(tempname, filepath)\n\n        return filepath\n\n    def load_datamanager(self) -> DATAMANAGER_TYPE:\n        filepath = self._get_datamanager_pickle_filename()\n        with open(filepath, \"rb\") as fh:\n            return cast(DATAMANAGER_TYPE, pickle.load(fh))\n\n    def get_runs_directory(self) -> str:\n        return os.path.join(self.internals_directory, \"runs\")\n\n    def get_numrun_directory(self, seed: int, num_run: int, budget: float) -> str:\n        return os.path.join(self.internals_directory, \"runs\", \"%d_%d_%s\" % (seed, num_run, budget))\n\n    def get_next_num_run(self, peek: bool = False) -> int:\n        \"\"\"\n        Every pipeline that is fitted by the estimator is stored with an\n        identifier called num_run. A dummy classifier will always have a num_run\n        equal to 1, and all other new configurations that are explored will\n        have a sequentially increasing identifier.\n\n        This method returns the next num_run a configuration should take.\n\n        Parameters\n        ----------\n        peek: bool\n            By default, the next num_rum will be returned, i.e. self.active_num_run + 1\n            Yet, if this bool parameter is equal to True, the value of the current\n            num_run is provided, i.e, self.active_num_run.\n            In other words, peek allows to get the current maximum identifier\n            of a configuration.\n\n        Returns\n        -------\n        num_run: int\n            An unique identifier for a configuration\n        \"\"\"\n\n        # If there are other num_runs, their name would be runs/<seed>_<num_run>_<budget>\n        other_num_runs = [\n            int(os.path.basename(run_dir).split(\"_\")[1])\n            for run_dir in glob.glob(os.path.join(self.internals_directory, \"runs\", \"*\"))\n            if self._is_run_dir(os.path.basename(run_dir))\n        ]\n        if len(other_num_runs) > 0:\n            # We track the number of runs from two forefronts:\n            # The physically available num_runs (which might be deleted or a crash could happen)\n            # From a internally kept attribute. The later should be sufficient, but we\n            # want to be robust against multiple backend copies on different workers\n            self.active_num_run = max([self.active_num_run] + other_num_runs)\n\n        # We are interested in the next run id\n        if not peek:\n            self.active_num_run += 1\n        return self.active_num_run\n\n    @staticmethod\n    def _is_run_dir(run_dir: str) -> bool:\n        \"\"\"\n        Run directories are stored in the format <seed>_<num_run>_<budget>.\n\n        Parameters\n        ----------\n        run_dir: str\n            string containing the base name of the run directory\n\n        Returns\n        -------\n        _: bool\n            whether the provided run directory matches the run_dir_pattern\n            signifying that it is a run directory\n        \"\"\"\n        run_dir_pattern = r\"\\d+_\\d+_\\d+\"\n        return bool(re.match(run_dir_pattern, run_dir))\n\n    def get_model_filename(self, seed: int, idx: int, budget: float) -> str:\n        return \"%s.%s.%s.model\" % (seed, idx, budget)\n\n    def get_cv_model_filename(self, seed: int, idx: int, budget: float) -> str:\n        return \"%s.%s.%s.cv_model\" % (seed, idx, budget)\n\n    def list_all_models(self, seed: int) -> List[str]:\n        runs_directory = self.get_runs_directory()\n        model_files = glob.glob(\n            os.path.join(glob.escape(runs_directory), \"%d_*\" % seed, \"%s.*.*.model\" % seed)\n        )\n        return model_files\n\n    def load_models_by_identifiers(\n        self, identifiers: List[PIPELINE_IDENTIFIER_TYPE]\n    ) -> Dict[PIPELINE_IDENTIFIER_TYPE, Pipeline]:\n        models = {}\n\n        for identifier in identifiers:\n            seed, idx, budget = identifier\n            models[identifier] = self.load_model_by_seed_and_id_and_budget(seed, idx, budget)\n\n        return models\n\n    def load_model_by_seed_and_id_and_budget(self, seed: int, idx: int, budget: float) -> Pipeline:\n        model_directory = self.get_numrun_directory(seed, idx, budget)\n\n        model_file_name = \"%s.%s.%s.model\" % (seed, idx, budget)\n        model_file_path = os.path.join(model_directory, model_file_name)\n        with open(model_file_path, \"rb\") as fh:\n            return pickle.load(fh)\n\n    def load_cv_models_by_identifiers(\n        self, identifiers: List[PIPELINE_IDENTIFIER_TYPE]\n    ) -> Dict[PIPELINE_IDENTIFIER_TYPE, Pipeline]:\n        models = {}\n\n        for identifier in identifiers:\n            seed, idx, budget = identifier\n            models[identifier] = self.load_cv_model_by_seed_and_id_and_budget(seed, idx, budget)\n\n        return models\n\n    def load_cv_model_by_seed_and_id_and_budget(\n        self, seed: int, idx: int, budget: float\n    ) -> Pipeline:\n        model_directory = self.get_numrun_directory(seed, idx, budget)\n\n        model_file_name = \"%s.%s.%s.cv_model\" % (seed, idx, budget)\n        model_file_path = os.path.join(model_directory, model_file_name)\n        with open(model_file_path, \"rb\") as fh:\n            return pickle.load(fh)\n\n    def save_numrun_to_dir(\n        self,\n        seed: int,\n        idx: int,\n        budget: float,\n        model: Optional[Pipeline],\n        cv_model: Optional[Pipeline],\n        ensemble_predictions: Optional[np.ndarray],\n        valid_predictions: Optional[np.ndarray],\n        test_predictions: Optional[np.ndarray],\n    ) -> None:\n        runs_directory = self.get_runs_directory()\n        tmpdir = tempfile.mkdtemp(dir=runs_directory)\n        if model is not None:\n            file_path = os.path.join(tmpdir, self.get_model_filename(seed, idx, budget))\n            with open(file_path, \"wb\") as fh:\n                pickle.dump(model, fh, -1)\n\n        if cv_model is not None:\n            file_path = os.path.join(tmpdir, self.get_cv_model_filename(seed, idx, budget))\n            with open(file_path, \"wb\") as fh:\n                pickle.dump(cv_model, fh, -1)\n\n        for preds, subset in (\n            (ensemble_predictions, \"ensemble\"),\n            (valid_predictions, \"valid\"),\n            (test_predictions, \"test\"),\n        ):\n            if preds is not None:\n                file_path = os.path.join(\n                    tmpdir, self.get_prediction_filename(subset, seed, idx, budget)\n                )\n                with open(file_path, \"wb\") as fh:\n                    pickle.dump(preds.astype(np.float32), fh, -1)\n        try:\n            os.rename(tmpdir, self.get_numrun_directory(seed, idx, budget))\n        except OSError:\n            if os.path.exists(self.get_numrun_directory(seed, idx, budget)):\n                os.rename(\n                    self.get_numrun_directory(seed, idx, budget),\n                    os.path.join(runs_directory, tmpdir + \".old\"),\n                )\n                os.rename(tmpdir, self.get_numrun_directory(seed, idx, budget))\n                shutil.rmtree(os.path.join(runs_directory, tmpdir + \".old\"))\n\n    def get_ensemble_dir(self) -> str:\n        return os.path.join(self.internals_directory, \"ensembles\")\n\n    def load_ensemble(self, seed: int) -> Optional[AbstractEnsemble]:\n        ensemble_dir = self.get_ensemble_dir()\n\n        if not os.path.exists(ensemble_dir):\n            if self.logger is not None:\n                self.logger.warning(\"Directory %s does not exist\" % ensemble_dir)\n            else:\n                warnings.warn(\"Directory %s does not exist\" % ensemble_dir)\n            return None\n\n        if seed >= 0:\n            indices_files = glob.glob(\n                os.path.join(glob.escape(ensemble_dir), \"%s.*.ensemble\" % seed)\n            )\n            indices_files.sort()\n        else:\n            indices_files = os.listdir(ensemble_dir)\n            indices_files = [os.path.join(ensemble_dir, f) for f in indices_files]\n            indices_files.sort(key=lambda f: time.ctime(os.path.getmtime(f)))\n\n        with open(indices_files[-1], \"rb\") as fh:\n            ensemble_members_run_numbers = cast(AbstractEnsemble, pickle.load(fh))\n\n        return ensemble_members_run_numbers\n\n    def save_ensemble(self, ensemble: AbstractEnsemble, idx: int, seed: int) -> None:\n        try:\n            os.makedirs(self.get_ensemble_dir())\n        except Exception:\n            pass\n\n        filepath = os.path.join(\n            self.get_ensemble_dir(), \"%s.%s.ensemble\" % (str(seed), str(idx).zfill(10))\n        )\n        with tempfile.NamedTemporaryFile(\"wb\", dir=os.path.dirname(filepath), delete=False) as fh:\n            pickle.dump(ensemble, fh)\n            tempname = fh.name\n        os.rename(tempname, filepath)\n\n    def get_prediction_filename(\n        self, subset: str, automl_seed: Union[str, int], idx: int, budget: float\n    ) -> str:\n        return \"predictions_%s_%s_%s_%s.npy\" % (subset, automl_seed, idx, budget)\n\n    def save_predictions_as_txt(\n        self,\n        predictions: np.ndarray,\n        subset: str,\n        idx: int,\n        precision: int,\n        prefix: Optional[str] = None,\n    ) -> None:\n        if not self.output_directory:\n            return\n        # Write prediction scores in prescribed format\n        filepath = os.path.join(\n            self.output_directory,\n            (\"%s_\" % prefix if prefix else \"\") + \"%s_%s.predict\" % (subset, str(idx)),\n        )\n\n        format_string = \"{:.%dg} \" % precision\n        with tempfile.NamedTemporaryFile(\n            \"w\", dir=os.path.dirname(filepath), delete=False\n        ) as output_file:\n            for row in predictions:\n                if not isinstance(row, np.ndarray) and not isinstance(row, list):\n                    row = [row]\n                for val in row:\n                    output_file.write(format_string.format(float(val)))\n                output_file.write(\"\\n\")\n            tempname = output_file.name\n        os.rename(tempname, filepath)\n\n    def write_txt_file(self, filepath: str, data: str, name: str) -> None:\n        with tempfile.NamedTemporaryFile(\"w\", dir=os.path.dirname(filepath), delete=False) as fh:\n            fh.write(data)\n            tempname = fh.name\n        os.rename(tempname, filepath)\n        if self.logger is not None:\n            self.logger.debug(\"Created %s file %s\" % (name, filepath))\n\n### Source File Dependency Files Content\n### Dependency File: abstract_ensemble.py\nfrom abc import ABCMeta, abstractmethod\nfrom typing import Dict, List, Tuple, Union\n\nimport numpy as np\n\nfrom sklearn.pipeline import Pipeline\n\n\nclass AbstractEnsemble(object, metaclass=ABCMeta):\n    @abstractmethod\n    def fit(\n        self,\n        base_models_predictions: np.ndarray,\n        true_targets: np.ndarray,\n        model_identifiers: List[Tuple[int, int, float]],\n    ) -> \"AbstractEnsemble\":\n        \"\"\"Fit an ensemble given predictions of base models and targets.\n\n        Ensemble building maximizes performance (in contrast to\n        hyperparameter optimization)!\n\n        Parameters\n        ----------\n        base_models_predictions : array of shape = [n_base_models, n_data_points, n_targets]\n            n_targets is the number of classes in case of classification,\n            n_targets is 0 or 1 in case of regression\n\n        true_targets : array of shape [n_targets]\n\n        model_identifiers : identifier for each base model.\n            Can be used for practical text output of the ensemble.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def predict(self, base_models_predictions: Union[np.ndarray, List[np.ndarray]]) -> np.ndarray:\n        \"\"\"Create ensemble predictions from the base model predictions.\n\n        Parameters\n        ----------\n        base_models_predictions : array of shape = [n_base_models, n_data_points, n_targets]\n            Same as in the fit method.\n\n        Returns\n        -------\n        array : [n_data_points]\n        \"\"\"\n        self\n\n    @abstractmethod\n    def get_models_with_weights(self, models: Dict[str, Pipeline]) -> List[Tuple[float, Pipeline]]:\n        \"\"\"Return a list of (weight, model) pairs\n\n        Parameters\n        ----------\n        models : dict {identifier : model object}\n            The identifiers are the same as the one presented to the fit()\n            method. Models can be used for nice printing.\n\n        Returns\n        -------\n        array : [(weight_1, model_1), ..., (weight_n, model_n)]\n        \"\"\"\n\n    @abstractmethod\n    def get_selected_model_identifiers(self) -> List[Tuple[int, int, float]]:\n        \"\"\"Return identifiers of models in the ensemble.\n\n        This includes models which have a weight of zero!\n\n        Returns\n        -------\n        list\n        \"\"\"\n\n    @abstractmethod\n    def get_validation_performance(self) -> float:\n        \"\"\"Return validation performance of ensemble.\n\n        Return\n        ------\n        float\n        \"\"\"\n\n\n### Dependency File: logging_.py\n# -*- encoding: utf-8 -*-\nimport logging\nimport logging.config\nimport logging.handlers\nimport multiprocessing\nimport os\nimport pickle\nimport random\nimport select\nimport socketserver\nimport struct\nimport threading\nfrom typing import Any, Dict, Optional, Type\n\nimport yaml\n\n\ndef setup_logger(\n    output_dir: str,\n    filename: Optional[str] = None,\n    distributedlog_filename: Optional[str] = None,\n    logging_config: Optional[Dict[str, Dict[str, Any]]] = None,\n) -> None:\n    # logging_config must be a dictionary object specifying the configuration\n    if logging_config is None:\n        with open(os.path.join(os.path.dirname(__file__), \"logging.yaml\"), \"r\") as fh:\n            logging_config = yaml.safe_load(fh)\n\n    if filename is None:\n        filename = logging_config[\"handlers\"][\"file_handler\"][\"filename\"]\n    logging_config[\"handlers\"][\"file_handler\"][\"filename\"] = os.path.join(output_dir, filename)\n\n    if distributedlog_filename is None:\n        distributedlog_filename = logging_config[\"handlers\"][\"distributed_logfile\"][\"filename\"]\n    logging_config[\"handlers\"][\"distributed_logfile\"][\"filename\"] = os.path.join(\n        output_dir, distributedlog_filename\n    )\n    logging.config.dictConfig(logging_config)\n\n\ndef _create_logger(name: str) -> logging.Logger:\n    return logging.getLogger(name)\n\n\ndef get_logger(name: str) -> \"PickableLoggerAdapter\":\n    logger = PickableLoggerAdapter(name)\n    return logger\n\n\nclass PickableLoggerAdapter(object):\n    def __init__(self, name: str):\n        self.name = name\n        self.logger = _create_logger(name)\n\n    def __getstate__(self) -> Dict[str, Any]:\n        \"\"\"\n        Method is called when pickle dumps an object.\n\n        Returns\n        -------\n        Dictionary, representing the object state to be pickled. Ignores\n        the self.logger field and only returns the logger name.\n        \"\"\"\n        return {\"name\": self.name}\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        \"\"\"\n        Method is called when pickle loads an object. Retrieves the name and\n        creates a logger.\n\n        Parameters\n        ----------\n        state - dictionary, containing the logger name.\n\n        \"\"\"\n        self.name = state[\"name\"]\n        self.logger = _create_logger(self.name)\n\n    def debug(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.debug(msg, *args, **kwargs)\n\n    def info(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.info(msg, *args, **kwargs)\n\n    def warning(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.warning(msg, *args, **kwargs)\n\n    def error(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.error(msg, *args, **kwargs)\n\n    def exception(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.exception(msg, *args, **kwargs)\n\n    def critical(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.critical(msg, *args, **kwargs)\n\n    def log(self, level: int, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level: int) -> bool:\n        return self.logger.isEnabledFor(level)\n\n\ndef get_named_client_logger(\n    name: str,\n    host: str = \"localhost\",\n    port: int = logging.handlers.DEFAULT_TCP_LOGGING_PORT,\n) -> \"PicklableClientLogger\":\n    logger = PicklableClientLogger(name=name, host=host, port=port)\n    return logger\n\n\ndef _get_named_client_logger(\n    name: str,\n    host: str = \"localhost\",\n    port: int = logging.handlers.DEFAULT_TCP_LOGGING_PORT,\n) -> logging.Logger:\n    \"\"\"\n    When working with a logging server, clients are expected to create a logger using\n    this method. For example, the automl object will create a master that awaits\n    for records sent through tcp to localhost.\n\n    Ensemble builder will then instantiate a logger object that will submit records\n    via a socket handler to the server.\n\n    We do not need to use any format as the server will render the msg as it\n    needs to.\n\n    Parameters\n    ----------\n        name: (str)\n            the name of the logger, used to tag the messages in the main log\n        host: (str)\n            Address of where the server is gonna look for messages\n        port: (str)\n            Port used to communicate with the server\n\n    Returns\n    -------\n        local_loger: a logger object that has a socket handler\n    \"\"\"\n    # Setup the logger configuration\n    # We add client not only to identify that this is the client\n    # communication part of the logger, but to make sure we have\n    # a new singleton with the desired socket handlers\n    local_logger = _create_logger(\"Client-\" + name)\n    local_logger.propagate = False\n    local_logger.setLevel(logging.DEBUG)\n\n    try:\n        ports = [getattr(handler, \"port\", None) for handler in local_logger.handlers]\n    except AttributeError:\n        # We do not want to log twice but adding multiple times the same\n        # handler. So we check to what ports we communicate to\n        # We can prevent errors with streamers not having a port with this try\n        # block -- but it is a scenario that is unlikely to happen\n        ports = []\n\n    if port not in ports:\n        socketHandler = logging.handlers.SocketHandler(host, port)\n        local_logger.addHandler(socketHandler)\n\n    return local_logger\n\n\nclass PicklableClientLogger(PickableLoggerAdapter):\n    def __init__(self, name: str, host: str, port: int):\n        self.name = name\n        self.host = host\n        self.port = port\n        self.logger = _get_named_client_logger(name=name, host=host, port=port)\n\n    def __getstate__(self) -> Dict[str, Any]:\n        \"\"\"\n        Method is called when pickle dumps an object.\n\n        Returns\n        -------\n        Dictionary, representing the object state to be pickled. Ignores\n        the self.logger field and only returns the logger name.\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"host\": self.host,\n            \"port\": self.port,\n        }\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        \"\"\"\n        Method is called when pickle loads an object. Retrieves the name and\n        creates a logger.\n\n        Parameters\n        ----------\n        state - dictionary, containing the logger name.\n\n        \"\"\"\n        self.name = state[\"name\"]\n        self.host = state[\"host\"]\n        self.port = state[\"port\"]\n        self.logger = _get_named_client_logger(\n            name=self.name,\n            host=self.host,\n            port=self.port,\n        )\n\n\nclass LogRecordStreamHandler(socketserver.StreamRequestHandler):\n    \"\"\"Handler for a streaming logging request.\n\n    This basically logs the record using whatever logging policy is\n    configured locally.\n    \"\"\"\n\n    def handle(self) -> None:\n        \"\"\"\n        Handle multiple requests - each expected to be a 4-byte length,\n        followed by the LogRecord in pickle format. Logs the record\n        according to whatever policy is configured locally.\n        \"\"\"\n        while True:\n            chunk = self.connection.recv(4)  # type: ignore[attr-defined]\n            if len(chunk) < 4:\n                break\n            slen = struct.unpack(\">L\", chunk)[0]\n            chunk = self.connection.recv(slen)  # type: ignore[attr-defined]\n            while len(chunk) < slen:\n                chunk = chunk + self.connection.recv(slen - len(chunk))  # type: ignore[attr-defined]  # noqa: E501\n            obj = self.unPickle(chunk)\n            record = logging.makeLogRecord(obj)\n            self.handleLogRecord(record)\n\n    def unPickle(self, data: Any) -> Any:\n        return pickle.loads(data)\n\n    def handleLogRecord(self, record: logging.LogRecord) -> None:\n        # logname is define in LogRecordSocketReceiver\n        # Yet Mypy Cannot see this. Ignore typing check for this\n        if self.server.logname is not None:  # type: ignore  # noqa\n            name = self.server.logname  # type: ignore  # noqa\n        else:\n            name = record.name\n        logger = logging.getLogger(name)\n        # N.B. EVERY record gets logged. This is because Logger.handle\n        # is normally called AFTER logger-level filtering. If you want\n        # to do filtering, do it at the client end to save wasting\n        # cycles and network bandwidth!\n        logger.handle(record)\n\n\ndef start_log_server(\n    host: str,\n    logname: str,\n    event: threading.Event,\n    port: multiprocessing.Value,\n    filename: str,\n    logging_config: Optional[Dict[str, Dict[str, Any]]],\n    output_dir: str,\n) -> None:\n    setup_logger(filename=filename, logging_config=logging_config, output_dir=output_dir)\n\n    while True:\n        # Loop until we find a valid port\n        _port = random.randint(10000, 65535)\n        try:\n            receiver = LogRecordSocketReceiver(\n                host=host,\n                port=_port,\n                logname=logname,\n                event=event,\n            )\n            with port.get_lock():\n                port.value = _port\n            receiver.serve_until_stopped()\n            break\n        except OSError:\n            continue\n\n\nclass LogRecordSocketReceiver(socketserver.ThreadingTCPServer):\n    \"\"\"\n    This class implement a entity that receives tcp messages on a given address\n    For further information, please check\n    https://docs.python.org/3/howto/logging-cookbook.html#configuration-server-example\n    \"\"\"\n\n    allow_reuse_address = True\n\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = logging.handlers.DEFAULT_TCP_LOGGING_PORT,\n        handler: Type[LogRecordStreamHandler] = LogRecordStreamHandler,\n        logname: Optional[str] = None,\n        event: Optional[threading.Event] = None,\n    ):\n        socketserver.ThreadingTCPServer.__init__(self, (host, port), handler)\n        self.timeout = 1\n        self.logname = logname\n        self.event = event\n\n    def serve_until_stopped(self) -> None:\n        while True:\n            rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout)\n            if rd:\n                self.handle_request()\n            if self.event is not None and self.event.is_set():\n                break\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:43:24"}, {"role": "assistant", "content": "```python\n# -*- encoding: utf-8 -*-\nimport builtins\n\nfrom autosklearn.automl_common.common.utils.backend import Backend\n\nimport unittest\nimport unittest.mock\n\n\nclass BackendModelsTest(unittest.TestCase):\n    class BackendStub(Backend):\n        def __init__(self):\n            self.__class__ = Backend\n\n    def setUp(self):\n        self.backend = self.BackendStub()\n        self.backend.internals_directory = \"/\"\n\n    @unittest.mock.patch(\"pickle.load\")\n    @unittest.mock.patch(\"os.path.exists\")\n    def test_load_model_by_seed_and_id(self, exists_mock, pickleLoadMock):\n        exists_mock.return_value = False\n        open_mock = unittest.mock.mock_open(read_data=\"Data\")\n        with unittest.mock.patch(\n            \"autosklearn.automl_common.common.utils.backend.open\",\n            open_mock,\n            create=True,\n        ):\n            seed = 13\n            idx = 17\n            budget = 50.0\n            expected_model = self._setup_load_model_mocks(\n                open_mock, pickleLoadMock, seed, idx, budget\n            )\n\n            actual_model = self.backend.load_model_by_seed_and_id_and_budget(\n                seed, idx, budget\n            )\n\n            self.assertEqual(expected_model, actual_model)\n\n    @unittest.mock.patch(\"pickle.load\")\n    @unittest.mock.patch.object(builtins, \"open\")\n    @unittest.mock.patch(\"os.path.exists\")\n    def test_loads_models_by_identifiers(self, exists_mock, openMock, pickleLoadMock):\n        exists_mock.return_value = True\n        seed = 13\n        idx = 17\n        budget = 50.0\n        expected_model = self._setup_load_model_mocks(\n            openMock, pickleLoadMock, seed, idx, budget\n        )\n        expected_dict = {(seed, idx, budget): expected_model}\n\n        actual_dict = self.backend.load_models_by_identifiers([(seed, idx, budget)])\n\n        self.assertIsInstance(actual_dict, dict)\n        self.assertDictEqual(expected_dict, actual_dict)\n\n    def _setup_load_model_mocks(self, openMock, pickleLoadMock, seed, idx, budget):\n        model_path = \"/runs/%s_%s_%s/%s.%s.%s.model\" % (\n            seed,\n            idx,\n            budget,\n            seed,\n            idx,\n            budget,\n        )\n        file_handler = \"file_handler\"\n        expected_model = \"model\"\n\n        fileMock = unittest.mock.MagicMock()\n        fileMock.__enter__.return_value = file_handler\n\n        openMock.side_effect = (\n            lambda path, flag: fileMock if path == model_path and flag == \"rb\" else None\n        )\n        pickleLoadMock.side_effect = (\n            lambda fh: expected_model if fh == file_handler else None\n        )\n\n        return expected_model\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: auto-sklearn\nTest File Path: auto-sklearn\\test_construction\\test_construction.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\" Provides simplified 2 use cases of dask that we consider\n\n1. A UserDask is when a user supplies a dask client, in which case\nwe don't close this down and leave it up to the user to control its lifetime.\n2.  A LocalDask is one we use when no user dask is supplied. In this case\nwe make sure to spin up and close down clients as needed.\n\nBoth of these can be uniformly accessed as a context manager.\n\n.. code:: python\n\n    # Locally controlled dask client\n    local_dask = LocalDask(n_jobs=2)\n    with local_dask as client:\n        # Do stuff with client\n        ...\n\n    # `client` is shutdown properly\n\n    # ----------------\n\n    # User controlled dask client\n    user_dask = UserDask(user_client)\n\n    with user_dask as client:\n        # Do stuff with (client == user_client)\n        ...\n\n    # `user_client` is still open and up to the user to close\n\"\"\"\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import Any\n\nimport tempfile\n\nfrom dask.distributed import Client, LocalCluster\n\nfrom autosklearn.util.single_thread_client import SingleThreadedClient\n\n\nclass Dask(ABC):\n    @abstractmethod\n    def client(self) -> Client:\n        \"\"\"Should return a dask client\"\"\"\n        ...\n\n    @abstractmethod\n    def close(self) -> None:\n        \"\"\"Should close up any resources needed for the dask client\"\"\"\n        ...\n\n    def __enter__(self) -> Client:\n        return self.client()\n\n    def __exit__(self, *args: Any, **kwargs: Any) -> None:\n        self.close()\n\n    @abstractmethod\n    def __repr__(self) -> str:\n        ...\n\n\nclass UserDask(Dask):\n    \"\"\"A dask instance created by a user\"\"\"\n\n    def __init__(self, client: Client):\n        \"\"\"\n        Parameters\n        ----------\n        client : Client\n            The client they passed in\n        \"\"\"\n        self._client = client\n\n    def client(self) -> Client:\n        \"\"\"The dask client\"\"\"\n        return self._client\n\n    def close(self) -> None:\n        \"\"\"Close the dask client\"\"\"\n        # We do nothing, it's user provided\n        pass\n\n    def __repr__(self) -> str:\n        return \"UserDask(...)\"\n\n\nclass LocalDask(Dask):\n    def __init__(self, n_jobs: int | None = None) -> None:\n        self.n_jobs = n_jobs\n        self._client: Client | None = None\n        self._cluster: LocalCluster | None = None\n\n    def client(self) -> Client:\n        \"\"\"Creates a usable dask client or returns an existing one\n\n        If there is not current client, because it has been closed, create\n        a new one.\n        * If ``n_jobs == 1``, create a ``SingleThreadedClient``\n        * Else create a ``Client`` with a ``LocalCluster``\n        \"\"\"\n        if self._client is not None:\n            return self._client\n\n        if self.n_jobs == 1:\n            cluster = None\n            client = SingleThreadedClient()\n        else:\n            cluster = LocalCluster(\n                n_workers=self.n_jobs,\n                processes=False,\n                threads_per_worker=1,\n                # We use tmpdir to save the workers as deleting workers takes\n                # more time than deleting backend directories.\n                # This prevent an error saying that the worker file was deleted,\n                # so the client could not close the worker properly\n                local_directory=tempfile.gettempdir(),\n                # Memory is handled by the pynisher, not by the dask worker/nanny\n                memory_limit=0,\n            )\n            client = Client(cluster, heartbeat_interval=10000)  # 10s\n\n        self._client = client\n        self._cluster = cluster\n        return self._client\n\n    def close(self) -> None:\n        \"\"\"Closes any open dask client\"\"\"\n        if self._client is None:\n            return\n\n        self._client.close()\n        if self._cluster is not None:\n            self._cluster.close()\n\n        self._client = None\n        self._cluster = None\n\n    def __repr__(self) -> str:\n        return f\"LocalDask(n_jobs = {self.n_jobs})\"\n\n\n### Source File Content:\nfrom typing import (\n    Any,\n    Dict,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import spmatrix\nfrom sklearn.model_selection import train_test_split\n\nfrom autosklearn.evaluation.splitter import CustomStratifiedShuffleSplit\n\n# TODO: TypedDict with python 3.8\n#\n#   When upgrading to python 3.8 as minimum version, this should be a TypedDict\n#   so that mypy can identify the fields types\nDatasetCompressionSpec = Dict[str, Union[float, List[str]]]\n\n# Default specification for arg `dataset_compression`\ndefault_dataset_compression_arg: DatasetCompressionSpec = {\n    \"memory_allocation\": 0.1,\n    \"methods\": [\"precision\", \"subsample\"],\n}\n\n\ndef validate_dataset_compression_arg(\n    dataset_compression: Mapping[str, Any], memory_limit: int\n) -> DatasetCompressionSpec:\n    \"\"\"Validates and return a correct dataset_compression argument\n\n    The returned value can be safely used with `reduce_dataset_size_if_too_large`.\n\n    Parameters\n    ----------\n    dataset_compression: Mapping[str, Any]\n        The arg to validate\n\n    Returns\n    -------\n    DatasetCompressionSpec\n        The validated and correct dataset compression spec\n    \"\"\"\n    if isinstance(dataset_compression, Mapping):\n        # Fill with defaults if they don't exist\n        dataset_compression = {**default_dataset_compression_arg, **dataset_compression}\n\n        parsed_keys = set(dataset_compression.keys())\n        default_keys = set(default_dataset_compression_arg.keys())\n\n        # Must contain known keys\n        if parsed_keys != default_keys:\n            raise ValueError(\n                f\"Unknown key(s) in ``dataset_compression``, {parsed_keys}.\"\n                f\"\\nPossible keys are {default_keys}\"\n            )\n\n        memory_allocation = dataset_compression[\"memory_allocation\"]\n\n        # \"memory_allocation\" must be float or int\n        if not (\n            isinstance(memory_allocation, float) or isinstance(memory_allocation, int)\n        ):\n            raise ValueError(\n                \"key 'memory_allocation' must be an `int` or `float`\"\n                f\"\\ntype = {memory_allocation}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        # \"memory_allocation\" must be in (0,1) if float\n        if isinstance(memory_allocation, float) and not (0.0 < memory_allocation < 1.0):\n            raise ValueError(\n                \"key 'memory_allocation' if float must be in (0, 1)\"\n                f\"\\nmemory_allocation = {memory_allocation}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        # \"memory_allocation\" if absolute, should be > 0 and < memory_limit\n        if isinstance(memory_allocation, int) and not (\n            0 < memory_allocation < memory_limit\n        ):\n            raise ValueError(\n                f\"key 'memory_allocation' if int must be in (0, {memory_limit})\"\n                f\"\\nmemory_allocation = {memory_allocation}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        # \"methods\" must be non-empty sequence\n        if (\n            not isinstance(dataset_compression[\"methods\"], Sequence)\n            or len(dataset_compression[\"methods\"]) <= 0\n        ):\n            raise ValueError(\n                \"key 'methods' must be a non-empty list\"\n                f\"\\nmethods = {dataset_compression['methods']}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        # \"methods\" must contain known methods\n        if any(\n            method\n            not in cast(Sequence, default_dataset_compression_arg[\"methods\"])  # mypy\n            for method in dataset_compression[\"methods\"]\n        ):\n            valid_methods = default_dataset_compression_arg[\"methods\"]\n            raise ValueError(\n                f\"key 'methods' can only contain {valid_methods}\"\n                f\"\\nmethods = {dataset_compression['methods']}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        return cast(DatasetCompressionSpec, dataset_compression)\n    else:\n        raise ValueError(\n            f\"Unknown type for `dataset_compression` {type(dataset_compression)}\"\n            f\"\\ndataset_compression = {dataset_compression}\"\n        )\n\n\nclass _DtypeReductionMapping(Mapping):\n    \"\"\"\n    Unfortuantly, mappings compare by hash(item) and not the __eq__ operator\n    between the key and the item.\n\n    Hence we wrap the dict in a Mapping class and implement our own __getitem__\n    such that we do use __eq__ between keys and query items.\n\n    >>> np.float32 == dtype('float32') # True, they are considered equal\n    >>>\n    >>> mydict = { np.float32: 'hello' }\n    >>>\n    >>> # Equal by __eq__ but dict operations fail\n    >>> np.dtype('float32') in mydict # False\n    >>> mydict[dtype('float32')]  # KeyError\n\n    This mapping class fixes that supporting the `in` operator as well as `__getitem__`\n\n    >>> reduction_mapping = _DtypeReductionMapping()\n    >>>\n    >>> reduction_mapping[np.dtype('float64')] # np.float32\n    >>> np.dtype('float32') in reduction_mapping # True\n    \"\"\"\n\n    # Information about dtype support\n    _mapping: Dict[type, type] = {\n        np.float32: np.float32,\n        np.float64: np.float32,\n    }\n\n    # In spite of the names, np.float96 and np.float128\n    # provide only as much precision as np.longdouble,\n    # that is, 80 bits on most x86 machines and 64 bits\n    # in standard Windows builds.\n    if hasattr(np, \"float96\"):\n        _mapping[np.float96] = np.float64\n\n    if hasattr(np, \"float128\"):\n        _mapping[np.float128] = np.float64\n\n    @classmethod\n    def __getitem__(cls, item: type) -> type:\n        for k, v in cls._mapping.items():\n            if k == item:\n                return v\n        raise KeyError(item)\n\n    @classmethod\n    def __iter__(cls) -> Iterator[type]:\n        return iter(cls._mapping.keys())\n\n    @classmethod\n    def __len__(cls) -> int:\n        return len(cls._mapping)\n\n\nreduction_mapping = _DtypeReductionMapping()\nsupported_precision_reductions = list(reduction_mapping)\n\n\ndef binarization(array: Union[List, np.ndarray]) -> np.ndarray:\n    # Takes a binary-class datafile and turn the max value (positive class)\n    # into 1 and the min into 0\n    array = np.array(array, dtype=float)  # conversion needed to use np.inf\n    if len(np.unique(array)) > 2:\n        raise ValueError(\n            \"The argument must be a binary-class datafile. \"\n            \"{} classes detected\".format(len(np.unique(array)))\n        )\n\n    # manipulation which aims at avoid error in data\n    # with for example classes '1' and '2'.\n    array[array == np.amax(array)] = np.inf\n    array[array == np.amin(array)] = 0\n    array[array == np.inf] = 1\n    return np.array(array, dtype=int)\n\n\ndef multilabel_to_multiclass(array: Union[List, np.ndarray]) -> np.ndarray:\n    array = binarization(array)\n    return np.array([np.nonzero(array[i, :])[0][0] for i in range(len(array))])\n\n\ndef convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Convert binary targets to numeric vector\n    typically classification target values\n    :param Ybin:\n    :return:\n    \"\"\"\n    result = np.array(Ybin)\n    if len(Ybin.shape) != 1:\n        result = np.dot(Ybin, range(Ybin.shape[1]))\n    return result\n\n\ndef convert_to_bin(Ycont: List, nval: int, verbose: bool = True) -> List:\n    # Convert numeric vector to binary (typically classification target values)\n    if verbose:\n        pass\n    Ybin = [[0] * nval for _ in range(len(Ycont))]\n    for i in range(len(Ybin)):\n        line = Ybin[i]\n        line[int(Ycont[i])] = 1\n        Ybin[i] = line\n    return Ybin\n\n\ndef predict_RAM_usage(X: np.ndarray, categorical: List[bool]) -> float:\n    # Return estimated RAM usage of dataset after OneHotEncoding in bytes.\n    estimated_columns = 0\n    for i, cat in enumerate(categorical):\n        if cat:\n            unique_values = np.unique(X[:, i])\n            num_unique_values = np.sum(np.isfinite(unique_values))\n            estimated_columns += num_unique_values\n        else:\n            estimated_columns += 1\n    estimated_ram = estimated_columns * X.shape[0] * X.dtype.itemsize\n    return estimated_ram\n\n\ndef subsample(\n    X: Union[np.ndarray, spmatrix],\n    y: np.ndarray,\n    is_classification: bool,\n    sample_size: Union[float, int],\n    random_state: Optional[Union[int, np.random.RandomState]] = None,\n) -> Tuple[Union[np.ndarray, spmatrix], np.ndarray]:\n    \"\"\"Subsamples data returning the same type as it recieved.\n\n    If `is_classification`, we split using a stratified shuffle split which\n    preserves unique labels in the training set.\n\n    NOTE:\n    It's highly unadvisable to use lists here. In order to preserver types,\n    we convert to a numpy array and then back to a list.\n\n    NOTE2:\n    Interestingly enough, StratifiedShuffleSplut and descendants don't support\n    sparse `y` in `split(): _check_array` call. Hence, neither do we.\n\n    Parameters\n    ----------\n    X: Union[np.ndarray, spmatrix]\n        The X's to subsample\n\n    y: np.ndarray\n        The Y's to subsample\n\n    is_classification: bool\n        Whether this is classification data or regression data. Required for\n        knowing how to split.\n\n    sample_size: float | int\n        If float, percentage of data to take otherwise if int, an absolute\n        count of samples to take.\n\n    random_state: int | RandomState = None\n        The random state to pass to the splitted\n\n    Returns\n    -------\n    (np.ndarray | spmatrix, np.ndarray)\n        The X and y subsampled according to sample_size\n    \"\"\"\n    if isinstance(X, List):\n        X = np.asarray(X)\n\n    if isinstance(y, List):\n        y = np.asarray(y)\n\n    if is_classification:\n        splitter = CustomStratifiedShuffleSplit(\n            train_size=sample_size, random_state=random_state\n        )\n        left_idxs, _ = next(splitter.split(X=X, y=y))\n\n        # This function supports pandas objects but they won't get here\n        # yet as we do not reduce the size of pandas dataframes.\n        if isinstance(X, pd.DataFrame):\n            idxs = X.index[left_idxs]\n            X = X.loc[idxs]\n        else:\n            X = X[left_idxs]\n\n        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            idxs = y.index[left_idxs]\n            y = y.loc[idxs]\n        else:\n            y = y[left_idxs]\n\n    else:\n        X, _, y, _ = train_test_split(  # type: ignore\n            X,\n            y,\n            train_size=sample_size,\n            random_state=random_state,\n        )\n\n    return X, y\n\n\ndef reduce_precision(\n    X: Union[np.ndarray, spmatrix]\n) -> Tuple[Union[np.ndarray, spmatrix], Type]:\n    \"\"\"Reduces the precision of a np.ndarray or spmatrix containing floats\n\n    Parameters\n    ----------\n    X:  np.ndarray | spmatrix\n        The data to reduce precision of.\n\n    Returns\n    -------\n    (ndarray | spmatrix, dtype)\n        Returns the reduced data X along with the dtype it was reduced to.\n    \"\"\"\n    if X.dtype not in supported_precision_reductions:\n        raise ValueError(\n            f\"X.dtype = {X.dtype} not equal to any supported\"\n            f\" {supported_precision_reductions}\"\n        )\n\n    precision = reduction_mapping[X.dtype]\n    return X.astype(precision), precision\n\n\ndef reduce_dataset_size_if_too_large(\n    X: Union[np.ndarray, spmatrix],\n    y: np.ndarray,\n    memory_limit: int,\n    is_classification: bool,\n    random_state: Union[int, np.random.RandomState] = None,\n    operations: List[str] = [\"precision\", \"subsample\"],\n    memory_allocation: Union[int, float] = 0.1,\n) -> Tuple[Union[np.ndarray, spmatrix], np.ndarray]:\n    f\"\"\"Reduces the size of the dataset if it's too close to the memory limit.\n\n    Follows the order of the operations passed in and retains the type of its\n    input.\n\n    Precision reduction will only work on the following float types:\n    -   {supported_precision_reductions}\n\n    Subsampling will ensure that the memory limit is satisfied while precision reduction\n    will only perform one level of precision reduction. Technically, you could supply\n    multiple rounds of precision reduction, i.e. to reduce np.float128 to np.float32\n    you could use `operations = ['precision'] * 2`.\n\n    However, if that's the use case, it'd be advised to simply use the function\n    `autosklearn.util.data.reduce_precision`.\n\n    NOTE: limitations\n    * Does not support dataframes yet\n        -   Requires implementing column wise precision reduction\n        -   Requires calculating memory usage\n\n    Parameters\n    ----------\n    X: np.ndarray | spmatrix\n        The features of the dataset.\n\n    y: np.ndarray\n        The labels of the dataset.\n\n    memory_limit: int\n        The amount of memory allocated in megabytes\n\n    is_classification: bool\n        Whether it's a classificaiton dataset or not. This is important when\n        considering how to subsample.\n\n    random_state: int | RandomState = None\n        The random_state to use for subsampling.\n\n    operations: List[str] = ['precision', 'subsampling']\n        A list of operations that are permitted to be performed to reduce\n        the size of the dataset.\n\n        **precision**\n\n        Reduce the precision of float types\n\n        **subsample**\n\n        Reduce the amount of samples of the dataset such that it fits into the allocated\n        memory. Ensures stratification and that unique labels are present\n\n    memory_allocation: Union[int, float] = 0.1\n        The amount of memory to allocate to the dataset. A float specifys that the\n        dataset will try to be fit into that percentage of memory. An int specifies an\n        absolute amount.\n\n    Returns\n    -------\n    (spmatrix | np.ndarray, np.ndarray)\n        The reduced X, y if reductions were needed\n    \"\"\"\n    # Validation\n    assert memory_limit > 0\n\n    if isinstance(memory_allocation, float):\n        if not (0.0 < memory_allocation < 1.0):\n            raise ValueError(\"memory_allocation if float must be in (0, 1)\")\n\n        allocated_memory = memory_limit * memory_allocation\n\n    elif isinstance(memory_allocation, int):\n        if not (0 < memory_allocation < memory_limit):\n            raise ValueError(\"memory_allocation if int must be in (0, memory_limit)\")\n\n        allocated_memory = memory_allocation\n\n    else:\n        raise ValueError(\n            f\"Unknown type for `memory_allocation` {type(memory_allocation)}\"\n        )\n\n    if \"precision\" in operations and X.dtype not in supported_precision_reductions:\n        raise ValueError(f\"Unsupported type `{X.dtype}` for precision reduction\")\n\n    def megabytes(arr: Union[np.ndarray, spmatrix]) -> float:\n        return (arr.nbytes if isinstance(X, np.ndarray) else arr.data.nbytes) / (\n            2**20\n        )\n\n    for operation in operations:\n\n        if operation == \"precision\":\n            # If the dataset is too big for the allocated memory,\n            # we then try to reduce the precision if it's a high precision dataset\n            if megabytes(X) > allocated_memory:\n                X, precision = reduce_precision(X)\n                warnings.warn(\n                    f\"Dataset too large for allocated memory {allocated_memory}MB, \"\n                    f\"reduced the precision from {X.dtype} to {precision}\",\n                )\n\n        elif operation == \"subsample\":\n            # If the dataset is still too big such that we couldn't fit\n            # into the allocated memory, we subsample it so that it does\n            if megabytes(X) > allocated_memory:\n\n                n_samples_before = X.shape[0]\n                sample_percentage = allocated_memory / megabytes(X)\n\n                # NOTE: type ignore\n                #\n                # Tried the generic `def subsample(X: T) -> T` approach but it was\n                # failing elsewhere, keeping it simple for now\n                X, y = subsample(  # type: ignore\n                    X,\n                    y,\n                    sample_size=sample_percentage,\n                    is_classification=is_classification,\n                    random_state=random_state,\n                )\n\n                n_samples_after = X.shape[0]\n                warnings.warn(\n                    f\"Dataset too large for allocated memory {allocated_memory}MB,\"\n                    f\" reduced number of samples from {n_samples_before} to\"\n                    f\" {n_samples_after}.\"\n                )\n\n        else:\n            raise ValueError(f\"Unknown operation `{operation}`\")\n\n    return X, y\n\n\n### Source File Content:\nimport typing\nfrom typing import Any\n\nfrom pathlib import Path\n\nimport dask.distributed\n\n\nclass DummyFuture(dask.distributed.Future):\n    \"\"\"\n    A class that mimics a distributed Future, the outcome of\n    performing submit on a distributed client.\n    \"\"\"\n\n    def __init__(self, result: typing.Any) -> None:\n        self._result = result  # type: typing.Any\n\n    def result(self, timeout: typing.Optional[int] = None) -> typing.Any:\n        return self._result\n\n    def cancel(self) -> None:\n        pass\n\n    def done(self) -> bool:\n        return True\n\n    def __repr__(self) -> str:\n        return \"DummyFuture: {}\".format(self._result)\n\n    def __del__(self) -> None:\n        pass\n\n\nclass SingleThreadedClient(dask.distributed.Client):\n    \"\"\"\n    A class to Mock the Distributed Client class, in case\n    Auto-Sklearn is meant to run in the current Thread.\n    \"\"\"\n\n    def __init__(self) -> None:\n\n        # Raise a not implemented error if using a method from Client\n        implemented_methods = [\n            \"submit\",\n            \"close\",\n            \"shutdown\",\n            \"write_scheduler_file\",\n            \"_get_scheduler_info\",\n            \"nthreads\",\n        ]\n        method_list = [\n            func\n            for func in dir(dask.distributed.Client)\n            if callable(getattr(dask.distributed.Client, func))\n            and not func.startswith(\"__\")\n        ]\n        for method in method_list:\n            if method in implemented_methods:\n                continue\n            setattr(self, method, self._unsupported_method)\n        pass\n\n    def _unsupported_method(self) -> None:\n        raise NotImplementedError()\n\n    def submit(\n        self,\n        func: typing.Callable,\n        *args: typing.List,\n        priority: int = 0,\n        key: Any = None,\n        workers: Any = None,\n        resources: Any = None,\n        retries: Any = None,\n        fifo_timeout: Any = \"100 ms\",\n        allow_other_workers: Any = False,\n        actor: Any = False,\n        actors: Any = False,\n        pure: Any = None,\n        **kwargs: Any,\n    ) -> typing.Any:\n        \"\"\"\n        Note\n        ----\n        The keyword arguments caught in `dask.distributed.Client` need to\n        be specified here so they don't get passed in as ``**kwargs`` to the\n        ``func``.\n        \"\"\"\n        return DummyFuture(func(*args, **kwargs))\n\n    def close(self) -> None:\n        pass\n\n    def shutdown(self) -> None:\n        pass\n\n    def write_scheduler_file(self, scheduler_file: str) -> None:\n        Path(scheduler_file).touch()\n        return\n\n    def _get_scheduler_info(self) -> typing.Dict:\n        return {\n            \"workers\": [\"127.0.0.1\"],\n            \"type\": \"Scheduler\",\n        }\n\n    def nthreads(self) -> typing.Dict:\n        return {\n            \"127.0.0.1\": 1,\n        }\n\n    def __repr__(self) -> str:\n        return \"SingleThreadedClient()\"\n\n    def __del__(self) -> None:\n        pass\n\n### Source File Dependency Files Content\n### Dependency File: splitter.py\nimport warnings\n\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.model_selection._split import _validate_shuffle_split\nfrom sklearn.utils import _approximate_mode, check_random_state, indexable\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import _num_samples, check_array, column_or_1d\n\n\nclass CustomStratifiedShuffleSplit(StratifiedShuffleSplit):\n    \"\"\"Splitter that deals with classes with too few samples\"\"\"\n\n    def _iter_indices(self, X, y, groups=None):  # type: ignore\n        n_samples = _num_samples(X)\n        y = check_array(y, ensure_2d=False, dtype=None)\n        n_train, n_test = _validate_shuffle_split(\n            n_samples,\n            self.test_size,\n            self.train_size,\n            default_test_size=self._default_test_size,\n        )\n\n        if y.ndim == 2:\n            # for multi-label y, map each distinct row to a string repr\n            # using join because str(row) uses an ellipsis if len(row) > 1000\n            y = np.array([\" \".join(row.astype(\"str\")) for row in y])\n\n        classes, y_indices = np.unique(y, return_inverse=True)\n        n_classes = classes.shape[0]\n\n        class_counts = np.bincount(y_indices)\n\n        if n_train < n_classes:\n            raise ValueError(\n                \"The train_size = %d should be greater or \"\n                \"equal to the number of classes = %d\" % (n_train, n_classes)\n            )\n        if n_test < n_classes:\n            raise ValueError(\n                \"The test_size = %d should be greater or \"\n                \"equal to the number of classes = %d\" % (n_test, n_classes)\n            )\n\n        # Find the sorted list of instances for each class:\n        # (np.unique above performs a sort, so code is O(n logn) already)\n        class_indices = np.split(\n            np.argsort(y_indices, kind=\"mergesort\"), np.cumsum(class_counts)[:-1]\n        )\n\n        rng = check_random_state(self.random_state)\n\n        for _ in range(self.n_splits):\n            # if there are ties in the class-counts, we want\n            # to make sure to break them anew in each iteration\n            n_i = _approximate_mode(class_counts, n_train, rng)\n            class_counts_remaining = class_counts - n_i\n            t_i = _approximate_mode(class_counts_remaining, n_test, rng)\n            train = []\n            test = []\n\n            # NOTE: Adapting for unique instances\n            #\n            #   Each list n_i, t_i represent the list of class in the\n            #   training_set and test_set resepectively.\n            #\n            #   n_i = [100, 100, 0, 3]  # 100 of class '0', 0 of class '2'\n            #   t_i = [300, 300, 1, 3]  # 300 of class '0', 1 of class '2'\n            #\n            #  To support unique labels such as class '2', which only has one sample\n            #  between both n_i and t_i, we need to make sure that n_i has at least\n            #  one sample of all classes. There is also the extra check to ensure\n            #  that the sizes stay the same.\n            #\n            #   n_i = [ 99, 100, 1, 3]  # 100 of class '0', 0 of class '2'\n            #            |       ^\n            #            v       |\n            #   t_i = [301, 300, 0, 3]  # 300 of class '0', 1 of class '2'\n            #\n            for i, class_count in enumerate(n_i):\n                if class_count == 0:\n                    t_i[i] -= 1\n                    n_i[i] += 1\n\n                    j = np.argmax(n_i)\n                    if n_i[j] == 1:\n                        warnings.warn(\n                            \"Can't respect size requirements for split.\",\n                            \" The training set must contain all of the unique\"\n                            \" labels that exist in the dataset.\",\n                        )\n                    else:\n                        n_i[j] -= 1\n                        t_i[j] += 1\n\n            for i in range(n_classes):\n                permutation = rng.permutation(class_counts[i])\n                perm_indices_class_i = class_indices[i].take(permutation, mode=\"clip\")\n\n                train.extend(perm_indices_class_i[: n_i[i]])\n                test.extend(perm_indices_class_i[n_i[i] : n_i[i] + t_i[i]])\n\n            train = rng.permutation(train)\n            test = rng.permutation(test)\n\n            yield train, test\n\n\nclass CustomStratifiedKFold(StratifiedKFold):\n    \"\"\"Stratified K-Folds cross-validator that ensures that there is always at least\n    1 sample per class in the training set.\n    \"\"\"\n\n    def _make_test_folds(self, X, y=None):  # type: ignore\n        rng = check_random_state(self.random_state)\n        y = np.asarray(y)\n        type_of_target_y = type_of_target(y)\n        allowed_target_types = (\"binary\", \"multiclass\")\n        if type_of_target_y not in allowed_target_types:\n            raise ValueError(\n                \"Supported target types are: {}. Got {!r} instead.\".format(\n                    allowed_target_types, type_of_target_y\n                )\n            )\n\n        y = column_or_1d(y)\n\n        _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)\n        # y_inv encodes y according to lexicographic order. We invert y_idx to\n        # map the classes so that they are encoded by order of appearance:\n        # 0 represents the first label appearing in y, 1 the second, etc.\n        _, class_perm = np.unique(y_idx, return_inverse=True)\n        y_encoded = class_perm[y_inv]\n\n        n_classes = len(y_idx)\n\n        # Determine the optimal number of samples from each class in each fold,\n        # using round robin over the sorted y. (This can be done direct from\n        # counts, but that code is unreadable.)\n        y_order = np.sort(y_encoded)\n        allocation = np.asarray(\n            [\n                np.bincount(y_order[i :: self.n_splits], minlength=n_classes)\n                for i in range(self.n_splits)\n            ]\n        )\n\n        # To maintain the data order dependencies as best as possible within\n        # the stratification constraint, we assign samples from each class in\n        # blocks (and then mess that up when shuffle=True).\n        test_folds = np.empty(len(y), dtype=\"i\")\n        for k in range(n_classes):\n            # since the kth column of allocation stores the number of samples\n            # of class k in each test set, this generates blocks of fold\n            # indices corresponding to the allocation for class k.\n            folds_for_class = np.arange(self.n_splits).repeat(allocation[:, k])\n            if self.shuffle:\n                rng.shuffle(folds_for_class)\n            test_folds[y_encoded == k] = folds_for_class\n        return test_folds\n\n    def split(self, X, y=None, groups=None):  # type: ignore\n\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        if self.n_splits > n_samples:\n            raise ValueError(\n                f\"Cannot have number of splits n_splits={self.n_splits} greater\"\n                f\" than the number of samples: n_samples={n_samples}.\"\n            )\n\n        for train, test in super().split(X, y, groups):\n            all_classes = np.unique(y)\n            train_classes = np.unique(y[train])\n            train = list(train)\n            test = list(test)\n            missing_classes = set(all_classes) - set(train_classes)\n            if len(missing_classes) > 0:\n                # print(missing_classes)\n                for diff in missing_classes:\n                    # print(len(train), len(test))\n                    to_move = np.where(y[test] == diff)[0][0]\n                    # print(y[test][to_move])\n                    train = train + [test[to_move]]\n                    del test[to_move]\n                    # print(len(train), len(test))\n            train = np.array(train, dtype=int)\n            test = np.array(test, dtype=int)\n\n            yield train, test\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:43:24"}, {"role": "assistant", "content": "```python\n\"\"\"Test things related to only constructing an AutoML instance\"\"\"\nfrom typing import Any, Dict, Optional, Union\n\nfrom autosklearn.automl import AutoML\nfrom autosklearn.util.dask import LocalDask\nfrom autosklearn.util.data import default_dataset_compression_arg\nfrom autosklearn.util.single_thread_client import SingleThreadedClient\n\nimport pytest\nfrom pytest_cases import parametrize\n\n\n@parametrize(\"disable_evaluator_output\", [(\"hello\", \"world\"), (\"model\", \"other\")])\ndef test_invalid_disable_eval_output_options(disable_evaluator_output: Any) -> None:\n    \"\"\"\n    Parameters\n    ----------\n    disable_evaluator_output : Iterable[str]\n        An iterator of invalid options\n\n    Expects\n    -------\n    * Should raise an error about invalid options\n    \"\"\"\n    with pytest.raises(ValueError, match=\"Unknown arg\"):\n        AutoML(\n            time_left_for_this_task=30,\n            per_run_time_limit=5,\n            disable_evaluator_output=disable_evaluator_output,\n        )\n\n\n@parametrize(\n    \"dataset_compression, expected\",\n    [\n        (False, None),\n        (True, default_dataset_compression_arg),\n        (\n            {\"memory_allocation\": 0.2},\n            {**default_dataset_compression_arg, **{\"memory_allocation\": 0.2}},\n        ),\n    ],\n)\ndef test_param_dataset_compression_args(\n    dataset_compression: Union[bool, Dict],\n    expected: Optional[Dict],\n) -> None:\n    \"\"\"\n    Parameters\n    ----------\n    dataset_compression: Union[bool, Dict]\n        The dataset_compression arg used\n\n    expected: Optional[Dict]\n        The expected internal variable setting\n\n    Expects\n    -------\n    * Setting the compression arg should result in the expected value\n        * False -> None, No dataset compression\n        * True -> default, The default settings\n        * dict -> default updated, The default should be updated with the args used\n    \"\"\"\n    auto = AutoML(\n        time_left_for_this_task=30,\n        per_run_time_limit=5,\n        dataset_compression=dataset_compression,\n    )\n    assert auto._dataset_compression == expected\n\n\ndef test_single_job_and_no_dask_client_sets_correct_multiprocessing_context() -> None:\n    \"\"\"\n    Expects\n    -------\n    * With n_jobs set to 1 and no dask client, we default to a SingleThreadedClient\n    with a \"fork\" _multiprocessing_context\n    \"\"\"\n    n_jobs = 1\n    dask_client = None\n\n    automl = AutoML(\n        time_left_for_this_task=30,\n        per_run_time_limit=5,\n        n_jobs=n_jobs,\n        dask_client=dask_client,\n    )\n\n    assert automl._multiprocessing_context == \"fork\"\n    assert automl._n_jobs == 1\n    assert isinstance(automl._dask, LocalDask)\n\n    with automl._dask as client:\n        assert isinstance(client, SingleThreadedClient)\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: auto-sklearn\nTest File Path: auto-sklearn\\test_data\\test_data.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom typing import (\n    Any,\n    Dict,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import spmatrix\nfrom sklearn.model_selection import train_test_split\n\nfrom autosklearn.evaluation.splitter import CustomStratifiedShuffleSplit\n\n# TODO: TypedDict with python 3.8\n#\n#   When upgrading to python 3.8 as minimum version, this should be a TypedDict\n#   so that mypy can identify the fields types\nDatasetCompressionSpec = Dict[str, Union[float, List[str]]]\n\n# Default specification for arg `dataset_compression`\ndefault_dataset_compression_arg: DatasetCompressionSpec = {\n    \"memory_allocation\": 0.1,\n    \"methods\": [\"precision\", \"subsample\"],\n}\n\n\ndef validate_dataset_compression_arg(\n    dataset_compression: Mapping[str, Any], memory_limit: int\n) -> DatasetCompressionSpec:\n    \"\"\"Validates and return a correct dataset_compression argument\n\n    The returned value can be safely used with `reduce_dataset_size_if_too_large`.\n\n    Parameters\n    ----------\n    dataset_compression: Mapping[str, Any]\n        The arg to validate\n\n    Returns\n    -------\n    DatasetCompressionSpec\n        The validated and correct dataset compression spec\n    \"\"\"\n    if isinstance(dataset_compression, Mapping):\n        # Fill with defaults if they don't exist\n        dataset_compression = {**default_dataset_compression_arg, **dataset_compression}\n\n        parsed_keys = set(dataset_compression.keys())\n        default_keys = set(default_dataset_compression_arg.keys())\n\n        # Must contain known keys\n        if parsed_keys != default_keys:\n            raise ValueError(\n                f\"Unknown key(s) in ``dataset_compression``, {parsed_keys}.\"\n                f\"\\nPossible keys are {default_keys}\"\n            )\n\n        memory_allocation = dataset_compression[\"memory_allocation\"]\n\n        # \"memory_allocation\" must be float or int\n        if not (\n            isinstance(memory_allocation, float) or isinstance(memory_allocation, int)\n        ):\n            raise ValueError(\n                \"key 'memory_allocation' must be an `int` or `float`\"\n                f\"\\ntype = {memory_allocation}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        # \"memory_allocation\" must be in (0,1) if float\n        if isinstance(memory_allocation, float) and not (0.0 < memory_allocation < 1.0):\n            raise ValueError(\n                \"key 'memory_allocation' if float must be in (0, 1)\"\n                f\"\\nmemory_allocation = {memory_allocation}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        # \"memory_allocation\" if absolute, should be > 0 and < memory_limit\n        if isinstance(memory_allocation, int) and not (\n            0 < memory_allocation < memory_limit\n        ):\n            raise ValueError(\n                f\"key 'memory_allocation' if int must be in (0, {memory_limit})\"\n                f\"\\nmemory_allocation = {memory_allocation}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        # \"methods\" must be non-empty sequence\n        if (\n            not isinstance(dataset_compression[\"methods\"], Sequence)\n            or len(dataset_compression[\"methods\"]) <= 0\n        ):\n            raise ValueError(\n                \"key 'methods' must be a non-empty list\"\n                f\"\\nmethods = {dataset_compression['methods']}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        # \"methods\" must contain known methods\n        if any(\n            method\n            not in cast(Sequence, default_dataset_compression_arg[\"methods\"])  # mypy\n            for method in dataset_compression[\"methods\"]\n        ):\n            valid_methods = default_dataset_compression_arg[\"methods\"]\n            raise ValueError(\n                f\"key 'methods' can only contain {valid_methods}\"\n                f\"\\nmethods = {dataset_compression['methods']}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        return cast(DatasetCompressionSpec, dataset_compression)\n    else:\n        raise ValueError(\n            f\"Unknown type for `dataset_compression` {type(dataset_compression)}\"\n            f\"\\ndataset_compression = {dataset_compression}\"\n        )\n\n\nclass _DtypeReductionMapping(Mapping):\n    \"\"\"\n    Unfortuantly, mappings compare by hash(item) and not the __eq__ operator\n    between the key and the item.\n\n    Hence we wrap the dict in a Mapping class and implement our own __getitem__\n    such that we do use __eq__ between keys and query items.\n\n    >>> np.float32 == dtype('float32') # True, they are considered equal\n    >>>\n    >>> mydict = { np.float32: 'hello' }\n    >>>\n    >>> # Equal by __eq__ but dict operations fail\n    >>> np.dtype('float32') in mydict # False\n    >>> mydict[dtype('float32')]  # KeyError\n\n    This mapping class fixes that supporting the `in` operator as well as `__getitem__`\n\n    >>> reduction_mapping = _DtypeReductionMapping()\n    >>>\n    >>> reduction_mapping[np.dtype('float64')] # np.float32\n    >>> np.dtype('float32') in reduction_mapping # True\n    \"\"\"\n\n    # Information about dtype support\n    _mapping: Dict[type, type] = {\n        np.float32: np.float32,\n        np.float64: np.float32,\n    }\n\n    # In spite of the names, np.float96 and np.float128\n    # provide only as much precision as np.longdouble,\n    # that is, 80 bits on most x86 machines and 64 bits\n    # in standard Windows builds.\n    if hasattr(np, \"float96\"):\n        _mapping[np.float96] = np.float64\n\n    if hasattr(np, \"float128\"):\n        _mapping[np.float128] = np.float64\n\n    @classmethod\n    def __getitem__(cls, item: type) -> type:\n        for k, v in cls._mapping.items():\n            if k == item:\n                return v\n        raise KeyError(item)\n\n    @classmethod\n    def __iter__(cls) -> Iterator[type]:\n        return iter(cls._mapping.keys())\n\n    @classmethod\n    def __len__(cls) -> int:\n        return len(cls._mapping)\n\n\nreduction_mapping = _DtypeReductionMapping()\nsupported_precision_reductions = list(reduction_mapping)\n\n\ndef binarization(array: Union[List, np.ndarray]) -> np.ndarray:\n    # Takes a binary-class datafile and turn the max value (positive class)\n    # into 1 and the min into 0\n    array = np.array(array, dtype=float)  # conversion needed to use np.inf\n    if len(np.unique(array)) > 2:\n        raise ValueError(\n            \"The argument must be a binary-class datafile. \"\n            \"{} classes detected\".format(len(np.unique(array)))\n        )\n\n    # manipulation which aims at avoid error in data\n    # with for example classes '1' and '2'.\n    array[array == np.amax(array)] = np.inf\n    array[array == np.amin(array)] = 0\n    array[array == np.inf] = 1\n    return np.array(array, dtype=int)\n\n\ndef multilabel_to_multiclass(array: Union[List, np.ndarray]) -> np.ndarray:\n    array = binarization(array)\n    return np.array([np.nonzero(array[i, :])[0][0] for i in range(len(array))])\n\n\ndef convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Convert binary targets to numeric vector\n    typically classification target values\n    :param Ybin:\n    :return:\n    \"\"\"\n    result = np.array(Ybin)\n    if len(Ybin.shape) != 1:\n        result = np.dot(Ybin, range(Ybin.shape[1]))\n    return result\n\n\ndef convert_to_bin(Ycont: List, nval: int, verbose: bool = True) -> List:\n    # Convert numeric vector to binary (typically classification target values)\n    if verbose:\n        pass\n    Ybin = [[0] * nval for _ in range(len(Ycont))]\n    for i in range(len(Ybin)):\n        line = Ybin[i]\n        line[int(Ycont[i])] = 1\n        Ybin[i] = line\n    return Ybin\n\n\ndef predict_RAM_usage(X: np.ndarray, categorical: List[bool]) -> float:\n    # Return estimated RAM usage of dataset after OneHotEncoding in bytes.\n    estimated_columns = 0\n    for i, cat in enumerate(categorical):\n        if cat:\n            unique_values = np.unique(X[:, i])\n            num_unique_values = np.sum(np.isfinite(unique_values))\n            estimated_columns += num_unique_values\n        else:\n            estimated_columns += 1\n    estimated_ram = estimated_columns * X.shape[0] * X.dtype.itemsize\n    return estimated_ram\n\n\ndef subsample(\n    X: Union[np.ndarray, spmatrix],\n    y: np.ndarray,\n    is_classification: bool,\n    sample_size: Union[float, int],\n    random_state: Optional[Union[int, np.random.RandomState]] = None,\n) -> Tuple[Union[np.ndarray, spmatrix], np.ndarray]:\n    \"\"\"Subsamples data returning the same type as it recieved.\n\n    If `is_classification`, we split using a stratified shuffle split which\n    preserves unique labels in the training set.\n\n    NOTE:\n    It's highly unadvisable to use lists here. In order to preserver types,\n    we convert to a numpy array and then back to a list.\n\n    NOTE2:\n    Interestingly enough, StratifiedShuffleSplut and descendants don't support\n    sparse `y` in `split(): _check_array` call. Hence, neither do we.\n\n    Parameters\n    ----------\n    X: Union[np.ndarray, spmatrix]\n        The X's to subsample\n\n    y: np.ndarray\n        The Y's to subsample\n\n    is_classification: bool\n        Whether this is classification data or regression data. Required for\n        knowing how to split.\n\n    sample_size: float | int\n        If float, percentage of data to take otherwise if int, an absolute\n        count of samples to take.\n\n    random_state: int | RandomState = None\n        The random state to pass to the splitted\n\n    Returns\n    -------\n    (np.ndarray | spmatrix, np.ndarray)\n        The X and y subsampled according to sample_size\n    \"\"\"\n    if isinstance(X, List):\n        X = np.asarray(X)\n\n    if isinstance(y, List):\n        y = np.asarray(y)\n\n    if is_classification:\n        splitter = CustomStratifiedShuffleSplit(\n            train_size=sample_size, random_state=random_state\n        )\n        left_idxs, _ = next(splitter.split(X=X, y=y))\n\n        # This function supports pandas objects but they won't get here\n        # yet as we do not reduce the size of pandas dataframes.\n        if isinstance(X, pd.DataFrame):\n            idxs = X.index[left_idxs]\n            X = X.loc[idxs]\n        else:\n            X = X[left_idxs]\n\n        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            idxs = y.index[left_idxs]\n            y = y.loc[idxs]\n        else:\n            y = y[left_idxs]\n\n    else:\n        X, _, y, _ = train_test_split(  # type: ignore\n            X,\n            y,\n            train_size=sample_size,\n            random_state=random_state,\n        )\n\n    return X, y\n\n\ndef reduce_precision(\n    X: Union[np.ndarray, spmatrix]\n) -> Tuple[Union[np.ndarray, spmatrix], Type]:\n    \"\"\"Reduces the precision of a np.ndarray or spmatrix containing floats\n\n    Parameters\n    ----------\n    X:  np.ndarray | spmatrix\n        The data to reduce precision of.\n\n    Returns\n    -------\n    (ndarray | spmatrix, dtype)\n        Returns the reduced data X along with the dtype it was reduced to.\n    \"\"\"\n    if X.dtype not in supported_precision_reductions:\n        raise ValueError(\n            f\"X.dtype = {X.dtype} not equal to any supported\"\n            f\" {supported_precision_reductions}\"\n        )\n\n    precision = reduction_mapping[X.dtype]\n    return X.astype(precision), precision\n\n\ndef reduce_dataset_size_if_too_large(\n    X: Union[np.ndarray, spmatrix],\n    y: np.ndarray,\n    memory_limit: int,\n    is_classification: bool,\n    random_state: Union[int, np.random.RandomState] = None,\n    operations: List[str] = [\"precision\", \"subsample\"],\n    memory_allocation: Union[int, float] = 0.1,\n) -> Tuple[Union[np.ndarray, spmatrix], np.ndarray]:\n    f\"\"\"Reduces the size of the dataset if it's too close to the memory limit.\n\n    Follows the order of the operations passed in and retains the type of its\n    input.\n\n    Precision reduction will only work on the following float types:\n    -   {supported_precision_reductions}\n\n    Subsampling will ensure that the memory limit is satisfied while precision reduction\n    will only perform one level of precision reduction. Technically, you could supply\n    multiple rounds of precision reduction, i.e. to reduce np.float128 to np.float32\n    you could use `operations = ['precision'] * 2`.\n\n    However, if that's the use case, it'd be advised to simply use the function\n    `autosklearn.util.data.reduce_precision`.\n\n    NOTE: limitations\n    * Does not support dataframes yet\n        -   Requires implementing column wise precision reduction\n        -   Requires calculating memory usage\n\n    Parameters\n    ----------\n    X: np.ndarray | spmatrix\n        The features of the dataset.\n\n    y: np.ndarray\n        The labels of the dataset.\n\n    memory_limit: int\n        The amount of memory allocated in megabytes\n\n    is_classification: bool\n        Whether it's a classificaiton dataset or not. This is important when\n        considering how to subsample.\n\n    random_state: int | RandomState = None\n        The random_state to use for subsampling.\n\n    operations: List[str] = ['precision', 'subsampling']\n        A list of operations that are permitted to be performed to reduce\n        the size of the dataset.\n\n        **precision**\n\n        Reduce the precision of float types\n\n        **subsample**\n\n        Reduce the amount of samples of the dataset such that it fits into the allocated\n        memory. Ensures stratification and that unique labels are present\n\n    memory_allocation: Union[int, float] = 0.1\n        The amount of memory to allocate to the dataset. A float specifys that the\n        dataset will try to be fit into that percentage of memory. An int specifies an\n        absolute amount.\n\n    Returns\n    -------\n    (spmatrix | np.ndarray, np.ndarray)\n        The reduced X, y if reductions were needed\n    \"\"\"\n    # Validation\n    assert memory_limit > 0\n\n    if isinstance(memory_allocation, float):\n        if not (0.0 < memory_allocation < 1.0):\n            raise ValueError(\"memory_allocation if float must be in (0, 1)\")\n\n        allocated_memory = memory_limit * memory_allocation\n\n    elif isinstance(memory_allocation, int):\n        if not (0 < memory_allocation < memory_limit):\n            raise ValueError(\"memory_allocation if int must be in (0, memory_limit)\")\n\n        allocated_memory = memory_allocation\n\n    else:\n        raise ValueError(\n            f\"Unknown type for `memory_allocation` {type(memory_allocation)}\"\n        )\n\n    if \"precision\" in operations and X.dtype not in supported_precision_reductions:\n        raise ValueError(f\"Unsupported type `{X.dtype}` for precision reduction\")\n\n    def megabytes(arr: Union[np.ndarray, spmatrix]) -> float:\n        return (arr.nbytes if isinstance(X, np.ndarray) else arr.data.nbytes) / (\n            2**20\n        )\n\n    for operation in operations:\n\n        if operation == \"precision\":\n            # If the dataset is too big for the allocated memory,\n            # we then try to reduce the precision if it's a high precision dataset\n            if megabytes(X) > allocated_memory:\n                X, precision = reduce_precision(X)\n                warnings.warn(\n                    f\"Dataset too large for allocated memory {allocated_memory}MB, \"\n                    f\"reduced the precision from {X.dtype} to {precision}\",\n                )\n\n        elif operation == \"subsample\":\n            # If the dataset is still too big such that we couldn't fit\n            # into the allocated memory, we subsample it so that it does\n            if megabytes(X) > allocated_memory:\n\n                n_samples_before = X.shape[0]\n                sample_percentage = allocated_memory / megabytes(X)\n\n                # NOTE: type ignore\n                #\n                # Tried the generic `def subsample(X: T) -> T` approach but it was\n                # failing elsewhere, keeping it simple for now\n                X, y = subsample(  # type: ignore\n                    X,\n                    y,\n                    sample_size=sample_percentage,\n                    is_classification=is_classification,\n                    random_state=random_state,\n                )\n\n                n_samples_after = X.shape[0]\n                warnings.warn(\n                    f\"Dataset too large for allocated memory {allocated_memory}MB,\"\n                    f\" reduced number of samples from {n_samples_before} to\"\n                    f\" {n_samples_after}.\"\n                )\n\n        else:\n            raise ValueError(f\"Unknown operation `{operation}`\")\n\n    return X, y\n\n### Source File Dependency Files Content\n### Dependency File: splitter.py\nimport warnings\n\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.model_selection._split import _validate_shuffle_split\nfrom sklearn.utils import _approximate_mode, check_random_state, indexable\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import _num_samples, check_array, column_or_1d\n\n\nclass CustomStratifiedShuffleSplit(StratifiedShuffleSplit):\n    \"\"\"Splitter that deals with classes with too few samples\"\"\"\n\n    def _iter_indices(self, X, y, groups=None):  # type: ignore\n        n_samples = _num_samples(X)\n        y = check_array(y, ensure_2d=False, dtype=None)\n        n_train, n_test = _validate_shuffle_split(\n            n_samples,\n            self.test_size,\n            self.train_size,\n            default_test_size=self._default_test_size,\n        )\n\n        if y.ndim == 2:\n            # for multi-label y, map each distinct row to a string repr\n            # using join because str(row) uses an ellipsis if len(row) > 1000\n            y = np.array([\" \".join(row.astype(\"str\")) for row in y])\n\n        classes, y_indices = np.unique(y, return_inverse=True)\n        n_classes = classes.shape[0]\n\n        class_counts = np.bincount(y_indices)\n\n        if n_train < n_classes:\n            raise ValueError(\n                \"The train_size = %d should be greater or \"\n                \"equal to the number of classes = %d\" % (n_train, n_classes)\n            )\n        if n_test < n_classes:\n            raise ValueError(\n                \"The test_size = %d should be greater or \"\n                \"equal to the number of classes = %d\" % (n_test, n_classes)\n            )\n\n        # Find the sorted list of instances for each class:\n        # (np.unique above performs a sort, so code is O(n logn) already)\n        class_indices = np.split(\n            np.argsort(y_indices, kind=\"mergesort\"), np.cumsum(class_counts)[:-1]\n        )\n\n        rng = check_random_state(self.random_state)\n\n        for _ in range(self.n_splits):\n            # if there are ties in the class-counts, we want\n            # to make sure to break them anew in each iteration\n            n_i = _approximate_mode(class_counts, n_train, rng)\n            class_counts_remaining = class_counts - n_i\n            t_i = _approximate_mode(class_counts_remaining, n_test, rng)\n            train = []\n            test = []\n\n            # NOTE: Adapting for unique instances\n            #\n            #   Each list n_i, t_i represent the list of class in the\n            #   training_set and test_set resepectively.\n            #\n            #   n_i = [100, 100, 0, 3]  # 100 of class '0', 0 of class '2'\n            #   t_i = [300, 300, 1, 3]  # 300 of class '0', 1 of class '2'\n            #\n            #  To support unique labels such as class '2', which only has one sample\n            #  between both n_i and t_i, we need to make sure that n_i has at least\n            #  one sample of all classes. There is also the extra check to ensure\n            #  that the sizes stay the same.\n            #\n            #   n_i = [ 99, 100, 1, 3]  # 100 of class '0', 0 of class '2'\n            #            |       ^\n            #            v       |\n            #   t_i = [301, 300, 0, 3]  # 300 of class '0', 1 of class '2'\n            #\n            for i, class_count in enumerate(n_i):\n                if class_count == 0:\n                    t_i[i] -= 1\n                    n_i[i] += 1\n\n                    j = np.argmax(n_i)\n                    if n_i[j] == 1:\n                        warnings.warn(\n                            \"Can't respect size requirements for split.\",\n                            \" The training set must contain all of the unique\"\n                            \" labels that exist in the dataset.\",\n                        )\n                    else:\n                        n_i[j] -= 1\n                        t_i[j] += 1\n\n            for i in range(n_classes):\n                permutation = rng.permutation(class_counts[i])\n                perm_indices_class_i = class_indices[i].take(permutation, mode=\"clip\")\n\n                train.extend(perm_indices_class_i[: n_i[i]])\n                test.extend(perm_indices_class_i[n_i[i] : n_i[i] + t_i[i]])\n\n            train = rng.permutation(train)\n            test = rng.permutation(test)\n\n            yield train, test\n\n\nclass CustomStratifiedKFold(StratifiedKFold):\n    \"\"\"Stratified K-Folds cross-validator that ensures that there is always at least\n    1 sample per class in the training set.\n    \"\"\"\n\n    def _make_test_folds(self, X, y=None):  # type: ignore\n        rng = check_random_state(self.random_state)\n        y = np.asarray(y)\n        type_of_target_y = type_of_target(y)\n        allowed_target_types = (\"binary\", \"multiclass\")\n        if type_of_target_y not in allowed_target_types:\n            raise ValueError(\n                \"Supported target types are: {}. Got {!r} instead.\".format(\n                    allowed_target_types, type_of_target_y\n                )\n            )\n\n        y = column_or_1d(y)\n\n        _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)\n        # y_inv encodes y according to lexicographic order. We invert y_idx to\n        # map the classes so that they are encoded by order of appearance:\n        # 0 represents the first label appearing in y, 1 the second, etc.\n        _, class_perm = np.unique(y_idx, return_inverse=True)\n        y_encoded = class_perm[y_inv]\n\n        n_classes = len(y_idx)\n\n        # Determine the optimal number of samples from each class in each fold,\n        # using round robin over the sorted y. (This can be done direct from\n        # counts, but that code is unreadable.)\n        y_order = np.sort(y_encoded)\n        allocation = np.asarray(\n            [\n                np.bincount(y_order[i :: self.n_splits], minlength=n_classes)\n                for i in range(self.n_splits)\n            ]\n        )\n\n        # To maintain the data order dependencies as best as possible within\n        # the stratification constraint, we assign samples from each class in\n        # blocks (and then mess that up when shuffle=True).\n        test_folds = np.empty(len(y), dtype=\"i\")\n        for k in range(n_classes):\n            # since the kth column of allocation stores the number of samples\n            # of class k in each test set, this generates blocks of fold\n            # indices corresponding to the allocation for class k.\n            folds_for_class = np.arange(self.n_splits).repeat(allocation[:, k])\n            if self.shuffle:\n                rng.shuffle(folds_for_class)\n            test_folds[y_encoded == k] = folds_for_class\n        return test_folds\n\n    def split(self, X, y=None, groups=None):  # type: ignore\n\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        if self.n_splits > n_samples:\n            raise ValueError(\n                f\"Cannot have number of splits n_splits={self.n_splits} greater\"\n                f\" than the number of samples: n_samples={n_samples}.\"\n            )\n\n        for train, test in super().split(X, y, groups):\n            all_classes = np.unique(y)\n            train_classes = np.unique(y[train])\n            train = list(train)\n            test = list(test)\n            missing_classes = set(all_classes) - set(train_classes)\n            if len(missing_classes) > 0:\n                # print(missing_classes)\n                for diff in missing_classes:\n                    # print(len(train), len(test))\n                    to_move = np.where(y[test] == diff)[0][0]\n                    # print(y[test][to_move])\n                    train = train + [test[to_move]]\n                    del test[to_move]\n                    # print(len(train), len(test))\n            train = np.array(train, dtype=int)\n            test = np.array(test, dtype=int)\n\n            yield train, test\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:43:24"}, {"role": "assistant", "content": "```python\nfrom typing import Any, Dict, List, Union\n\nimport warnings\nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.datasets\nfrom scipy.sparse import csr_matrix, spmatrix\n\nfrom autosklearn.constants import (\n    BINARY_CLASSIFICATION,\n    CLASSIFICATION_TASKS,\n    MULTICLASS_CLASSIFICATION,\n    MULTILABEL_CLASSIFICATION,\n    MULTIOUTPUT_REGRESSION,\n    REGRESSION,\n    REGRESSION_TASKS,\n)\nfrom autosklearn.util.data import (\n    default_dataset_compression_arg,\n    reduce_dataset_size_if_too_large,\n    reduce_precision,\n    reduction_mapping,\n    subsample,\n    supported_precision_reductions,\n    validate_dataset_compression_arg,\n)\n\nimport pytest\n\nparametrize = pytest.mark.parametrize\n\n\n@parametrize(\"arg\", [default_dataset_compression_arg])\ndef test_validate_dataset_compression_arg_returns_original_dict(arg: Dict[str, Any]):\n    \"\"\"\n    Parameters\n    ----------\n    arg: Dict[str, any]\n\n    Expects\n    -------\n    * Should validate without error\n    * Should return a dict identical to the default\n    \"\"\"\n    validate_arg = validate_dataset_compression_arg(arg, memory_limit=10)\n    assert validate_arg == arg\n\n\n@parametrize(\"memory_allocation\", [10, 50, 0.1, 0.5])\ndef test_validate_dataset_compression_arg_returns_with_memory_allocation(\n    memory_allocation: Union[float, int],\n):\n    \"\"\"\n    Parameters\n    ----------\n    memory_allocation: Union[float, int]\n        A valid memory_allocation\n\n    Expects\n    -------\n    * Should validate without error\n    * Should not modify the memory_allocation arg\n    * Should populate the default methods\n    \"\"\"\n    arg = {\"memory_allocation\": memory_allocation}\n    validate_arg = validate_dataset_compression_arg(arg, memory_limit=100)\n\n    expected_methods = default_dataset_compression_arg[\"methods\"]\n\n    assert validate_arg[\"memory_allocation\"] == memory_allocation\n    assert validate_arg[\"methods\"] == expected_methods\n\n\n@parametrize(\n    \"methods\",\n    [\n        [\"precision\"],\n        [\"precision\", \"subsample\"],\n        [\"precision\", \"precision\", \"subsample\"],\n    ],\n)\ndef test_validate_dataset_compression_arg_returns_with_same_methods(\n    methods: List[str],\n):\n    \"\"\"\n    Parameters\n    ----------\n    methods: List[str]\n        A valid list of methods\n\n    Expects\n    -------\n    * Should validate without error\n    * Should not modify the methods arg\n    * Should populate memory_allocation with the default\n    \"\"\"\n    arg = {\"methods\": methods}\n    validate_arg = validate_dataset_compression_arg(arg, memory_limit=10)\n\n    expected_memory_allocation = default_dataset_compression_arg[\"memory_allocation\"]\n\n    assert validate_arg[\"memory_allocation\"] == expected_memory_allocation\n    assert validate_arg[\"methods\"] == methods\n\n\n@parametrize(\"bad_arg\", [[1337], \"hello\"])\ndef test_validate_dataset_compression_arg_raises_error_with_bad_arg(bad_arg: Any):\n    \"\"\"\n    Parameters\n    ----------\n    bad_arg: Any\n        An arg which is not a Mapping\n\n    Expects\n    -------\n    * Should raise a ValueError\n    \"\"\"\n    with pytest.raises(ValueError, match=r\"Unknown type\"):\n        validate_dataset_compression_arg(bad_arg, memory_limit=10)\n\n\n@parametrize(\"key\", [\"hello\", \"hi\", \"hey\"])\ndef test_validate_dataset_compression_arg_raises_error_with_bad_key(key: str):\n    \"\"\"\n    Parameters\n    ----------\n    key: str\n        A key which should not be in the parameters\n\n    Expects\n    -------\n    * Should raise a ValueError\n    \"\"\"\n    bad_arg = {**default_dataset_compression_arg, key: 1337}\n    with pytest.raises(ValueError, match=r\"Unknown key\"):\n        validate_dataset_compression_arg(bad_arg, memory_limit=10)\n\n\n@parametrize(\"memory_allocation\", [\"hello\", {}, [1, 2, 3]])\ndef test_validate_dataset_compression_arg_raises_error_with_bad_memory_allocation_type(\n    memory_allocation: Any,\n):\n    \"\"\"\n    Parameters\n    ----------\n    memory_allocation: Any\n        A bad type for memory_allocation\n\n    Expects\n    -------\n    * Should raise a ValueError\n    \"\"\"\n    bad_arg = {\"memory_allocation\": memory_allocation}\n    with pytest.raises(\n        ValueError, match=r\"key 'memory_allocation' must be an `int` or `float`\"\n    ):\n        validate_dataset_compression_arg(bad_arg, memory_limit=10)\n\n\n@parametrize(\"memory_allocation\", [-0.5, 0.0, 1.0, 1.5])\ndef test_validate_dataset_compression_arg_raises_error_with_bad_memory_allocation_float(\n    memory_allocation: float,\n):\n    \"\"\"\n    Parameters\n    ----------\n    memory_allocation: Any\n        A bad float value for memory_allocation\n\n    Expects\n    -------\n    * Should raise a ValueError\n    \"\"\"\n    bad_arg = {\"memory_allocation\": memory_allocation}\n\n    with pytest.raises(\n        ValueError, match=r\"key 'memory_allocation' if float must be in \\(0, 1\\)\"\n    ):\n        validate_dataset_compression_arg(bad_arg, memory_limit=10)\n\n\n@parametrize(\n    \"memory_allocation, memory_limit\", [(0, 10), (10, 10), (-20, 10), (20, 10)]\n)\ndef test_validate_dataset_compression_arg_raises_error_with_bad_memory_allocation_int(\n    memory_allocation: int, memory_limit: int\n):\n    \"\"\"\n    Parameters\n    ----------\n    memory_allocation: int\n        A bad float int for memory_allocation\n\n    memory_limit: int\n        The memory limit\n\n    Expects\n    -------\n    * Should raise a ValueError\n    \"\"\"\n    bad_arg = {\"memory_allocation\": memory_allocation}\n    with pytest.raises(\n        ValueError, match=r\"key 'memory_allocation' if int must be in \\(0,\"\n    ):\n        validate_dataset_compression_arg(bad_arg, memory_limit=memory_limit)\n\n\n@parametrize(\"methods\", [10, {\"hello\", \"world\"}, []])\ndef test_validate_dataset_compression_arg_raises_error_with_bad_methods_type(\n    methods: Any,\n):\n    \"\"\"\n    Parameters\n    ----------\n    methods: int\n        A bad type for key methods\n\n    Expects\n    -------\n    * Should raise a ValueError\n    \"\"\"\n    bad_arg = {\"methods\": methods}\n    with pytest.raises(ValueError, match=r\"key 'methods' must be a non-empty list\"):\n        validate_dataset_compression_arg(bad_arg, memory_limit=10)\n\n\n@parametrize(\n    \"methods\",\n    [\n        [\"bad\", \"worse\"],\n        [\"precision\", \"kind_of_bad\"],\n        [\"still_bad\", \"precision\", \"subsample\"],\n    ],\n)\ndef test_validate_dataset_compression_arg_raises_error_with_bad_methods_entries(\n    methods: Any,\n):\n    \"\"\"\n    Parameters\n    ----------\n    methods: int\n        A bad type for key methods\n\n    Expects\n    -------\n    * Should raise a ValueError\n    \"\"\"\n    bad_arg = {\"methods\": methods}\n    with pytest.raises(ValueError, match=r\"key 'methods' can only contain\"):\n        validate_dataset_compression_arg(bad_arg, memory_limit=10)\n\n\n@parametrize(\n    \"y\",\n    [\n        np.asarray(9999 * [0] + 1 * [1]),\n        np.asarray(4999 * [1] + 4999 * [2] + 1 * [3] + 1 * [4]),\n        np.asarray(\n            4999 * [[0, 1, 1]] + 4999 * [[1, 1, 0]] + 1 * [[1, 0, 1]] + 1 * [[0, 0, 0]]\n        ),\n    ],\n)\n@parametrize(\"random_state\", list(range(5)))\ndef test_subsample_classification_unique_labels_stay_in_training_set(y, random_state):\n    n_samples = len(y)\n    X = np.random.random(size=(n_samples, 3))\n    sample_size = 100\n\n    values, counts = np.unique(y, axis=0, return_counts=True)\n    unique_labels = [value for value, count in zip(values, counts) if count == 1]\n    assert len(unique_labels), \"Ensure we have unique labels in the test\"\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        X_sampled, y_sampled = subsample(\n            X,\n            y,\n            random_state=random_state,\n            sample_size=sample_size,\n            is_classification=True,\n        )\n\n    assert X_sampled.dtype == X.dtype and y_sampled.dtype == y.dtype\n    assert len(y_sampled) == sample_size\n    assert all(\n        label in y_sampled for label in unique_labels\n    ), f\"sampled unique = {np.unique(y_sampled)}, original unique = {unique_labels}\"\n\n\n@parametrize(\"X\", [np.asarray([[1, 1, 1]] * 30)])\n@parametrize(\"x_type\", [list, np.ndarray, csr_matrix, pd.DataFrame])\n@parametrize(\n    \"y, task\",\n    [\n        (np.asarray([0] * 15 + [1] * 15), BINARY_CLASSIFICATION),\n        (np.asarray([0] * 10 + [1] * 10 + [2] * 10), MULTICLASS_CLASSIFICATION),\n        (np.asarray([[1, 0, 1]] * 30), MULTILABEL_CLASSIFICATION),\n        (np.asarray([1.0] * 30), REGRESSION),\n        (np.asarray([[1.0, 1.0, 1.0]] * 30), MULTIOUTPUT_REGRESSION),\n    ],\n)\n@parametrize(\"y_type\", [list, np.ndarray, pd.DataFrame, pd.Series])\n@parametrize(\"random_state\", [0])\n@parametrize(\"sample_size\", [0.25, 0.5, 5, 10])\ndef test_subsample_validity(X, x_type, y, y_type, random_state, sample_size, task):\n    \"\"\"Asserts the validity of the function with all valid types\n\n    We want to make sure that `subsample` works correctly with all the types listed\n    as x_type and y_type.\n\n    We also want to make sure it works with all kinds of target types.\n\n    The output should maintain the types, and subsample the correct amount.\n    \"\"\"\n    assert len(X) == len(y)  # Make sure our test data is correct\n\n    if y_type == pd.Series and task in [\n        MULTILABEL_CLASSIFICATION,\n        MULTIOUTPUT_REGRESSION,\n    ]:\n        # We can't have a pd.Series with multiple values as it's 1 dimensional\n        pytest.skip(\"Can't have pd.Series as y when task is n-dimensional\")\n\n    # Convert our data to its given x_type or y_type\n    def convert(arr, objtype):\n        if objtype == np.ndarray:\n            return arr\n        elif objtype == list:\n            return arr.tolist()\n        else:\n            return objtype(arr)\n\n    X = convert(X, x_type)\n    y = convert(y, y_type)\n\n    # Subsample the data, ignoring any warnings\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        X_sampled, y_sampled = subsample(\n            X,\n            y,\n            random_state=random_state,\n            sample_size=sample_size,\n            is_classification=task in CLASSIFICATION_TASKS,\n        )\n\n    # Function to get the type of an obj\n    def dtype(obj):\n        if isinstance(obj, List):\n            if isinstance(obj[0], List):\n                return type(obj[0][0])\n            else:\n                return type(obj[0])\n\n        elif isinstance(obj, pd.DataFrame):\n            return obj.dtypes\n\n        else:\n            return obj.dtype\n\n    # Check that the types of X remain the same after subsampling\n    if isinstance(X, pd.DataFrame):\n        # Dataframe can have multiple types, one per column\n        assert list(dtype(X_sampled)) == list(dtype(X))\n    else:\n        assert dtype(X_sampled) == dtype(X)\n\n    # Check that the types of y remain the same after subsampling\n    if isinstance(y, pd.DataFrame):\n        assert list(dtype(y_sampled)) == list(dtype(y))\n    else:\n        assert dtype(y_sampled) == dtype(y)\n\n    # Function to get the size of an object\n    def size(obj):\n        if isinstance(obj, spmatrix):  # spmatrix doesn't support __len__\n            return obj.shape[0] if obj.shape[0] > 1 else obj.shape[1]\n        else:\n            return len(obj)\n\n    # check the right amount of samples were taken\n    if sample_size < 1:\n        assert size(X_sampled) == int(sample_size * size(X))\n    else:\n        assert size(X_sampled) == sample_size\n\n\n@parametrize(\"X\", [np.asarray([[0, 0, 1]] * 10)])\n@parametrize(\n    \"dtype\", supported_precision_reductions + [np.dtype(\"float32\"), np.dtype(\"float64\")]\n)\n@parametrize(\"x_type\", [np.ndarray, csr_matrix])\ndef test_reduce_precision_correctly_reduces_precision(X, dtype, x_type):\n    X = X.astype(dtype)\n    if x_type == csr_matrix:\n        X = x_type(X)\n\n    X_reduced, precision = reduce_precision(X)\n\n    # Check the reduced precision is correctly returned\n    assert X_reduced.dtype == precision\n\n    # Check that it was reduce to the correct precision\n    expected: Dict[type, type] = {\n        np.float32: np.float32,\n        np.float64: np.float32,\n        np.dtype(\"float32\"): np.float32,\n        np.dtype(\"float64\"): np.float32,\n    }\n    if hasattr(np, \"float96\"):\n        expected[np.float96] = np.float64\n\n    if hasattr(np, \"float128\"):\n        expected[np.float128] = np.float64\n\n    assert precision == expected[dtype]\n\n    # Check that X's shape was not modified in any way\n    assert X.shape == X_reduced.shape\n\n    # Check that the return type is the one as we passed in\n    assert type(X) == type(X_reduced)\n\n\n@parametrize(\"X\", [np.asarray([0, 0, 1]) * 10])\n@parametrize(\"dtype\", [np.int32, np.int64, np.complex128])\ndef test_reduce_precision_with_unsupported_dtypes(X, dtype):\n    X = X.astype(dtype)\n    with pytest.raises(ValueError) as err:\n        reduce_precision(X)\n\n    expected = (\n        f\"X.dtype = {X.dtype} not equal to any supported \"\n        f\"{supported_precision_reductions}\"\n    )\n\n    assert err.value.args[0] == expected\n\n\n@parametrize(\n    \"X\",\n    [\n        np.ones(\n            (100000, 10), dtype=np.float64\n        )  # Make it big for reductions to take place\n    ],\n)\n@parametrize(\"x_type\", [csr_matrix, np.ndarray])\n@parametrize(\"dtype\", supported_precision_reductions)\n@parametrize(\n    \"y, is_classification\",\n    [\n        (np.ones((100000,)), True),\n        (np.ones((100000,)), False),\n    ],\n)\n@parametrize(\"memory_allocation\", [0.1, 1 / 5.2, 1 / 8, 1])\n@parametrize(\"operations\", [[\"precision\"], [\"subsample\"], [\"precision\", \"subsample\"]])\ndef test_reduce_dataset_reduces_size_and_precision(\n    X, x_type, dtype, y, is_classification, memory_allocation, operations\n):\n    assert len(X) == len(y)\n    X = X.astype(dtype)\n    if x_type == csr_matrix:\n        X = x_type(X)\n\n    random_state = 0\n    memory_limit = 2  # Force reductions\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\")\n\n        X_out, y_out = reduce_dataset_size_if_too_large(\n            X=X,\n            y=y,\n            random_state=random_state,\n            memory_limit=memory_limit,\n            operations=operations,\n            memory_allocation=memory_allocation,\n            is_classification=is_classification,\n        )\n\n    def bytes(arr):\n        return arr.nbytes if isinstance(arr, np.ndarray) else arr.data.nbytes\n\n    # If we expect some precision reduction unless at float32 already\n    if \"precision\" in operations and dtype != np.float32:\n        expected = reduction_mapping[X.dtype]\n        assert X_out.dtype == expected\n        assert bytes(X_out) < bytes(X)\n\n    # If we expect some subsampling\n    if \"subsample\" in operations:\n        assert X_out.shape[0] < X.shape[0]\n        assert y_out.shape[0] < y.shape[0]\n        assert bytes(X_out) < bytes(X)\n\n\ndef test_reduce_dataset_invalid_dtype_for_precision_reduction():\n    dtype = int\n    X = np.asarray([1, 2, 3], dtype=dtype)\n\n    with pytest.raises(ValueError) as err:\n        reduce_dataset_size_if_too_large(\n            X=X,\n            y=X,\n            operations=[\"precision\"],\n            memory_limit=1,\n            memory_allocation=0.1,\n            is_classification=False,\n        )\n\n    expected_err = f\"Unsupported type `{X.dtype}` for precision reduction\"\n    assert err.value.args[0] == expected_err\n\n\ndef test_reduce_dataset_invalid_operations():\n    invalid_op = \"invalid\"\n\n    X = np.asarray([1, 2, 3], dtype=float)\n    with pytest.raises(ValueError) as err:\n        reduce_dataset_size_if_too_large(\n            X=X,\n            y=X,\n            operations=[invalid_op],\n            memory_limit=1,\n            memory_allocation=0.1,\n            is_classification=False,\n        )\n\n    expected_err = f\"Unknown operation `{invalid_op}`\"\n    assert err.value.args[0] == expected_err\n\n\n@parametrize(\"memory_allocation\", [-0.5, 0.0, 1.0, 1.5])\ndef test_reduce_dataset_invalid_memory_allocation_float(memory_allocation: float):\n    \"\"\"\n    Parameters\n    ----------\n    memory_allocation: float\n        An invalid memory allocation as a float\n\n    Expects\n    -------\n    * Should raise a ValueError\n    \"\"\"\n    with pytest.raises(\n        ValueError, match=r\"memory_allocation if float must be in \\(0, 1\\)\"\n    ):\n        reduce_dataset_size_if_too_large(\n            X=np.empty(1),\n            y=np.empty(1),\n            memory_limit=100,\n            is_classification=True,\n            memory_allocation=memory_allocation,\n        )\n\n\n@parametrize(\"memory_allocation\", [-1, 0, 100, 101])\ndef test_reduce_dataset_invalid_memory_allocation_int(memory_allocation: int):\n    \"\"\"\n    Parameters\n    ----------\n    memory_allocation: float\n        An invalid memory allocation as a int\n\n    Expects\n    -------\n    * Should raise a ValueError\n    \"\"\"\n    with pytest.raises(\n        ValueError, match=r\"memory_allocation if int must be in \\(0, memory_limit\"\n    ):\n        reduce_dataset_size_if_too_large(\n            X=np.empty(1),\n            y=np.empty(1),\n            is_classification=True,\n            memory_limit=100,\n            memory_allocation=memory_allocation,\n        )\n\n\n@parametrize(\"memory_allocation\", [\"100\", {\"a\": 1}, [100]])\ndef test_reduce_dataset_invalid_memory_allocation_type(memory_allocation: Any):\n    \"\"\"\n    Parameters\n    ----------\n    memory_allocation: Any\n        An invalid memory allocation type\n\n    Expects\n    -------\n    * Should raise a ValueError\n    \"\"\"\n    with pytest.raises(ValueError, match=r\"Unknown type for `memory_allocation`\"):\n        reduce_dataset_size_if_too_large(\n            X=np.empty(1),\n            y=np.empty(1),\n            memory_limit=100,\n            is_classification=True,\n            memory_allocation=memory_allocation,\n        )\n\n\n@pytest.mark.parametrize(\n    \"memory_limit,precision,task\",\n    [\n        (memory_limit, precision, task)\n        for task in chain(CLASSIFICATION_TASKS, REGRESSION_TASKS)\n        for precision in (float, np.float32, np.float64, np.float128)\n        for memory_limit in (1, 100)\n    ],\n)\ndef test_reduce_dataset_subsampling_explicit_values(memory_limit, precision, task):\n    random_state = 0\n    fixture = {\n        BINARY_CLASSIFICATION: {\n            1: {float: 2621, np.float32: 2621, np.float64: 2621, np.float128: 1310},\n            100: {\n                float: 12000,\n                np.float32: 12000,\n                np.float64: 12000,\n                np.float128: 12000,\n            },\n        },\n        MULTICLASS_CLASSIFICATION: {\n            1: {float: 409, np.float32: 409, np.float64: 409, np.float128: 204},\n            100: {float: 1797, np.float32: 1797, np.float64: 1797, np.float128: 1797},\n        },\n        MULTILABEL_CLASSIFICATION: {\n            1: {float: 409, np.float32: 409, np.float64: 409, np.float128: 204},\n            100: {float: 1797, np.float32: 1797, np.float64: 1797, np.float128: 1797},\n        },\n        REGRESSION: {\n            1: {float: 1310, np.float32: 1310, np.float64: 1310, np.float128: 655},\n            100: {float: 5000, np.float32: 5000, np.float64: 5000, np.float128: 5000},\n        },\n        MULTIOUTPUT_REGRESSION: {\n            1: {float: 1310, np.float32: 1310, np.float64: 1310, np.float128: 655},\n            100: {float: 5000, np.float32: 5000, np.float64: 5000, np.float128: 5000},\n        },\n    }\n\n    # Create the task and data\n    if task == BINARY_CLASSIFICATION:\n        X, y = sklearn.datasets.make_hastie_10_2()\n    elif task == MULTICLASS_CLASSIFICATION:\n        X, y = sklearn.datasets.load_digits(return_X_y=True)\n    elif task == MULTILABEL_CLASSIFICATION:\n        X, y_ = sklearn.datasets.load_digits(return_X_y=True)\n        y = np.zeros((X.shape[0], 10))\n        for i, j in enumerate(y_):\n            y[i, j] = 1\n    elif task == REGRESSION:\n        X, y = sklearn.datasets.make_friedman1(n_samples=5000, n_features=20)\n    elif task == MULTIOUTPUT_REGRESSION:\n        X, y = sklearn.datasets.make_friedman1(n_samples=5000, n_features=20)\n        y = np.vstack((y, y)).transpose()\n    else:\n        raise ValueError(task)\n\n    # Validate the test data and make sure X and y have the same number of rows\n    assert X.shape[0] == y.shape[0]\n\n    # Convert X to the dtype we are testing\n    X = X.astype(precision)\n\n    # Preform the subsampling through `reduce_dataset_size_if_too_large`\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        X_new, y_new = reduce_dataset_size_if_too_large(\n            X=X,\n            y=y,\n            random_state=random_state,\n            memory_limit=memory_limit,\n            is_classification=task in CLASSIFICATION_TASKS,\n            operations=[\"precision\", \"subsample\"],\n            memory_allocation=0.1,\n        )\n\n    # Assert the new number of samples\n    assert X_new.shape[0] == fixture[task][memory_limit][precision]\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: auto-sklearn\nTest File Path: auto-sklearn\\test_feature_validator\\test_feature_validator.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom typing import (\n    Any,\n    Dict,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import spmatrix\nfrom sklearn.model_selection import train_test_split\n\nfrom autosklearn.evaluation.splitter import CustomStratifiedShuffleSplit\n\n# TODO: TypedDict with python 3.8\n#\n#   When upgrading to python 3.8 as minimum version, this should be a TypedDict\n#   so that mypy can identify the fields types\nDatasetCompressionSpec = Dict[str, Union[float, List[str]]]\n\n# Default specification for arg `dataset_compression`\ndefault_dataset_compression_arg: DatasetCompressionSpec = {\n    \"memory_allocation\": 0.1,\n    \"methods\": [\"precision\", \"subsample\"],\n}\n\n\ndef validate_dataset_compression_arg(\n    dataset_compression: Mapping[str, Any], memory_limit: int\n) -> DatasetCompressionSpec:\n    \"\"\"Validates and return a correct dataset_compression argument\n\n    The returned value can be safely used with `reduce_dataset_size_if_too_large`.\n\n    Parameters\n    ----------\n    dataset_compression: Mapping[str, Any]\n        The arg to validate\n\n    Returns\n    -------\n    DatasetCompressionSpec\n        The validated and correct dataset compression spec\n    \"\"\"\n    if isinstance(dataset_compression, Mapping):\n        # Fill with defaults if they don't exist\n        dataset_compression = {**default_dataset_compression_arg, **dataset_compression}\n\n        parsed_keys = set(dataset_compression.keys())\n        default_keys = set(default_dataset_compression_arg.keys())\n\n        # Must contain known keys\n        if parsed_keys != default_keys:\n            raise ValueError(\n                f\"Unknown key(s) in ``dataset_compression``, {parsed_keys}.\"\n                f\"\\nPossible keys are {default_keys}\"\n            )\n\n        memory_allocation = dataset_compression[\"memory_allocation\"]\n\n        # \"memory_allocation\" must be float or int\n        if not (\n            isinstance(memory_allocation, float) or isinstance(memory_allocation, int)\n        ):\n            raise ValueError(\n                \"key 'memory_allocation' must be an `int` or `float`\"\n                f\"\\ntype = {memory_allocation}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        # \"memory_allocation\" must be in (0,1) if float\n        if isinstance(memory_allocation, float) and not (0.0 < memory_allocation < 1.0):\n            raise ValueError(\n                \"key 'memory_allocation' if float must be in (0, 1)\"\n                f\"\\nmemory_allocation = {memory_allocation}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        # \"memory_allocation\" if absolute, should be > 0 and < memory_limit\n        if isinstance(memory_allocation, int) and not (\n            0 < memory_allocation < memory_limit\n        ):\n            raise ValueError(\n                f\"key 'memory_allocation' if int must be in (0, {memory_limit})\"\n                f\"\\nmemory_allocation = {memory_allocation}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        # \"methods\" must be non-empty sequence\n        if (\n            not isinstance(dataset_compression[\"methods\"], Sequence)\n            or len(dataset_compression[\"methods\"]) <= 0\n        ):\n            raise ValueError(\n                \"key 'methods' must be a non-empty list\"\n                f\"\\nmethods = {dataset_compression['methods']}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        # \"methods\" must contain known methods\n        if any(\n            method\n            not in cast(Sequence, default_dataset_compression_arg[\"methods\"])  # mypy\n            for method in dataset_compression[\"methods\"]\n        ):\n            valid_methods = default_dataset_compression_arg[\"methods\"]\n            raise ValueError(\n                f\"key 'methods' can only contain {valid_methods}\"\n                f\"\\nmethods = {dataset_compression['methods']}\"\n                f\"\\ndataset_compression = {dataset_compression}\"\n            )\n\n        return cast(DatasetCompressionSpec, dataset_compression)\n    else:\n        raise ValueError(\n            f\"Unknown type for `dataset_compression` {type(dataset_compression)}\"\n            f\"\\ndataset_compression = {dataset_compression}\"\n        )\n\n\nclass _DtypeReductionMapping(Mapping):\n    \"\"\"\n    Unfortuantly, mappings compare by hash(item) and not the __eq__ operator\n    between the key and the item.\n\n    Hence we wrap the dict in a Mapping class and implement our own __getitem__\n    such that we do use __eq__ between keys and query items.\n\n    >>> np.float32 == dtype('float32') # True, they are considered equal\n    >>>\n    >>> mydict = { np.float32: 'hello' }\n    >>>\n    >>> # Equal by __eq__ but dict operations fail\n    >>> np.dtype('float32') in mydict # False\n    >>> mydict[dtype('float32')]  # KeyError\n\n    This mapping class fixes that supporting the `in` operator as well as `__getitem__`\n\n    >>> reduction_mapping = _DtypeReductionMapping()\n    >>>\n    >>> reduction_mapping[np.dtype('float64')] # np.float32\n    >>> np.dtype('float32') in reduction_mapping # True\n    \"\"\"\n\n    # Information about dtype support\n    _mapping: Dict[type, type] = {\n        np.float32: np.float32,\n        np.float64: np.float32,\n    }\n\n    # In spite of the names, np.float96 and np.float128\n    # provide only as much precision as np.longdouble,\n    # that is, 80 bits on most x86 machines and 64 bits\n    # in standard Windows builds.\n    if hasattr(np, \"float96\"):\n        _mapping[np.float96] = np.float64\n\n    if hasattr(np, \"float128\"):\n        _mapping[np.float128] = np.float64\n\n    @classmethod\n    def __getitem__(cls, item: type) -> type:\n        for k, v in cls._mapping.items():\n            if k == item:\n                return v\n        raise KeyError(item)\n\n    @classmethod\n    def __iter__(cls) -> Iterator[type]:\n        return iter(cls._mapping.keys())\n\n    @classmethod\n    def __len__(cls) -> int:\n        return len(cls._mapping)\n\n\nreduction_mapping = _DtypeReductionMapping()\nsupported_precision_reductions = list(reduction_mapping)\n\n\ndef binarization(array: Union[List, np.ndarray]) -> np.ndarray:\n    # Takes a binary-class datafile and turn the max value (positive class)\n    # into 1 and the min into 0\n    array = np.array(array, dtype=float)  # conversion needed to use np.inf\n    if len(np.unique(array)) > 2:\n        raise ValueError(\n            \"The argument must be a binary-class datafile. \"\n            \"{} classes detected\".format(len(np.unique(array)))\n        )\n\n    # manipulation which aims at avoid error in data\n    # with for example classes '1' and '2'.\n    array[array == np.amax(array)] = np.inf\n    array[array == np.amin(array)] = 0\n    array[array == np.inf] = 1\n    return np.array(array, dtype=int)\n\n\ndef multilabel_to_multiclass(array: Union[List, np.ndarray]) -> np.ndarray:\n    array = binarization(array)\n    return np.array([np.nonzero(array[i, :])[0][0] for i in range(len(array))])\n\n\ndef convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Convert binary targets to numeric vector\n    typically classification target values\n    :param Ybin:\n    :return:\n    \"\"\"\n    result = np.array(Ybin)\n    if len(Ybin.shape) != 1:\n        result = np.dot(Ybin, range(Ybin.shape[1]))\n    return result\n\n\ndef convert_to_bin(Ycont: List, nval: int, verbose: bool = True) -> List:\n    # Convert numeric vector to binary (typically classification target values)\n    if verbose:\n        pass\n    Ybin = [[0] * nval for _ in range(len(Ycont))]\n    for i in range(len(Ybin)):\n        line = Ybin[i]\n        line[int(Ycont[i])] = 1\n        Ybin[i] = line\n    return Ybin\n\n\ndef predict_RAM_usage(X: np.ndarray, categorical: List[bool]) -> float:\n    # Return estimated RAM usage of dataset after OneHotEncoding in bytes.\n    estimated_columns = 0\n    for i, cat in enumerate(categorical):\n        if cat:\n            unique_values = np.unique(X[:, i])\n            num_unique_values = np.sum(np.isfinite(unique_values))\n            estimated_columns += num_unique_values\n        else:\n            estimated_columns += 1\n    estimated_ram = estimated_columns * X.shape[0] * X.dtype.itemsize\n    return estimated_ram\n\n\ndef subsample(\n    X: Union[np.ndarray, spmatrix],\n    y: np.ndarray,\n    is_classification: bool,\n    sample_size: Union[float, int],\n    random_state: Optional[Union[int, np.random.RandomState]] = None,\n) -> Tuple[Union[np.ndarray, spmatrix], np.ndarray]:\n    \"\"\"Subsamples data returning the same type as it recieved.\n\n    If `is_classification`, we split using a stratified shuffle split which\n    preserves unique labels in the training set.\n\n    NOTE:\n    It's highly unadvisable to use lists here. In order to preserver types,\n    we convert to a numpy array and then back to a list.\n\n    NOTE2:\n    Interestingly enough, StratifiedShuffleSplut and descendants don't support\n    sparse `y` in `split(): _check_array` call. Hence, neither do we.\n\n    Parameters\n    ----------\n    X: Union[np.ndarray, spmatrix]\n        The X's to subsample\n\n    y: np.ndarray\n        The Y's to subsample\n\n    is_classification: bool\n        Whether this is classification data or regression data. Required for\n        knowing how to split.\n\n    sample_size: float | int\n        If float, percentage of data to take otherwise if int, an absolute\n        count of samples to take.\n\n    random_state: int | RandomState = None\n        The random state to pass to the splitted\n\n    Returns\n    -------\n    (np.ndarray | spmatrix, np.ndarray)\n        The X and y subsampled according to sample_size\n    \"\"\"\n    if isinstance(X, List):\n        X = np.asarray(X)\n\n    if isinstance(y, List):\n        y = np.asarray(y)\n\n    if is_classification:\n        splitter = CustomStratifiedShuffleSplit(\n            train_size=sample_size, random_state=random_state\n        )\n        left_idxs, _ = next(splitter.split(X=X, y=y))\n\n        # This function supports pandas objects but they won't get here\n        # yet as we do not reduce the size of pandas dataframes.\n        if isinstance(X, pd.DataFrame):\n            idxs = X.index[left_idxs]\n            X = X.loc[idxs]\n        else:\n            X = X[left_idxs]\n\n        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            idxs = y.index[left_idxs]\n            y = y.loc[idxs]\n        else:\n            y = y[left_idxs]\n\n    else:\n        X, _, y, _ = train_test_split(  # type: ignore\n            X,\n            y,\n            train_size=sample_size,\n            random_state=random_state,\n        )\n\n    return X, y\n\n\ndef reduce_precision(\n    X: Union[np.ndarray, spmatrix]\n) -> Tuple[Union[np.ndarray, spmatrix], Type]:\n    \"\"\"Reduces the precision of a np.ndarray or spmatrix containing floats\n\n    Parameters\n    ----------\n    X:  np.ndarray | spmatrix\n        The data to reduce precision of.\n\n    Returns\n    -------\n    (ndarray | spmatrix, dtype)\n        Returns the reduced data X along with the dtype it was reduced to.\n    \"\"\"\n    if X.dtype not in supported_precision_reductions:\n        raise ValueError(\n            f\"X.dtype = {X.dtype} not equal to any supported\"\n            f\" {supported_precision_reductions}\"\n        )\n\n    precision = reduction_mapping[X.dtype]\n    return X.astype(precision), precision\n\n\ndef reduce_dataset_size_if_too_large(\n    X: Union[np.ndarray, spmatrix],\n    y: np.ndarray,\n    memory_limit: int,\n    is_classification: bool,\n    random_state: Union[int, np.random.RandomState] = None,\n    operations: List[str] = [\"precision\", \"subsample\"],\n    memory_allocation: Union[int, float] = 0.1,\n) -> Tuple[Union[np.ndarray, spmatrix], np.ndarray]:\n    f\"\"\"Reduces the size of the dataset if it's too close to the memory limit.\n\n    Follows the order of the operations passed in and retains the type of its\n    input.\n\n    Precision reduction will only work on the following float types:\n    -   {supported_precision_reductions}\n\n    Subsampling will ensure that the memory limit is satisfied while precision reduction\n    will only perform one level of precision reduction. Technically, you could supply\n    multiple rounds of precision reduction, i.e. to reduce np.float128 to np.float32\n    you could use `operations = ['precision'] * 2`.\n\n    However, if that's the use case, it'd be advised to simply use the function\n    `autosklearn.util.data.reduce_precision`.\n\n    NOTE: limitations\n    * Does not support dataframes yet\n        -   Requires implementing column wise precision reduction\n        -   Requires calculating memory usage\n\n    Parameters\n    ----------\n    X: np.ndarray | spmatrix\n        The features of the dataset.\n\n    y: np.ndarray\n        The labels of the dataset.\n\n    memory_limit: int\n        The amount of memory allocated in megabytes\n\n    is_classification: bool\n        Whether it's a classificaiton dataset or not. This is important when\n        considering how to subsample.\n\n    random_state: int | RandomState = None\n        The random_state to use for subsampling.\n\n    operations: List[str] = ['precision', 'subsampling']\n        A list of operations that are permitted to be performed to reduce\n        the size of the dataset.\n\n        **precision**\n\n        Reduce the precision of float types\n\n        **subsample**\n\n        Reduce the amount of samples of the dataset such that it fits into the allocated\n        memory. Ensures stratification and that unique labels are present\n\n    memory_allocation: Union[int, float] = 0.1\n        The amount of memory to allocate to the dataset. A float specifys that the\n        dataset will try to be fit into that percentage of memory. An int specifies an\n        absolute amount.\n\n    Returns\n    -------\n    (spmatrix | np.ndarray, np.ndarray)\n        The reduced X, y if reductions were needed\n    \"\"\"\n    # Validation\n    assert memory_limit > 0\n\n    if isinstance(memory_allocation, float):\n        if not (0.0 < memory_allocation < 1.0):\n            raise ValueError(\"memory_allocation if float must be in (0, 1)\")\n\n        allocated_memory = memory_limit * memory_allocation\n\n    elif isinstance(memory_allocation, int):\n        if not (0 < memory_allocation < memory_limit):\n            raise ValueError(\"memory_allocation if int must be in (0, memory_limit)\")\n\n        allocated_memory = memory_allocation\n\n    else:\n        raise ValueError(\n            f\"Unknown type for `memory_allocation` {type(memory_allocation)}\"\n        )\n\n    if \"precision\" in operations and X.dtype not in supported_precision_reductions:\n        raise ValueError(f\"Unsupported type `{X.dtype}` for precision reduction\")\n\n    def megabytes(arr: Union[np.ndarray, spmatrix]) -> float:\n        return (arr.nbytes if isinstance(X, np.ndarray) else arr.data.nbytes) / (\n            2**20\n        )\n\n    for operation in operations:\n\n        if operation == \"precision\":\n            # If the dataset is too big for the allocated memory,\n            # we then try to reduce the precision if it's a high precision dataset\n            if megabytes(X) > allocated_memory:\n                X, precision = reduce_precision(X)\n                warnings.warn(\n                    f\"Dataset too large for allocated memory {allocated_memory}MB, \"\n                    f\"reduced the precision from {X.dtype} to {precision}\",\n                )\n\n        elif operation == \"subsample\":\n            # If the dataset is still too big such that we couldn't fit\n            # into the allocated memory, we subsample it so that it does\n            if megabytes(X) > allocated_memory:\n\n                n_samples_before = X.shape[0]\n                sample_percentage = allocated_memory / megabytes(X)\n\n                # NOTE: type ignore\n                #\n                # Tried the generic `def subsample(X: T) -> T` approach but it was\n                # failing elsewhere, keeping it simple for now\n                X, y = subsample(  # type: ignore\n                    X,\n                    y,\n                    sample_size=sample_percentage,\n                    is_classification=is_classification,\n                    random_state=random_state,\n                )\n\n                n_samples_after = X.shape[0]\n                warnings.warn(\n                    f\"Dataset too large for allocated memory {allocated_memory}MB,\"\n                    f\" reduced number of samples from {n_samples_before} to\"\n                    f\" {n_samples_after}.\"\n                )\n\n        else:\n            raise ValueError(f\"Unknown operation `{operation}`\")\n\n    return X, y\n\n\n### Source File Content:\nfrom typing import Dict, List, Optional, Tuple, Union, cast\n\nimport logging\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.api.types import is_numeric_dtype, is_sparse\nfrom scipy.sparse import csr_matrix, spmatrix\nfrom sklearn.base import BaseEstimator\nfrom sklearn.exceptions import NotFittedError\n\nfrom autosklearn.util.logging_ import PickableLoggerAdapter\n\nSUPPORTED_FEAT_TYPES = Union[List, pd.DataFrame, np.ndarray, spmatrix]\n\n\nclass FeatureValidator(BaseEstimator):\n    \"\"\"\n    Checks the input data to Auto-Sklearn.\n\n    It also determines what columns are categorical and which ones are numerical,\n    so that the pre-processing pipeline can process this columns accordingly.\n\n    Attributes\n    ----------\n    feat_type: Optional[List[str]] = None\n        In case the dataset is not a pandas DataFrame:\n            +   If provided, this list indicates which columns should be treated as\n                categorical it is internally transformed into a dictionary that\n                indicates a mapping from column index to categorical/numerical.\n            +   If not provided, by default all columns are treated as numerical\n\n        If the input dataset is of type pandas dataframe, this argument\n        must be none, as the column type will be inferred from the pandas dtypes.\n\n    data_type:\n        Class name of the data type provided during fit.\n    \"\"\"\n\n    def __init__(\n        self,\n        feat_type: Optional[List[str]] = None,\n        logger: Optional[PickableLoggerAdapter] = None,\n        allow_string_features: bool = True,\n    ) -> None:\n        # If a dataframe was provided, we populate\n        # this attribute with a mapping from column to\n        # {numerical | categorical | string}\n        self.feat_type: Optional[Dict[Union[str, int], str]] = None\n        if feat_type is not None:\n            if isinstance(feat_type, dict):\n                self.feat_type = feat_type\n            elif not isinstance(feat_type, List):\n                raise ValueError(\n                    \"Auto-Sklearn expects a list of categorical/\"\n                    \"numerical/string feature types, yet a\"\n                    \" {} was provided\".format(type(feat_type))\n                )\n            else:\n\n                # Convert to a dictionary which will be passed to the ColumnTransformer\n                # Column Transformer supports strings or integer indexes\n                self.feat_type = {i: feat for i, feat in enumerate(feat_type)}\n\n        # Register types to detect unsupported data format changes\n        self.data_type = None  # type: Optional[type]\n        self.dtypes = {}  # type: Dict[str, str]\n\n        self.logger = logger if logger is not None else logging.getLogger(__name__)\n\n        self._is_fitted = False\n        self.allow_string_features = allow_string_features\n\n    def fit(\n        self,\n        X_train: SUPPORTED_FEAT_TYPES,\n        X_test: Optional[SUPPORTED_FEAT_TYPES] = None,\n    ) -> \"FeatureValidator\":\n        \"\"\"\n        Validates input data to Auto-Sklearn.\n        The supported data types are List, numpy arrays and pandas DataFrames.\n        CSR sparse data types are also supported\n\n        Parameters\n        ----------\n        X_train: SUPPORTED_FEAT_TYPES\n            A set of features that are going to be validated (type and dimensionality\n            checks) and a encoder fitted in the case the data needs encoding\n        X_test: Optional[SUPPORTED_FEAT_TYPES]\n            A hold out set of data used for checking\n        \"\"\"\n        # If a list was provided, it will be converted to pandas\n        if isinstance(X_train, List):\n            X_train, X_test = self.list_to_dataframe(X_train, X_test)\n\n        self._check_data(X_train)\n\n        # Handle categorical feature identification for the pipeline\n        if hasattr(X_train, \"iloc\"):\n            if self.feat_type is not None:\n                raise ValueError(\n                    \"When providing a DataFrame to Auto-Sklearn, we extract \"\n                    \"the feature types from the DataFrame.dtypes. That is, \"\n                    \"providing the option feat_type to the fit method is not \"\n                    \"supported when using a Dataframe. Please make sure that the \"\n                    \"type of each column in your DataFrame is properly set. \"\n                    \"More details about having the correct data type in your \"\n                    \"DataFrame can be seen in \"\n                    \"https://pandas.pydata.org/pandas-docs/stable/reference\"\n                    \"/api/pandas.DataFrame.astype.html\"\n                )\n            else:\n                self.feat_type = self.get_feat_type_from_columns(X_train)\n        else:\n            # Numpy array was provided\n            if self.feat_type is None:\n                # Assume numerical columns if a numpy array has no feature types\n                self.feat_type = {i: \"numerical\" for i in range(np.shape(X_train)[1])}\n            else:\n                # Check The feat type provided\n                if len(self.feat_type) != np.shape(X_train)[1]:\n                    raise ValueError(\n                        \"Array feat_type does not have same number of \"\n                        \"variables as X has features. %d vs %d.\"\n                        % (len(self.feat_type), np.shape(X_train)[1])\n                    )\n                if not all([isinstance(f, str) for f in self.feat_type.values()]):\n                    raise ValueError(\n                        \"feat_type must only contain strings: {}\".format(\n                            list(self.feat_type.values()),\n                        )\n                    )\n\n                for ft in self.feat_type.values():\n                    if ft.lower() not in [\"categorical\", \"numerical\", \"string\"]:\n                        raise ValueError(\n                            \"Only `Categorical`, `Numerical` and `String` are \"\n                            \"valid feature types\"\n                        )\n\n        if X_test is not None:\n            self._check_data(X_test)\n\n            if np.shape(X_train)[1] != np.shape(X_test)[1]:\n                raise ValueError(\n                    \"The feature dimensionality of the train and test \"\n                    \"data does not match train({}) != test({})\".format(\n                        np.shape(X_train)[1], np.shape(X_test)[1]\n                    )\n                )\n\n        self._is_fitted = True\n\n        return self\n\n    def transform(\n        self,\n        X: SUPPORTED_FEAT_TYPES,\n    ) -> Union[np.ndarray, spmatrix, pd.DataFrame]:\n        \"\"\"\n        Validates and fit a categorical encoder (if needed) to the features.\n        The supported data types are List, numpy arrays and pandas DataFrames.\n\n        Parameters\n        ----------\n        X_train: SUPPORTED_FEAT_TYPES\n            A set of features, whose categorical features are going to be\n            transformed\n\n        Return\n        ------\n        np.ndarray | spmatrix | pd.DataFrame:\n            The transformed array\n        \"\"\"\n        if not self._is_fitted:\n            raise NotFittedError(\n                \"Cannot call transform on a validator that is not fitted\"\n            )\n\n        # If a list was provided, it will be converted to pandas\n        if isinstance(X, List):\n            X_transformed, _ = self.list_to_dataframe(X)\n        else:\n            X_transformed = X\n\n        # Check the data here so we catch problems on new test data\n        self._check_data(X_transformed)\n\n        # Sparse related transformations\n        # Not all sparse format support index sorting\n        if isinstance(X_transformed, spmatrix):\n            if not isinstance(X_transformed, csr_matrix):\n                self.logger.warning(\n                    f\"Sparse data provided is of type {type(X_transformed)} \"\n                    \"yet Auto-Sklearn only support csr_matrix. Auto-sklearn \"\n                    \"will convert the provided data to the csr_matrix format.\"\n                )\n                X_transformed = X_transformed.tocsr(copy=False)\n\n            X_transformed.sort_indices()\n\n        return X_transformed\n\n    def _check_data(\n        self,\n        X: SUPPORTED_FEAT_TYPES,\n    ) -> None:\n        \"\"\"\n        Feature dimensionality and data type checks\n\n        Parameters\n        ----------\n        X: SUPPORTED_FEAT_TYPES\n            A set of features that are going to be validated (type and dimensionality)\n            and a encoder fitted in the case the data needs encoding\n        \"\"\"\n        # We consider columns that are all nan in a pandas frame as category\n        if hasattr(X, \"columns\"):\n            for column in cast(pd.DataFrame, X).columns:\n                if X[column].isna().all():\n                    X[column] = X[column].astype(\"category\")\n\n        if not isinstance(X, (np.ndarray, pd.DataFrame)) and not isinstance(\n            X, spmatrix\n        ):\n            raise ValueError(\n                \"Auto-sklearn only supports Numpy arrays, Pandas DataFrames,\"\n                \" scipy sparse and Python Lists, yet, the provided input is\"\n                \" of type {}\".format(type(X))\n            )\n\n        if self.data_type is None:\n            self.data_type = type(X)\n\n        if self.data_type != type(X):\n            self.logger.warning(\n                f\"Auto-sklearn previously received features of type {self.data_type} \"\n                f\"yet the current features have type {type(X)}. Changing the dtype \"\n                \"of inputs to an estimator might cause problems\"\n            )\n\n        # Do not support category/string numpy data. Only numbers\n        if hasattr(X, \"dtype\"):\n            if not np.issubdtype(X.dtype.type, np.number):  # type: ignore[union-attr]\n                raise ValueError(\n                    \"When providing a numpy array to Auto-sklearn, the only valid\"\n                    f\" dtypes are numerical ones. The provided data type {X.dtype.type}\"\n                    \" is not supported.\"\n                )\n\n        # Then for Pandas, we do not support Nan in categorical columns\n        if hasattr(X, \"iloc\"):\n            # If entered here, we have a pandas dataframe\n            X = cast(pd.DataFrame, X)\n\n            dtypes = {col: X[col].dtype.name.lower() for col in X.columns}\n            if len(self.dtypes) > 0:\n                if self.dtypes != dtypes:\n                    # To support list, we need to support object inference.\n                    # In extreme cases, the train column might be all integer,\n                    # and the test column might be float.\n                    self.logger.warning(\n                        \"Changing the dtype of the features after fit() is \"\n                        \"not recommended. Fit() method was called with \"\n                        \"{} whereas the new features have {} as type\".format(\n                            self.dtypes,\n                            dtypes,\n                        )\n                    )\n            else:\n                self.dtypes = dtypes\n\n    def get_feat_type_from_columns(\n        self,\n        X: pd.DataFrame,\n    ) -> Dict[Union[str, int], str]:\n        \"\"\"\n        Returns a dictionary that maps pandas dataframe columns to a feature type.\n        This feature type can be categorical, numerical or string\n\n        Parameters\n        ----------\n        X: pd.DataFrame\n            A set of features that are going to be validated (type and dimensionality\n            checks) and a encoder fitted in the case the data needs encoding\n\n        Returns\n        -------\n        feat_type:\n            dictionary with column to feature type mapping\n        \"\"\"\n        # Also, register the feature types for the estimator\n        feat_type = {}\n\n        # Make sure each column is a valid type\n        for i, column in enumerate(X.columns):\n            if is_sparse(X[column]):\n                raise ValueError(\n                    \"Auto-sklearn does not yet support sparse pandas Series.\"\n                    f\" Please convert {column} to a dense format.\"\n                )\n            elif X[column].dtype.name in [\"category\", \"bool\"]:\n                feat_type[column] = \"categorical\"\n            elif X[column].dtype.name == \"string\":\n                if self.allow_string_features:\n                    feat_type[column] = \"string\"\n                else:\n                    feat_type[column] = \"categorical\"\n                    warnings.warn(\n                        f\"you disabled text encoding column {column} will be \"\n                        f\"encoded as category\"\n                    )\n            # Move away from np.issubdtype as it causes\n            # TypeError: data type not understood in certain pandas types\n            elif not is_numeric_dtype(X[column]):\n                if X[column].dtype.name == \"object\":\n                    warnings.warn(\n                        f\"Input Column {column} has generic type object. \"\n                        f\"Autosklearn will treat this column as string. \"\n                        f\"Please ensure that this setting is suitable for your task.\",\n                        UserWarning,\n                    )\n                    if self.allow_string_features:\n                        feat_type[column] = \"string\"\n                    else:\n                        feat_type[column] = \"categorical\"\n                        warnings.warn(\n                            f\"you disabled text encoding column {column} will be\"\n                            f\"encoded as category\"\n                        )\n                elif pd.core.dtypes.common.is_datetime_or_timedelta_dtype(\n                    X[column].dtype\n                ):\n                    raise ValueError(\n                        \"Auto-sklearn does not support time and/or date datatype as \"\n                        f\"given in column {column}. Please convert the time \"\n                        \" information to a numerical value first. One example on how to\"\n                        \" do this can be found on \"\n                        \" https://stats.stackexchange.com/questions/311494/\"\n                    )\n                else:\n                    raise ValueError(\n                        \"Input Column {} has unsupported dtype {}. \"\n                        \"Supported column types are categorical/bool/numerical dtypes. \"\n                        \"Make sure your data is formatted in a correct way, \"\n                        \"before feeding it to Auto-Sklearn.\".format(\n                            column,\n                            X[column].dtype.name,\n                        )\n                    )\n            else:\n                feat_type[column] = \"numerical\"\n        return feat_type\n\n    def list_to_dataframe(\n        self,\n        X_train: SUPPORTED_FEAT_TYPES,\n        X_test: Optional[SUPPORTED_FEAT_TYPES] = None,\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n        \"\"\"\n        Converts a list to a DataFrame. In this process, column types are inferred.\n\n        If test data is provided, we proactively match it to train data\n\n        Parameters\n        ----------\n        X_train: SUPPORTED_FEAT_TYPES\n            A set of features that are going to be validated (type and dimensionality\n            checks) and a encoder fitted in the case the data needs encoding\n\n        X_test: Optional[SUPPORTED_FEAT_TYPES]\n            A hold out set of data used for checking\n\n        Returns\n        -------\n        Union[pd.DataFrame, pd.DataFrame]:\n            transformed (train, test) data from list to pandas DataFrame\n        \"\"\"\n        # If a list was provided, it will be converted to pandas\n        X_train = pd.DataFrame(data=X_train).convert_dtypes()\n\n        # Store the dtypes and use in case of re-fit\n        if len(self.dtypes) == 0:\n            self.dtypes = {\n                col: X_train[col].dtype.name.lower() for col in X_train.columns\n            }\n        else:\n            for col in X_train.columns:\n                # Try to convert to the original dtype used to fit the validator\n                # But also be robust to extreme cases (for example, the train data for a\n                # column was all np.int-like and the test data is np.float-type)\n                try:\n                    X_train[col] = X_train[col].astype(self.dtypes[col])\n                except Exception as e:\n                    self.logger.warning(\n                        f\"Failed to format column {col} as {self.dtypes[col]}: {e}\"\n                    )\n                    self.dtypes[col] = X_train[col].dtype.name.lower()\n\n        self.logger.warning(\n            \"The provided feature types to autosklearn are of type list.\"\n            \"Features have been interpreted as: {}\".format(\n                [(col, t) for col, t in zip(X_train.columns, X_train.dtypes)]\n            )\n        )\n        if X_test is not None:\n            if not isinstance(X_test, List):\n                self.logger.warning(\n                    \"Train features are a list while the provided test data\"\n                    \"is {}. X_test will be casted as DataFrame.\".format(type(X_test))\n                )\n            X_test = pd.DataFrame(data=X_test)\n            for col in X_test.columns:\n                try:\n                    X_test[col] = X_test[col].astype(self.dtypes[col])\n                except Exception as e:\n                    self.logger.warning(\n                        f\"Failed to format column {col} as {self.dtypes[col]}: {e}\"\n                    )\n                    self.dtypes[col] = X_test[col].dtype.name.lower()\n\n        return X_train, X_test\n\n### Source File Dependency Files Content\n### Dependency File: logging_.py\n\"\"\"\nFor accessing a logger, please default to using\n`get_named_client_logger(name, host, port)`\n\"\"\"\nfrom typing import Any, Dict, Iterator, Optional, TextIO, Type, cast\n\nimport logging\nimport logging.config\nimport logging.handlers\nimport multiprocessing\nimport os\nimport pickle\nimport random\nimport select\nimport socketserver\nimport struct\nimport threading\nimport warnings\nfrom contextlib import contextmanager\n\nimport yaml\n\n\ndef setup_logger(\n    output_dir: str,\n    filename: Optional[str] = None,\n    distributedlog_filename: Optional[str] = None,\n    logging_config: Optional[Dict] = None,\n) -> None:\n    # logging_config must be a dictionary object specifying the configuration\n    # for the loggers to be used in auto-sklearn.\n    if logging_config is None:\n        with open(os.path.join(os.path.dirname(__file__), \"logging.yaml\"), \"r\") as fh:\n            logging_config = yaml.safe_load(fh)\n\n    # Make sure we have a filename handler\n    if \"handlers\" not in logging_config:\n        logging_config[\"handlers\"] = {}\n    if \"file_handler\" not in logging_config[\"handlers\"]:\n        logging_config[\"handlers\"][\"file_handler\"] = {\n            \"class\": \"logging.FileHandler\",\n            \"level\": \"DEBUG\",\n            \"filename\": \"autosklearn.log\",\n        }\n    if \"distributed_logfile\" not in logging_config[\"handlers\"]:\n        # We have to create a file handler\n        logging_config[\"handlers\"][\"distributed_logfile\"] = {\n            \"class\": \"logging.FileHandler\",\n            \"level\": \"DEBUG\",\n            \"filename\": \"distributed.log\",\n        }\n\n    if filename is None:\n        filename = logging_config[\"handlers\"][\"file_handler\"][\"filename\"]\n\n    if distributedlog_filename is None:\n        distributedlog_filename = logging_config[\"handlers\"][\"distributed_logfile\"][\n            \"filename\"\n        ]\n\n    # Make path absolute only if required\n    # This is needed because this function might be called multiple times with the same\n    # dict, and we don't want /path/path/<name>.log but rather just /path/<name>.log\n    if os.path.sep not in logging_config[\"handlers\"][\"file_handler\"][\"filename\"]:\n        logging_config[\"handlers\"][\"file_handler\"][\"filename\"] = os.path.join(\n            output_dir, filename\n        )\n    if os.path.sep not in logging_config[\"handlers\"][\"distributed_logfile\"][\"filename\"]:\n        logging_config[\"handlers\"][\"distributed_logfile\"][\"filename\"] = os.path.join(\n            output_dir, distributedlog_filename\n        )\n    logging.config.dictConfig(logging_config)\n\n\ndef _create_logger(name: str) -> logging.Logger:\n    return logging.getLogger(name)\n\n\ndef get_logger(name: str) -> \"PickableLoggerAdapter\":\n    logger = PickableLoggerAdapter(name)\n    return logger\n\n\nclass PickableLoggerAdapter:\n    def __init__(self, name: str):\n        self.name = name\n        self.logger = _create_logger(name)\n\n    def __getstate__(self) -> Dict[str, Any]:\n        \"\"\"\n        Method is called when pickle dumps an object.\n\n        Returns\n        -------\n        Dictionary, representing the object state to be pickled. Ignores\n        the self.logger field and only returns the logger name.\n        \"\"\"\n        return {\"name\": self.name}\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        \"\"\"\n        Method is called when pickle loads an object. Retrieves the name and\n        creates a logger.\n\n        Parameters\n        ----------\n        state - dictionary, containing the logger name.\n\n        \"\"\"\n        self.name = state[\"name\"]\n        self.logger = _create_logger(self.name)\n\n    def debug(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.debug(msg, *args, **kwargs)\n\n    def info(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.info(msg, *args, **kwargs)\n\n    def warning(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.warning(msg, *args, **kwargs)\n\n    def error(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.error(msg, *args, **kwargs)\n\n    def exception(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.exception(msg, *args, **kwargs)\n\n    def critical(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.critical(msg, *args, **kwargs)\n\n    def log(self, level: int, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level: int) -> bool:\n        return self.logger.isEnabledFor(level)\n\n\ndef get_named_client_logger(\n    name: str,\n    host: str = \"localhost\",\n    port: int = logging.handlers.DEFAULT_TCP_LOGGING_PORT,\n) -> \"PicklableClientLogger\":\n    logger = PicklableClientLogger(name=name, host=host, port=port)\n    return logger\n\n\ndef _get_named_client_logger(\n    name: str,\n    host: str = \"localhost\",\n    port: int = logging.handlers.DEFAULT_TCP_LOGGING_PORT,\n) -> logging.Logger:\n    \"\"\"\n    When working with a logging server, clients are expected to create a logger using\n    this method. For example, the automl object will create a master that awaits\n    for records sent through tcp to localhost.\n\n    Ensemble builder will then instantiate a logger object that will submit records\n    via a socket handler to the server.\n\n    We do not need to use any format as the server will render the msg as it\n    needs to.\n\n    Parameters\n    ----------\n        name: (str)\n            the name of the logger, used to tag the messages in the main log\n        host: (str)\n            Address of where the server is gonna look for messages\n        port: (str)\n            Port used to communicate with the server\n\n    Returns\n    -------\n        local_loger: a logger object that has a socket handler\n    \"\"\"\n    # Setup the logger configuration\n    # We add client not only to identify that this is the client\n    # communication part of the logger, but to make sure we have\n    # a new singleton with the desired socket handlers\n    local_logger = _create_logger(\"Client-\" + name)\n    local_logger.propagate = False\n    local_logger.setLevel(logging.DEBUG)\n\n    try:\n        # Ignore mypy logging.handlers.SocketHandler has no attribute port\n        # This is not the case clearly, yet MyPy assumes this is not the case\n        # Even when using direct casting or getattr\n        ports = [\n            getattr(handler, \"port\", None) for handler in local_logger.handlers\n        ]  # type: ignore[attr-defined]\n    except AttributeError:\n        # We do not want to log twice but adding multiple times the same\n        # handler. So we check to what ports we communicate to\n        # We can prevent errors with streamers not having a port with this try\n        # block -- but it is a scenario that is unlikely to happen\n        ports = []\n\n    if port not in ports:\n        socketHandler = logging.handlers.SocketHandler(host, port)\n        local_logger.addHandler(socketHandler)\n\n    return local_logger\n\n\nclass PicklableClientLogger(PickableLoggerAdapter):\n    def __init__(self, name: str, host: str, port: int):\n        self.name = name\n        self.host = host\n        self.port = port\n        self.logger = _get_named_client_logger(name=name, host=host, port=port)\n\n    def __getstate__(self) -> Dict[str, Any]:\n        \"\"\"\n        Method is called when pickle dumps an object.\n\n        Returns\n        -------\n        Dictionary, representing the object state to be pickled. Ignores\n        the self.logger field and only returns the logger name.\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"host\": self.host,\n            \"port\": self.port,\n        }\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        \"\"\"\n        Method is called when pickle loads an object. Retrieves the name and\n        creates a logger.\n\n        Parameters\n        ----------\n        state - dictionary, containing the logger name.\n\n        \"\"\"\n        self.name = state[\"name\"]\n        self.host = state[\"host\"]\n        self.port = state[\"port\"]\n        self.logger = _get_named_client_logger(\n            name=self.name,\n            host=self.host,\n            port=self.port,\n        )\n\n\nclass LogRecordStreamHandler(socketserver.StreamRequestHandler):\n    \"\"\"Handler for a streaming logging request.\n\n    This basically logs the record using whatever logging policy is\n    configured locally.\n    \"\"\"\n\n    def handle(self) -> None:\n        \"\"\"\n        Handle multiple requests - each expected to be a 4-byte length,\n        followed by the LogRecord in pickle format. Logs the record\n        according to whatever policy is configured locally.\n        \"\"\"\n        while True:\n            chunk = self.connection.recv(4)  # type: ignore[attr-defined]\n            if len(chunk) < 4:\n                break\n            slen = struct.unpack(\">L\", chunk)[0]\n            chunk = self.connection.recv(slen)  # type: ignore[attr-defined]\n            while len(chunk) < slen:\n                chunk = chunk + self.connection.recv(slen - len(chunk))  # type: ignore[attr-defined]  # noqa: E501\n            obj = self.unPickle(chunk)\n            record = logging.makeLogRecord(obj)\n            self.handleLogRecord(record)\n\n    def unPickle(self, data: Any) -> Any:\n        return pickle.loads(data)\n\n    def handleLogRecord(self, record: logging.LogRecord) -> None:\n        # logname is define in LogRecordSocketReceiver\n        # Yet Mypy Cannot see this. This is needed so that we can\n        # re-use the logging setup for autosklearn into the recieved\n        # records\n        if self.server.logname is not None:  # type: ignore  # noqa\n            name = self.server.logname  # type: ignore  # noqa\n        else:\n            name = record.name\n        logger = logging.getLogger(name)\n        # N.B. EVERY record gets logged. This is because Logger.handle\n        # is normally called AFTER logger-level filtering. If you want\n        # to do filtering, do it at the client end to save wasting\n        # cycles and network bandwidth!\n        logger.handle(record)\n\n\ndef start_log_server(\n    host: str,\n    logname: str,\n    event: threading.Event,\n    port: multiprocessing.Value,\n    filename: str,\n    logging_config: Dict,\n    output_dir: str,\n) -> None:\n    setup_logger(\n        filename=filename, logging_config=logging_config, output_dir=output_dir\n    )\n\n    while True:\n        # Loop until we find a valid port\n        _port = random.randint(10000, 65535)\n        try:\n            receiver = LogRecordSocketReceiver(\n                host=host,\n                port=_port,\n                logname=logname,\n                event=event,\n            )\n            with port.get_lock():\n                port.value = _port\n            receiver.serve_until_stopped()\n            break\n        except OSError:\n            continue\n\n\nclass LogRecordSocketReceiver(socketserver.ThreadingTCPServer):\n    \"\"\"\n    This class implement a entity that receives tcp messages on a given address\n    For further information, please check\n    https://docs.python.org/3/howto/logging-cookbook.html#configuration-server-example\n    \"\"\"\n\n    allow_reuse_address = True\n\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = logging.handlers.DEFAULT_TCP_LOGGING_PORT,\n        handler: Type[LogRecordStreamHandler] = LogRecordStreamHandler,\n        logname: Optional[str] = None,\n        event: threading.Event = None,\n    ):\n        socketserver.ThreadingTCPServer.__init__(self, (host, port), handler)\n        self.timeout = 1\n        self.logname = logname\n        self.event = event\n\n    def serve_until_stopped(self) -> None:\n        while True:\n            rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout)\n            if rd:\n                self.handle_request()\n            if self.event is not None and self.event.is_set():\n                break\n\n\n@contextmanager\ndef warnings_to(logger: Optional[PicklableClientLogger] = None) -> Iterator[None]:\n    \"\"\"A context manager to catch warnings and send them to the logger\n\n    If no logger is passed, warnings propogate as they normally would.\n\n    Parameters\n    ----------\n    logger: Optional[PicklableClientLogger] = None\n        The logger to send the warnings to\n    \"\"\"\n    # Send to a logger if one exists\n    if logger:\n\n        with warnings.catch_warnings():\n\n            def to_log(\n                logger: PicklableClientLogger,\n                message: str,\n                category: Type[Warning],  # Weird but that's what it is\n                filename: str,\n                lineno: int,\n                file: Optional[TextIO] = None,\n                line: Optional[str] = None,\n            ) -> None:\n                logger.warning(f\"{filename}:{lineno} {category.__name__}:{message}\")\n\n            # Mypy was complaining that logger didn't exist in `to_log` see here:\n            # https://mypy.readthedocs.io/en/stable/common_issues.html#narrowing-and-inner-functions\n            # we explicitly pass it in and have to force it's type with `cast`\n            warnings.showwarning = lambda *args: to_log(\n                cast(PicklableClientLogger, logger), *args\n            )\n\n            yield\n    # Else do nothing, warnings go to wherever they would without this context\n    else:\n        yield\n\n\n### Dependency File: splitter.py\nimport warnings\n\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.model_selection._split import _validate_shuffle_split\nfrom sklearn.utils import _approximate_mode, check_random_state, indexable\nfrom sklearn.utils.multiclass import type_of_target\nfrom sklearn.utils.validation import _num_samples, check_array, column_or_1d\n\n\nclass CustomStratifiedShuffleSplit(StratifiedShuffleSplit):\n    \"\"\"Splitter that deals with classes with too few samples\"\"\"\n\n    def _iter_indices(self, X, y, groups=None):  # type: ignore\n        n_samples = _num_samples(X)\n        y = check_array(y, ensure_2d=False, dtype=None)\n        n_train, n_test = _validate_shuffle_split(\n            n_samples,\n            self.test_size,\n            self.train_size,\n            default_test_size=self._default_test_size,\n        )\n\n        if y.ndim == 2:\n            # for multi-label y, map each distinct row to a string repr\n            # using join because str(row) uses an ellipsis if len(row) > 1000\n            y = np.array([\" \".join(row.astype(\"str\")) for row in y])\n\n        classes, y_indices = np.unique(y, return_inverse=True)\n        n_classes = classes.shape[0]\n\n        class_counts = np.bincount(y_indices)\n\n        if n_train < n_classes:\n            raise ValueError(\n                \"The train_size = %d should be greater or \"\n                \"equal to the number of classes = %d\" % (n_train, n_classes)\n            )\n        if n_test < n_classes:\n            raise ValueError(\n                \"The test_size = %d should be greater or \"\n                \"equal to the number of classes = %d\" % (n_test, n_classes)\n            )\n\n        # Find the sorted list of instances for each class:\n        # (np.unique above performs a sort, so code is O(n logn) already)\n        class_indices = np.split(\n            np.argsort(y_indices, kind=\"mergesort\"), np.cumsum(class_counts)[:-1]\n        )\n\n        rng = check_random_state(self.random_state)\n\n        for _ in range(self.n_splits):\n            # if there are ties in the class-counts, we want\n            # to make sure to break them anew in each iteration\n            n_i = _approximate_mode(class_counts, n_train, rng)\n            class_counts_remaining = class_counts - n_i\n            t_i = _approximate_mode(class_counts_remaining, n_test, rng)\n            train = []\n            test = []\n\n            # NOTE: Adapting for unique instances\n            #\n            #   Each list n_i, t_i represent the list of class in the\n            #   training_set and test_set resepectively.\n            #\n            #   n_i = [100, 100, 0, 3]  # 100 of class '0', 0 of class '2'\n            #   t_i = [300, 300, 1, 3]  # 300 of class '0', 1 of class '2'\n            #\n            #  To support unique labels such as class '2', which only has one sample\n            #  between both n_i and t_i, we need to make sure that n_i has at least\n            #  one sample of all classes. There is also the extra check to ensure\n            #  that the sizes stay the same.\n            #\n            #   n_i = [ 99, 100, 1, 3]  # 100 of class '0', 0 of class '2'\n            #            |       ^\n            #            v       |\n            #   t_i = [301, 300, 0, 3]  # 300 of class '0', 1 of class '2'\n            #\n            for i, class_count in enumerate(n_i):\n                if class_count == 0:\n                    t_i[i] -= 1\n                    n_i[i] += 1\n\n                    j = np.argmax(n_i)\n                    if n_i[j] == 1:\n                        warnings.warn(\n                            \"Can't respect size requirements for split.\",\n                            \" The training set must contain all of the unique\"\n                            \" labels that exist in the dataset.\",\n                        )\n                    else:\n                        n_i[j] -= 1\n                        t_i[j] += 1\n\n            for i in range(n_classes):\n                permutation = rng.permutation(class_counts[i])\n                perm_indices_class_i = class_indices[i].take(permutation, mode=\"clip\")\n\n                train.extend(perm_indices_class_i[: n_i[i]])\n                test.extend(perm_indices_class_i[n_i[i] : n_i[i] + t_i[i]])\n\n            train = rng.permutation(train)\n            test = rng.permutation(test)\n\n            yield train, test\n\n\nclass CustomStratifiedKFold(StratifiedKFold):\n    \"\"\"Stratified K-Folds cross-validator that ensures that there is always at least\n    1 sample per class in the training set.\n    \"\"\"\n\n    def _make_test_folds(self, X, y=None):  # type: ignore\n        rng = check_random_state(self.random_state)\n        y = np.asarray(y)\n        type_of_target_y = type_of_target(y)\n        allowed_target_types = (\"binary\", \"multiclass\")\n        if type_of_target_y not in allowed_target_types:\n            raise ValueError(\n                \"Supported target types are: {}. Got {!r} instead.\".format(\n                    allowed_target_types, type_of_target_y\n                )\n            )\n\n        y = column_or_1d(y)\n\n        _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)\n        # y_inv encodes y according to lexicographic order. We invert y_idx to\n        # map the classes so that they are encoded by order of appearance:\n        # 0 represents the first label appearing in y, 1 the second, etc.\n        _, class_perm = np.unique(y_idx, return_inverse=True)\n        y_encoded = class_perm[y_inv]\n\n        n_classes = len(y_idx)\n\n        # Determine the optimal number of samples from each class in each fold,\n        # using round robin over the sorted y. (This can be done direct from\n        # counts, but that code is unreadable.)\n        y_order = np.sort(y_encoded)\n        allocation = np.asarray(\n            [\n                np.bincount(y_order[i :: self.n_splits], minlength=n_classes)\n                for i in range(self.n_splits)\n            ]\n        )\n\n        # To maintain the data order dependencies as best as possible within\n        # the stratification constraint, we assign samples from each class in\n        # blocks (and then mess that up when shuffle=True).\n        test_folds = np.empty(len(y), dtype=\"i\")\n        for k in range(n_classes):\n            # since the kth column of allocation stores the number of samples\n            # of class k in each test set, this generates blocks of fold\n            # indices corresponding to the allocation for class k.\n            folds_for_class = np.arange(self.n_splits).repeat(allocation[:, k])\n            if self.shuffle:\n                rng.shuffle(folds_for_class)\n            test_folds[y_encoded == k] = folds_for_class\n        return test_folds\n\n    def split(self, X, y=None, groups=None):  # type: ignore\n\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        if self.n_splits > n_samples:\n            raise ValueError(\n                f\"Cannot have number of splits n_splits={self.n_splits} greater\"\n                f\" than the number of samples: n_samples={n_samples}.\"\n            )\n\n        for train, test in super().split(X, y, groups):\n            all_classes = np.unique(y)\n            train_classes = np.unique(y[train])\n            train = list(train)\n            test = list(test)\n            missing_classes = set(all_classes) - set(train_classes)\n            if len(missing_classes) > 0:\n                # print(missing_classes)\n                for diff in missing_classes:\n                    # print(len(train), len(test))\n                    to_move = np.where(y[test] == diff)[0][0]\n                    # print(y[test][to_move])\n                    train = train + [test[to_move]]\n                    del test[to_move]\n                    # print(len(train), len(test))\n            train = np.array(train, dtype=int)\n            test = np.array(test, dtype=int)\n\n            yield train, test\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:43:24"}, {"role": "assistant", "content": "```python\nimport numpy as np\nimport pandas as pd\nimport sklearn.datasets\nimport sklearn.model_selection\nfrom pandas.api.types import is_categorical_dtype, is_numeric_dtype, is_string_dtype\nfrom scipy import sparse\n\nfrom autosklearn.data.feature_validator import FeatureValidator\n\nimport pytest\n\n\n# Fixtures to be used in this class. By default all elements have 100 datapoints\n@pytest.fixture\ndef input_data_featuretest(request):\n    if request.param == \"numpy_categoricalonly_nonan\":\n        return np.random.randint(10, size=(100, 10))\n    elif request.param == \"numpy_numericalonly_nonan\":\n        return np.random.uniform(10, size=(100, 10))\n    elif request.param == \"numpy_mixed_nonan\":\n        return np.column_stack(\n            [\n                np.random.uniform(10, size=(100, 3)),\n                np.random.randint(10, size=(100, 3)),\n                np.random.uniform(10, size=(100, 3)),\n                np.random.randint(10, size=(100, 1)),\n            ]\n        )\n    elif request.param == \"numpy_string_nonan\":\n        return np.array(\n            [\n                [\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"],\n                [\"a\", \"b\", \"d\", \"r\", \"b\", \"c\"],\n            ]\n        )\n    elif request.param == \"numpy_categoricalonly_nan\":\n        array = np.random.randint(10, size=(100, 10)).astype(\"float\")\n        array[50, 0:5] = np.nan\n        return array\n    elif request.param == \"numpy_numericalonly_nan\":\n        array = np.random.uniform(10, size=(100, 10)).astype(\"float\")\n        array[50, 0:5] = np.nan\n        # Somehow array is changed to dtype object after np.nan\n        return array.astype(\"float\")\n    elif request.param == \"numpy_mixed_nan\":\n        array = np.column_stack(\n            [\n                np.random.uniform(10, size=(100, 3)),\n                np.random.randint(10, size=(100, 3)),\n                np.random.uniform(10, size=(100, 3)),\n                np.random.randint(10, size=(100, 1)),\n            ]\n        )\n        array[50, 0:5] = np.nan\n        return array\n    elif request.param == \"numpy_string_nan\":\n        return np.array(\n            [\n                [\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"],\n                [np.nan, \"b\", \"d\", \"r\", \"b\", \"c\"],\n            ]\n        )\n    elif request.param == \"pandas_categoricalonly_nonan\":\n        return pd.DataFrame(\n            [\n                {\"A\": 1, \"B\": 2},\n                {\"A\": 3, \"B\": 4},\n            ],\n            dtype=\"category\",\n        )\n    elif request.param == \"pandas_numericalonly_nonan\":\n        return pd.DataFrame(\n            [\n                {\"A\": 1, \"B\": 2},\n                {\"A\": 3, \"B\": 4},\n            ],\n            dtype=\"float\",\n        )\n    elif request.param == \"pandas_mixed_nonan\":\n        frame = pd.DataFrame(\n            [\n                {\"A\": 1, \"B\": 2},\n                {\"A\": 3, \"B\": 4},\n            ],\n            dtype=\"category\",\n        )\n        frame[\"B\"] = pd.to_numeric(frame[\"B\"])\n        return frame\n    elif request.param == \"pandas_categoricalonly_nan\":\n        return pd.DataFrame(\n            [\n                {\"A\": 1, \"B\": 2, \"C\": np.nan},\n                {\"A\": 3, \"C\": np.nan},\n            ],\n            dtype=\"category\",\n        )\n    elif request.param == \"pandas_numericalonly_nan\":\n        return pd.DataFrame(\n            [\n                {\"A\": 1, \"B\": 2, \"C\": np.nan},\n                {\"A\": 3, \"C\": np.nan},\n            ],\n            dtype=\"float\",\n        )\n    elif request.param == \"pandas_mixed_nan\":\n        frame = pd.DataFrame(\n            [\n                {\"A\": 1, \"B\": 2, \"C\": 8},\n                {\"A\": 3, \"B\": 4},\n            ],\n            dtype=\"category\",\n        )\n        frame[\"B\"] = pd.to_numeric(frame[\"B\"])\n        return frame\n    elif request.param == \"pandas_string_nonan\":\n        return pd.DataFrame(\n            [\n                {\"A\": 1, \"B\": 2},\n                {\"A\": 3, \"B\": 4},\n            ],\n            dtype=\"string\",\n        )\n    elif request.param == \"list_categoricalonly_nonan\":\n        return [\n            [\"a\", \"b\", \"c\", \"d\"],\n            [\"e\", \"f\", \"c\", \"d\"],\n        ]\n    elif request.param == \"list_numericalonly_nonan\":\n        return [[1, 2, 3, 4], [5, 6, 7, 8]]\n    elif request.param == \"list_mixed_nonan\":\n        return [[\"a\", 2, 3, 4], [\"b\", 6, 7, 8]]\n    elif request.param == \"list_categoricalonly_nan\":\n        return [\n            [\"a\", \"b\", \"c\", np.nan],\n            [\"e\", \"f\", \"c\", \"d\"],\n        ]\n    elif request.param == \"list_numericalonly_nan\":\n        return [[1, 2, 3, np.nan], [5, 6, 7, 8]]\n    elif request.param == \"list_mixed_nan\":\n        return [[\"a\", np.nan, 3, 4], [\"b\", 6, 7, 8]]\n    elif \"sparse\" in request.param:\n        # We expect the names to be of the type sparse_csc_nonan\n        sparse_, type_, nan_ = request.param.split(\"_\")\n        if \"nonan\" in nan_:\n            data = np.ones(3)\n        else:\n            data = np.array([1, 2, np.nan])\n\n        # Then the type of sparse\n        row_ind = np.array([0, 1, 2])\n        col_ind = np.array([1, 2, 1])\n        if \"csc\" in type_:\n            return sparse.csc_matrix((data, (row_ind, col_ind)))\n        elif \"csr\" in type_:\n            return sparse.csr_matrix((data, (row_ind, col_ind)))\n        elif \"coo\" in type_:\n            return sparse.coo_matrix((data, (row_ind, col_ind)))\n        elif \"bsr\" in type_:\n            return sparse.bsr_matrix((data, (row_ind, col_ind)))\n        elif \"lil\" in type_:\n            return sparse.lil_matrix((data))\n        elif \"dok\" in type_:\n            return sparse.dok_matrix(np.vstack((data, data, data)))\n        elif \"dia\" in type_:\n            return sparse.dia_matrix(np.vstack((data, data, data)))\n        else:\n            ValueError(\"Unsupported indirect fixture {}\".format(request.param))\n    elif \"openml\" in request.param:\n        _, openml_id = request.param.split(\"_\")\n        X, y = sklearn.datasets.fetch_openml(\n            data_id=int(openml_id), return_X_y=True, as_frame=True\n        )\n        return X\n    else:\n        ValueError(\"Unsupported indirect fixture {}\".format(request.param))\n\n\n# Actual checks for the features\n@pytest.mark.parametrize(\n    \"input_data_featuretest\",\n    (\n        \"numpy_categoricalonly_nonan\",\n        \"numpy_numericalonly_nonan\",\n        \"numpy_mixed_nonan\",\n        \"numpy_categoricalonly_nan\",\n        \"numpy_numericalonly_nan\",\n        \"numpy_mixed_nan\",\n        \"pandas_categoricalonly_nonan\",\n        \"pandas_numericalonly_nonan\",\n        \"pandas_mixed_nonan\",\n        \"pandas_numericalonly_nan\",\n        \"list_numericalonly_nonan\",\n        \"list_numericalonly_nan\",\n        \"sparse_bsr_nonan\",\n        \"sparse_bsr_nan\",\n        \"sparse_coo_nonan\",\n        \"sparse_coo_nan\",\n        \"sparse_csc_nonan\",\n        \"sparse_csc_nan\",\n        \"sparse_csr_nonan\",\n        \"sparse_csr_nan\",\n        \"sparse_dia_nonan\",\n        \"sparse_dia_nan\",\n        \"sparse_dok_nonan\",\n        \"sparse_dok_nan\",\n        \"sparse_lil_nonan\",\n        \"sparse_lil_nan\",\n        \"openml_40981\",  # Australian\n    ),\n    indirect=True,\n)\ndef test_featurevalidator_supported_types(input_data_featuretest):\n    validator = FeatureValidator()\n    validator.fit(input_data_featuretest, input_data_featuretest)\n    transformed_X = validator.transform(input_data_featuretest)\n    if sparse.issparse(input_data_featuretest):\n        assert sparse.issparse(transformed_X)\n    elif isinstance(input_data_featuretest, list):\n        assert isinstance(transformed_X, pd.DataFrame)\n    else:\n        assert isinstance(transformed_X, type(input_data_featuretest))\n    assert np.shape(input_data_featuretest) == np.shape(transformed_X)\n    assert validator._is_fitted\n\n\n@pytest.mark.parametrize(\n    \"input_data_featuretest\",\n    (\n        \"numpy_string_nonan\",\n        \"numpy_string_nan\",\n    ),\n    indirect=True,\n)\ndef test_featurevalidator_unsupported_numpy(input_data_featuretest):\n    validator = FeatureValidator()\n    with pytest.raises(\n        ValueError, match=r\".*When providing a numpy array.*not supported.\"\n    ):\n        validator.fit(input_data_featuretest)\n\n\n@pytest.mark.parametrize(\n    \"input_data_featuretest\",\n    (\n        \"numpy_categoricalonly_nonan\",\n        \"numpy_mixed_nonan\",\n        \"numpy_categoricalonly_nan\",\n        \"numpy_mixed_nan\",\n        \"pandas_categoricalonly_nonan\",\n        \"pandas_mixed_nonan\",\n        \"sparse_bsr_nonan\",\n        \"sparse_bsr_nan\",\n        \"sparse_coo_nonan\",\n        \"sparse_coo_nan\",\n        \"sparse_csc_nonan\",\n        \"sparse_csc_nan\",\n        \"sparse_csr_nonan\",\n        \"sparse_csr_nan\",\n        \"sparse_dia_nonan\",\n        \"sparse_dia_nan\",\n        \"sparse_dok_nonan\",\n        \"sparse_dok_nan\",\n        \"sparse_lil_nonan\",\n    ),\n    indirect=True,\n)\ndef test_featurevalidator_fitontypeA_transformtypeB(input_data_featuretest):\n    \"\"\"\n    Check if we can fit in a given type (numpy) yet transform\n    if the user changes the type (pandas then)\n    \"\"\"\n    validator = FeatureValidator()\n    validator.fit(input_data_featuretest, input_data_featuretest)\n    if isinstance(input_data_featuretest, pd.DataFrame):\n        complementary_type = input_data_featuretest.to_numpy()\n    elif isinstance(input_data_featuretest, np.ndarray):\n        complementary_type = pd.DataFrame(input_data_featuretest)\n    elif isinstance(input_data_featuretest, list):\n        complementary_type = pd.DataFrame(input_data_featuretest)\n    elif sparse.issparse(input_data_featuretest):\n        complementary_type = sparse.csr_matrix(input_data_featuretest.todense())\n    else:\n        raise ValueError(type(input_data_featuretest))\n    transformed_X = validator.transform(complementary_type)\n    assert np.shape(input_data_featuretest) == np.shape(transformed_X)\n    assert validator._is_fitted\n\n\ndef test_featurevalidatorget_feat_type_from_columns():\n    \"\"\"\n    Makes sure that encoded columns are returned by get_feat_type_from_columns\n    whereas numerical columns are not returned\n    \"\"\"\n    validator = FeatureValidator()\n\n    df = pd.DataFrame(\n        [\n            {\"int\": 1, \"float\": 1.0, \"category\": \"one\", \"bool\": True},\n            {\"int\": 2, \"float\": 2.0, \"category\": \"two\", \"bool\": False},\n        ]\n    )\n\n    for col in df.columns:\n        df[col] = df[col].astype(col)\n\n    feature_types = validator.get_feat_type_from_columns(df)\n\n    assert feature_types == {\n        \"int\": \"numerical\",\n        \"float\": \"numerical\",\n        \"category\": \"categorical\",\n        \"bool\": \"categorical\",\n    }\n\n\ndef test_features_unsupported_calls_are_raised():\n    \"\"\"\n    Makes sure we raise a proper message to the user,\n    when providing not supported data input or using the validator in a way that is not\n    expected\n    \"\"\"\n    validator = FeatureValidator()\n    with pytest.raises(ValueError, match=r\"Auto-sklearn does not support time\"):\n        validator.fit(pd.DataFrame({\"datetime\": [pd.Timestamp(\"20180310\")]}))\n    with pytest.raises(\n        ValueError, match=r\"Auto-sklearn only supports.*yet, the provided input\"\n    ):\n        validator.fit({\"input1\": 1, \"input2\": 2})\n    validator = FeatureValidator()\n    with pytest.raises(\n        ValueError, match=r\"The feature dimensionality of the train and test\"\n    ):\n        validator.fit(\n            X_train=np.array([[1, 2, 3], [4, 5, 6]]),\n            X_test=np.array([[1, 2, 3, 4], [4, 5, 6, 7]]),\n        )\n    with pytest.raises(\n        ValueError, match=r\"Cannot call transform on a validator that is not fit\"\n    ):\n        validator.transform(np.array([[1, 2, 3], [4, 5, 6]]))\n    validator = FeatureValidator(feat_type=[\"Numerical\"])\n    with pytest.raises(\n        ValueError, match=r\"providing the option feat_type to the fit method is.*\"\n    ):\n        validator.fit(pd.DataFrame([[1, 2, 3], [4, 5, 6]]))\n    with pytest.raises(ValueError, match=r\"feat_type does not have same number of.*\"):\n        validator.fit(np.array([[1, 2, 3], [4, 5, 6]]))\n    validator = FeatureValidator(feat_type=[1, 2, 3])\n    with pytest.raises(ValueError, match=r\"feat_type must only contain strings.*\"):\n        validator.fit(np.array([[1, 2, 3], [4, 5, 6]]))\n    validator = FeatureValidator(feat_type=[\"1\", \"2\", \"3\"])\n    with pytest.raises(\n        ValueError, match=r\"Only `Categorical`, `Numerical` and `String` are.*\"\n    ):\n        validator.fit(np.array([[1, 2, 3], [4, 5, 6]]))\n\n\ndef test_no_new_category_after_fit():\n    \"\"\"\n    This test makes sure that we can actually pass new categories to the estimator\n    without throwing an error\n    \"\"\"\n    # Then make sure we catch categorical extra categories\n    x = pd.DataFrame({\"A\": [1, 2, 3, 4], \"B\": [5, 6, 7, 8]}, dtype=\"category\")\n    validator = FeatureValidator()\n    validator.fit(x)\n    x[\"A\"] = x[\"A\"].apply(lambda x: x * x)\n    validator.transform(x)\n\n\n# Actual checks for the features\n@pytest.mark.parametrize(\n    \"openml_id\",\n    (\n        40981,  # Australian\n        3,  # kr-vs-kp\n        1468,  # cnae-9\n        40975,  # car\n        40984,  # Segment\n    ),\n)\n@pytest.mark.parametrize(\"train_data_type\", (\"numpy\", \"pandas\", \"list\"))\n@pytest.mark.parametrize(\"test_data_type\", (\"numpy\", \"pandas\", \"list\"))\ndef test_featurevalidator_new_data_after_fit(\n    openml_id, train_data_type, test_data_type\n):\n    # List is currently not supported as infer_objects\n    # cast list objects to type objects\n    if train_data_type == \"list\" or test_data_type == \"list\":\n        pytest.skip()\n\n    validator = FeatureValidator()\n\n    if train_data_type == \"numpy\":\n        X, y = sklearn.datasets.fetch_openml(\n            data_id=openml_id, return_X_y=True, as_frame=False\n        )\n    elif train_data_type == \"pandas\":\n        X, y = sklearn.datasets.fetch_openml(\n            data_id=openml_id, return_X_y=True, as_frame=True\n        )\n    else:\n        X, y = sklearn.datasets.fetch_openml(\n            data_id=openml_id, return_X_y=True, as_frame=True\n        )\n        X = X.values.tolist()\n        y = y.values.tolist()\n\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n        X, y, random_state=1\n    )\n\n    validator.fit(X_train)\n\n    transformed_X = validator.transform(X_test)\n\n    # Basic Checking\n    if sparse.issparse(input_data_featuretest):\n        assert sparse.issparse(transformed_X)\n    elif isinstance(input_data_featuretest, list):\n        assert isinstance(transformed_X, pd.DataFrame)\n    else:\n        assert isinstance(transformed_X, type(X_train))\n    assert np.shape(X_test) == np.shape(transformed_X)\n\n\n@pytest.mark.parametrize(\n    \"openml_id\",\n    (\n        40981,  # Australian\n        3,  # kr-vs-kp\n        1468,  # cnae-9\n        40975,  # car\n        40984,  # Segment\n        2,  # anneal\n    ),\n)\ndef test_list_to_dataframe(openml_id):\n    X_pandas, y_pandas = sklearn.datasets.fetch_openml(\n        data_id=openml_id, return_X_y=True, as_frame=True\n    )\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n        X_pandas, y_pandas, random_state=1\n    )\n\n    X_list = X_train.values.tolist()\n    validator = FeatureValidator()\n    validator.fit(X_list)\n    transformed_X = validator.transform(X_list)\n    for i, col in enumerate(X_pandas.columns):\n        if is_numeric_dtype(X_pandas[col].dtype):\n            # convert dtype translates 72.0 to 72. Be robust against this!\n            assert is_numeric_dtype(transformed_X[i].dtype)\n        elif is_string_dtype(X_pandas[col].dtype):\n            assert is_string_dtype(X_pandas[col].dtype)\n        elif is_categorical_dtype(X_pandas[col].dtype):\n            assert is_categorical_dtype(X_pandas[col].dtype)\n        else:\n            raise NotImplementedError\n\n    # Also make sure that at testing time\n    # this work\n    transformed_X = validator.transform(X_test.values.tolist())\n    for i, col in enumerate(X_pandas.columns):\n        if is_numeric_dtype(X_pandas[col].dtype):\n            # convert dtype translates 72.0 to 72. Be robust against this!\n            assert is_numeric_dtype(transformed_X[i].dtype)\n        elif is_string_dtype(X_pandas[col].dtype):\n            assert is_string_dtype(X_pandas[col].dtype)\n        elif is_categorical_dtype(X_pandas[col].dtype):\n            assert is_categorical_dtype(X_pandas[col].dtype)\n        else:\n            raise NotImplementedError\n\n\n@pytest.mark.parametrize(\n    \"input_data_featuretest\",\n    (\n        \"sparse_bsr_nonan\",\n        \"sparse_bsr_nan\",\n        \"sparse_coo_nonan\",\n        \"sparse_coo_nan\",\n        \"sparse_csc_nonan\",\n        \"sparse_csc_nan\",\n        \"sparse_csr_nonan\",\n        \"sparse_csr_nan\",\n        \"sparse_dia_nonan\",\n        \"sparse_dia_nan\",\n        \"sparse_dok_nonan\",\n        \"sparse_dok_nan\",\n        \"sparse_lil_nonan\",\n        \"sparse_lil_nan\",\n    ),\n    indirect=True,\n)\ndef test_sparse_output_is_csr(input_data_featuretest):\n    validator = FeatureValidator()\n    validator.fit(input_data_featuretest, input_data_featuretest)\n    transformed_X = validator.transform(input_data_featuretest)\n    assert sparse.issparse(transformed_X)\n    assert isinstance(transformed_X, sparse.csr_matrix)\n\n\ndef test_unsupported_dataframe_sparse():\n    df = pd.DataFrame({\"A\": pd.Series(pd.arrays.SparseArray(np.random.randn(10)))})\n    validator = FeatureValidator()\n    with pytest.raises(\n        ValueError, match=r\"Auto-sklearn does not yet support sparse pandas\"\n    ):\n        validator.fit(df)\n\n\ndef test_object_columns():\n    class Dummy:\n        def __init__(self, x):\n            self.x = x\n\n        def __call__(self):\n            print(self.x)\n\n        def dummy_func(self):\n            for i in range(100):\n                print(\"do something 100 times\")\n\n    dummy_object = Dummy(1)\n    lst = [1, 2, 3]\n    array = np.array([1, 2, 3])\n    dummy_string = \"dummy string\"\n\n    df = pd.DataFrame(\n        {\n            \"dummy_object\": [dummy_object] * 4,\n            \"dummy_lst\": [lst] * 4,\n            \"dummy_array\": [array] * 4,\n            \"dummy_string\": [dummy_string] * 4,\n            \"type_mix_column\": [dummy_string, dummy_object, array, lst],\n            \"cat_column\": [\"a\", \"b\", \"a\", \"b\"],\n        }\n    )\n    df[\"cat_column\"] = df[\"cat_column\"].astype(\"category\")\n\n    with pytest.warns(\n        UserWarning,\n        match=r\"Input Column dummy_object has \"\n        r\"generic type object. \"\n        r\"Autosklearn will treat \"\n        r\"this column as string. \"\n        r\"Please ensure that this setting \"\n        r\"is suitable for your task.\",\n    ):\n        validator = FeatureValidator()\n        feat_type = validator.get_feat_type_from_columns(df)\n\n    column_types = {\n        \"dummy_object\": \"string\",\n        \"dummy_lst\": \"string\",\n        \"dummy_array\": \"string\",\n        \"dummy_string\": \"string\",\n        \"type_mix_column\": \"string\",\n        \"cat_column\": \"categorical\",\n    }\n\n    assert feat_type == column_types\n\n\ndef test_allow_string_feature():\n    df = pd.DataFrame({\"Text\": [\"Hello\", \"how are you?\"]})\n    with pytest.warns(\n        UserWarning,\n        match=r\"Input Column Text has generic type object. \"\n        r\"Autosklearn will treat this column as string. \"\n        r\"Please ensure that this setting is suitable for your task.\",\n    ):\n        validator = FeatureValidator(allow_string_features=False)\n        feat_type = validator.get_feat_type_from_columns(df)\n\n    column_types = {\"Text\": \"categorical\"}\n    assert feat_type == column_types\n\n    df[\"Text\"] = df[\"Text\"].astype(\"string\")\n    with pytest.warns(\n        UserWarning,\n        match=r\"you disabled text encoding column Text will be \" r\"encoded as category\",\n    ):\n        validator = FeatureValidator(allow_string_features=False)\n        feat_type = validator.get_feat_type_from_columns(df)\n\n    column_types = {\"Text\": \"categorical\"}\n    assert feat_type == column_types\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: auto-sklearn\nTest File Path: auto-sklearn\\test_meta_base\\test_meta_base.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom typing import Optional, Union\n\nimport copy\nfrom itertools import product\n\nimport numpy as np\nfrom ConfigSpace.configuration_space import Configuration, ConfigurationSpace\nfrom ConfigSpace.forbidden import ForbiddenAndConjunction, ForbiddenEqualsClause\nfrom sklearn.base import ClassifierMixin\n\nfrom autosklearn.askl_typing import FEAT_TYPE_TYPE\nfrom autosklearn.pipeline.base import BasePipeline\nfrom autosklearn.pipeline.components.classification import ClassifierChoice\nfrom autosklearn.pipeline.components.data_preprocessing import DataPreprocessorChoice\nfrom autosklearn.pipeline.components.data_preprocessing.balancing.balancing import (\n    Balancing,\n)\nfrom autosklearn.pipeline.components.feature_preprocessing import (\n    FeaturePreprocessorChoice,\n)\nfrom autosklearn.pipeline.constants import SPARSE\n\n\nclass SimpleClassificationPipeline(BasePipeline, ClassifierMixin):\n    \"\"\"This class implements the classification task.\n\n    It implements a pipeline, which includes one preprocessing step and one\n    classification algorithm. It can render a search space including all known\n    classification and preprocessing algorithms.\n\n    Contrary to the sklearn API it is not possible to enumerate the\n    possible parameters in the __init__ function because we only know the\n    available classifiers at runtime. For this reason the user must\n    specifiy the parameters by passing an instance of\n    ConfigSpace.configuration_space.Configuration.\n\n    Parameters\n    ----------\n    config : ConfigSpace.configuration_space.Configuration\n        The configuration to evaluate.\n\n    random_state : Optional[int | RandomState]\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance\n        used by `np.random`.\n\n    Attributes\n    ----------\n    _estimator : The underlying scikit-learn classification model. This\n        variable is assigned after a call to the\n        :meth:`autosklearn.pipeline.classification.SimpleClassificationPipeline\n        .fit` method.\n\n    _preprocessor : The underlying scikit-learn preprocessing algorithm. This\n        variable is only assigned if a preprocessor is specified and\n        after a call to the\n        :meth:`autosklearn.pipeline.classification.SimpleClassificationPipeline\n        .fit` method.\n\n    See also\n    --------\n\n    References\n    ----------\n\n    Examples\n    --------\n\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[Configuration] = None,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        steps=None,\n        dataset_properties=None,\n        include=None,\n        exclude=None,\n        random_state: Optional[Union[int, np.random.RandomState]] = None,\n        init_params=None,\n    ):\n        self._output_dtype = np.int32\n        if dataset_properties is None:\n            dataset_properties = dict()\n        if \"target_type\" not in dataset_properties:\n            dataset_properties[\"target_type\"] = \"classification\"\n        super().__init__(\n            feat_type=feat_type,\n            config=config,\n            steps=steps,\n            dataset_properties=dataset_properties,\n            include=include,\n            exclude=exclude,\n            random_state=random_state,\n            init_params=init_params,\n        )\n\n    def fit_transformer(self, X, y, fit_params=None):\n\n        if fit_params is None:\n            fit_params = {}\n\n        if self.config[\"balancing:strategy\"] == \"weighting\":\n            balancing = Balancing(strategy=\"weighting\")\n            _init_params, _fit_params = balancing.get_weights(\n                y,\n                self.config[\"classifier:__choice__\"],\n                self.config[\"feature_preprocessor:__choice__\"],\n                {},\n                {},\n            )\n            _init_params.update(self.init_params)\n            self.set_hyperparameters(\n                feat_type=self.feat_type,\n                configuration=self.config,\n                init_params=_init_params,\n            )\n\n            if _fit_params is not None:\n                fit_params.update(_fit_params)\n\n        X, fit_params = super().fit_transformer(X, y, fit_params=fit_params)\n\n        return X, fit_params\n\n    def predict_proba(self, X, batch_size=None):\n        \"\"\"predict_proba.\n\n        Parameters\n        ----------\n        X : array-like, shape = (n_samples, n_features)\n\n        batch_size: int or None, defaults to None\n            batch_size controls whether the pipeline will be\n            called on small chunks of the data. Useful when calling the\n            predict method on the whole array X results in a MemoryError.\n\n        Returns\n        -------\n        array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n        \"\"\"\n        if batch_size is None:\n            return super().predict_proba(X)\n\n        else:\n            if not isinstance(batch_size, int):\n                raise ValueError(\n                    \"Argument 'batch_size' must be of type int, \"\n                    \"but is '%s'\" % type(batch_size)\n                )\n            if batch_size <= 0:\n                raise ValueError(\n                    \"Argument 'batch_size' must be positive, \" \"but is %d\" % batch_size\n                )\n\n            else:\n                # Probe for the target array dimensions\n                target = self.predict_proba(X[0:2].copy())\n\n                y = np.zeros((X.shape[0], target.shape[1]), dtype=np.float32)\n\n                for k in range(max(1, int(np.ceil(float(X.shape[0]) / batch_size)))):\n                    batch_from = k * batch_size\n                    batch_to = min([(k + 1) * batch_size, X.shape[0]])\n                    pred_prob = self.predict_proba(\n                        X[batch_from:batch_to], batch_size=None\n                    )\n                    y[batch_from:batch_to] = pred_prob.astype(np.float32)\n\n                return y\n\n    def _get_hyperparameter_search_space(\n        self,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        include=None,\n        exclude=None,\n        dataset_properties=None,\n    ):\n        \"\"\"Create the hyperparameter configuration space.\n\n        Parameters\n        ----------\n        feat_type : dict, maps columns to there datatypes\n\n        include : dict (optional, default=None)\n\n        Returns\n        -------\n        cs : ConfigSpace.configuration_space.Configuration\n            The configuration space describing the SimpleRegressionClassifier.\n        \"\"\"\n        cs = ConfigurationSpace()\n\n        if dataset_properties is None or not isinstance(dataset_properties, dict):\n            dataset_properties = dict()\n        if \"target_type\" not in dataset_properties:\n            dataset_properties[\"target_type\"] = \"classification\"\n        if dataset_properties[\"target_type\"] != \"classification\":\n            dataset_properties[\"target_type\"] = \"classification\"\n\n        if \"sparse\" not in dataset_properties:\n            # This dataset is probably dense\n            dataset_properties[\"sparse\"] = False\n\n        cs = self._get_base_search_space(\n            cs=cs,\n            feat_type=feat_type,\n            dataset_properties=dataset_properties,\n            exclude=exclude,\n            include=include,\n            pipeline=self.steps,\n        )\n\n        classifiers = cs.get_hyperparameter(\"classifier:__choice__\").choices\n        preprocessors = cs.get_hyperparameter(\"feature_preprocessor:__choice__\").choices\n        available_classifiers = self._final_estimator.get_available_components(\n            dataset_properties\n        )\n\n        possible_default_classifier = copy.copy(list(available_classifiers.keys()))\n        default = cs.get_hyperparameter(\"classifier:__choice__\").default_value\n        del possible_default_classifier[possible_default_classifier.index(default)]\n\n        # A classifier which can handle sparse data after the densifier is\n        # forbidden for memory issues\n        for key in classifiers:\n            if SPARSE in available_classifiers[key].get_properties()[\"input\"]:\n                if \"densifier\" in preprocessors:\n                    while True:\n                        try:\n                            forb_cls = ForbiddenEqualsClause(\n                                cs.get_hyperparameter(\"classifier:__choice__\"), key\n                            )\n                            forb_fpp = ForbiddenEqualsClause(\n                                cs.get_hyperparameter(\n                                    \"feature_preprocessor:__choice__\"\n                                ),\n                                \"densifier\",\n                            )\n                            cs.add_forbidden_clause(\n                                ForbiddenAndConjunction(forb_cls, forb_fpp)\n                            )\n                            # Success\n                            break\n                        except ValueError:\n                            # Change the default and try again\n                            try:\n                                default = possible_default_classifier.pop()\n                            except IndexError:\n                                raise ValueError(\n                                    \"Cannot find a legal default configuration.\"\n                                )\n                            cs.get_hyperparameter(\n                                \"classifier:__choice__\"\n                            ).default_value = default\n\n        # which would take too long\n        # Combinations of non-linear models with feature learning:\n        classifiers_ = [\n            \"adaboost\",\n            \"decision_tree\",\n            \"extra_trees\",\n            \"gradient_boosting\",\n            \"k_nearest_neighbors\",\n            \"libsvm_svc\",\n            \"mlp\",\n            \"random_forest\",\n            \"gaussian_nb\",\n        ]\n        feature_learning = [\n            \"kernel_pca\",\n            \"kitchen_sinks\",\n            \"nystroem_sampler\",\n        ]\n\n        for c, f in product(classifiers_, feature_learning):\n            if c not in classifiers:\n                continue\n            if f not in preprocessors:\n                continue\n            while True:\n                try:\n                    cs.add_forbidden_clause(\n                        ForbiddenAndConjunction(\n                            ForbiddenEqualsClause(\n                                cs.get_hyperparameter(\"classifier:__choice__\"), c\n                            ),\n                            ForbiddenEqualsClause(\n                                cs.get_hyperparameter(\n                                    \"feature_preprocessor:__choice__\"\n                                ),\n                                f,\n                            ),\n                        )\n                    )\n                    break\n                except KeyError:\n                    break\n                except ValueError:\n                    # Change the default and try again\n                    try:\n                        default = possible_default_classifier.pop()\n                    except IndexError:\n                        raise ValueError(\"Cannot find a legal default configuration.\")\n                    cs.get_hyperparameter(\n                        \"classifier:__choice__\"\n                    ).default_value = default\n\n        # Won't work\n        # Multinomial NB etc don't use with features learning, pca etc\n        classifiers_ = [\"multinomial_nb\"]\n        preproc_with_negative_X = [\n            \"kitchen_sinks\",\n            \"pca\",\n            \"truncatedSVD\",\n            \"fast_ica\",\n            \"kernel_pca\",\n            \"nystroem_sampler\",\n        ]\n\n        for c, f in product(classifiers_, preproc_with_negative_X):\n            if c not in classifiers:\n                continue\n            if f not in preprocessors:\n                continue\n            while True:\n                try:\n                    cs.add_forbidden_clause(\n                        ForbiddenAndConjunction(\n                            ForbiddenEqualsClause(\n                                cs.get_hyperparameter(\n                                    \"feature_preprocessor:__choice__\"\n                                ),\n                                f,\n                            ),\n                            ForbiddenEqualsClause(\n                                cs.get_hyperparameter(\"classifier:__choice__\"), c\n                            ),\n                        )\n                    )\n                    break\n                except KeyError:\n                    break\n                except ValueError:\n                    # Change the default and try again\n                    try:\n                        default = possible_default_classifier.pop()\n                    except IndexError:\n                        raise ValueError(\"Cannot find a legal default configuration.\")\n                    cs.get_hyperparameter(\n                        \"classifier:__choice__\"\n                    ).default_value = default\n\n        self.configuration_space = cs\n        self.dataset_properties = dataset_properties\n        return cs\n\n    def _get_pipeline_steps(\n        self, dataset_properties, feat_type: Optional[FEAT_TYPE_TYPE] = None\n    ):\n        steps = []\n\n        default_dataset_properties = {\"target_type\": \"classification\"}\n        if dataset_properties is not None and isinstance(dataset_properties, dict):\n            default_dataset_properties.update(dataset_properties)\n\n        steps.extend(\n            [\n                [\n                    \"data_preprocessor\",\n                    DataPreprocessorChoice(\n                        feat_type=feat_type,\n                        dataset_properties=default_dataset_properties,\n                        random_state=self.random_state,\n                    ),\n                ],\n                [\"balancing\", Balancing(random_state=self.random_state)],\n                [\n                    \"feature_preprocessor\",\n                    FeaturePreprocessorChoice(\n                        feat_type=feat_type,\n                        dataset_properties=default_dataset_properties,\n                        random_state=self.random_state,\n                    ),\n                ],\n                [\n                    \"classifier\",\n                    ClassifierChoice(\n                        feat_type=feat_type,\n                        dataset_properties=default_dataset_properties,\n                        random_state=self.random_state,\n                    ),\n                ],\n            ]\n        )\n\n        return steps\n\n    def _get_estimator_hyperparameter_name(self):\n        return \"classifier\"\n\n\n### Source File Content:\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\nfrom ConfigSpace.configuration_space import Configuration\nfrom ConfigSpace.util import deactivate_inactive_hyperparameters\n\nfrom ..input import aslib_simple\nfrom ..metafeatures.metafeature import DatasetMetafeatures\n\n\nclass Run(object):\n    def __init__(self, configuration, result, runtime):\n        self.configuration = configuration\n        self.result = result\n        self.runtime = runtime\n\n    def __repr__(self):\n        return \"Run:\\nresult: %3.3f\\nruntime: %3.3f\\n%s\" % (\n            self.result,\n            self.runtime,\n            str(self.configuration),\n        )\n\n\nclass Instance(object):\n    def __init__(self, name, features):\n        self.name = name\n        self.features = features\n\n\nclass MetaBase(object):\n    def __init__(self, configuration_space, aslib_directory, logger):\n        \"\"\"Container for dataset metadata and experiment results.\n\n        Constructor arguments:\n        - The configuration space\n        - aslib_directory: directory with a problem instance in the aslib format\n        \"\"\"\n\n        self.logger = logger\n\n        self.configuration_space = configuration_space\n        self.default_configuration_space_dict = (\n            configuration_space.get_default_configuration().get_dictionary()\n        )\n        self.aslib_directory = aslib_directory\n\n        aslib_reader = aslib_simple.AlgorithmSelectionProblem(\n            self.aslib_directory, self.configuration_space\n        )\n        self.metafeatures = aslib_reader.metafeatures\n        self.algorithm_runs: OrderedDict[\n            str, pd.DataFrame\n        ] = aslib_reader.algorithm_runs\n        self.configurations = aslib_reader.configurations\n\n        configurations = dict()\n        for algorithm_id in self.configurations:\n            configuration = self.configurations[algorithm_id]\n            try:\n                for key in self.default_configuration_space_dict.keys():\n                    if key not in configuration:\n                        configuration[key] = self.default_configuration_space_dict[key]\n                configuration = Configuration(\n                    configuration_space,\n                    values=configuration,\n                    allow_inactive_with_values=True,\n                )\n                configuration = deactivate_inactive_hyperparameters(\n                    configuration, configuration_space\n                )\n                configurations[str(algorithm_id)] = configuration\n            except (ValueError, KeyError) as e:\n                self.logger.debug(\"Error reading configurations: %s\", e)\n\n        self.configurations = configurations\n\n    def add_dataset(self, name, metafeatures):\n        metafeatures.name = name\n        if isinstance(metafeatures, DatasetMetafeatures):\n            data_ = {\n                mf.name: mf.value for mf in metafeatures.metafeature_values.values()\n            }\n            metafeatures = pd.Series(name=name, data=data_, dtype=np.float64)\n        if name.lower() in self.metafeatures.index:\n            self.logger.warning(\n                \"Dataset %s already in meta-data. Removing occurence.\", name.lower()\n            )\n            self.metafeatures.drop(name.lower(), inplace=True)\n        self.metafeatures = pd.concat([self.metafeatures, pd.DataFrame(metafeatures).T])\n\n    def get_runs(self, dataset_name, performance_measure=None):\n        \"\"\"Return a list of all runs for a dataset.\"\"\"\n        if performance_measure is None:\n            performance_measure = list(self.algorithm_runs.keys())[0]\n        return self.algorithm_runs[performance_measure].loc[dataset_name]\n\n    def get_all_runs(self, performance_measure=None):\n        \"\"\"Return a dictionary with a list of all runs\"\"\"\n        if performance_measure is None:\n            performance_measure = list(self.algorithm_runs.keys())[0]\n        return self.algorithm_runs[performance_measure]\n\n    def get_metafeatures(self, dataset_name=None, features=None):\n        if features is not None:\n            metafeatures = self._get_metafeatures(features)\n        else:\n            metafeatures = self.metafeatures\n        if dataset_name is not None:\n            return metafeatures.loc[dataset_name]\n        else:\n            return metafeatures\n\n    def _get_metafeatures(self, features):\n        \"\"\"This is inside an extra function for testing purpose\"\"\"\n        # Load the task\n\n        self.logger.info(\"Going to use the following metafeature subset: %s\", features)\n        all_metafeatures = self.metafeatures\n        all_metafeatures = all_metafeatures.loc[:, features]\n\n        return all_metafeatures\n\n    def get_configuration_from_algorithm_index(self, idx):\n        return self.configurations[str(idx)]\n        # configuration = self.configurations[idx]\n        # configuration = Configuration(self.configuration_space,\n        # **configuration)\n        # return configuration\n\n    def get_algorithm_index_from_configuration(self, configuration):\n        for idx in self.configurations.keys():\n            if configuration == self.configurations[idx]:\n                return idx\n\n        raise ValueError(configuration)\n\n    def get_all_dataset_names(self):\n        return list(self.metafeatures.index)\n\n### Source File Dependency Files Content\n### Dependency File: askl_typing.py\nfrom typing import Dict, Union\n\nFEAT_TYPE_TYPE = Dict[Union[str, int], str]\n\n\n### Dependency File: aslib_simple.py\nimport csv\nimport logging\nimport os\nfrom collections import OrderedDict, defaultdict\n\nimport arff\nimport pandas as pd\nfrom ConfigSpace.configuration_space import ConfigurationSpace\n\n\nclass AlgorithmSelectionProblem(object):\n    def __init__(self, directory: str, cs: ConfigurationSpace):\n        self.logger = logging.getLogger(__name__)\n\n        # Create data structures\n        self.cs = cs\n        self.dir_ = directory\n        self.algorithm_runs = None\n        self.configurations = None\n        self.metafeatures = None\n        self.read_funcs = {\n            # \"description.txt\": self._read_description,\n            \"algorithm_runs.arff\": self._read_algorithm_runs,\n            # \"feature_costs.arff\": self._read_feature_costs,\n            \"feature_values.arff\": self._read_feature_values,\n            # \"feature_runstatus.arff\": self._read_feature_runstatus,\n            # \"ground_truth.arff\": self._read_ground_truth,\n            # \"cv.arff\": self._read_cv,\n            \"configurations.csv\": self._read_configurations,\n        }\n        self.found_files = []\n\n        # Read ASLib files\n        self._find_files()\n        self._read_files()\n\n    def _find_files(self):\n        \"\"\"\n        find all expected files in self.dir_\n        fills self.found_files\n        \"\"\"\n        expected = [\n            # \"description.txt\",\n            \"algorithm_runs.arff\",\n            \"feature_values.arff\",\n            # \"feature_runstatus.arff\",\n        ]\n        optional = [\n            \"ground_truth.arff\",\n            \"feature_costs.arff\",\n            \"citation.bib\",\n            \"cv.arff\",\n            \"configurations.csv\",\n        ]\n\n        for expected_file in expected:\n            full_path = os.path.join(self.dir_, expected_file)\n            if not os.path.isfile(full_path):\n                self.logger.error(\"Not found: %s (has to be added)\" % (full_path))\n            else:\n                self.found_files.append(full_path)\n\n        for expected_file in optional:\n            full_path = os.path.join(self.dir_, expected_file)\n            if not os.path.isfile(full_path):\n                # self.logger.warning(\n                #     \"Not found: %s (maybe you want to add it)\" % (full_path))\n                pass\n            else:\n                self.found_files.append(full_path)\n\n    def _read_files(self):\n        \"\"\"\n        iterates over all found files (self.found_files) and\n        calls the corresponding function to validate file\n        \"\"\"\n        for file_ in self.found_files:\n            read_func = self.read_funcs.get(os.path.basename(file_))\n            if read_func:\n                read_func(file_)\n\n    def _read_algorithm_runs(self, filename):\n        with open(filename) as fh:\n            arff_dict = arff.load(fh)\n\n        if arff_dict[\"attributes\"][0][0].upper() != \"INSTANCE_ID\":\n            self.logger.error(\n                \"instance_id as first attribute is missing in %s\" % (filename)\n            )\n        if arff_dict[\"attributes\"][1][0].upper() != \"REPETITION\":\n            self.logger.error(\n                \"repetition as second attribute is missing in %s\" % (filename)\n            )\n        if arff_dict[\"attributes\"][2][0].upper() != \"ALGORITHM\":\n            self.logger.error(\n                \"algorithm as third attribute is missing in %s\" % (filename)\n            )\n\n        performance_measures = [pm[0] for pm in arff_dict[\"attributes\"][3:-1]]\n\n        measure_instance_algorithm_triples = defaultdict(lambda: defaultdict(dict))\n        for data in arff_dict[\"data\"]:\n            inst_name = str(data[0])\n            # repetition = data[1]\n            algorithm = str(data[2])\n            perf_list = data[3:-1]\n            status = data[-1]\n\n            if status != \"ok\":\n                continue\n\n            for i, performance_measure in enumerate(performance_measures):\n                measure_instance_algorithm_triples[performance_measure][inst_name][\n                    algorithm\n                ] = perf_list[i]\n\n        # TODO: this does not support any repetitions!\n        measure_algorithm_matrices = OrderedDict()\n        for pm in performance_measures:\n            measure_algorithm_matrices[pm] = pd.DataFrame(\n                measure_instance_algorithm_triples[pm]\n            ).transpose()\n\n        self.algorithm_runs = measure_algorithm_matrices\n\n    def _read_feature_values(self, filename):\n        with open(filename) as fh:\n            arff_dict = arff.load(fh)\n\n        metafeatures = dict()\n        for data in arff_dict[\"data\"]:\n            inst_name = data[0]\n            # repetition = data[1]\n            features = data[2:]\n\n            metafeatures[inst_name] = {\n                feature[0]: feature_value\n                for feature, feature_value in zip(arff_dict[\"attributes\"][2:], features)\n            }\n\n        self.metafeatures = pd.DataFrame(metafeatures).transpose()\n\n    def _read_configurations(self, filename):\n        with open(filename) as fh:\n            csv_reader = csv.DictReader(fh)\n\n            configurations = dict()\n            hp_names = self.cs.get_hyperparameter_names()\n            for line in csv_reader:\n                configuration = dict()\n                algorithm_id = line[\"idx\"]\n                for hp_name, value in line.items():\n                    if not value or hp_name == \"idx\":\n                        continue\n                    if hp_name not in hp_names:\n                        # skip hyperparameter\n                        # if it is not existing in the current search space\n                        continue\n                    try:\n                        value = int(value)\n                    except Exception:\n                        try:\n                            value = float(value)\n                        except Exception:\n                            pass\n\n                    configuration[hp_name] = value\n                configurations[algorithm_id] = configuration\n        self.configurations = configurations\n\n\n### Dependency File: base.py\nfrom abc import ABCMeta\nfrom typing import Dict, Optional, Union\n\nimport numpy as np\nimport scipy.sparse\nfrom ConfigSpace import Configuration\nfrom sklearn.pipeline import Pipeline\n\nimport autosklearn.pipeline.create_searchspace_util\nfrom autosklearn.askl_typing import FEAT_TYPE_TYPE\n\nfrom .components.base import AutoSklearnChoice, AutoSklearnComponent\n\nDATASET_PROPERTIES_TYPE = Dict[str, Union[str, int, bool]]\nPIPELINE_DATA_DTYPE = Union[\n    np.ndarray,\n    scipy.sparse.bsr_matrix,\n    scipy.sparse.coo_matrix,\n    scipy.sparse.csc_matrix,\n    scipy.sparse.csr_matrix,\n    scipy.sparse.dia_matrix,\n    scipy.sparse.dok_matrix,\n    scipy.sparse.lil_matrix,\n]\n\n\nclass BasePipeline(Pipeline):\n    \"\"\"Base class for all pipeline objects.\n\n    Notes\n    -----\n    This class should not be instantiated, only subclassed.\"\"\"\n\n    __metaclass__ = ABCMeta\n\n    def __init__(\n        self,\n        config=None,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        steps=None,\n        dataset_properties=None,\n        include=None,\n        exclude=None,\n        random_state=None,\n        init_params=None,\n    ):\n\n        self.init_params = init_params if init_params is not None else {}\n        self.include = include if include is not None else {}\n        self.exclude = exclude if exclude is not None else {}\n        self.dataset_properties = (\n            dataset_properties if dataset_properties is not None else {}\n        )\n        self.random_state = random_state\n        self.feat_type = feat_type\n\n        if steps is None:\n            self.steps = self._get_pipeline_steps(\n                feat_type=feat_type, dataset_properties=dataset_properties\n            )\n        else:\n            self.steps = steps\n\n        self._validate_include_exclude_params()\n\n        self.config_space = self.get_hyperparameter_search_space(feat_type=feat_type)\n\n        if config is None:\n            self.config = self.config_space.get_default_configuration()\n        else:\n            if isinstance(config, dict):\n                config = Configuration(self.config_space, config)\n            if self.config_space != config.configuration_space:\n                print(self.config_space._children)\n                print(config.configuration_space._children)\n                import difflib\n\n                diff = difflib.unified_diff(\n                    str(self.config_space).splitlines(),\n                    str(config.configuration_space).splitlines(),\n                )\n                diff = \"\\n\".join(diff)\n                raise ValueError(\n                    \"Configuration passed does not come from the \"\n                    \"same configuration space. Differences are: \"\n                    \"%s\" % diff\n                )\n            self.config = config\n\n        self.set_hyperparameters(\n            self.config, feat_type=feat_type, init_params=init_params\n        )\n\n        super().__init__(steps=self.steps)\n\n        self._additional_run_info = {}\n\n    def fit(self, X, y, **fit_params):\n        \"\"\"Fit the selected algorithm to the training data.\n\n        Parameters\n        ----------\n        X : array-like or sparse, shape = (n_samples, n_features)\n            Training data. The preferred type of the matrix (dense or sparse)\n            depends on the estimator selected.\n\n        y : array-like\n            Targets\n\n        fit_params : dict\n            See the documentation of sklearn.pipeline.Pipeline for formatting\n            instructions.\n\n        Returns\n        -------\n        self : returns an instance of self.\n\n        Raises\n        ------\n        NoModelException\n            NoModelException is raised if fit() is called without specifying\n            a classification algorithm first.\n        \"\"\"\n        X, fit_params = self.fit_transformer(X, y, **fit_params)\n        self.fit_estimator(X, y, **fit_params)\n        return self\n\n    def fit_transformer(self, X, y, fit_params=None):\n        self.num_targets = 1 if len(y.shape) == 1 else y.shape[1]\n        if fit_params is None:\n            fit_params = {}\n        fit_params = {\n            key.replace(\":\", \"__\"): value for key, value in fit_params.items()\n        }\n        fit_params_steps = self._check_fit_params(**fit_params)\n        Xt = self._fit(X, y, **fit_params_steps)\n        return Xt, fit_params_steps[self.steps[-1][0]]\n\n    def fit_estimator(self, X, y, **fit_params):\n        fit_params = {\n            key.replace(\":\", \"__\"): value for key, value in fit_params.items()\n        }\n        self._final_estimator.fit(X, y, **fit_params)\n        return self\n\n    def iterative_fit(self, X, y, n_iter=1, **fit_params):\n        self._final_estimator.iterative_fit(X, y, n_iter=n_iter, **fit_params)\n\n    def estimator_supports_iterative_fit(self):\n        return self._final_estimator.estimator_supports_iterative_fit()\n\n    def get_max_iter(self):\n        if self.estimator_supports_iterative_fit():\n            return self._final_estimator.get_max_iter()\n        else:\n            raise NotImplementedError()\n\n    def configuration_fully_fitted(self):\n        return self._final_estimator.configuration_fully_fitted()\n\n    def get_current_iter(self):\n        return self._final_estimator.get_current_iter()\n\n    def predict(self, X, batch_size=None):\n        \"\"\"Predict the classes using the selected model.\n\n        Parameters\n        ----------\n        X : array-like, shape = (n_samples, n_features)\n\n        batch_size: int or None, defaults to None\n            batch_size controls whether the pipeline will be\n            called on small chunks of the data. Useful when calling the\n            predict method on the whole array X results in a MemoryError.\n\n        Returns\n        -------\n        array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n            Returns the predicted values\"\"\"\n\n        if batch_size is None:\n            return super().predict(X).astype(self._output_dtype)\n        else:\n            if not isinstance(batch_size, int):\n                raise ValueError(\n                    \"Argument 'batch_size' must be of type int, \"\n                    \"but is '%s'\" % type(batch_size)\n                )\n            if batch_size <= 0:\n                raise ValueError(\n                    \"Argument 'batch_size' must be positive, \" \"but is %d\" % batch_size\n                )\n\n            else:\n                if self.num_targets == 1:\n                    y = np.zeros((X.shape[0],), dtype=self._output_dtype)\n                else:\n                    y = np.zeros(\n                        (X.shape[0], self.num_targets), dtype=self._output_dtype\n                    )\n\n                # Copied and adapted from the scikit-learn GP code\n                for k in range(max(1, int(np.ceil(float(X.shape[0]) / batch_size)))):\n                    batch_from = k * batch_size\n                    batch_to = min([(k + 1) * batch_size, X.shape[0]])\n                    y[batch_from:batch_to] = self.predict(\n                        X[batch_from:batch_to], batch_size=None\n                    )\n\n                return y\n\n    def set_hyperparameters(\n        self,\n        configuration,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        init_params=None,\n    ):\n        self.config = configuration\n\n        for node_idx, n_ in enumerate(self.steps):\n            node_name, node = n_\n\n            sub_configuration_space = node.get_hyperparameter_search_space(\n                feat_type=feat_type, dataset_properties=self.dataset_properties\n            )\n            sub_config_dict = {}\n            for param in configuration:\n                if param.startswith(\"%s:\" % node_name):\n                    value = configuration[param]\n                    new_name = param.replace(\"%s:\" % node_name, \"\", 1)\n                    sub_config_dict[new_name] = value\n\n            sub_configuration = Configuration(\n                sub_configuration_space, values=sub_config_dict\n            )\n\n            if init_params is not None:\n                sub_init_params_dict = {}\n                for param in init_params:\n                    if param.startswith(\"%s:\" % node_name):\n                        value = init_params[param]\n                        new_name = param.replace(\"%s:\" % node_name, \"\", 1)\n                        sub_init_params_dict[new_name] = value\n            else:\n                sub_init_params_dict = None\n\n            if isinstance(\n                node, (AutoSklearnChoice, AutoSklearnComponent, BasePipeline)\n            ):\n                node.set_hyperparameters(\n                    feat_type=feat_type,\n                    configuration=sub_configuration,\n                    init_params=sub_init_params_dict,\n                )\n            else:\n                raise NotImplementedError(\"Not supported yet!\")\n\n        # In-code check to make sure init params\n        # is checked after pipeline creation\n        self._check_init_params_honored(init_params)\n\n        return self\n\n    def get_hyperparameter_search_space(\n        self, feat_type: Optional[FEAT_TYPE_TYPE] = None, dataset_properties=None\n    ):\n        \"\"\"Return the configuration space for the CASH problem.\n\n        Returns\n        -------\n        cs : ConfigSpace.configuration_space.Configuration\n            The configuration space describing the AutoSklearnClassifier.\n\n        \"\"\"\n        if not hasattr(self, \"config_space\") or self.config_space is None:\n            self.config_space = self._get_hyperparameter_search_space(\n                feat_type=feat_type,\n                include=self.include,\n                exclude=self.exclude,\n                dataset_properties=self.dataset_properties,\n            )\n        return self.config_space\n\n    def _get_hyperparameter_search_space(\n        self,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        include=None,\n        exclude=None,\n        dataset_properties=None,\n    ):\n        \"\"\"Return the configuration space for the CASH problem.\n\n        This method should be called by the method\n        get_hyperparameter_search_space of a subclass. After the subclass\n        assembles a list of available estimators and preprocessor components,\n        _get_hyperparameter_search_space can be called to do the work of\n        creating the actual\n        ConfigSpace.configuration_space.ConfigurationSpace object.\n\n        Parameters\n        ----------\n        feat_type: dict\n            python dictionary which maps the columns of the dataset to the data types\n\n        estimator_name : str\n            Name of the estimator hyperparameter which will be used in the\n            configuration space. For a classification task, this would be\n            'classifier'.\n\n        estimator_components : dict {name: component}\n            Dictionary with all estimator components to be included in the\n            configuration space.\n\n        preprocessor_components : dict {name: component}\n            Dictionary with all preprocessor components to be included in the\n            configuration space. .\n\n        always_active : list of str\n            A list of components which will always be active in the pipeline.\n            This is useful for components like imputation which have\n            hyperparameters to be configured, but which do not have any parent.\n\n        default_estimator : str\n            Default value for the estimator hyperparameter.\n\n        Returns\n        -------\n        cs : ConfigSpace.configuration_space.Configuration\n            The configuration space describing the AutoSklearnClassifier.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _get_base_search_space(\n        self,\n        cs,\n        dataset_properties,\n        include,\n        exclude,\n        pipeline,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n    ):\n        if include is None:\n            if self.include is None:\n                include = {}\n            else:\n                include = self.include\n\n        keys = [pair[0] for pair in pipeline]\n        for key in include:\n            if key not in keys:\n                raise ValueError(\n                    \"Invalid key in include: %s; should be one \" \"of %s\" % (key, keys)\n                )\n\n        if exclude is None:\n            if self.exclude is None:\n                exclude = {}\n            else:\n                exclude = self.exclude\n\n        keys = [pair[0] for pair in pipeline]\n        for key in exclude:\n            if key not in keys:\n                raise ValueError(\n                    \"Invalid key in exclude: %s; should be one \" \"of %s\" % (key, keys)\n                )\n\n        if \"sparse\" not in dataset_properties:\n            # This dataset is probably dense\n            dataset_properties[\"sparse\"] = False\n        if \"signed\" not in dataset_properties:\n            # This dataset probably contains unsigned data\n            dataset_properties[\"signed\"] = False\n\n        matches = autosklearn.pipeline.create_searchspace_util.get_match_array(\n            pipeline=pipeline,\n            dataset_properties=dataset_properties,\n            include=include,\n            exclude=exclude,\n        )\n\n        # Now we have only legal combinations at this step of the pipeline\n        # Simple sanity checks\n        assert np.sum(matches) != 0, \"No valid pipeline found.\"\n\n        assert np.sum(matches) <= np.size(\n            matches\n        ), \"'matches' is not binary; %s <= %d, %s\" % (\n            str(np.sum(matches)),\n            np.size(matches),\n            str(matches.shape),\n        )\n\n        # Iterate each dimension of the matches array (each step of the\n        # pipeline) to see if we can add a hyperparameter for that step\n        for node_idx, n_ in enumerate(pipeline):\n            node_name, node = n_\n\n            is_choice = isinstance(node, AutoSklearnChoice)\n\n            # if the node isn't a choice we can add it immediately because it\n            #  must be active (if it wasn't, np.sum(matches) would be zero\n            if not is_choice:\n                cs.add_configuration_space(\n                    node_name,\n                    node.get_hyperparameter_search_space(\n                        dataset_properties=dataset_properties, feat_type=feat_type\n                    ),\n                )\n            # If the node is a choice, we have to figure out which of its\n            #  choices are actually legal choices\n            else:\n                choices_list = (\n                    autosklearn.pipeline.create_searchspace_util.find_active_choices(\n                        matches,\n                        node,\n                        node_idx,\n                        dataset_properties,\n                        include.get(node_name),\n                        exclude.get(node_name),\n                    )\n                )\n                sub_config_space = node.get_hyperparameter_search_space(\n                    feat_type=feat_type,\n                    dataset_properties=dataset_properties,\n                    include=choices_list,\n                )\n                cs.add_configuration_space(node_name, sub_config_space)\n\n        # And now add forbidden parameter configurations\n        # According to matches\n        if np.sum(matches) < np.size(matches):\n            cs = autosklearn.pipeline.create_searchspace_util.add_forbidden(\n                conf_space=cs,\n                pipeline=pipeline,\n                matches=matches,\n                dataset_properties=dataset_properties,\n                include=include,\n                exclude=exclude,\n            )\n\n        return cs\n\n    def _check_init_params_honored(self, init_params):\n        \"\"\"\n        Makes sure that init params is honored at the implementation level\n        \"\"\"\n        if init_params is None or len(init_params) < 1:\n            # None/empty dict, so no further check required\n            return\n\n        # There is the scenario, where instance is passed as an argument to the\n        # init_params 'instance': '{\"task_id\": \"73543c4a360aa24498c0967fbc2f926b\"}'}\n        # coming from smac instance. Remove this key to make the testing stricter\n        init_params.pop(\"instance\", None)\n\n        for key, value in init_params.items():\n\n            if \":\" not in key:\n                raise ValueError(\n                    \"Unsupported argument to init_params {}.\"\n                    \"When using init_params, a hierarchical format like \"\n                    \"node_name:parameter must be provided.\".format(key)\n                )\n            node_name = key.split(\":\", 1)[0]\n            if node_name not in self.named_steps.keys():\n                raise ValueError(\n                    \"The current node name specified via key={} of init_params \"\n                    \"is not valid. Valid node names are {}\".format(\n                        key, self.named_steps.keys()\n                    )\n                )\n                continue\n            variable_name = key.split(\":\")[-1]\n            node = self.named_steps[node_name]\n            if isinstance(node, BasePipeline):\n                # If dealing with a sub pipe,\n                # Call the child _check_init_params_honored with the updated config\n                node._check_init_params_honored(\n                    {key.replace(\"%s:\" % node_name, \"\", 1): value}\n                )\n                continue\n\n            if isinstance(node, AutoSklearnComponent):\n                node_dict = vars(node)\n            elif isinstance(node, AutoSklearnChoice):\n                node_dict = vars(node.choice)\n            else:\n                raise ValueError(\"Unsupported node type {}\".format(type(node)))\n\n            if variable_name not in node_dict or node_dict[variable_name] != value:\n                raise ValueError(\n                    \"Cannot properly set the pair {}->{} via init_params\"\n                    \"\".format(key, value)\n                )\n\n    def __repr__(self):\n        class_name = self.__class__.__name__\n\n        configuration = {}\n        self.config._populate_values()\n        for hp_name in self.config:\n            if self.config[hp_name] is not None:\n                configuration[hp_name] = self.config[hp_name]\n\n        configuration_string = \"\".join(\n            [\n                \"configuration={\\n  \",\n                \",\\n  \".join(\n                    [\n                        \"'%s': %s\" % (hp_name, repr(configuration[hp_name]))\n                        for hp_name in sorted(configuration)\n                    ]\n                ),\n                \"}\",\n            ]\n        )\n\n        if len(self.dataset_properties) > 0:\n            dataset_properties_string = []\n            dataset_properties_string.append(\"dataset_properties={\")\n            for i, item in enumerate(self.dataset_properties.items()):\n                if i != 0:\n                    dataset_properties_string.append(\",\\n  \")\n                else:\n                    dataset_properties_string.append(\"\\n  \")\n\n                if isinstance(item[1], str):\n                    dataset_properties_string.append(\"'%s': '%s'\" % (item[0], item[1]))\n                else:\n                    dataset_properties_string.append(\"'%s': %s\" % (item[0], item[1]))\n            dataset_properties_string.append(\"}\")\n            dataset_properties_string = \"\".join(dataset_properties_string)\n\n            return_value = \"%s(%s,\\n%s)\" % (\n                class_name,\n                configuration,\n                dataset_properties_string,\n            )\n        else:\n            return_value = \"%s(%s)\" % (class_name, configuration_string)\n\n        return return_value\n\n    def _get_pipeline_steps(\n        self, dataset_properties, feat_type: Optional[FEAT_TYPE_TYPE] = None\n    ):\n        raise NotImplementedError()\n\n    def _get_estimator_hyperparameter_name(self):\n        raise NotImplementedError()\n\n    def get_additional_run_info(self):\n        \"\"\"Allows retrieving additional run information from the pipeline.\n\n        Can be overridden by subclasses to return additional information to\n        the optimization algorithm.\n        \"\"\"\n        return self._additional_run_info\n\n    def _validate_include_exclude_params(self):\n        if self.include is not None and self.exclude is not None:\n            for key in self.include.keys():\n                if key in self.exclude.keys():\n                    raise ValueError(\n                        \"Cannot specify include and exclude for same step '{}'.\".format(\n                            key\n                        )\n                    )\n\n        supported_steps = {\n            step[0]: step[1]\n            for step in self.steps\n            if isinstance(step[1], AutoSklearnChoice)\n        }\n        for arg in [\"include\", \"exclude\"]:\n            argument = getattr(self, arg)\n            if not argument:\n                continue\n            for key in list(argument.keys()):\n                if key not in supported_steps:\n                    raise ValueError(\n                        \"The provided key '{}' in the '{}' argument is not valid. The\"\n                        \" only supported keys for this task are {}\".format(\n                            key, arg, list(supported_steps.keys())\n                        )\n                    )\n\n                candidate_components = argument[key]\n                if not (\n                    isinstance(candidate_components, list) and candidate_components\n                ):\n                    raise ValueError(\n                        \"The provided value of the key '{}' in the '{}' argument is \"\n                        \"not valid. The value must be a non-empty list.\".format(\n                            key, arg\n                        )\n                    )\n\n                available_components = list(\n                    supported_steps[key]\n                    .get_available_components(\n                        dataset_properties=self.dataset_properties\n                    )\n                    .keys()\n                )\n                for component in candidate_components:\n                    if component not in available_components:\n                        raise ValueError(\n                            \"The provided component '{}' for the key '{}' in the '{}'\"\n                            \" argument is not valid. The supported components for the\"\n                            \" step '{}' for this task are {}\".format(\n                                component, key, arg, key, available_components\n                            )\n                        )\n\n\n### Dependency File: metafeature.py\nfrom abc import ABCMeta, abstractmethod\n\nimport time\nfrom io import StringIO\n\nimport arff\nimport scipy.sparse\n\n\nclass AbstractMetaFeature(object):\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def _calculate(cls, X, y, logger, feat_type):\n        pass\n\n    def __call__(self, X, y, logger, feat_type=None):\n        if feat_type is None:\n            feat_type = {i: \"numerical\" for i in range(X.shape[1])}\n        starttime = time.time()\n        try:\n            if scipy.sparse.issparse(X) and hasattr(self, \"_calculate_sparse\"):\n                value = self._calculate_sparse(X, y, logger, feat_type)\n            else:\n                value = self._calculate(X, y, logger, feat_type)\n            comment = \"\"\n        except MemoryError:\n            value = None\n            comment = \"Memory Error\"\n\n        endtime = time.time()\n        return MetaFeatureValue(\n            self.__class__.__name__,\n            self.type_,\n            0,\n            0,\n            value,\n            endtime - starttime,\n            comment=comment,\n        )\n\n\nclass MetaFeature(AbstractMetaFeature):\n    def __init__(self):\n        super(MetaFeature, self).__init__()\n        self.type_ = \"METAFEATURE\"\n\n\nclass HelperFunction(AbstractMetaFeature):\n    def __init__(self):\n        super(HelperFunction, self).__init__()\n        self.type_ = \"HELPERFUNCTION\"\n\n\nclass MetaFeatureValue(object):\n    def __init__(self, name, type_, fold, repeat, value, time, comment=\"\"):\n        self.name = name\n        self.type_ = type_\n        self.fold = fold\n        self.repeat = repeat\n        self.value = value\n        self.time = time\n        self.comment = comment\n\n    def to_arff_row(self):\n        if self.type_ == \"METAFEATURE\":\n            value = self.value\n        else:\n            value = \"?\"\n\n        return [\n            self.name,\n            self.type_,\n            self.fold,\n            self.repeat,\n            value,\n            self.time,\n            self.comment,\n        ]\n\n    def __repr__(self):\n        repr = (\n            \"%s (type: %s, fold: %d, repeat: %d, value: %s, time: %3.3f, \"\n            \"comment: %s)\"\n        )\n        repr = repr % tuple(\n            self.to_arff_row()[:4]\n            + [str(self.to_arff_row()[4])]\n            + self.to_arff_row()[5:]\n        )\n        return repr\n\n\nclass DatasetMetafeatures(object):\n    def __init__(self, dataset_name, metafeature_values):\n        self.dataset_name = dataset_name\n        self.metafeature_values = metafeature_values\n\n    def _get_arff(self):\n        output = dict()\n        output[\"relation\"] = \"metafeatures_%s\" % (self.dataset_name)\n        output[\"description\"] = \"\"\n        output[\"attributes\"] = [\n            (\"name\", \"STRING\"),\n            (\"type\", \"STRING\"),\n            (\"fold\", \"NUMERIC\"),\n            (\"repeat\", \"NUMERIC\"),\n            (\"value\", \"NUMERIC\"),\n            (\"time\", \"NUMERIC\"),\n            (\"comment\", \"STRING\"),\n        ]\n        output[\"data\"] = []\n\n        for key in sorted(self.metafeature_values):\n            output[\"data\"].append(self.metafeature_values[key].to_arff_row())\n        return output\n\n    def dumps(self):\n        return self._get_arff()\n\n    def dump(self, path_or_filehandle):\n        output = self._get_arff()\n\n        if isinstance(path_or_filehandle, str):\n            with open(path_or_filehandle, \"w\") as fh:\n                arff.dump(output, fh)\n        else:\n            arff.dump(output, path_or_filehandle)\n\n    @classmethod\n    def load(cls, path_or_filehandle):\n\n        if isinstance(path_or_filehandle, str):\n            with open(path_or_filehandle) as fh:\n                input = arff.load(fh)\n        else:\n            input = arff.load(path_or_filehandle)\n\n        dataset_name = input[\"relation\"].replace(\"metafeatures_\", \"\")\n        metafeature_values = []\n        for item in input[\"data\"]:\n            mf = MetaFeatureValue(*item)\n            metafeature_values.append(mf)\n\n        return cls(dataset_name, metafeature_values)\n\n    def __repr__(self, verbosity=0):\n        repr = StringIO()\n        repr.write(\"Metafeatures for dataset %s\\n\" % self.dataset_name)\n        for name in self.metafeature_values:\n            if verbosity == 0 and self.metafeature_values[name].type_ != \"METAFEATURE\":\n                continue\n            if verbosity == 0:\n                repr.write(\n                    \"  %s: %s\\n\" % (str(name), str(self.metafeature_values[name].value))\n                )\n            elif verbosity >= 1:\n                repr.write(\n                    \"  %s: %10s  (%10fs)\\n\"\n                    % (\n                        str(name),\n                        str(self.metafeature_values[name].value)[:10],\n                        self.metafeature_values[name].time,\n                    )\n                )\n\n            # Add the reason for a crash if one happened!\n            if verbosity > 1 and self.metafeature_values[name].comment:\n                repr.write(\"    %s\\n\" % self.metafeature_values[name].comment)\n\n        return repr.getvalue()\n\n    def keys(self):\n        return self.metafeature_values.keys()\n\n    def __getitem__(self, item):\n        return self.metafeature_values[item]\n\n\n### Dependency File: __init__.py\nfrom typing import Optional, Type\n\nimport os\nfrom collections import OrderedDict\n\nfrom ConfigSpace.configuration_space import ConfigurationSpace\nfrom ConfigSpace.hyperparameters import CategoricalHyperparameter\n\nfrom autosklearn.askl_typing import FEAT_TYPE_TYPE\n\nfrom ..base import (\n    AutoSklearnChoice,\n    AutoSklearnPreprocessingAlgorithm,\n    ThirdPartyComponents,\n    _addons,\n    find_components,\n)\n\nclassifier_directory = os.path.split(__file__)[0]\n_preprocessors = find_components(\n    __package__, classifier_directory, AutoSklearnPreprocessingAlgorithm\n)\nadditional_components = ThirdPartyComponents(AutoSklearnPreprocessingAlgorithm)\n_addons[\"feature_preprocessing\"] = additional_components\n\n\ndef add_preprocessor(preprocessor: Type[AutoSklearnPreprocessingAlgorithm]) -> None:\n    additional_components.add_component(preprocessor)\n\n\nclass FeaturePreprocessorChoice(AutoSklearnChoice):\n    @classmethod\n    def get_components(cls):\n        components = OrderedDict()\n        components.update(_preprocessors)\n        components.update(additional_components.components)\n        return components\n\n    def get_available_components(\n        self, dataset_properties=None, include=None, exclude=None\n    ):\n        if dataset_properties is None:\n            dataset_properties = {}\n\n        if include is not None and exclude is not None:\n            raise ValueError(\n                \"The argument include and exclude cannot be used together.\"\n            )\n\n        available_comp = self.get_components()\n\n        if include is not None:\n            for incl in include:\n                if incl not in available_comp:\n                    raise ValueError(\n                        \"Trying to include unknown component: \" \"%s\" % incl\n                    )\n\n        # TODO check for task type classification and/or regression!\n\n        components_dict = OrderedDict()\n        for name in available_comp:\n            if include is not None and name not in include:\n                continue\n            elif exclude is not None and name in exclude:\n                continue\n\n            entry = available_comp[name]\n\n            # Exclude itself to avoid infinite loop\n            if entry == FeaturePreprocessorChoice or hasattr(entry, \"get_components\"):\n                continue\n\n            target_type = dataset_properties[\"target_type\"]\n            if target_type == \"classification\":\n                if entry.get_properties()[\"handles_classification\"] is False:\n                    continue\n                if (\n                    dataset_properties.get(\"multiclass\") is True\n                    and entry.get_properties()[\"handles_multiclass\"] is False\n                ):\n                    continue\n                if (\n                    dataset_properties.get(\"multilabel\") is True\n                    and entry.get_properties()[\"handles_multilabel\"] is False\n                ):\n                    continue\n\n            elif target_type == \"regression\":\n                if entry.get_properties()[\"handles_regression\"] is False:\n                    continue\n                if (\n                    dataset_properties.get(\"multioutput\") is True\n                    and entry.get_properties()[\"handles_multioutput\"] is False\n                ):\n                    continue\n\n            else:\n                raise ValueError(\"Unknown target type %s\" % target_type)\n\n            components_dict[name] = entry\n\n        return components_dict\n\n    def get_hyperparameter_search_space(\n        self,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        dataset_properties=None,\n        default=None,\n        include=None,\n        exclude=None,\n    ):\n        cs = ConfigurationSpace()\n\n        if dataset_properties is None:\n            dataset_properties = {}\n\n        # Compile a list of legal preprocessors for this problem\n        available_preprocessors = self.get_available_components(\n            dataset_properties=dataset_properties, include=include, exclude=exclude\n        )\n\n        if len(available_preprocessors) == 0:\n            raise ValueError(\"No preprocessors found, please add NoPreprocessing\")\n\n        if default is None:\n            defaults = [\"no_preprocessing\", \"select_percentile\", \"pca\", \"truncatedSVD\"]\n            for default_ in defaults:\n                if default_ in available_preprocessors:\n                    default = default_\n                    break\n\n        preprocessor = CategoricalHyperparameter(\n            \"__choice__\", list(available_preprocessors.keys()), default_value=default\n        )\n        cs.add_hyperparameter(preprocessor)\n        for name in available_preprocessors:\n            preprocessor_configuration_space = available_preprocessors[\n                name\n            ].get_hyperparameter_search_space(dataset_properties=dataset_properties)\n            parent_hyperparameter = {\"parent\": preprocessor, \"value\": name}\n            cs.add_configuration_space(\n                name,\n                preprocessor_configuration_space,\n                parent_hyperparameter=parent_hyperparameter,\n            )\n\n        return cs\n\n    def transform(self, X):\n        return self.choice.transform(X)\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:43:24"}, {"role": "assistant", "content": "```python\nimport logging\nimport os\n\nimport pandas as pd\n\nimport autosklearn.pipeline.classification\nfrom autosklearn.metalearning.metalearning.meta_base import MetaBase\n\nimport unittest\n\n\nclass MetaBaseTest(unittest.TestCase):\n    _multiprocess_can_split_ = True\n\n    def setUp(self):\n        self.cwd = os.getcwd()\n        data_dir = os.path.dirname(__file__)\n        data_dir = os.path.join(data_dir, \"test_meta_base_data\")\n        os.chdir(data_dir)\n\n        pipeline = autosklearn.pipeline.classification.SimpleClassificationPipeline()\n        cs = pipeline.get_hyperparameter_search_space()\n\n        self.logger = logging.getLogger()\n        self.base = MetaBase(cs, data_dir, logger=self.logger)\n\n    def tearDown(self):\n        os.chdir(self.cwd)\n\n    def test_get_all_runs(self):\n        runs = self.base.get_all_runs()\n        self.assertIsInstance(runs, pd.DataFrame)\n        # TODO update this ASAP\n        self.assertEqual((125, 125), runs.shape)\n\n    def test_get_runs(self):\n        runs = self.base.get_runs(\"233\")\n        # TODO update this ASAP\n        self.assertEqual(125, len(runs))\n        self.assertIsInstance(runs, pd.Series)\n\n    def test_get_metafeatures_single_dataset(self):\n        mf = self.base.get_metafeatures(\"233\")\n        self.assertIsInstance(mf, pd.Series)\n        self.assertEqual(mf.name, \"233\")\n        self.assertEqual(mf.loc[\"NumberOfInstances\"], 2142.0)\n\n    def test_get_metafeatures_single_feature(self):\n        mf = self.base.get_metafeatures(features=\"NumberOfInstances\")\n        self.assertIsInstance(mf, pd.Series)\n        self.assertEqual(mf.shape, (132,))\n\n    def test_get_metafeatures_single_dataset_and_single_feature(self):\n        mf = self.base.get_metafeatures(\"233\", features=\"NumberOfInstances\")\n        self.assertEqual(mf.shape, ())\n\n    def test_get_metafeatures_multiple_datasets(self):\n        mf = self.base.get_metafeatures([\"233\", \"236\"])\n        self.assertIsInstance(mf, pd.DataFrame)\n        self.assertEqual(mf.shape, (2, 46))\n\n    def test_get_metafeatures_multiple_features(self):\n        mf = self.base.get_metafeatures(\n            features=[\"NumberOfInstances\", \"NumberOfClasses\"]\n        )\n        self.assertIsInstance(mf, pd.DataFrame)\n        self.assertEqual(mf.shape, (132, 2))\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: auto-sklearn\nTest File Path: auto-sklearn\\test_metrics\\test_metrics.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n# -*- encoding: utf-8 -*-\nimport numpy as np\n\n\ndef sanitize_array(array: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Replace NaN and Inf (there should not be any!)\n    :param array:\n    :return:\n    \"\"\"\n    a = np.ravel(array)\n    maxi = np.nanmax(a[np.isfinite(a)])\n    mini = np.nanmin(a[np.isfinite(a)])\n    array[array == float(\"inf\")] = maxi\n    array[array == float(\"-inf\")] = mini\n    mid = (maxi + mini) / 2\n    array[np.isnan(array)] = mid\n    return array\n\n\n### Source File Content:\nfrom abc import ABCMeta, abstractmethod\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Union\n\nimport collections\nfrom functools import partial\nfrom itertools import product\n\nimport numpy as np\nimport sklearn.metrics\nfrom sklearn.utils.multiclass import type_of_target\nfrom smac.utils.constants import MAXINT\n\nfrom autosklearn.constants import (\n    BINARY_CLASSIFICATION,\n    MULTICLASS_CLASSIFICATION,\n    MULTILABEL_CLASSIFICATION,\n    MULTIOUTPUT_REGRESSION,\n    REGRESSION,\n    REGRESSION_TASKS,\n    TASK_TYPES,\n)\nfrom autosklearn.data.target_validator import SUPPORTED_XDATA_TYPES\n\nfrom .util import sanitize_array\n\n\nclass Scorer(object, metaclass=ABCMeta):\n    def __init__(\n        self,\n        name: str,\n        score_func: Callable,\n        optimum: float,\n        worst_possible_result: float,\n        sign: float,\n        kwargs: Any,\n        needs_X: bool = False,\n    ) -> None:\n        self.name = name\n        self._kwargs = kwargs\n        self._score_func = score_func\n        self._optimum = optimum\n        self._needs_X = needs_X\n        self._worst_possible_result = worst_possible_result\n        self._sign = sign\n\n    @abstractmethod\n    def __call__(\n        self,\n        y_true: np.ndarray,\n        y_pred: np.ndarray,\n        *,\n        X_data: Optional[SUPPORTED_XDATA_TYPES] = None,\n        sample_weight: Optional[List[float]] = None,\n    ) -> float:\n        pass\n\n    def __repr__(self) -> str:\n        return self.name\n\n\nclass _PredictScorer(Scorer):\n    def __call__(\n        self,\n        y_true: np.ndarray,\n        y_pred: np.ndarray,\n        *,\n        X_data: Optional[SUPPORTED_XDATA_TYPES] = None,\n        sample_weight: Optional[List[float]] = None,\n    ) -> float:\n        \"\"\"Evaluate predicted target values for X relative to y_true.\n\n        Parameters\n        ----------\n        y_true : array-like\n            Gold standard target values for X.\n\n        y_pred : array-like, [n_samples x n_classes]\n            Model predictions\n\n        X_data : array-like [n_samples x n_features]\n            X data used to obtain the predictions: each row x_j corresponds to the input\n             used to obtain predictions y_j\n\n        sample_weight : array-like, optional (default=None)\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Score function applied to prediction of estimator on X.\n        \"\"\"\n        type_true = type_of_target(y_true)\n        if (\n            type_true == \"binary\"\n            and type_of_target(y_pred) == \"continuous\"\n            and len(y_pred.shape) == 1\n        ):\n            # For a pred scorer, no threshold, nor probability is required\n            # If y_true is binary, and y_pred is continuous\n            # it means that a rounding is necessary to obtain the binary class\n            y_pred = np.around(y_pred, decimals=0)\n        elif (\n            len(y_pred.shape) == 1 or y_pred.shape[1] == 1 or type_true == \"continuous\"\n        ):\n            # must be regression, all other task types would return at least\n            # two probabilities\n            pass\n        elif type_true in [\"binary\", \"multiclass\"]:\n            y_pred = np.argmax(y_pred, axis=1)\n        elif type_true == \"multilabel-indicator\":\n            y_pred[y_pred > 0.5] = 1.0\n            y_pred[y_pred <= 0.5] = 0.0\n        elif type_true == \"continuous-multioutput\":\n            pass\n        else:\n            raise ValueError(type_true)\n\n        scorer_kwargs = {}  # type: Dict[str, Union[List[float], np.ndarray]]\n        if sample_weight is not None:\n            scorer_kwargs[\"sample_weight\"] = sample_weight\n        if self._needs_X is True:\n            scorer_kwargs[\"X_data\"] = X_data\n\n        return self._sign * self._score_func(\n            y_true, y_pred, **scorer_kwargs, **self._kwargs\n        )\n\n\nclass _ProbaScorer(Scorer):\n    def __call__(\n        self,\n        y_true: np.ndarray,\n        y_pred: np.ndarray,\n        *,\n        X_data: Optional[SUPPORTED_XDATA_TYPES] = None,\n        sample_weight: Optional[List[float]] = None,\n    ) -> float:\n        \"\"\"Evaluate predicted probabilities for X relative to y_true.\n        Parameters\n        ----------\n        y_true : array-like\n            Gold standard target values for X. These must be class labels,\n            not probabilities.\n\n        y_pred : array-like, [n_samples x n_classes]\n            Model predictions\n\n        X_data : array-like [n_samples x n_features]\n            X data used to obtain the predictions: each row x_j corresponds to the input\n             used to obtain predictions y_j\n\n        sample_weight : array-like, optional (default=None)\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Score function applied to prediction of estimator on X.\n        \"\"\"\n\n        if self._score_func is sklearn.metrics.log_loss:\n            n_labels_pred = np.array(y_pred).reshape((len(y_pred), -1)).shape[1]\n            n_labels_test = len(np.unique(y_true))\n            if n_labels_pred != n_labels_test:\n                labels = list(range(n_labels_pred))\n                if sample_weight is not None:\n                    return self._sign * self._score_func(\n                        y_true,\n                        y_pred,\n                        sample_weight=sample_weight,\n                        labels=labels,\n                        **self._kwargs,\n                    )\n                else:\n                    return self._sign * self._score_func(\n                        y_true, y_pred, labels=labels, **self._kwargs\n                    )\n\n        scorer_kwargs = {}  # type: Dict[str, Union[List[float], np.ndarray]]\n        if sample_weight is not None:\n            scorer_kwargs[\"sample_weight\"] = sample_weight\n        if self._needs_X is True:\n            scorer_kwargs[\"X_data\"] = X_data\n\n        return self._sign * self._score_func(\n            y_true, y_pred, **scorer_kwargs, **self._kwargs\n        )\n\n\nclass _ThresholdScorer(Scorer):\n    def __call__(\n        self,\n        y_true: np.ndarray,\n        y_pred: np.ndarray,\n        *,\n        X_data: Optional[SUPPORTED_XDATA_TYPES] = None,\n        sample_weight: Optional[List[float]] = None,\n    ) -> float:\n        \"\"\"Evaluate decision function output for X relative to y_true.\n        Parameters\n        ----------\n        y_true : array-like\n            Gold standard target values for X. These must be class labels,\n            not probabilities.\n\n        y_pred : array-like, [n_samples x n_classes]\n            Model predictions\n\n        X_data : array-like [n_samples x n_features]\n            X data used to obtain the predictions: each row x_j corresponds to the input\n             used to obtain predictions y_j\n\n        sample_weight : array-like, optional (default=None)\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Score function applied to prediction of estimator on X.\n        \"\"\"\n        y_type = type_of_target(y_true)\n        if y_type not in (\"binary\", \"multilabel-indicator\"):\n            raise ValueError(\"{0} format is not supported\".format(y_type))\n\n        if y_type == \"binary\":\n            if y_pred.ndim > 1:\n                y_pred = y_pred[:, 1]\n        elif isinstance(y_pred, list):\n            y_pred = np.vstack([p[:, -1] for p in y_pred]).T\n\n        scorer_kwargs = {}  # type: Dict[str, Union[List[float], np.ndarray]]\n        if sample_weight is not None:\n            scorer_kwargs[\"sample_weight\"] = sample_weight\n        if self._needs_X is True:\n            scorer_kwargs[\"X_data\"] = X_data\n\n        return self._sign * self._score_func(\n            y_true, y_pred, **scorer_kwargs, **self._kwargs\n        )\n\n\ndef make_scorer(\n    name: str,\n    score_func: Callable,\n    *,\n    optimum: float = 1.0,\n    worst_possible_result: float = 0.0,\n    greater_is_better: bool = True,\n    needs_proba: bool = False,\n    needs_threshold: bool = False,\n    needs_X: bool = False,\n    **kwargs: Any,\n) -> Scorer:\n    \"\"\"Make a scorer from a performance metric or loss function.\n\n    Factory inspired by scikit-learn which wraps scikit-learn scoring functions\n    to be used in auto-sklearn.\n\n    Parameters\n    ----------\n    name: str\n        Descriptive name of the metric\n\n    score_func : callable\n        Score function (or loss function) with signature\n        ``score_func(y, y_pred, **kwargs)``.\n\n    optimum : int or float, default=1\n        The best score achievable by the score function, i.e. maximum in case of\n        scorer function and minimum in case of loss function.\n\n    worst_possible_result : int of float, default=0\n        The worst score achievable by the score function, i.e. minimum in case of\n        scorer function and maximum in case of loss function.\n\n    greater_is_better : boolean, default=True\n        Whether score_func is a score function (default), meaning high is good,\n        or a loss function, meaning low is good. In the latter case, the\n        scorer object will sign-flip the outcome of the score_func.\n\n    needs_proba : boolean, default=False\n        Whether score_func requires predict_proba to get probability estimates\n        out of a classifier.\n\n    needs_threshold : boolean, default=False\n        Whether score_func takes a continuous decision certainty.\n        This only works for binary classification.\n\n    needs_X : boolean, default=False\n        Whether score_func requires X in __call__ to compute a metric.\n\n    **kwargs : additional arguments\n        Additional parameters to be passed to score_func.\n\n    Returns\n    -------\n    scorer : callable\n        Callable object that returns a scalar score; greater is better or set\n        greater_is_better to False.\n    \"\"\"\n    sign = 1 if greater_is_better else -1\n    if needs_proba and needs_threshold:\n        raise ValueError(\n            \"Set either needs_proba or needs_threshold to True, but not both.\"\n        )\n\n    cls = None  # type: Any\n    if needs_proba:\n        cls = _ProbaScorer\n    elif needs_threshold:\n        cls = _ThresholdScorer\n    else:\n        cls = _PredictScorer\n    return cls(\n        name, score_func, optimum, worst_possible_result, sign, kwargs, needs_X=needs_X\n    )\n\n\n# Standard regression scores\nmean_absolute_error = make_scorer(\n    \"mean_absolute_error\",\n    sklearn.metrics.mean_absolute_error,\n    optimum=0,\n    worst_possible_result=MAXINT,\n    greater_is_better=False,\n)\nmean_squared_error = make_scorer(\n    \"mean_squared_error\",\n    sklearn.metrics.mean_squared_error,\n    optimum=0,\n    worst_possible_result=MAXINT,\n    greater_is_better=False,\n    squared=True,\n)\nroot_mean_squared_error = make_scorer(\n    \"root_mean_squared_error\",\n    sklearn.metrics.mean_squared_error,\n    optimum=0,\n    worst_possible_result=MAXINT,\n    greater_is_better=False,\n    squared=False,\n)\nmean_squared_log_error = make_scorer(\n    \"mean_squared_log_error\",\n    sklearn.metrics.mean_squared_log_error,\n    optimum=0,\n    worst_possible_result=MAXINT,\n    greater_is_better=False,\n)\nmedian_absolute_error = make_scorer(\n    \"median_absolute_error\",\n    sklearn.metrics.median_absolute_error,\n    optimum=0,\n    worst_possible_result=MAXINT,\n    greater_is_better=False,\n)\n\nr2 = make_scorer(\"r2\", sklearn.metrics.r2_score)\n\n# Standard Classification Scores\naccuracy = make_scorer(\"accuracy\", sklearn.metrics.accuracy_score)\nbalanced_accuracy = make_scorer(\n    \"balanced_accuracy\", sklearn.metrics.balanced_accuracy_score\n)\n\n# Score functions that need decision values\nroc_auc = make_scorer(\n    \"roc_auc\",\n    sklearn.metrics.roc_auc_score,\n    greater_is_better=True,\n    needs_threshold=True,\n)\naverage_precision = make_scorer(\n    \"average_precision\", sklearn.metrics.average_precision_score, needs_threshold=True\n)\n\n# NOTE: zero_division\n#\n#   Specified as the explicit default, see sklearn docs:\n#   https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn-metrics-precision-score\nprecision = make_scorer(\n    \"precision\", partial(sklearn.metrics.precision_score, zero_division=0)\n)\nrecall = make_scorer(\"recall\", partial(sklearn.metrics.recall_score, zero_division=0))\nf1 = make_scorer(\"f1\", partial(sklearn.metrics.f1_score, zero_division=0))\n\n# Score function for probabilistic classification\nlog_loss = make_scorer(\n    \"log_loss\",\n    sklearn.metrics.log_loss,\n    optimum=0,\n    worst_possible_result=MAXINT,\n    greater_is_better=False,\n    needs_proba=True,\n)\n# TODO what about mathews correlation coefficient etc?\n\n\nREGRESSION_METRICS = {\n    scorer.name: scorer\n    for scorer in [\n        mean_absolute_error,\n        mean_squared_error,\n        root_mean_squared_error,\n        mean_squared_log_error,\n        median_absolute_error,\n        r2,\n    ]\n}\n\nCLASSIFICATION_METRICS = {\n    scorer.name: scorer\n    for scorer in [accuracy, balanced_accuracy, roc_auc, average_precision, log_loss]\n}\n\n# NOTE: zero_division\n#\n#   Specified as the explicit default, see sklearn docs:\n#   https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn-metrics-precision-score\nfor (base_name, sklearn_metric), average in product(\n    [\n        (\"precision\", sklearn.metrics.precision_score),\n        (\"recall\", sklearn.metrics.recall_score),\n        (\"f1\", sklearn.metrics.f1_score),\n    ],\n    [\"macro\", \"micro\", \"samples\", \"weighted\"],\n):\n    name = f\"{base_name}_{average}\"\n    scorer = make_scorer(\n        name, partial(sklearn_metric, pos_label=None, average=average, zero_division=0)\n    )\n    globals()[name] = scorer  # Adds scorer to the module scope\n    CLASSIFICATION_METRICS[name] = scorer\n\n\ndef _validate_metrics(\n    metrics: Sequence[Scorer],\n    scoring_functions: Optional[List[Scorer]] = None,\n) -> None:\n    \"\"\"\n    Validate metrics given to Auto-sklearn. Raises an Exception in case of a problem.\n\n    metrics: Sequence[Scorer]\n        A list of objects that hosts a function to calculate how good the\n        prediction is according to the solution.\n    scoring_functions: Optional[List[Scorer]]\n        A list of metrics to calculate multiple losses\n    \"\"\"\n\n    to_score = list(metrics)\n    if scoring_functions:\n        to_score.extend(scoring_functions)\n\n    if len(metrics) == 0:\n        raise ValueError(\"Number of metrics to compute must be greater than zero.\")\n\n    metric_counter = collections.Counter(to_score)\n    metric_names_counter = collections.Counter(metric.name for metric in to_score)\n    if len(metric_counter) != len(metric_names_counter):\n        raise ValueError(\n            \"Error in metrics passed to Auto-sklearn. A metric name was used \"\n            \"multiple times for different metrics!\"\n        )\n\n\ndef calculate_scores(\n    solution: np.ndarray,\n    prediction: np.ndarray,\n    task_type: int,\n    metrics: Sequence[Scorer],\n    *,\n    X_data: Optional[SUPPORTED_XDATA_TYPES] = None,\n    scoring_functions: Optional[List[Scorer]] = None,\n) -> Dict[str, float]:\n    \"\"\"\n    Returns the scores (a magnitude that allows casting the\n    optimization problem as a maximization one) for the\n    given Auto-Sklearn Scorer objects.\n\n    Parameters\n    ----------\n    solution: np.ndarray\n        The ground truth of the targets\n    prediction: np.ndarray\n        The best estimate from the model, of the given targets\n    task_type: int\n        To understand if the problem task is classification\n        or regression\n    metrics: Sequence[Scorer]\n        A list of objects that hosts a function to calculate how good the\n        prediction is according to the solution.\n    X_data : array-like [n_samples x n_features]\n        X data used to obtain the predictions\n    scoring_functions: List[Scorer]\n        A list of metrics to calculate multiple losses\n    Returns\n    -------\n    Dict[str, float]\n    \"\"\"\n    if task_type not in TASK_TYPES:\n        raise NotImplementedError(task_type)\n\n    _validate_metrics(metrics=metrics, scoring_functions=scoring_functions)\n\n    to_score = list(metrics)\n    if scoring_functions:\n        to_score.extend(scoring_functions)\n\n    score_dict = dict()\n    if task_type in REGRESSION_TASKS:\n        for metric_ in to_score:\n\n            try:\n                score_dict[metric_.name] = _compute_single_scorer(\n                    metric=metric_,\n                    prediction=prediction,\n                    solution=solution,\n                    task_type=task_type,\n                    X_data=X_data,\n                )\n            except ValueError as e:\n                print(e, e.args[0])\n                if (\n                    e.args[0] == \"Mean Squared Logarithmic Error cannot be used when \"\n                    \"targets contain negative values.\"\n                ):\n                    continue\n                else:\n                    raise e\n\n    else:\n        for metric_ in to_score:\n\n            # TODO maybe annotate metrics to define which cases they can\n            # handle?\n\n            try:\n                score_dict[metric_.name] = _compute_single_scorer(\n                    metric=metric_,\n                    prediction=prediction,\n                    solution=solution,\n                    task_type=task_type,\n                    X_data=X_data,\n                )\n            except ValueError as e:\n                if e.args[0] == \"multiclass format is not supported\":\n                    continue\n                elif (\n                    e.args[0] == \"Samplewise metrics are not available \"\n                    \"outside of multilabel classification.\"\n                ):\n                    continue\n                elif (\n                    e.args[0] == \"Target is multiclass but \"\n                    \"average='binary'. Please choose another average \"\n                    \"setting, one of [None, 'micro', 'macro', 'weighted'].\"\n                ):\n                    continue\n                else:\n                    raise e\n\n    return score_dict\n\n\ndef calculate_loss(\n    solution: np.ndarray,\n    prediction: np.ndarray,\n    task_type: int,\n    metric: Scorer,\n    X_data: Optional[SUPPORTED_XDATA_TYPES] = None,\n) -> float:\n    \"\"\"Calculate the loss with a given metric\n\n    Parameters\n    ----------\n    solution: np.ndarray\n        The solutions\n\n    prediction: np.ndarray\n        The predictions generated\n\n    task_type: int\n        The task type of the problem\n\n    metric: Scorer\n        The metric to use\n\n    X_data: Optional[SUPPORTED_XDATA_TYPES]\n        X data used to obtain the predictions\n    \"\"\"\n    losses = calculate_losses(\n        solution=solution,\n        prediction=prediction,\n        task_type=task_type,\n        metrics=[metric],\n        X_data=X_data,\n    )\n    return losses[metric.name]\n\n\ndef calculate_losses(\n    solution: np.ndarray,\n    prediction: np.ndarray,\n    task_type: int,\n    metrics: Sequence[Scorer],\n    *,\n    X_data: Optional[SUPPORTED_XDATA_TYPES] = None,\n    scoring_functions: Optional[List[Scorer]] = None,\n) -> Dict[str, float]:\n    \"\"\"\n    Returns the losses (a magnitude that allows casting the\n    optimization problem as a minimization one) for the\n    given Auto-Sklearn Scorer objects.\n\n    Parameters\n    ----------\n    solution: np.ndarray\n        The ground truth of the targets\n    prediction: np.ndarray\n        The best estimate from the model, of the given targets\n    task_type: int\n        To understand if the problem task is classification\n        or regression\n    metrics: Sequence[Scorer]\n        A list of objects that hosts a function to calculate how good the\n        prediction is according to the solution.\n    X_data: Optional[SUPPORTED_XDATA_TYPES]\n        X data used to obtain the predictions\n    scoring_functions: List[Scorer]\n        A list of metrics to calculate multiple losses\n\n    Returns\n    -------\n    Dict[str, float]\n        A loss function for each of the provided scorer objects\n    \"\"\"\n    score = calculate_scores(\n        solution=solution,\n        prediction=prediction,\n        X_data=X_data,\n        task_type=task_type,\n        metrics=metrics,\n        scoring_functions=scoring_functions,\n    )\n    scoring_functions = scoring_functions if scoring_functions else []\n\n    # we expect a dict() object for which we should calculate the loss\n    loss_dict = dict()\n    for metric_ in scoring_functions + list(metrics):\n        # maybe metric argument is not in scoring_functions\n        # TODO: When metrics are annotated with type_of_target support\n        # we can remove this check\n        if metric_.name not in score:\n            continue\n        loss_dict[metric_.name] = metric_._optimum - score[metric_.name]\n    return loss_dict\n\n\ndef compute_single_metric(\n    metric: Scorer,\n    prediction: np.ndarray,\n    solution: np.ndarray,\n    task_type: int,\n    X_data: Optional[SUPPORTED_XDATA_TYPES] = None,\n) -> float:\n    \"\"\"\n    Returns a metric for the given Auto-Sklearn Scorer object.\n    It's direction is determined by the metric itself.\n\n    Parameters\n    ----------\n    solution: np.ndarray\n        The ground truth of the targets\n    prediction: np.ndarray\n        The best estimate from the model, of the given targets\n    task_type: int\n        To understand if the problem task is classification\n        or regression\n    metric: Scorer\n        Object that host a function to calculate how good the\n        prediction is according to the solution.\n    X_data : array-like [n_samples x n_features]\n        X data used to obtain the predictions\n\n    Returns\n    -------\n    float\n    \"\"\"\n    score = _compute_single_scorer(\n        solution=solution,\n        prediction=prediction,\n        metric=metric,\n        X_data=X_data,\n        task_type=task_type,\n    )\n    return metric._sign * score\n\n\ndef _compute_single_scorer(\n    metric: Scorer,\n    prediction: np.ndarray,\n    solution: np.ndarray,\n    task_type: int,\n    X_data: Optional[SUPPORTED_XDATA_TYPES] = None,\n) -> float:\n    \"\"\"\n    Returns a score (a magnitude that allows casting the\n    optimization problem as a maximization one) for the\n    given Auto-Sklearn Scorer object\n\n    Parameters\n    ----------\n    solution: np.ndarray\n        The ground truth of the targets\n    prediction: np.ndarray\n        The best estimate from the model, of the given targets\n    task_type: int\n        To understand if the problem task is classification\n        or regression\n    metric: Scorer\n        Object that host a function to calculate how good the\n        prediction is according to the solution.\n    X_data : array-like [n_samples x n_features]\n        X data used to obtain the predictions\n    Returns\n    -------\n    float\n    \"\"\"\n    if metric._needs_X:\n        if X_data is None:\n            raise ValueError(\n                f\"Metric {metric.name} needs X_data, but X_data is {X_data}\"\n            )\n        elif X_data.shape[0] != solution.shape[0]:\n            raise ValueError(\n                f\"X_data has wrong length. \"\n                f\"Should be {solution.shape[0]}, but is {X_data.shape[0]}\"\n            )\n        if task_type in REGRESSION_TASKS:\n            # TODO put this into the regression metric itself\n            cprediction = sanitize_array(prediction)\n            score = metric(solution, cprediction, X_data=X_data)\n        else:\n            score = metric(solution, prediction, X_data=X_data)\n        return score\n\n    if task_type in REGRESSION_TASKS:\n        # TODO put this into the regression metric itself\n        cprediction = sanitize_array(prediction)\n        score = metric(solution, cprediction)\n    else:\n        score = metric(solution, prediction)\n    return score\n\n\n# Must be at bottom so all metrics are defined\ndefault_metric_for_task: Dict[int, Scorer] = {\n    BINARY_CLASSIFICATION: CLASSIFICATION_METRICS[\"accuracy\"],\n    MULTICLASS_CLASSIFICATION: CLASSIFICATION_METRICS[\"accuracy\"],\n    MULTILABEL_CLASSIFICATION: CLASSIFICATION_METRICS[\"f1_macro\"],\n    REGRESSION: REGRESSION_METRICS[\"r2\"],\n    MULTIOUTPUT_REGRESSION: REGRESSION_METRICS[\"r2\"],\n}\n\n### Source File Dependency Files Content\n### Dependency File: target_validator.py\nfrom typing import List, Optional, Type, Union, cast\n\nimport logging\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.utils\nfrom pandas.api.types import is_numeric_dtype\nfrom pandas.core.dtypes.base import ExtensionDtype\nfrom scipy.sparse import spmatrix\nfrom sklearn.base import BaseEstimator\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.utils.multiclass import type_of_target\n\nfrom autosklearn.util.logging_ import PickableLoggerAdapter\n\nSUPPORTED_TARGET_TYPES = Union[List, pd.Series, pd.DataFrame, np.ndarray, spmatrix]\nSUPPORTED_XDATA_TYPES = Union[pd.Series, pd.DataFrame, np.ndarray, spmatrix]\n\n\nclass TargetValidator(BaseEstimator):\n    \"\"\"A class to pre-process targets.\n\n    It validates the data provided during fit to make sure it matches Sklearn\n    expectation as well as encoding the targets in case of classification\n\n    Attributes\n    ----------\n    is_classification: bool\n        A bool that indicates if the validator should operate in classification mode.\n        During classification, the targets are encoded.\n\n    encoder: Optional[BaseEstimator]\n        Host a encoder object if the data requires transformation (for example, if\n        provided a categorical column in a pandas DataFrame).\n\n    enc_columns: List[str]\n        List of columns that where encoded\n    \"\"\"\n\n    def __init__(\n        self,\n        is_classification: bool = False,\n        logger: Optional[PickableLoggerAdapter] = None,\n    ) -> None:\n        self.is_classification = is_classification\n\n        self.data_type = None  # type: Optional[type]\n\n        # NOTE: type update\n        #   Encoders don't seems to have a unified base class for\n        #   the methods 'transform'. Could make a `prototype` class that\n        #   duck types for the 'transform', 'fit_transform' methods\n        #   This is however fine while we only use ordinal encoder\n        self.encoder = None  # type: Optional[OrdinalEncoder]\n\n        self.out_dimensionality = None  # type: Optional[int]\n        self.type_of_target = None  # type: Optional[str]\n\n        self.logger = logger if logger is not None else logging.getLogger(__name__)\n\n        # Store the dtype for remapping to correct type\n        self.dtype = None  # type: Optional[Type]\n\n        self._is_fitted = False\n\n    def fit(\n        self,\n        y_train: Union[List, np.ndarray, pd.Series, pd.DataFrame],\n        y_test: Optional[Union[List, np.ndarray, pd.Series, pd.DataFrame]] = None,\n    ) -> \"TargetValidator\":\n        \"\"\"\n        Validates and fit a categorical encoder (if needed) to the targets\n        The supported data types are List, numpy arrays and pandas DataFrames.\n\n        Parameters\n        ----------\n            y_train: SUPPORTED_TARGET_TYPES\n                A set of targets set aside for training\n            y_test: Union[SUPPORTED_TARGET_TYPES]\n                A hold out set of data used of the targets. It is also used to fit the\n                categories of the encoder.\n        \"\"\"\n        # Check that the data is valid\n        self._check_data(y_train)\n\n        shape = np.shape(y_train)\n        if y_test is not None:\n            self._check_data(y_test)\n\n            if len(shape) != len(np.shape(y_test)) or (\n                len(shape) > 1 and (shape[1] != np.shape(y_test)[1])\n            ):\n                raise ValueError(\n                    \"The dimensionality of the train and test\"\n                    \" targets do not match\"\n                    f\" train {np.shape(y_train)}\"\n                    f\" != test {np.shape(y_test)}\"\n                )\n\n            if isinstance(y_train, pd.DataFrame):\n                if not isinstance(y_test, pd.DataFrame):\n                    y_test = pd.DataFrame(y_test)\n\n                if y_train.columns.tolist() != y_test.columns.tolist():\n                    raise ValueError(\n                        \"Train and test targets must both have the\"\n                        f\" same columns, yet y={y_train.columns}\"\n                        f\" and y_test={y_test.columns}\"\n                    )\n\n                if list(y_train.dtypes) != list(y_test.dtypes):\n                    raise ValueError(\n                        \"Train and test targets must both have the same dtypes\"\n                    )\n\n        if self.out_dimensionality is None:\n            self.out_dimensionality = 1 if len(shape) == 1 else shape[1]\n        else:\n            _n_outputs = 1 if len(shape) == 1 else shape[1]\n            if self.out_dimensionality != _n_outputs:\n                raise ValueError(\n                    \"Number of outputs changed from %d to %d!\"\n                    % (self.out_dimensionality, _n_outputs)\n                )\n\n        # Fit on the training data\n        self._fit(y_train, y_test)\n\n        self._is_fitted = True\n\n        return self\n\n    def _fit(\n        self,\n        y_train: Union[List, np.ndarray, pd.Series, pd.DataFrame],\n        y_test: Optional[Union[List, np.ndarray, pd.Series, pd.DataFrame]] = None,\n    ) -> \"TargetValidator\":\n        \"\"\"\n        If dealing with classification, this utility encodes the targets.\n\n        It does so by also using the classes from the test data, to prevent encoding\n        errors\n\n        Parameters\n        ----------\n            y_train: SUPPORTED_TARGET_TYPES\n                The labels of the current task. They are going to be encoded in case\n                of classification\n            y_test: Optional[SUPPORTED_TARGET_TYPES]\n                A holdout set of labels\n        \"\"\"\n        if not self.is_classification or self.type_of_target == \"multilabel-indicator\":\n            # Only fit an encoder for classification tasks\n            # Also, encoding multilabel indicator data makes the data multiclass\n            # Let the user employ a MultiLabelBinarizer if needed\n            self.encoder = None\n            return self\n\n        # Validate the shape of the input\n        shape = np.shape(y_train)\n        ndim = len(shape)\n        if ndim > 1 and shape[1] != 1:\n            # We should not reach this if statement, we check for type of targets before\n            raise ValueError(\n                \"Multi-dimensional classification is not yet\"\n                \" supported. Encoding multidimensional data\"\n                \" converts multiple columns to a 1 dimensional encoding.\"\n                f\" Data involved = {shape}/{self.type_of_target}\"\n            )\n\n        # Create the encoder\n        self.encoder = OrdinalEncoder(handle_unknown=\"error\")\n\n        # Clear typing to just numpy arrays and pandas\n        if isinstance(y_train, List):\n            y = np.asarray(y_train)\n        else:\n            y = y_train\n\n        # Attempt to store dtype if it's numeric, we use this in\n        # inverse_transform to try corretly restore it's dtype\n        if isinstance(y, pd.Series):\n            if isinstance(y.dtype, ExtensionDtype):\n                warnings.warn(\n                    \"Fitting transformer with a pandas series which\"\n                    f\" has the dtype {y.dtype}. Inverse transform\"\n                    \" may not be able preserve dtype when converting\"\n                    \" to np.ndarray\"\n                )\n            if is_numeric_dtype(y.dtype):\n                self.dtype = y.dtype\n        elif isinstance(y, pd.DataFrame):\n            if is_numeric_dtype(y.dtypes[0]):\n                self.dtype = y.dtypes[0]\n        else:\n            if is_numeric_dtype(y.dtype):\n                self.dtype = y.dtype\n\n        # Merge y_test and y_train for encoding\n        if y_test is not None:\n            if isinstance(y, (pd.Series, pd.DataFrame)):\n                if isinstance(y, pd.Series):\n                    y_test = pd.Series(y_test)\n                else:\n                    y_test = pd.DataFrame(y_test)\n\n                y = pd.concat([y, y_test], ignore_index=True, sort=False)\n            else:\n                y = np.concatenate([y, y_test])\n\n        # If one dimensional, we need to reshape it\n        # [1, 2, 3] -> [[1], [2], [3]]\n        if y.ndim == 1:\n            if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n                y = y.to_numpy()\n\n            y = y.reshape(-1, 1)\n\n        # Fit encoder\n        self.encoder.fit(y)\n\n        return self\n\n    def transform(\n        self,\n        y: Union[List, pd.Series, pd.DataFrame, np.ndarray],\n    ) -> np.ndarray:\n        \"\"\"\n        Validates and fit a categorical encoder (if needed) to the features.\n        The supported data types are List, numpy arrays and pandas DataFrames.\n\n        Parameters\n        ----------\n            y: SUPPORTED_TARGET_TYPES\n                A set of targets that are going to be encoded if the current task\n                is classification\n        Return\n        ------\n            np.ndarray:\n                The transformed array\n        \"\"\"\n        if not self._is_fitted:\n            raise NotFittedError(\"TargetValidator must have fit() called first\")\n\n        # Check the data here so we catch problems on new test data\n        self._check_data(y)\n\n        # Clear the types List and DataFrame off of y\n        if isinstance(y, List):\n            y_transformed = np.asarray(y)\n        elif isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            y_transformed = y.to_numpy()\n        else:\n            y_transformed = y\n\n        if self.encoder is None:\n            return y_transformed\n\n        # The Ordinal encoder expects a 2 dimensional input.\n        # The targets are 1 dimensional, so reshape to match the expected shape\n        if y_transformed.ndim == 1:\n            y_transformed = y_transformed.reshape(-1, 1)\n            y_transformed = self.encoder.transform(y_transformed).reshape(-1)\n        else:\n            y_transformed = self.encoder.transform(y_transformed)\n\n        # check_array ensures correct numerical features for array\n        y_transformed = sklearn.utils.check_array(\n            y_transformed,\n            force_all_finite=True,\n            accept_sparse=\"csr\",\n            ensure_2d=False,\n        )\n\n        # Ensure we flatten any arrays [[1], [2], [1], [1], ...] to [1,2,1,1,...]\n        if y_transformed.ndim == 2 and y_transformed.shape[1] == 1:\n            y_transformed = y_transformed.ravel()\n\n        return y_transformed\n\n    def inverse_transform(\n        self,\n        y: Union[List, pd.Series, pd.DataFrame, np.ndarray],\n    ) -> np.ndarray:\n        \"\"\"Revert any encoding transformation done on a target array.\n\n        Parameters\n        ----------\n        y: Union[List, np.ndarray, pd.DataFrame, pd.Series]\n            Target array to be transformed back to original form before encoding\n\n        Return\n        ------\n        np.ndarray:\n            The transformed array\n        \"\"\"\n        if not self._is_fitted:\n            raise NotFittedError(\"TargetValidator must have fit() called first\")\n\n        # If there's no encoder, just return it as the expected ndarray\n        if self.encoder is None:\n            return np.asarray(y)\n\n        # If it's one dimensional, we need to convert it\n        # [1,2,3] -> [[1], [2], [3]]\n        shape = np.shape(y)\n        if len(shape) == 1:\n\n            if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n                y = y.to_numpy()\n            elif isinstance(y, List):\n                y = np.asarray(y)\n\n            # Perform the transformation and reshape it\n            y = y.reshape(-1, 1)\n            y_inv = self.encoder.inverse_transform(y)\n            y_inv = y_inv.reshape(-1)\n\n        else:\n            # Otherwise we can can just invert it as expected\n            y_inv = self.encoder.inverse_transform(y)\n\n        # Inverse transform returns a numpy array of type object\n        # This breaks certain metrics as accuracy, which makes type_of_target be unknown\n        # If while fit a dtype was observed, we try to honor that dtype\n        if self.dtype is not None:\n            y_inv = y_inv.astype(self.dtype)\n\n        return y_inv\n\n    def is_single_column_target(self) -> bool:\n        \"\"\"Output is encoded with a single column encoding\"\"\"\n        return self.out_dimensionality == 1\n\n    def _check_data(\n        self,\n        y: SUPPORTED_TARGET_TYPES,\n    ) -> None:\n        \"\"\"\n        Perform dimensionality and data type checks on the targets\n\n        Parameters\n        ----------\n        y: Union[np.ndarray, pd.DataFrame, pd.Series]\n            A set of features whose dimensionality and data type is going to be checked\n        \"\"\"\n        if not isinstance(\n            y, (np.ndarray, pd.DataFrame, List, pd.Series)\n        ) and not isinstance(y, spmatrix):\n            raise ValueError(\n                \"Auto-sklearn only supports Numpy arrays, Pandas DataFrames,\"\n                \" pd.Series, sparse data and Python Lists as targets, yet, \"\n                \"the provided input is of type {}\".format(type(y))\n            )\n\n        if isinstance(y, spmatrix) and not np.issubdtype(y.dtype.type, np.number):\n            raise ValueError(\n                \"When providing a sparse matrix as targets, the only supported \"\n                \"values are numerical. Please consider using a dense\"\n                \" instead.\"\n            )\n\n        if self.data_type is None:\n            self.data_type = type(y)\n        if self.data_type != type(y):\n            self.logger.warning(\n                \"Auto-sklearn previously received targets of type %s \"\n                \"yet the current features have type %s. Changing the dtype \"\n                \"of inputs to an estimator might cause problems\"\n                % (\n                    str(self.data_type),\n                    str(type(y)),\n                ),\n            )\n\n        # No Nan is supported\n        has_nan_values = False\n        if hasattr(y, \"iloc\"):\n            has_nan_values = cast(pd.DataFrame, y).isnull().values.any()\n\n        if isinstance(y, spmatrix):\n            has_nan_values = not np.array_equal(y.data, y.data)\n        else:\n            # List and array like values are considered here\n            # np.isnan cannot work on strings, so we have to check for every element\n            # but NaN, are not equal to themselves:\n            has_nan_values = not np.array_equal(y, y)\n        if has_nan_values:\n            raise ValueError(\n                \"Target values cannot contain missing/NaN values. \"\n                \"This is not supported by scikit-learn. \"\n            )\n\n        # Pandas Series is not supported for multi-label indicator\n        # This format checks are done by type of target\n        try:\n            self.type_of_target = type_of_target(y)\n        except Exception as e:\n            raise ValueError(\n                \"The provided data could not be interpreted by Sklearn. \"\n                \"While determining the type of the targets via type_of_target \"\n                \"run into exception: {}.\".format(e)\n            )\n\n        supported_output_types = (\n            \"binary\",\n            \"continuous\",\n            \"continuous-multioutput\",\n            \"multiclass\",\n            \"multilabel-indicator\",\n            # Notice unknown/multiclass-multioutput are not supported\n            # This can only happen during testing only as estimators\n            # should filter out unsupported types.\n        )\n        if self.type_of_target not in supported_output_types:\n            raise ValueError(\n                \"Provided targets are not supported by Auto-Sklearn. \"\n                \"Provided type is {} whereas supported types are {}.\".format(\n                    self.type_of_target, supported_output_types\n                )\n            )\n\n    @property\n    def classes_(self) -> np.ndarray:\n        \"\"\"\n        Complies with scikit learn classes_ attribute,\n        which consist of a ndarray of shape (n_classes,)\n        where n_classes are the number of classes seen while fitting\n        a encoder to the targets.\n\n        Returns\n        -------\n        classes_: np.ndarray\n            The unique classes seen during encoding of a classifier\n        \"\"\"\n        if self.encoder is None:\n            return np.array([])\n        else:\n            return self.encoder.categories_[0]\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:43:24"}, {"role": "assistant", "content": "```python\nimport warnings\n\nimport numpy as np\nimport sklearn.metrics\n\nimport autosklearn.metrics\nfrom autosklearn.constants import BINARY_CLASSIFICATION, REGRESSION\nfrom autosklearn.metrics import (\n    calculate_losses,\n    calculate_scores,\n    compute_single_metric,\n)\n\nimport pytest\nimport unittest\n\n\nclass TestScorer(unittest.TestCase):\n    def test_needs_X(self):\n        y_true = np.array([0, 0, 1, 1])\n        y_pred = np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]])\n\n        def dummy_metric(y_true, y_pred, X_data=None, **kwargs):\n            if not np.array_equal(np.array([45]), X_data):\n                raise ValueError(f\"is {X_data}\")\n            return 1\n\n        scorer = autosklearn.metrics._PredictScorer(\n            \"accuracy\", dummy_metric, 1, 0, 1, {}, needs_X=True\n        )\n        scorer(y_true, y_pred, X_data=np.array([45]))\n\n        scorer_nox = autosklearn.metrics._PredictScorer(\n            \"accuracy\", dummy_metric, 1, 0, 1, {}, needs_X=False\n        )\n        with self.assertRaises(ValueError) as cm:\n            scorer_nox(y_true, y_pred, X_data=np.array([32]))\n        the_exception = cm.exception\n        # X_data is not forwarded\n        self.assertEqual(the_exception.args[0], \"is None\")\n\n        scorer_nox = autosklearn.metrics._PredictScorer(\n            \"accuracy\", sklearn.metrics.accuracy_score, 1, 0, 1, {}, needs_X=False\n        )\n        scorer_nox(y_true, y_pred, X_data=np.array([32]))\n\n\n@pytest.mark.parametrize(\n    \"y_pred, y_true, scorer, expected_score\",\n    [\n        (\n            np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]),\n            np.array([0, 0, 1, 1]),\n            autosklearn.metrics.accuracy,\n            1.0,\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]),\n            np.array([0, 0, 1, 1]),\n            autosklearn.metrics.accuracy,\n            0.5,\n        ),\n        (\n            np.array([[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]),\n            np.array([0, 0, 1, 1]),\n            autosklearn.metrics.balanced_accuracy,\n            0.5,\n        ),\n        (\n            np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            np.array([0, 1, 2]),\n            autosklearn.metrics.accuracy,\n            1.0,\n        ),\n        (\n            np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]),\n            np.array([0, 1, 2]),\n            autosklearn.metrics.accuracy,\n            0.333333333,\n        ),\n        (\n            np.array([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]),\n            np.array([0, 1, 2]),\n            autosklearn.metrics.accuracy,\n            0.333333333,\n        ),\n        (\n            np.array([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]),\n            np.array([0, 1, 2]),\n            autosklearn.metrics.balanced_accuracy,\n            0.333333333,\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]),\n            np.array([[0, 0], [0, 1], [1, 0], [1, 1]]),\n            autosklearn.metrics.accuracy,\n            1.0,\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]),\n            np.array([[0, 0], [0, 1], [1, 0], [1, 1]]),\n            autosklearn.metrics.accuracy,\n            0.25,\n        ),\n        (\n            np.array([[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]),\n            np.array([[0, 0], [0, 1], [1, 0], [1, 1]]),\n            autosklearn.metrics.accuracy,\n            0.25,\n        ),\n        (\n            np.arange(0, 1.01, 0.1),\n            np.arange(0, 1.01, 0.1),\n            autosklearn.metrics.r2,\n            1.0,\n        ),\n        (\n            np.ones(np.arange(0, 1.01, 0.1).shape) * np.mean(np.arange(0, 1.01, 0.1)),\n            np.arange(0, 1.01, 0.1),\n            autosklearn.metrics.r2,\n            0.0,\n        ),\n        (\n            np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]),\n            np.array([0, 0, 1, 1]),\n            autosklearn.metrics.log_loss,\n            0.0,\n        ),\n        (\n            np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            np.array([0, 1, 2]),\n            autosklearn.metrics.log_loss,\n            0.0,\n        ),\n        (\n            np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]),\n            np.array([0, 0, 1, 1]),\n            autosklearn.metrics.roc_auc,\n            1.0,\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]),\n            np.array([0, 0, 1, 1]),\n            autosklearn.metrics.roc_auc,\n            0.5,\n        ),\n        (\n            np.array([[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]),\n            np.array([0, 0, 1, 1]),\n            autosklearn.metrics.roc_auc,\n            0.5,\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]),\n            np.array([[0, 0], [0, 1], [1, 0], [1, 1]]),\n            autosklearn.metrics.roc_auc,\n            1.0,\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]),\n            np.array([[0, 0], [0, 1], [1, 0], [1, 1]]),\n            autosklearn.metrics.roc_auc,\n            0.5,\n        ),\n        (\n            np.array([[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]),\n            np.array([[0, 0], [0, 1], [1, 0], [1, 1]]),\n            autosklearn.metrics.roc_auc,\n            0.5,\n        ),\n    ],\n)\ndef test_scorer(\n    y_pred: np.ndarray,\n    y_true: np.ndarray,\n    scorer: autosklearn.metrics.Scorer,\n    expected_score: float,\n) -> None:\n    \"\"\"\n    Expects\n    -------\n    * Expected scores are equal to scores gained from implementing assembled scorers.\n    \"\"\"\n    result_score = scorer(y_true, y_pred)\n    assert expected_score == pytest.approx(result_score)\n\n\n@pytest.mark.parametrize(\n    \"y_pred, y_true, expected_score\",\n    [\n        (\n            np.arange(0, 1.01, 0.1) + 1.0,\n            np.arange(0, 1.01, 0.1),\n            -9.0,\n        ),\n        (\n            np.arange(0, 1.01, 0.1) + 0.5,\n            np.arange(0, 1.01, 0.1),\n            -1.5,\n        ),\n        (\n            np.arange(0, 1.01, 0.1),\n            np.arange(0, 1.01, 0.1),\n            1.0,\n        ),\n    ],\n)\ndef test_sign_flip(\n    y_pred: np.array,\n    y_true: np.array,\n    expected_score: float,\n) -> None:\n    \"\"\"\n    Expects\n    -------\n    * Flipping greater_is_better for r2_score result in flipped signs of its output.\n    \"\"\"\n    greater_true_scorer = autosklearn.metrics.make_scorer(\n        \"r2\", sklearn.metrics.r2_score, greater_is_better=True\n    )\n    greater_true_score = greater_true_scorer(y_true, y_pred)\n    assert expected_score == pytest.approx(greater_true_score)\n\n    greater_false_scorer = autosklearn.metrics.make_scorer(\n        \"r2\", sklearn.metrics.r2_score, greater_is_better=False\n    )\n    greater_false_score = greater_false_scorer(y_true, y_pred)\n    assert (expected_score * -1.0) == pytest.approx(greater_false_score)\n\n\ndef test_regression_metrics():\n    \"\"\"\n    Expects\n    -------\n    * Test metrics do not change output for autosklearn.metrics.REGRESSION_METRICS.\n    \"\"\"\n    for metric, scorer in autosklearn.metrics.REGRESSION_METRICS.items():\n        y_true = np.random.random(100).reshape((-1, 1))\n        y_pred = y_true.copy() + np.random.randn(100, 1) * 0.1\n\n        if metric == \"mean_squared_log_error\":\n            y_true = np.abs(y_true)\n            y_pred = np.abs(y_pred)\n\n        y_true_2 = y_true.copy()\n        y_pred_2 = y_pred.copy()\n        assert np.isfinite(scorer(y_true_2, y_pred_2))\n        np.testing.assert_array_almost_equal(y_true, y_true_2, err_msg=metric)\n        np.testing.assert_array_almost_equal(y_pred, y_pred_2, err_msg=metric)\n\n\ndef test_classification_metrics():\n    \"\"\"\n    Expects\n    -------\n    * Test metrics do not change output for autosklearn.metrics.CLASSIFICATION_METRICS.\n    \"\"\"\n    for metric, scorer in autosklearn.metrics.CLASSIFICATION_METRICS.items():\n        y_true = np.random.randint(0, 2, size=(100, 1))\n        y_pred = np.random.random(200).reshape((-1, 2))\n        y_pred = np.array([y_pred[i] / np.sum(y_pred[i]) for i in range(100)])\n\n        y_true_2 = y_true.copy()\n        y_pred_2 = y_pred.copy()\n        try:\n            assert np.isfinite(scorer(y_true_2, y_pred_2))\n            np.testing.assert_array_almost_equal(y_true, y_true_2, err_msg=metric)\n            np.testing.assert_array_almost_equal(y_pred, y_pred_2, err_msg=metric)\n        except ValueError as e:\n            if (\n                e.args[0] == \"Samplewise metrics are not available outside\"\n                \" of multilabel classification.\"\n            ):\n                pass\n            else:\n                raise e\n\n\ndef test_regression_all():\n    \"\"\"\n    Expects\n    -------\n    * Correct scores from REGRESSION_METRICS.\n    \"\"\"\n    for metric, scorer in autosklearn.metrics.REGRESSION_METRICS.items():\n        if scorer.name == \"mean_squared_log_error\":\n            continue\n\n        y_true = np.array([1, 2, 3, 4])\n\n        y_pred_list = [\n            np.array([1, 2, 3, 4]),\n            np.array([3, 4, 5, 6]),\n            np.array([-1, 0, -1, 0]),\n            np.array([-5, 10, 7, -3]),\n        ]\n\n        score_list = [scorer(y_true, y_pred) for y_pred in y_pred_list]\n\n        assert scorer._optimum == pytest.approx(score_list[0])\n        assert score_list == sorted(score_list, reverse=True)\n\n\ndef test_classification_binary():\n    \"\"\"\n    Expects\n    -------\n    * Correct scores from CLASSIFICATION_METRICS for binary classification.\n    \"\"\"\n    for metric, scorer in autosklearn.metrics.CLASSIFICATION_METRICS.items():\n        # Skip functions not applicable for binary classification.\n        # TODO: Average precision should work for binary classification,\n        # TODO: but its behavior is not right. When y_pred is completely\n        # TODO: wrong, it does return 0.5, but when it is not completely\n        # TODO: wrong, it returns value smaller than 0.5.\n        if metric in [\n            \"average_precision\",\n            \"precision_samples\",\n            \"recall_samples\",\n            \"f1_samples\",\n        ]:\n            continue\n\n        y_true = np.array([1.0, 1.0, 1.0, 0.0, 0.0, 0.0])\n\n        y_pred_list = [\n            np.array(\n                [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n            ),\n            np.array(\n                [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]\n            ),\n            np.array(\n                [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n            ),\n            np.array(\n                [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n            ),\n        ]\n\n        score_list = [scorer(y_true, y_pred) for y_pred in y_pred_list]\n\n        assert scorer._optimum == pytest.approx(score_list[0])\n        assert score_list == sorted(score_list, reverse=True)\n\n\ndef test_classification_multiclass():\n    \"\"\"\n    Expects\n    -------\n    * Correct scores from CLASSIFICATION_METRICS for multiclass classification.\n    \"\"\"\n    # The last check in this test has a mismatch between the number of\n    # labels predicted in y_pred and the number of labels in y_true.\n    # This triggers several warnings but we are aware.\n    #\n    # TODO convert to pytest with fixture\n    #\n    #   This test should be parameterized so we can identify which metrics\n    #   cause which warning specifically and rectify if needed.\n    ignored_warnings = [(UserWarning, \"y_pred contains classes not in y_true\")]\n\n    for metric, scorer in autosklearn.metrics.CLASSIFICATION_METRICS.items():\n        # Skip functions not applicable for multiclass classification.\n        if metric in [\n            \"roc_auc\",\n            \"average_precision\",\n            \"precision\",\n            \"recall\",\n            \"f1\",\n            \"precision_samples\",\n            \"recall_samples\",\n            \"f1_samples\",\n        ]:\n            continue\n\n        y_true = np.array([0.0, 0.0, 1.0, 1.0, 2.0])\n\n        y_pred_list = [\n            np.array(\n                [\n                    [1.0, 0.0, 0.0],\n                    [1.0, 0.0, 0.0],\n                    [0.0, 1.0, 0.0],\n                    [0.0, 1.0, 0.0],\n                    [0.0, 0.0, 1.0],\n                ]\n            ),\n            np.array(\n                [\n                    [1.0, 0.0, 0.0],\n                    [1.0, 0.0, 0.0],\n                    [1.0, 0.0, 0.0],\n                    [0.0, 1.0, 0.0],\n                    [0.0, 0.0, 1.0],\n                ]\n            ),\n            np.array(\n                [\n                    [0.0, 0.0, 1.0],\n                    [0.0, 1.0, 0.0],\n                    [1.0, 0.0, 0.0],\n                    [0.0, 1.0, 0.0],\n                    [0.0, 1.0, 0.0],\n                ]\n            ),\n            np.array(\n                [\n                    [0.0, 0.0, 1.0],\n                    [0.0, 0.0, 1.0],\n                    [1.0, 0.0, 0.0],\n                    [1.0, 0.0, 0.0],\n                    [0.0, 1.0, 0.0],\n                ]\n            ),\n        ]\n\n        score_list = [scorer(y_true, y_pred) for y_pred in y_pred_list]\n\n        assert scorer._optimum == pytest.approx(score_list[0])\n        assert score_list == sorted(score_list, reverse=True)\n\n        # less labels in the targets than in the predictions\n        y_true = np.array([0.0, 0.0, 1.0, 1.0])\n        y_pred = np.array(\n            [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]\n        )\n\n        with warnings.catch_warnings():\n            for category, message in ignored_warnings:\n                warnings.filterwarnings(\"ignore\", category=category, message=message)\n\n            score = scorer(y_true, y_pred)\n            assert np.isfinite(score)\n\n\ndef test_classification_multilabel():\n    \"\"\"\n    Expects\n    -------\n    * Correct scores from CLASSIFICATION_METRICS for multi-label classification.\n    \"\"\"\n    for metric, scorer in autosklearn.metrics.CLASSIFICATION_METRICS.items():\n        # Skip functions not applicable for multi-label classification.\n        if metric in [\n            \"roc_auc\",\n            \"log_loss\",\n            \"precision\",\n            \"recall\",\n            \"f1\",\n            \"balanced_accuracy\",\n        ]:\n            continue\n        y_true = np.array([[1, 0, 0], [1, 1, 0], [0, 1, 1], [1, 1, 1]])\n\n        y_pred_list = [\n            np.array([[1, 0, 0], [1, 1, 0], [0, 1, 1], [1, 1, 1]]),\n            np.array([[1, 0, 0], [0, 0, 1], [0, 1, 1], [1, 1, 1]]),\n            np.array([[1, 0, 0], [0, 0, 1], [1, 0, 1], [1, 1, 0]]),\n            np.array([[0, 1, 1], [0, 0, 1], [1, 0, 0], [0, 0, 0]]),\n        ]\n\n        score_list = [scorer(y_true, y_pred) for y_pred in y_pred_list]\n\n        assert scorer._optimum == pytest.approx(score_list[0])\n        assert score_list == sorted(score_list, reverse=True)\n\n\nclass TestCalculateScore(unittest.TestCase):\n    def test_unsupported_task_type(self):\n        y_true = np.array([1.0, 1.0, 1.0, 0.0, 0.0, 0.0])\n        y_pred = np.array(\n            [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n        )\n        scorer = autosklearn.metrics.accuracy\n\n        raised = False\n        try:\n            calculate_scores(y_true, y_pred, 6, scorer)\n        except NotImplementedError:\n            raised = True\n        self.assertTrue(raised)\n\n    def test_classification_scoring_functions(self):\n\n        scoring_functions = list(autosklearn.metrics.CLASSIFICATION_METRICS.values())\n        scoring_functions.remove(autosklearn.metrics.accuracy)\n        fail_metrics = [\"precision_samples\", \"recall_samples\", \"f1_samples\"]\n        success_metrics = list(autosklearn.metrics.CLASSIFICATION_METRICS.keys())\n        for metric in fail_metrics:\n            success_metrics.remove(metric)\n\n        y_true = np.array([1.0, 1.0, 1.0, 0.0, 0.0, 0.0])\n        y_pred = np.array(\n            [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n        )\n        score_dict = calculate_scores(\n            y_true,\n            y_pred,\n            BINARY_CLASSIFICATION,\n            [autosklearn.metrics.accuracy],\n            scoring_functions=scoring_functions,\n        )\n\n        self.assertIsInstance(score_dict, dict)\n        self.assertTrue(len(success_metrics), len(score_dict))\n        for metric in fail_metrics:\n            self.assertNotIn(metric, score_dict.keys())\n        for metric in success_metrics:\n            self.assertIn(metric, score_dict.keys())\n            self.assertAlmostEqual(\n                autosklearn.metrics.CLASSIFICATION_METRICS[metric]._optimum,\n                score_dict[metric],\n            )\n\n    def test_regression_scoring_functions(self):\n\n        scoring_functions = list(autosklearn.metrics.REGRESSION_METRICS.values())\n        scoring_functions.remove(autosklearn.metrics.root_mean_squared_error)\n\n        metrics = list(autosklearn.metrics.REGRESSION_METRICS.keys())\n        metrics.remove(\"mean_squared_log_error\")\n\n        y_true = np.array([1, 2, 3, -4])\n        y_pred = y_true.copy()\n\n        score_dict = calculate_scores(\n            y_true,\n            y_pred,\n            REGRESSION,\n            [autosklearn.metrics.root_mean_squared_error],\n            scoring_functions=scoring_functions,\n        )\n\n        self.assertIsInstance(score_dict, dict)\n        self.assertTrue(len(metrics), len(score_dict))\n        for metric in metrics:\n            self.assertIn(metric, score_dict.keys())\n            self.assertAlmostEqual(\n                autosklearn.metrics.REGRESSION_METRICS[metric]._optimum,\n                score_dict[metric],\n            )\n\n    def test_classification_only_metric(self):\n        y_true = np.array([1.0, 1.0, 1.0, 0.0, 0.0, 0.0])\n        y_pred = np.array(\n            [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n        )\n        scorer = autosklearn.metrics.accuracy\n\n        score = calculate_scores(y_true, y_pred, BINARY_CLASSIFICATION, [scorer])[\n            \"accuracy\"\n        ]\n\n        previous_score = scorer._optimum\n        self.assertAlmostEqual(score, previous_score)\n\n    def test_regression_only_metric(self):\n        y_true = np.array([1, 2, 3, 4])\n        y_pred = y_true.copy()\n        scorer = autosklearn.metrics.root_mean_squared_error\n\n        score = calculate_scores(y_true, y_pred, REGRESSION, [scorer])[\n            \"root_mean_squared_error\"\n        ]\n        previous_score = scorer._optimum\n        self.assertAlmostEqual(score, previous_score)\n\n\ndef test_calculate_losses():\n    # In a 0-1 ranged scorer, make sure that the loss\n    # has an expected positive value\n    y_pred = np.array([0, 1, 0, 1, 1, 1, 0, 0, 0, 0])\n    y_true = np.array([0, 1, 0, 1, 1, 0, 0, 0, 0, 0])\n    score = sklearn.metrics.accuracy_score(y_true, y_pred)\n    assert {\"accuracy\": pytest.approx(score)} == calculate_scores(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=BINARY_CLASSIFICATION,\n        metrics=[autosklearn.metrics.accuracy],\n    )\n    assert {\"accuracy\": pytest.approx(1.0 - score)} == calculate_losses(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=BINARY_CLASSIFICATION,\n        metrics=[autosklearn.metrics.accuracy],\n    )\n\n    # Test two metrics\n    score_dict = calculate_scores(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=BINARY_CLASSIFICATION,\n        metrics=[\n            autosklearn.metrics.accuracy,\n            autosklearn.metrics.balanced_accuracy,\n        ],\n    )\n    expected_score_dict = {\n        \"accuracy\": 0.9,\n        \"balanced_accuracy\": 0.9285714285714286,\n    }\n    loss_dict = calculate_losses(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=BINARY_CLASSIFICATION,\n        metrics=[\n            autosklearn.metrics.accuracy,\n            autosklearn.metrics.balanced_accuracy,\n        ],\n    )\n    for expected_metric, expected_score in expected_score_dict.items():\n        assert pytest.approx(expected_score) == score_dict[expected_metric]\n        assert pytest.approx(1 - expected_score) == loss_dict[expected_metric]\n\n    # Test no metric\n    with pytest.raises(\n        ValueError, match=\"Number of metrics to compute must be greater than zero.\"\n    ):\n        calculate_scores(\n            solution=y_true,\n            prediction=y_pred,\n            task_type=BINARY_CLASSIFICATION,\n            metrics=[],\n        )\n\n    with pytest.raises(\n        ValueError, match=\"Number of metrics to compute must be greater than zero.\"\n    ):\n        calculate_scores(\n            solution=y_true,\n            prediction=y_pred,\n            task_type=BINARY_CLASSIFICATION,\n            metrics=[],\n            scoring_functions=[\n                autosklearn.metrics.accuracy,\n                autosklearn.metrics.balanced_accuracy,\n            ],\n        )\n\n    # Test the same metric twice\n    accuracy_fixture = {\"accuracy\": pytest.approx(0.9)}\n    assert accuracy_fixture == calculate_scores(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=BINARY_CLASSIFICATION,\n        metrics=[autosklearn.metrics.accuracy, autosklearn.metrics.accuracy],\n    )\n    assert accuracy_fixture == calculate_scores(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=BINARY_CLASSIFICATION,\n        metrics=[autosklearn.metrics.accuracy],\n        scoring_functions=[autosklearn.metrics.accuracy],\n    )\n    assert accuracy_fixture == calculate_scores(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=BINARY_CLASSIFICATION,\n        metrics=[autosklearn.metrics.accuracy],\n        scoring_functions=[autosklearn.metrics.accuracy, autosklearn.metrics.accuracy],\n    )\n\n    # Test the same name for multiple metrics!\n    bogus_accuracy = autosklearn.metrics.make_scorer(\n        \"accuracy\",\n        score_func=sklearn.metrics.roc_auc_score,\n    )\n    with pytest.raises(ValueError, match=\"used multiple times\"):\n        calculate_scores(\n            solution=y_true,\n            prediction=y_pred,\n            task_type=BINARY_CLASSIFICATION,\n            metrics=[autosklearn.metrics.accuracy, bogus_accuracy],\n        )\n\n    # Test additional scoring functions\n    score_dict = calculate_scores(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=BINARY_CLASSIFICATION,\n        metrics=[autosklearn.metrics.accuracy],\n        scoring_functions=[\n            autosklearn.metrics.accuracy,\n            autosklearn.metrics.balanced_accuracy,\n        ],\n    )\n    expected_score_dict = {\n        \"accuracy\": 0.9,\n        \"balanced_accuracy\": 0.9285714285714286,\n    }\n    loss_dict = calculate_losses(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=BINARY_CLASSIFICATION,\n        metrics=[autosklearn.metrics.accuracy],\n        scoring_functions=[\n            autosklearn.metrics.accuracy,\n            autosklearn.metrics.balanced_accuracy,\n        ],\n    )\n    for expected_metric, expected_score in expected_score_dict.items():\n        assert pytest.approx(expected_score) == score_dict[expected_metric]\n        assert pytest.approx(1 - expected_score) == loss_dict[expected_metric]\n\n    # Lastly make sure that metrics whose optimum is zero\n    # are also properly working\n    y_true = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n    y_pred = np.array([0.11, 0.22, 0.33, 0.44, 0.55, 0.66])\n    score = sklearn.metrics.mean_squared_error(y_true, y_pred)\n    assert {\"mean_squared_error\": pytest.approx(0 - score)} == calculate_scores(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=REGRESSION,\n        metrics=[autosklearn.metrics.mean_squared_error],\n    )\n    assert {\"mean_squared_error\": pytest.approx(score)} == calculate_losses(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=REGRESSION,\n        metrics=[autosklearn.metrics.mean_squared_error],\n    )\n\n\ndef test_calculate_metric():\n    # metric to be maximized\n    y_pred = np.array([0, 1, 0, 1, 1, 1, 0, 0, 0, 0])\n    y_true = np.array([0, 1, 0, 1, 1, 0, 0, 0, 0, 0])\n    score = sklearn.metrics.accuracy_score(y_true, y_pred)\n    assert pytest.approx(score) == compute_single_metric(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=BINARY_CLASSIFICATION,\n        metric=autosklearn.metrics.accuracy,\n    )\n\n    # metric to be minimized\n    y_true = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n    y_pred = np.array([0.11, 0.22, 0.33, 0.44, 0.55, 0.66])\n    score = sklearn.metrics.mean_squared_error(y_true, y_pred)\n    assert pytest.approx(score) == compute_single_metric(\n        solution=y_true,\n        prediction=y_pred,\n        task_type=REGRESSION,\n        metric=autosklearn.metrics.mean_squared_error,\n    )\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: auto-sklearn\nTest File Path: auto-sklearn\\test_regression\\test_regression.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom typing import Optional, Union\n\nimport copy\nfrom itertools import product\n\nimport numpy as np\nfrom ConfigSpace.configuration_space import Configuration, ConfigurationSpace\nfrom ConfigSpace.forbidden import ForbiddenAndConjunction, ForbiddenEqualsClause\nfrom sklearn.base import RegressorMixin\n\nfrom autosklearn.askl_typing import FEAT_TYPE_TYPE\nfrom autosklearn.pipeline.base import BasePipeline\nfrom autosklearn.pipeline.components import (\n    feature_preprocessing as feature_preprocessing_components,\n)\nfrom autosklearn.pipeline.components import regression as regression_components\nfrom autosklearn.pipeline.components.data_preprocessing import DataPreprocessorChoice\nfrom autosklearn.pipeline.constants import SPARSE\n\n\nclass SimpleRegressionPipeline(RegressorMixin, BasePipeline):\n    \"\"\"This class implements the regression task.\n\n    It implements a pipeline, which includes one preprocessing step and one\n    regression algorithm. It can render a search space including all known\n    regression and preprocessing algorithms.\n\n    Contrary to the sklearn API it is not possible to enumerate the\n    possible parameters in the __init__ function because we only know the\n    available regressors at runtime. For this reason the user must\n    specifiy the parameters by passing an instance of\n    ConfigSpace.configuration_space.Configuration.\n\n    Parameters\n    ----------\n    config : ConfigSpace.configuration_space.Configuration\n        The configuration to evaluate.\n\n    random_state : Optional[int | RandomState]\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance\n        used by `np.random`.\n\n    Attributes\n    ----------\n    _estimator : The underlying scikit-learn regression model. This\n        variable is assigned after a call to the\n        :meth:`autosklearn.pipeline.regression.SimpleRegressionPipeline.fit`\n        method.\n\n    _preprocessor : The underlying scikit-learn preprocessing algorithm. This\n        variable is only assigned if a preprocessor is specified and\n        after a call to the\n        :meth:`autosklearn.pipeline.regression.SimpleRegressionPipeline.fit`\n        method.\n\n    See also\n    --------\n\n    References\n    ----------\n\n    Examples\n    --------\n\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[Configuration] = None,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        steps=None,\n        dataset_properties=None,\n        include=None,\n        exclude=None,\n        random_state: Optional[Union[int, np.random.RandomState]] = None,\n        init_params=None,\n    ):\n        self._output_dtype = np.float32\n        if dataset_properties is None:\n            dataset_properties = dict()\n        if \"target_type\" not in dataset_properties:\n            dataset_properties[\"target_type\"] = \"regression\"\n        super().__init__(\n            feat_type=feat_type,\n            config=config,\n            steps=steps,\n            dataset_properties=dataset_properties,\n            include=include,\n            exclude=exclude,\n            random_state=random_state,\n            init_params=init_params,\n        )\n\n    def fit_estimator(self, X, y, **fit_params):\n        self.y_max_ = np.nanmax(y)\n        self.y_min_ = np.nanmin(y)\n        return super(SimpleRegressionPipeline, self).fit_estimator(X, y, **fit_params)\n\n    def iterative_fit(self, X, y, n_iter=1, **fit_params):\n        self.y_max_ = np.nanmax(y)\n        self.y_min_ = np.nanmin(y)\n        return super(SimpleRegressionPipeline, self).iterative_fit(\n            X, y, n_iter=n_iter, **fit_params\n        )\n\n    def predict(self, X, batch_size=None):\n        \"\"\"Predict the classes using the selected model.\n\n        Predicted values are capped to approximately the maximum and minimum labels\n        seen during training.\n\n        Parameters\n        ----------\n        X : array-like, shape = (n_samples, n_features)\n\n        batch_size: int or None, defaults to None\n            batch_size controls whether the pipeline will be\n            called on small chunks of the data. Useful when calling the\n            predict method on the whole array X results in a MemoryError.\n\n        Returns\n        -------\n        array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n            Returns the predicted values\"\"\"\n        y = super().predict(X, batch_size=batch_size)\n\n        if self.y_max_ > 0:\n            y[y > (2 * self.y_max_)] = 2 * self.y_max_\n        elif self.y_max_ < 0:\n            y[y > (0.5 * self.y_max_)] = 0.5 * self.y_max_\n        if self.y_min_ < 0:\n            y[y < (2 * self.y_min_)] = 2 * self.y_min_\n        elif self.y_min_ > 0:\n            y[y < (0.5 * self.y_min_)] = 0.5 * self.y_min_\n\n        return y\n\n    def _get_hyperparameter_search_space(\n        self,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        include=None,\n        exclude=None,\n        dataset_properties=None,\n    ):\n        \"\"\"Return the configuration space for the CASH problem.\n\n        Parameters\n        ----------\n        include : dict\n            If include is given, only the modules specified for nodes\n            are used. Specify them by their module name; e.g., to include\n            only the SVM use :python:`include={'regressor':['svr']}`.\n\n        exclude : dict\n            If exclude is given, only the components specified for nodes\n            are used. Specify them by their module name; e.g., to include\n            all regressors except the SVM use\n            :python:`exclude=['regressor': 'svr']`.\n\n        Returns\n        -------\n        cs : ConfigSpace.configuration_space.Configuration\n            The configuration space describing the SimpleRegressionClassifier.\n        \"\"\"\n        cs = ConfigurationSpace()\n\n        if dataset_properties is None or not isinstance(dataset_properties, dict):\n            dataset_properties = dict()\n        if \"target_type\" not in dataset_properties:\n            dataset_properties[\"target_type\"] = \"regression\"\n        if dataset_properties[\"target_type\"] != \"regression\":\n            dataset_properties[\"target_type\"] = \"regression\"\n\n        if \"sparse\" not in dataset_properties:\n            # This dataset is probably dense\n            dataset_properties[\"sparse\"] = False\n\n        cs = self._get_base_search_space(\n            cs=cs,\n            feat_type=feat_type,\n            dataset_properties=dataset_properties,\n            exclude=exclude,\n            include=include,\n            pipeline=self.steps,\n        )\n\n        regressors = cs.get_hyperparameter(\"regressor:__choice__\").choices\n        preprocessors = cs.get_hyperparameter(\"feature_preprocessor:__choice__\").choices\n        available_regressors = self._final_estimator.get_available_components(\n            dataset_properties\n        )\n\n        possible_default_regressor = copy.copy(list(available_regressors.keys()))\n        default = cs.get_hyperparameter(\"regressor:__choice__\").default_value\n        del possible_default_regressor[possible_default_regressor.index(default)]\n\n        # A regressor which can handle sparse data after the densifier is\n        # forbidden for memory issues\n        for key in regressors:\n            if (\n                SPARSE\n                in available_regressors[key].get_properties(dataset_properties=None)[\n                    \"input\"\n                ]\n            ):\n                if \"densifier\" in preprocessors:\n                    while True:\n                        try:\n                            forb_reg = ForbiddenEqualsClause(\n                                cs.get_hyperparameter(\"regressor:__choice__\"), key\n                            )\n                            forb_fpp = ForbiddenEqualsClause(\n                                cs.get_hyperparameter(\n                                    \"feature_preprocessor:__choice__\"\n                                ),\n                                \"densifier\",\n                            )\n                            cs.add_forbidden_clause(\n                                ForbiddenAndConjunction(forb_reg, forb_fpp)\n                            )\n                            # Success\n                            break\n                        except ValueError:\n                            # Change the default and try again\n                            try:\n                                default = possible_default_regressor.pop()\n                            except IndexError:\n                                raise ValueError(\n                                    \"Cannot find a legal default configuration.\"\n                                )\n                            cs.get_hyperparameter(\n                                \"regressor:__choice__\"\n                            ).default_value = default\n\n        # which would take too long\n        # Combinations of tree-based models with feature learning:\n        regressors_ = [\n            \"adaboost\",\n            \"ard_regression\",\n            \"decision_tree\",\n            \"extra_trees\",\n            \"gaussian_process\",\n            \"gradient_boosting\",\n            \"k_nearest_neighbors\",\n            \"libsvm_svr\",\n            \"mlp\",\n            \"random_forest\",\n        ]\n        feature_learning_ = [\"kitchen_sinks\", \"kernel_pca\", \"nystroem_sampler\"]\n\n        for r, f in product(regressors_, feature_learning_):\n            if r not in regressors:\n                continue\n            if f not in preprocessors:\n                continue\n            while True:\n                try:\n                    cs.add_forbidden_clause(\n                        ForbiddenAndConjunction(\n                            ForbiddenEqualsClause(\n                                cs.get_hyperparameter(\"regressor:__choice__\"), r\n                            ),\n                            ForbiddenEqualsClause(\n                                cs.get_hyperparameter(\n                                    \"feature_preprocessor:__choice__\"\n                                ),\n                                f,\n                            ),\n                        )\n                    )\n                    break\n                except KeyError:\n                    break\n                except ValueError:\n                    # Change the default and try again\n                    try:\n                        default = possible_default_regressor.pop()\n                    except IndexError:\n                        raise ValueError(\"Cannot find a legal default configuration.\")\n                    cs.get_hyperparameter(\n                        \"regressor:__choice__\"\n                    ).default_value = default\n\n        self.configuration_space = cs\n        self.dataset_properties = dataset_properties\n        return cs\n\n    def _get_estimator_components(self):\n        return regression_components._regressors\n\n    def _get_pipeline_steps(\n        self, dataset_properties, feat_type: Optional[FEAT_TYPE_TYPE] = None\n    ):\n        steps = []\n\n        default_dataset_properties = {\"target_type\": \"regression\"}\n        if dataset_properties is not None and isinstance(dataset_properties, dict):\n            default_dataset_properties.update(dataset_properties)\n\n        steps.extend(\n            [\n                [\n                    \"data_preprocessor\",\n                    DataPreprocessorChoice(\n                        feat_type=feat_type,\n                        dataset_properties=default_dataset_properties,\n                        random_state=self.random_state,\n                    ),\n                ],\n                [\n                    \"feature_preprocessor\",\n                    feature_preprocessing_components.FeaturePreprocessorChoice(\n                        feat_type=feat_type,\n                        dataset_properties=default_dataset_properties,\n                        random_state=self.random_state,\n                    ),\n                ],\n                [\n                    \"regressor\",\n                    regression_components.RegressorChoice(\n                        feat_type=feat_type,\n                        dataset_properties=default_dataset_properties,\n                        random_state=self.random_state,\n                    ),\n                ],\n            ]\n        )\n\n        return steps\n\n    def _get_estimator_hyperparameter_name(self):\n        return \"regressor\"\n\n### Source File Dependency Files Content\n### Dependency File: askl_typing.py\nfrom typing import Dict, Union\n\nFEAT_TYPE_TYPE = Dict[Union[str, int], str]\n\n\n### Dependency File: base.py\nfrom abc import ABCMeta\nfrom typing import Dict, Optional, Union\n\nimport numpy as np\nimport scipy.sparse\nfrom ConfigSpace import Configuration\nfrom sklearn.pipeline import Pipeline\n\nimport autosklearn.pipeline.create_searchspace_util\nfrom autosklearn.askl_typing import FEAT_TYPE_TYPE\n\nfrom .components.base import AutoSklearnChoice, AutoSklearnComponent\n\nDATASET_PROPERTIES_TYPE = Dict[str, Union[str, int, bool]]\nPIPELINE_DATA_DTYPE = Union[\n    np.ndarray,\n    scipy.sparse.bsr_matrix,\n    scipy.sparse.coo_matrix,\n    scipy.sparse.csc_matrix,\n    scipy.sparse.csr_matrix,\n    scipy.sparse.dia_matrix,\n    scipy.sparse.dok_matrix,\n    scipy.sparse.lil_matrix,\n]\n\n\nclass BasePipeline(Pipeline):\n    \"\"\"Base class for all pipeline objects.\n\n    Notes\n    -----\n    This class should not be instantiated, only subclassed.\"\"\"\n\n    __metaclass__ = ABCMeta\n\n    def __init__(\n        self,\n        config=None,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        steps=None,\n        dataset_properties=None,\n        include=None,\n        exclude=None,\n        random_state=None,\n        init_params=None,\n    ):\n\n        self.init_params = init_params if init_params is not None else {}\n        self.include = include if include is not None else {}\n        self.exclude = exclude if exclude is not None else {}\n        self.dataset_properties = (\n            dataset_properties if dataset_properties is not None else {}\n        )\n        self.random_state = random_state\n        self.feat_type = feat_type\n\n        if steps is None:\n            self.steps = self._get_pipeline_steps(\n                feat_type=feat_type, dataset_properties=dataset_properties\n            )\n        else:\n            self.steps = steps\n\n        self._validate_include_exclude_params()\n\n        self.config_space = self.get_hyperparameter_search_space(feat_type=feat_type)\n\n        if config is None:\n            self.config = self.config_space.get_default_configuration()\n        else:\n            if isinstance(config, dict):\n                config = Configuration(self.config_space, config)\n            if self.config_space != config.configuration_space:\n                print(self.config_space._children)\n                print(config.configuration_space._children)\n                import difflib\n\n                diff = difflib.unified_diff(\n                    str(self.config_space).splitlines(),\n                    str(config.configuration_space).splitlines(),\n                )\n                diff = \"\\n\".join(diff)\n                raise ValueError(\n                    \"Configuration passed does not come from the \"\n                    \"same configuration space. Differences are: \"\n                    \"%s\" % diff\n                )\n            self.config = config\n\n        self.set_hyperparameters(\n            self.config, feat_type=feat_type, init_params=init_params\n        )\n\n        super().__init__(steps=self.steps)\n\n        self._additional_run_info = {}\n\n    def fit(self, X, y, **fit_params):\n        \"\"\"Fit the selected algorithm to the training data.\n\n        Parameters\n        ----------\n        X : array-like or sparse, shape = (n_samples, n_features)\n            Training data. The preferred type of the matrix (dense or sparse)\n            depends on the estimator selected.\n\n        y : array-like\n            Targets\n\n        fit_params : dict\n            See the documentation of sklearn.pipeline.Pipeline for formatting\n            instructions.\n\n        Returns\n        -------\n        self : returns an instance of self.\n\n        Raises\n        ------\n        NoModelException\n            NoModelException is raised if fit() is called without specifying\n            a classification algorithm first.\n        \"\"\"\n        X, fit_params = self.fit_transformer(X, y, **fit_params)\n        self.fit_estimator(X, y, **fit_params)\n        return self\n\n    def fit_transformer(self, X, y, fit_params=None):\n        self.num_targets = 1 if len(y.shape) == 1 else y.shape[1]\n        if fit_params is None:\n            fit_params = {}\n        fit_params = {\n            key.replace(\":\", \"__\"): value for key, value in fit_params.items()\n        }\n        fit_params_steps = self._check_fit_params(**fit_params)\n        Xt = self._fit(X, y, **fit_params_steps)\n        return Xt, fit_params_steps[self.steps[-1][0]]\n\n    def fit_estimator(self, X, y, **fit_params):\n        fit_params = {\n            key.replace(\":\", \"__\"): value for key, value in fit_params.items()\n        }\n        self._final_estimator.fit(X, y, **fit_params)\n        return self\n\n    def iterative_fit(self, X, y, n_iter=1, **fit_params):\n        self._final_estimator.iterative_fit(X, y, n_iter=n_iter, **fit_params)\n\n    def estimator_supports_iterative_fit(self):\n        return self._final_estimator.estimator_supports_iterative_fit()\n\n    def get_max_iter(self):\n        if self.estimator_supports_iterative_fit():\n            return self._final_estimator.get_max_iter()\n        else:\n            raise NotImplementedError()\n\n    def configuration_fully_fitted(self):\n        return self._final_estimator.configuration_fully_fitted()\n\n    def get_current_iter(self):\n        return self._final_estimator.get_current_iter()\n\n    def predict(self, X, batch_size=None):\n        \"\"\"Predict the classes using the selected model.\n\n        Parameters\n        ----------\n        X : array-like, shape = (n_samples, n_features)\n\n        batch_size: int or None, defaults to None\n            batch_size controls whether the pipeline will be\n            called on small chunks of the data. Useful when calling the\n            predict method on the whole array X results in a MemoryError.\n\n        Returns\n        -------\n        array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n            Returns the predicted values\"\"\"\n\n        if batch_size is None:\n            return super().predict(X).astype(self._output_dtype)\n        else:\n            if not isinstance(batch_size, int):\n                raise ValueError(\n                    \"Argument 'batch_size' must be of type int, \"\n                    \"but is '%s'\" % type(batch_size)\n                )\n            if batch_size <= 0:\n                raise ValueError(\n                    \"Argument 'batch_size' must be positive, \" \"but is %d\" % batch_size\n                )\n\n            else:\n                if self.num_targets == 1:\n                    y = np.zeros((X.shape[0],), dtype=self._output_dtype)\n                else:\n                    y = np.zeros(\n                        (X.shape[0], self.num_targets), dtype=self._output_dtype\n                    )\n\n                # Copied and adapted from the scikit-learn GP code\n                for k in range(max(1, int(np.ceil(float(X.shape[0]) / batch_size)))):\n                    batch_from = k * batch_size\n                    batch_to = min([(k + 1) * batch_size, X.shape[0]])\n                    y[batch_from:batch_to] = self.predict(\n                        X[batch_from:batch_to], batch_size=None\n                    )\n\n                return y\n\n    def set_hyperparameters(\n        self,\n        configuration,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        init_params=None,\n    ):\n        self.config = configuration\n\n        for node_idx, n_ in enumerate(self.steps):\n            node_name, node = n_\n\n            sub_configuration_space = node.get_hyperparameter_search_space(\n                feat_type=feat_type, dataset_properties=self.dataset_properties\n            )\n            sub_config_dict = {}\n            for param in configuration:\n                if param.startswith(\"%s:\" % node_name):\n                    value = configuration[param]\n                    new_name = param.replace(\"%s:\" % node_name, \"\", 1)\n                    sub_config_dict[new_name] = value\n\n            sub_configuration = Configuration(\n                sub_configuration_space, values=sub_config_dict\n            )\n\n            if init_params is not None:\n                sub_init_params_dict = {}\n                for param in init_params:\n                    if param.startswith(\"%s:\" % node_name):\n                        value = init_params[param]\n                        new_name = param.replace(\"%s:\" % node_name, \"\", 1)\n                        sub_init_params_dict[new_name] = value\n            else:\n                sub_init_params_dict = None\n\n            if isinstance(\n                node, (AutoSklearnChoice, AutoSklearnComponent, BasePipeline)\n            ):\n                node.set_hyperparameters(\n                    feat_type=feat_type,\n                    configuration=sub_configuration,\n                    init_params=sub_init_params_dict,\n                )\n            else:\n                raise NotImplementedError(\"Not supported yet!\")\n\n        # In-code check to make sure init params\n        # is checked after pipeline creation\n        self._check_init_params_honored(init_params)\n\n        return self\n\n    def get_hyperparameter_search_space(\n        self, feat_type: Optional[FEAT_TYPE_TYPE] = None, dataset_properties=None\n    ):\n        \"\"\"Return the configuration space for the CASH problem.\n\n        Returns\n        -------\n        cs : ConfigSpace.configuration_space.Configuration\n            The configuration space describing the AutoSklearnClassifier.\n\n        \"\"\"\n        if not hasattr(self, \"config_space\") or self.config_space is None:\n            self.config_space = self._get_hyperparameter_search_space(\n                feat_type=feat_type,\n                include=self.include,\n                exclude=self.exclude,\n                dataset_properties=self.dataset_properties,\n            )\n        return self.config_space\n\n    def _get_hyperparameter_search_space(\n        self,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        include=None,\n        exclude=None,\n        dataset_properties=None,\n    ):\n        \"\"\"Return the configuration space for the CASH problem.\n\n        This method should be called by the method\n        get_hyperparameter_search_space of a subclass. After the subclass\n        assembles a list of available estimators and preprocessor components,\n        _get_hyperparameter_search_space can be called to do the work of\n        creating the actual\n        ConfigSpace.configuration_space.ConfigurationSpace object.\n\n        Parameters\n        ----------\n        feat_type: dict\n            python dictionary which maps the columns of the dataset to the data types\n\n        estimator_name : str\n            Name of the estimator hyperparameter which will be used in the\n            configuration space. For a classification task, this would be\n            'classifier'.\n\n        estimator_components : dict {name: component}\n            Dictionary with all estimator components to be included in the\n            configuration space.\n\n        preprocessor_components : dict {name: component}\n            Dictionary with all preprocessor components to be included in the\n            configuration space. .\n\n        always_active : list of str\n            A list of components which will always be active in the pipeline.\n            This is useful for components like imputation which have\n            hyperparameters to be configured, but which do not have any parent.\n\n        default_estimator : str\n            Default value for the estimator hyperparameter.\n\n        Returns\n        -------\n        cs : ConfigSpace.configuration_space.Configuration\n            The configuration space describing the AutoSklearnClassifier.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _get_base_search_space(\n        self,\n        cs,\n        dataset_properties,\n        include,\n        exclude,\n        pipeline,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n    ):\n        if include is None:\n            if self.include is None:\n                include = {}\n            else:\n                include = self.include\n\n        keys = [pair[0] for pair in pipeline]\n        for key in include:\n            if key not in keys:\n                raise ValueError(\n                    \"Invalid key in include: %s; should be one \" \"of %s\" % (key, keys)\n                )\n\n        if exclude is None:\n            if self.exclude is None:\n                exclude = {}\n            else:\n                exclude = self.exclude\n\n        keys = [pair[0] for pair in pipeline]\n        for key in exclude:\n            if key not in keys:\n                raise ValueError(\n                    \"Invalid key in exclude: %s; should be one \" \"of %s\" % (key, keys)\n                )\n\n        if \"sparse\" not in dataset_properties:\n            # This dataset is probably dense\n            dataset_properties[\"sparse\"] = False\n        if \"signed\" not in dataset_properties:\n            # This dataset probably contains unsigned data\n            dataset_properties[\"signed\"] = False\n\n        matches = autosklearn.pipeline.create_searchspace_util.get_match_array(\n            pipeline=pipeline,\n            dataset_properties=dataset_properties,\n            include=include,\n            exclude=exclude,\n        )\n\n        # Now we have only legal combinations at this step of the pipeline\n        # Simple sanity checks\n        assert np.sum(matches) != 0, \"No valid pipeline found.\"\n\n        assert np.sum(matches) <= np.size(\n            matches\n        ), \"'matches' is not binary; %s <= %d, %s\" % (\n            str(np.sum(matches)),\n            np.size(matches),\n            str(matches.shape),\n        )\n\n        # Iterate each dimension of the matches array (each step of the\n        # pipeline) to see if we can add a hyperparameter for that step\n        for node_idx, n_ in enumerate(pipeline):\n            node_name, node = n_\n\n            is_choice = isinstance(node, AutoSklearnChoice)\n\n            # if the node isn't a choice we can add it immediately because it\n            #  must be active (if it wasn't, np.sum(matches) would be zero\n            if not is_choice:\n                cs.add_configuration_space(\n                    node_name,\n                    node.get_hyperparameter_search_space(\n                        dataset_properties=dataset_properties, feat_type=feat_type\n                    ),\n                )\n            # If the node is a choice, we have to figure out which of its\n            #  choices are actually legal choices\n            else:\n                choices_list = (\n                    autosklearn.pipeline.create_searchspace_util.find_active_choices(\n                        matches,\n                        node,\n                        node_idx,\n                        dataset_properties,\n                        include.get(node_name),\n                        exclude.get(node_name),\n                    )\n                )\n                sub_config_space = node.get_hyperparameter_search_space(\n                    feat_type=feat_type,\n                    dataset_properties=dataset_properties,\n                    include=choices_list,\n                )\n                cs.add_configuration_space(node_name, sub_config_space)\n\n        # And now add forbidden parameter configurations\n        # According to matches\n        if np.sum(matches) < np.size(matches):\n            cs = autosklearn.pipeline.create_searchspace_util.add_forbidden(\n                conf_space=cs,\n                pipeline=pipeline,\n                matches=matches,\n                dataset_properties=dataset_properties,\n                include=include,\n                exclude=exclude,\n            )\n\n        return cs\n\n    def _check_init_params_honored(self, init_params):\n        \"\"\"\n        Makes sure that init params is honored at the implementation level\n        \"\"\"\n        if init_params is None or len(init_params) < 1:\n            # None/empty dict, so no further check required\n            return\n\n        # There is the scenario, where instance is passed as an argument to the\n        # init_params 'instance': '{\"task_id\": \"73543c4a360aa24498c0967fbc2f926b\"}'}\n        # coming from smac instance. Remove this key to make the testing stricter\n        init_params.pop(\"instance\", None)\n\n        for key, value in init_params.items():\n\n            if \":\" not in key:\n                raise ValueError(\n                    \"Unsupported argument to init_params {}.\"\n                    \"When using init_params, a hierarchical format like \"\n                    \"node_name:parameter must be provided.\".format(key)\n                )\n            node_name = key.split(\":\", 1)[0]\n            if node_name not in self.named_steps.keys():\n                raise ValueError(\n                    \"The current node name specified via key={} of init_params \"\n                    \"is not valid. Valid node names are {}\".format(\n                        key, self.named_steps.keys()\n                    )\n                )\n                continue\n            variable_name = key.split(\":\")[-1]\n            node = self.named_steps[node_name]\n            if isinstance(node, BasePipeline):\n                # If dealing with a sub pipe,\n                # Call the child _check_init_params_honored with the updated config\n                node._check_init_params_honored(\n                    {key.replace(\"%s:\" % node_name, \"\", 1): value}\n                )\n                continue\n\n            if isinstance(node, AutoSklearnComponent):\n                node_dict = vars(node)\n            elif isinstance(node, AutoSklearnChoice):\n                node_dict = vars(node.choice)\n            else:\n                raise ValueError(\"Unsupported node type {}\".format(type(node)))\n\n            if variable_name not in node_dict or node_dict[variable_name] != value:\n                raise ValueError(\n                    \"Cannot properly set the pair {}->{} via init_params\"\n                    \"\".format(key, value)\n                )\n\n    def __repr__(self):\n        class_name = self.__class__.__name__\n\n        configuration = {}\n        self.config._populate_values()\n        for hp_name in self.config:\n            if self.config[hp_name] is not None:\n                configuration[hp_name] = self.config[hp_name]\n\n        configuration_string = \"\".join(\n            [\n                \"configuration={\\n  \",\n                \",\\n  \".join(\n                    [\n                        \"'%s': %s\" % (hp_name, repr(configuration[hp_name]))\n                        for hp_name in sorted(configuration)\n                    ]\n                ),\n                \"}\",\n            ]\n        )\n\n        if len(self.dataset_properties) > 0:\n            dataset_properties_string = []\n            dataset_properties_string.append(\"dataset_properties={\")\n            for i, item in enumerate(self.dataset_properties.items()):\n                if i != 0:\n                    dataset_properties_string.append(\",\\n  \")\n                else:\n                    dataset_properties_string.append(\"\\n  \")\n\n                if isinstance(item[1], str):\n                    dataset_properties_string.append(\"'%s': '%s'\" % (item[0], item[1]))\n                else:\n                    dataset_properties_string.append(\"'%s': %s\" % (item[0], item[1]))\n            dataset_properties_string.append(\"}\")\n            dataset_properties_string = \"\".join(dataset_properties_string)\n\n            return_value = \"%s(%s,\\n%s)\" % (\n                class_name,\n                configuration,\n                dataset_properties_string,\n            )\n        else:\n            return_value = \"%s(%s)\" % (class_name, configuration_string)\n\n        return return_value\n\n    def _get_pipeline_steps(\n        self, dataset_properties, feat_type: Optional[FEAT_TYPE_TYPE] = None\n    ):\n        raise NotImplementedError()\n\n    def _get_estimator_hyperparameter_name(self):\n        raise NotImplementedError()\n\n    def get_additional_run_info(self):\n        \"\"\"Allows retrieving additional run information from the pipeline.\n\n        Can be overridden by subclasses to return additional information to\n        the optimization algorithm.\n        \"\"\"\n        return self._additional_run_info\n\n    def _validate_include_exclude_params(self):\n        if self.include is not None and self.exclude is not None:\n            for key in self.include.keys():\n                if key in self.exclude.keys():\n                    raise ValueError(\n                        \"Cannot specify include and exclude for same step '{}'.\".format(\n                            key\n                        )\n                    )\n\n        supported_steps = {\n            step[0]: step[1]\n            for step in self.steps\n            if isinstance(step[1], AutoSklearnChoice)\n        }\n        for arg in [\"include\", \"exclude\"]:\n            argument = getattr(self, arg)\n            if not argument:\n                continue\n            for key in list(argument.keys()):\n                if key not in supported_steps:\n                    raise ValueError(\n                        \"The provided key '{}' in the '{}' argument is not valid. The\"\n                        \" only supported keys for this task are {}\".format(\n                            key, arg, list(supported_steps.keys())\n                        )\n                    )\n\n                candidate_components = argument[key]\n                if not (\n                    isinstance(candidate_components, list) and candidate_components\n                ):\n                    raise ValueError(\n                        \"The provided value of the key '{}' in the '{}' argument is \"\n                        \"not valid. The value must be a non-empty list.\".format(\n                            key, arg\n                        )\n                    )\n\n                available_components = list(\n                    supported_steps[key]\n                    .get_available_components(\n                        dataset_properties=self.dataset_properties\n                    )\n                    .keys()\n                )\n                for component in candidate_components:\n                    if component not in available_components:\n                        raise ValueError(\n                            \"The provided component '{}' for the key '{}' in the '{}'\"\n                            \" argument is not valid. The supported components for the\"\n                            \" step '{}' for this task are {}\".format(\n                                component, key, arg, key, available_components\n                            )\n                        )\n\n\n### Dependency File: __init__.py\nfrom typing import Optional, Type\n\nimport os\nfrom collections import OrderedDict\n\nfrom ConfigSpace.configuration_space import ConfigurationSpace\nfrom ConfigSpace.hyperparameters import CategoricalHyperparameter\n\nfrom autosklearn.askl_typing import FEAT_TYPE_TYPE\n\nfrom ..base import (\n    AutoSklearnChoice,\n    AutoSklearnPreprocessingAlgorithm,\n    ThirdPartyComponents,\n    _addons,\n    find_components,\n)\n\nclassifier_directory = os.path.split(__file__)[0]\n_preprocessors = find_components(\n    __package__, classifier_directory, AutoSklearnPreprocessingAlgorithm\n)\nadditional_components = ThirdPartyComponents(AutoSklearnPreprocessingAlgorithm)\n_addons[\"feature_preprocessing\"] = additional_components\n\n\ndef add_preprocessor(preprocessor: Type[AutoSklearnPreprocessingAlgorithm]) -> None:\n    additional_components.add_component(preprocessor)\n\n\nclass FeaturePreprocessorChoice(AutoSklearnChoice):\n    @classmethod\n    def get_components(cls):\n        components = OrderedDict()\n        components.update(_preprocessors)\n        components.update(additional_components.components)\n        return components\n\n    def get_available_components(\n        self, dataset_properties=None, include=None, exclude=None\n    ):\n        if dataset_properties is None:\n            dataset_properties = {}\n\n        if include is not None and exclude is not None:\n            raise ValueError(\n                \"The argument include and exclude cannot be used together.\"\n            )\n\n        available_comp = self.get_components()\n\n        if include is not None:\n            for incl in include:\n                if incl not in available_comp:\n                    raise ValueError(\n                        \"Trying to include unknown component: \" \"%s\" % incl\n                    )\n\n        # TODO check for task type classification and/or regression!\n\n        components_dict = OrderedDict()\n        for name in available_comp:\n            if include is not None and name not in include:\n                continue\n            elif exclude is not None and name in exclude:\n                continue\n\n            entry = available_comp[name]\n\n            # Exclude itself to avoid infinite loop\n            if entry == FeaturePreprocessorChoice or hasattr(entry, \"get_components\"):\n                continue\n\n            target_type = dataset_properties[\"target_type\"]\n            if target_type == \"classification\":\n                if entry.get_properties()[\"handles_classification\"] is False:\n                    continue\n                if (\n                    dataset_properties.get(\"multiclass\") is True\n                    and entry.get_properties()[\"handles_multiclass\"] is False\n                ):\n                    continue\n                if (\n                    dataset_properties.get(\"multilabel\") is True\n                    and entry.get_properties()[\"handles_multilabel\"] is False\n                ):\n                    continue\n\n            elif target_type == \"regression\":\n                if entry.get_properties()[\"handles_regression\"] is False:\n                    continue\n                if (\n                    dataset_properties.get(\"multioutput\") is True\n                    and entry.get_properties()[\"handles_multioutput\"] is False\n                ):\n                    continue\n\n            else:\n                raise ValueError(\"Unknown target type %s\" % target_type)\n\n            components_dict[name] = entry\n\n        return components_dict\n\n    def get_hyperparameter_search_space(\n        self,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        dataset_properties=None,\n        default=None,\n        include=None,\n        exclude=None,\n    ):\n        cs = ConfigurationSpace()\n\n        if dataset_properties is None:\n            dataset_properties = {}\n\n        # Compile a list of legal preprocessors for this problem\n        available_preprocessors = self.get_available_components(\n            dataset_properties=dataset_properties, include=include, exclude=exclude\n        )\n\n        if len(available_preprocessors) == 0:\n            raise ValueError(\"No preprocessors found, please add NoPreprocessing\")\n\n        if default is None:\n            defaults = [\"no_preprocessing\", \"select_percentile\", \"pca\", \"truncatedSVD\"]\n            for default_ in defaults:\n                if default_ in available_preprocessors:\n                    default = default_\n                    break\n\n        preprocessor = CategoricalHyperparameter(\n            \"__choice__\", list(available_preprocessors.keys()), default_value=default\n        )\n        cs.add_hyperparameter(preprocessor)\n        for name in available_preprocessors:\n            preprocessor_configuration_space = available_preprocessors[\n                name\n            ].get_hyperparameter_search_space(dataset_properties=dataset_properties)\n            parent_hyperparameter = {\"parent\": preprocessor, \"value\": name}\n            cs.add_configuration_space(\n                name,\n                preprocessor_configuration_space,\n                parent_hyperparameter=parent_hyperparameter,\n            )\n\n        return cs\n\n    def transform(self, X):\n        return self.choice.transform(X)\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:43:24"}, {"role": "assistant", "content": "```python\nimport copy\nimport itertools\nimport resource\nimport tempfile\n\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.decomposition\nimport sklearn.ensemble\nimport sklearn.svm\nfrom ConfigSpace.configuration_space import ConfigurationSpace\nfrom ConfigSpace.hyperparameters import CategoricalHyperparameter\nfrom joblib import Memory\nfrom sklearn.base import clone\nfrom sklearn.utils.validation import check_is_fitted\n\nimport autosklearn.pipeline.components.feature_preprocessing as preprocessing_components\nimport autosklearn.pipeline.components.regression as regression_components\nfrom autosklearn.pipeline.components.base import (\n    AutoSklearnChoice,\n    AutoSklearnComponent,\n    AutoSklearnPreprocessingAlgorithm,\n    AutoSklearnRegressionAlgorithm,\n)\nfrom autosklearn.pipeline.constants import (\n    DENSE,\n    PREDICTIONS,\n    SIGNED_DATA,\n    SPARSE,\n    UNSIGNED_DATA,\n)\nfrom autosklearn.pipeline.regression import SimpleRegressionPipeline\nfrom autosklearn.pipeline.util import get_dataset\n\nimport unittest\nimport unittest.mock\n\nfrom test.test_pipeline.ignored_warnings import ignore_warnings, regressor_warnings\n\n\nclass SimpleRegressionPipelineTest(unittest.TestCase):\n    _multiprocess_can_split_ = True\n\n    def test_io_dict(self):\n        regressors = regression_components._regressors\n        for r in regressors:\n            if regressors[r] == regression_components.RegressorChoice:\n                continue\n            props = regressors[r].get_properties()\n            self.assertIn(\"input\", props)\n            self.assertIn(\"output\", props)\n            inp = props[\"input\"]\n            output = props[\"output\"]\n\n            self.assertIsInstance(inp, tuple)\n            self.assertIsInstance(output, tuple)\n            for i in inp:\n                self.assertIn(i, (SPARSE, DENSE, SIGNED_DATA, UNSIGNED_DATA))\n            self.assertEqual(output, (PREDICTIONS,))\n            self.assertIn(\"handles_regression\", props)\n            self.assertTrue(props[\"handles_regression\"])\n            self.assertIn(\"handles_classification\", props)\n            self.assertIn(\"handles_multiclass\", props)\n            self.assertIn(\"handles_multilabel\", props)\n            self.assertIn(\"handles_multioutput\", props)\n            self.assertFalse(props[\"handles_classification\"])\n            self.assertFalse(props[\"handles_multiclass\"])\n            self.assertFalse(props[\"handles_multilabel\"])\n\n    def test_find_regressors(self):\n        regressors = regression_components._regressors\n        self.assertGreaterEqual(len(regressors), 1)\n        for key in regressors:\n            if hasattr(regressors[key], \"get_components\"):\n                continue\n            self.assertIn(AutoSklearnRegressionAlgorithm, regressors[key].__bases__)\n\n    def test_find_preprocessors(self):\n        preprocessors = preprocessing_components._preprocessors\n        self.assertGreaterEqual(len(preprocessors), 1)\n        for key in preprocessors:\n            if hasattr(preprocessors[key], \"get_components\"):\n                continue\n            self.assertIn(\n                AutoSklearnPreprocessingAlgorithm, preprocessors[key].__bases__\n            )\n\n    def test_configurations(self):\n        cs = SimpleRegressionPipeline().get_hyperparameter_search_space()\n\n        self._test_configurations(cs)\n\n    def test_configurations_signed_data(self):\n        dataset_properties = {\"signed\": True}\n        cs = SimpleRegressionPipeline(\n            dataset_properties=dataset_properties\n        ).get_hyperparameter_search_space()\n\n        self._test_configurations(\n            configurations_space=cs, dataset_properties=dataset_properties\n        )\n\n    def test_configurations_sparse(self):\n        dataset_properties = {\"sparse\": True}\n        cs = SimpleRegressionPipeline(\n            dataset_properties=dataset_properties\n        ).get_hyperparameter_search_space()\n\n        self._test_configurations(\n            cs, make_sparse=True, dataset_properties=dataset_properties\n        )\n\n    def test_multioutput(self):\n        cache = Memory(location=tempfile.gettempdir())\n        cached_func = cache.cache(sklearn.datasets.make_regression)\n        X, Y = cached_func(\n            n_samples=250,\n            n_features=20,\n            n_informative=9,\n            n_targets=4,\n            bias=0.5,\n            effective_rank=10,\n            tail_strength=0.4,\n            noise=0.3,\n            shuffle=True,\n            coef=False,\n            random_state=1,\n        )\n        X_train = X[:200, :]\n        Y_train = Y[:200, :]\n        X_test = X[200:, :]\n        Y_test = Y[200:, :]\n\n        data = {\n            \"X_train\": X_train,\n            \"Y_train\": Y_train,\n            \"X_test\": X_test,\n            \"Y_test\": Y_test,\n        }\n\n        dataset_properties = {\"multioutput\": True}\n        pipeline = SimpleRegressionPipeline(dataset_properties=dataset_properties)\n        cs = pipeline.get_hyperparameter_search_space()\n\n        self._test_configurations(cs, data=data, dataset_properties=dataset_properties)\n\n    def _test_configurations(\n        self,\n        configurations_space,\n        make_sparse=False,\n        data=None,\n        dataset_properties=None,\n    ):\n        # Use a limit of ~4GiB\n        limit = 3072 * 1024 * 1024\n        resource.setrlimit(resource.RLIMIT_AS, (limit, limit))\n\n        configurations_space.seed(1)\n\n        for i in range(10):\n            config = configurations_space.sample_configuration()\n            config._populate_values()\n\n            # Restrict configurations which could take too long on travis-ci\n            restrictions = {\n                \"regressor:adaboost:n_estimators\": 50,\n                \"regressor:adaboost:max_depth\": 1,\n                \"feature_preprocessor:kernel_pca:n_components\": 10,\n                \"feature_preprocessor:kitchen_sinks:n_components\": 50,\n                \"regressor:libsvm_svc:degree\": 2,\n                \"regressor:libsvm_svr:degree\": 2,\n                \"regressor:libsvm_svr:C\": 1.0,\n                \"feature_preprocessor:truncatedSVD:target_dim\": 10,\n                \"feature_preprocessor:polynomial:degree\": 2,\n                \"regressor:lda:n_components\": 10,\n            }\n\n            for restrict_parameter in restrictions:\n                restrict_to = restrictions[restrict_parameter]\n                if (\n                    restrict_parameter in config\n                    and config[restrict_parameter] is not None\n                ):\n                    config._values[restrict_parameter] = restrict_to\n\n            if data is None:\n                X_train, Y_train, X_test, Y_test = get_dataset(\n                    dataset=\"boston\", make_sparse=make_sparse, add_NaNs=True\n                )\n            else:\n                X_train = data[\"X_train\"].copy()\n                Y_train = data[\"Y_train\"].copy()\n                X_test = data[\"X_test\"].copy()\n                data[\"Y_test\"].copy()\n\n            cls = SimpleRegressionPipeline(\n                random_state=1, dataset_properties=dataset_properties\n            )\n            cls.set_hyperparameters(config)\n\n            # First make sure that for this configuration, setting the parameters\n            # does not mistakenly set the estimator as fitted\n            for name, step in cls.named_steps.items():\n                with self.assertRaisesRegex(\n                    sklearn.exceptions.NotFittedError, \"instance is not fitted yet\"\n                ):\n                    check_is_fitted(step)\n\n            try:\n                with ignore_warnings(regressor_warnings):\n                    cls.fit(X_train, Y_train)\n\n                # After fit, all components should be tagged as fitted\n                # by sklearn. Check is fitted raises an exception if that\n                # is not the case\n                try:\n                    for name, step in cls.named_steps.items():\n                        check_is_fitted(step)\n                except sklearn.exceptions.NotFittedError:\n                    self.fail(\n                        \"config={} raised NotFittedError unexpectedly!\".format(config)\n                    )\n\n                cls.predict(X_test)\n            except MemoryError:\n                continue\n            except np.linalg.LinAlgError:\n                continue\n            except ValueError as e:\n                if \"Floating-point under-/overflow occurred at epoch\" in e.args[0]:\n                    continue\n                elif \"removed all features\" in e.args[0]:\n                    continue\n                elif \"all features are discarded\" in e.args[0]:\n                    continue\n                elif \"Numerical problems in QDA\" in e.args[0]:\n                    continue\n                elif \"Bug in scikit-learn\" in e.args[0]:\n                    continue\n                elif (\n                    \"The condensed distance matrix must contain only finite \"\n                    \"values.\" in e.args[0]\n                ):\n                    continue\n                elif (\n                    \"zero-size array to reduction operation maximum which has no \"\n                    \"identity\" in e.args[0]\n                ):\n                    continue\n                else:\n                    e.args += (f\"config={config}\",)\n                    raise e\n\n            except RuntimeWarning as e:\n                if \"invalid value encountered in sqrt\" in e.args[0]:\n                    continue\n                elif \"divide by zero encountered in\" in e.args[0]:\n                    continue\n                elif \"invalid value encountered in divide\" in e.args[0]:\n                    continue\n                elif \"invalid value encountered in true_divide\" in e.args[0]:\n                    continue\n                elif \"invalid value encountered in multiply\" in e.args[0]:\n                    continue\n                else:\n                    e.args += (f\"config={config}\",)\n                    raise e\n\n            except UserWarning as e:\n                if \"FastICA did not converge\" in e.args[0]:\n                    continue\n                else:\n                    e.args += (f\"config={config}\",)\n                    raise e\n\n            except Exception as e:\n                if (\n                    \"Multiple input features cannot have the same target value\"\n                    in e.args[0]\n                ):\n                    continue\n                else:\n                    e.args += (f\"config={config}\",)\n                    raise e\n\n    def test_default_configuration(self):\n        for i in range(2):\n            X_train, Y_train, X_test, Y_test = get_dataset(dataset=\"diabetes\")\n            auto = SimpleRegressionPipeline(random_state=1)\n            auto = auto.fit(X_train, Y_train)\n            predictions = auto.predict(copy.deepcopy(X_test))\n            # The lower the worse\n            r2_score = sklearn.metrics.r2_score(Y_test, predictions)\n            self.assertAlmostEqual(0.3458397471855429, r2_score, places=2)\n            model_score = auto.score(copy.deepcopy(X_test), Y_test)\n            self.assertAlmostEqual(model_score, r2_score, places=5)\n\n    def test_default_configuration_iterative_fit(self):\n        regressor = SimpleRegressionPipeline(\n            random_state=1,\n            include={\n                \"regressor\": [\"random_forest\"],\n                \"feature_preprocessor\": [\"no_preprocessing\"],\n            },\n        )\n        X_train, Y_train, X_test, Y_test = get_dataset(dataset=\"boston\")\n        regressor.fit_transformer(X_train, Y_train)\n        for i in range(1, 11):\n            regressor.iterative_fit(X_train, Y_train)\n            self.assertEqual(regressor.steps[-1][-1].choice.estimator.n_estimators, i)\n\n    def test_repr(self):\n        representation = repr(SimpleRegressionPipeline())\n        cls = eval(representation)\n        self.assertIsInstance(cls, SimpleRegressionPipeline)\n\n    def test_get_hyperparameter_search_space(self):\n        cs = SimpleRegressionPipeline().get_hyperparameter_search_space()\n        self.assertIsInstance(cs, ConfigurationSpace)\n        conditions = cs.get_conditions()\n        hyperparameters = cs.get_hyperparameters()\n        forbiddens = cs.get_forbiddens()\n        self.assertEqual(167, len(hyperparameters))\n        self.assertEqual(len(hyperparameters) - 3, len(conditions))\n        self.assertEqual(len(forbiddens), 35)\n\n    def test_get_hyperparameter_search_space_include_exclude_models(self):\n        regressor = SimpleRegressionPipeline(include={\"regressor\": [\"random_forest\"]})\n        cs = regressor.get_hyperparameter_search_space()\n        self.assertEqual(\n            cs.get_hyperparameter(\"regressor:__choice__\"),\n            CategoricalHyperparameter(\"regressor:__choice__\", [\"random_forest\"]),\n        )\n\n        # TODO add this test when more than one regressor is present\n        regressor = SimpleRegressionPipeline(exclude={\"regressor\": [\"random_forest\"]})\n        cs = regressor.get_hyperparameter_search_space()\n        self.assertNotIn(\"random_forest\", str(cs))\n\n        regressor = SimpleRegressionPipeline(include={\"feature_preprocessor\": [\"pca\"]})\n        cs = regressor.get_hyperparameter_search_space()\n        self.assertEqual(\n            cs.get_hyperparameter(\"feature_preprocessor:__choice__\"),\n            CategoricalHyperparameter(\"feature_preprocessor:__choice__\", [\"pca\"]),\n        )\n\n        regressor = SimpleRegressionPipeline(\n            exclude={\"feature_preprocessor\": [\"no_preprocessing\"]}\n        )\n        cs = regressor.get_hyperparameter_search_space()\n        self.assertNotIn(\"no_preprocessing\", str(cs))\n\n    def test_get_hyperparameter_search_space_preprocessor_contradicts_default(\n        self,\n    ):\n        regressor = SimpleRegressionPipeline(\n            include={\"feature_preprocessor\": [\"densifier\"]},\n            dataset_properties={\"sparse\": True},\n        )\n        cs = regressor.get_hyperparameter_search_space()\n        self.assertEqual(\n            cs.get_hyperparameter(\"regressor:__choice__\").default_value,\n            \"gradient_boosting\",\n        )\n\n        regressor = SimpleRegressionPipeline(\n            include={\"feature_preprocessor\": [\"nystroem_sampler\"]}\n        )\n        cs = regressor.get_hyperparameter_search_space()\n        self.assertEqual(\n            cs.get_hyperparameter(\"regressor:__choice__\").default_value, \"sgd\"\n        )\n\n    def test_get_hyperparameter_search_space_only_forbidden_combinations(self):\n        self.assertRaisesRegex(\n            ValueError,\n            \"Cannot find a legal default configuration.\",\n            SimpleRegressionPipeline,\n            include={\n                \"regressor\": [\"random_forest\"],\n                \"feature_preprocessor\": [\"kitchen_sinks\"],\n            },\n        )\n\n        # It must also be catched that no classifiers which can handle sparse\n        # data are located behind the densifier\n        self.assertRaisesRegex(\n            ValueError,\n            \"Cannot find a legal default configuration\",\n            SimpleRegressionPipeline,\n            include={\n                \"regressor\": [\"extra_trees\"],\n                \"feature_preprocessor\": [\"densifier\"],\n            },\n            dataset_properties={\"sparse\": True},\n        )\n\n    @unittest.skip(\n        \"test_get_hyperparameter_search_space_dataset_properties\"\n        + \" Not yet Implemented\"\n    )\n    def test_get_hyperparameter_search_space_dataset_properties(self):\n        # TODO: We do not have any dataset properties for regression, so this\n        # test is somewhat stupid\n        pass\n        \"\"\"\n        full_cs = SimpleRegressionPipeline.get_hyperparameter_search_space()\n        cs_mc = SimpleRegressionPipeline.get_hyperparameter_search_space()\n        self.assertEqual(full_cs, cs_mc)\n\n        cs_ml = SimpleRegressionPipeline.get_hyperparameter_search_space()\n        self.assertNotIn('k_nearest_neighbors', str(cs_ml))\n        self.assertNotIn('liblinear', str(cs_ml))\n        self.assertNotIn('libsvm_svc', str(cs_ml))\n        self.assertNotIn('sgd', str(cs_ml))\n\n        cs_sp = SimpleRegressionPipeline.get_hyperparameter_search_space(\n            sparse=True)\n        self.assertNotIn('extra_trees', str(cs_sp))\n        self.assertNotIn('gradient_boosting', str(cs_sp))\n        self.assertNotIn('random_forest', str(cs_sp))\n\n        cs_mc_ml = SimpleRegressionPipeline.get_hyperparameter_search_space()\n        self.assertEqual(cs_ml, cs_mc_ml)\n\n        self.assertRaisesRegex(ValueError,\n                                \"No regressor to build a configuration space \"\n                                \"for...\", SimpleRegressionPipeline.\n                                get_hyperparameter_search_space,\n                                multiclass=True, multilabel=True, sparse=True)\n    \"\"\"\n\n    def test_predict_batched(self):\n        include = {\"regressor\": [\"decision_tree\"]}\n        cs = SimpleRegressionPipeline(include=include).get_hyperparameter_search_space()\n        default = cs.get_default_configuration()\n        regressor = SimpleRegressionPipeline(\n            config=default, random_state=1, include=include\n        )\n\n        X_train, Y_train, X_test, Y_test = get_dataset(dataset=\"boston\")\n        regressor.fit(X_train, Y_train)\n        X_test_ = X_test.copy()\n        prediction_ = regressor.predict(X_test_)\n        mock_predict = unittest.mock.Mock(wraps=regressor.steps[-1][-1].predict)\n        regressor.steps[-1][-1].predict = mock_predict\n        prediction = regressor.predict(X_test, batch_size=20)\n        self.assertEqual((356,), prediction.shape)\n        self.assertEqual(18, mock_predict.call_count)\n        np.testing.assert_array_almost_equal(prediction_, prediction)\n\n    def test_predict_batched_sparse(self):\n        dataset_properties = {\"sparse\": True}\n        include = {\"regressor\": [\"decision_tree\"]}\n\n        cs = SimpleRegressionPipeline(\n            dataset_properties=dataset_properties, include=include\n        ).get_hyperparameter_search_space()\n\n        default = cs.get_default_configuration()\n        regressor = SimpleRegressionPipeline(\n            config=default,\n            random_state=1,\n            dataset_properties=dataset_properties,\n            include=include,\n        )\n\n        X_train, Y_train, X_test, Y_test = get_dataset(\n            dataset=\"boston\", make_sparse=True\n        )\n        regressor.fit(X_train, Y_train)\n        X_test_ = X_test.copy()\n        prediction_ = regressor.predict(X_test_)\n        mock_predict = unittest.mock.Mock(wraps=regressor.steps[-1][-1].predict)\n        regressor.steps[-1][-1].predict = mock_predict\n        prediction = regressor.predict(X_test, batch_size=20)\n        self.assertEqual((356,), prediction.shape)\n        self.assertEqual(18, mock_predict.call_count)\n        np.testing.assert_array_almost_equal(prediction_, prediction)\n\n    @unittest.skip(\"test_check_random_state Not yet Implemented\")\n    def test_check_random_state(self):\n        raise NotImplementedError()\n\n    @unittest.skip(\"test_validate_input_X Not yet Implemented\")\n    def test_validate_input_X(self):\n        raise NotImplementedError()\n\n    @unittest.skip(\"test_validate_input_Y Not yet Implemented\")\n    def test_validate_input_Y(self):\n        raise NotImplementedError()\n\n    def test_pipeline_clonability(self):\n        X_train, Y_train, X_test, Y_test = get_dataset(dataset=\"boston\")\n        auto = SimpleRegressionPipeline(random_state=1)\n        auto = auto.fit(X_train, Y_train)\n        auto_clone = clone(auto)\n        auto_clone_params = auto_clone.get_params()\n\n        # Make sure all keys are copied properly\n        for k, v in auto.get_params().items():\n            self.assertIn(k, auto_clone_params)\n\n        # Make sure the params getter of estimator are honored\n        klass = auto.__class__\n        new_object_params = auto.get_params(deep=False)\n        for name, param in new_object_params.items():\n            new_object_params[name] = clone(param, safe=False)\n        new_object = klass(**new_object_params)\n        params_set = new_object.get_params(deep=False)\n\n        for name in new_object_params:\n            param1 = new_object_params[name]\n            param2 = params_set[name]\n            self.assertEqual(param1, param2)\n\n    def test_set_params(self):\n        pass\n\n    def test_get_params(self):\n        pass\n\n    def _test_set_hyperparameter_choice(\n        self, expected_key, implementation, config_dict\n    ):\n        \"\"\"\n        Given a configuration in config, this procedure makes sure that\n        the given implementation, which should be a Choice component, honors\n        the type of the object, and any hyperparameter associated to it\n        \"\"\"\n        keys_checked = [expected_key]\n        implementation_type = config_dict[expected_key]\n        expected_type = implementation.get_components()[implementation_type]\n        self.assertIsInstance(implementation.choice, expected_type)\n\n        # Are there further hyperparams?\n        # A choice component might have attribute requirements that we need to check\n        expected_sub_key = (\n            expected_key.replace(\":__choice__\", \":\") + implementation_type\n        )\n        expected_attributes = {}\n        if \"data_preprocessor:__choice__\" in expected_key:\n            # We have to check both the numerical and categorical\n            to_check = {\n                \"numerical_transformer\": implementation.choice.numer_ppl.named_steps,\n                \"categorical_transformer\": implementation.choice.categ_ppl.named_steps,\n                \"text_transformer\": implementation.choice.txt_ppl.named_steps,\n            }\n\n            for data_type, pipeline in to_check.items():\n                for sub_name, sub_step in pipeline.items():\n                    # If it is a Choice, make sure it is the correct one!\n                    if isinstance(sub_step, AutoSklearnChoice):\n                        key = \"data_preprocessor:feature_type:{}:{}:__choice__\".format(\n                            data_type, sub_name\n                        )\n                        keys_checked.extend(\n                            self._test_set_hyperparameter_choice(\n                                key, sub_step, config_dict\n                            )\n                        )\n                    # If it is a component, make sure it has the correct hyperparams\n                    elif isinstance(sub_step, AutoSklearnComponent):\n                        keys_checked.extend(\n                            self._test_set_hyperparameter_component(\n                                \"data_preprocessor:feature_type:{}:{}\".format(\n                                    data_type, sub_name\n                                ),\n                                sub_step,\n                                config_dict,\n                            )\n                        )\n                    else:\n                        raise ValueError(\"New type of pipeline component!\")\n            return keys_checked\n        else:\n            for key, value in config_dict.items():\n                if key != expected_key and expected_sub_key in key:\n                    expected_attributes[key.split(\":\")[-1]] = value\n                    keys_checked.append(key)\n        if expected_attributes:\n            attributes = vars(implementation.choice)\n            # Cannot check the whole dictionary, just names, as some\n            # classes map the text hyperparameter directly to a function!\n            for expected_attribute in expected_attributes.keys():\n                self.assertIn(expected_attribute, attributes.keys())\n        return keys_checked\n\n    def _test_set_hyperparameter_component(\n        self, expected_key, implementation, config_dict\n    ):\n        \"\"\"\n        Given a configuration in config, this procedure makes sure that\n        the given implementation, which should be a autosklearn component, honors\n        is created with the desired hyperparameters stated in config_dict\n        \"\"\"\n        keys_checked = []\n        attributes = vars(implementation)\n        expected_attributes = {}\n        for key, value in config_dict.items():\n            if expected_key in key:\n                keys_checked.append(key)\n                key = key.replace(expected_key + \":\", \"\")\n                if \":\" in key:\n                    raise ValueError(\n                        \"This utility should only be called with a \"\n                        \"matching string that produces leaf configurations, \"\n                        \"that is no further colons are expected, yet key={}\"\n                        \"\".format(key)\n                    )\n                expected_attributes[key] = value\n        # Cannot check the whole dictionary, just names, as some\n        # classes map the text hyperparameter directly to a function!\n        for expected_attribute in expected_attributes.keys():\n            self.assertIn(expected_attribute, attributes.keys())\n        return keys_checked\n\n    def test_set_hyperparameters_honors_configuration(self):\n        \"\"\"Makes sure that a given configuration is honored in practice.\n\n        This method tests that the set hyperparameters actually create objects\n        that comply with the given configuration. It iterates trough the pipeline to\n        make sure we did not miss a step, but also checks at the end that every\n        configuration from Config was checked\n\n        Also considers random_state and ensures pipeline steps correctly recieve\n        the right random_state\n        \"\"\"\n\n        all_combinations = list(itertools.product([True, False], repeat=4))\n        for (\n            sparse,\n            multilabel,\n            signed,\n            multiclass,\n        ) in all_combinations:\n            dataset_properties = {\n                \"sparse\": sparse,\n                \"multilabel\": multilabel,\n                \"multiclass\": multiclass,\n                \"signed\": signed,\n            }\n            random_state = 1\n            auto = SimpleRegressionPipeline(\n                random_state=random_state,\n                dataset_properties=dataset_properties,\n            )\n            cs = auto.get_hyperparameter_search_space()\n            config = cs.sample_configuration()\n\n            # Set hyperparameters takes a given config and translate\n            # a config to an actual implementation\n            auto.set_hyperparameters(config)\n            config_dict = config.get_dictionary()\n\n            # keys to check is our mechanism to ensure that every\n            # every config key is checked\n            keys_checked = []\n\n            for name, step in auto.named_steps.items():\n                if name == \"data_preprocessor\":\n                    keys_checked.extend(\n                        self._test_set_hyperparameter_choice(\n                            \"data_preprocessor:__choice__\", step, config_dict\n                        )\n                    )\n                    self.assertEqual(step.random_state, random_state)\n                elif name == \"feature_preprocessor\":\n                    keys_checked.extend(\n                        self._test_set_hyperparameter_choice(\n                            \"feature_preprocessor:__choice__\", step, config_dict\n                        )\n                    )\n                    self.assertEqual(step.random_state, random_state)\n                elif name == \"regressor\":\n                    keys_checked.extend(\n                        self._test_set_hyperparameter_choice(\n                            \"regressor:__choice__\", step, config_dict\n                        )\n                    )\n                    self.assertEqual(step.random_state, random_state)\n                else:\n                    raise ValueError(\n                        \"Found another type of step! Need to update this check\"\n                        \" {}. \".format(name)\n                    )\n\n            # Make sure we checked the whole configuration\n            self.assertSetEqual(set(config_dict.keys()), set(keys_checked))\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: auto-sklearn\nTest File Path: auto-sklearn\\test_single_thread_client\\test_single_thread_client.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nimport typing\nfrom typing import Any\n\nfrom pathlib import Path\n\nimport dask.distributed\n\n\nclass DummyFuture(dask.distributed.Future):\n    \"\"\"\n    A class that mimics a distributed Future, the outcome of\n    performing submit on a distributed client.\n    \"\"\"\n\n    def __init__(self, result: typing.Any) -> None:\n        self._result = result  # type: typing.Any\n\n    def result(self, timeout: typing.Optional[int] = None) -> typing.Any:\n        return self._result\n\n    def cancel(self) -> None:\n        pass\n\n    def done(self) -> bool:\n        return True\n\n    def __repr__(self) -> str:\n        return \"DummyFuture: {}\".format(self._result)\n\n    def __del__(self) -> None:\n        pass\n\n\nclass SingleThreadedClient(dask.distributed.Client):\n    \"\"\"\n    A class to Mock the Distributed Client class, in case\n    Auto-Sklearn is meant to run in the current Thread.\n    \"\"\"\n\n    def __init__(self) -> None:\n\n        # Raise a not implemented error if using a method from Client\n        implemented_methods = [\n            \"submit\",\n            \"close\",\n            \"shutdown\",\n            \"write_scheduler_file\",\n            \"_get_scheduler_info\",\n            \"nthreads\",\n        ]\n        method_list = [\n            func\n            for func in dir(dask.distributed.Client)\n            if callable(getattr(dask.distributed.Client, func))\n            and not func.startswith(\"__\")\n        ]\n        for method in method_list:\n            if method in implemented_methods:\n                continue\n            setattr(self, method, self._unsupported_method)\n        pass\n\n    def _unsupported_method(self) -> None:\n        raise NotImplementedError()\n\n    def submit(\n        self,\n        func: typing.Callable,\n        *args: typing.List,\n        priority: int = 0,\n        key: Any = None,\n        workers: Any = None,\n        resources: Any = None,\n        retries: Any = None,\n        fifo_timeout: Any = \"100 ms\",\n        allow_other_workers: Any = False,\n        actor: Any = False,\n        actors: Any = False,\n        pure: Any = None,\n        **kwargs: Any,\n    ) -> typing.Any:\n        \"\"\"\n        Note\n        ----\n        The keyword arguments caught in `dask.distributed.Client` need to\n        be specified here so they don't get passed in as ``**kwargs`` to the\n        ``func``.\n        \"\"\"\n        return DummyFuture(func(*args, **kwargs))\n\n    def close(self) -> None:\n        pass\n\n    def shutdown(self) -> None:\n        pass\n\n    def write_scheduler_file(self, scheduler_file: str) -> None:\n        Path(scheduler_file).touch()\n        return\n\n    def _get_scheduler_info(self) -> typing.Dict:\n        return {\n            \"workers\": [\"127.0.0.1\"],\n            \"type\": \"Scheduler\",\n        }\n\n    def nthreads(self) -> typing.Dict:\n        return {\n            \"127.0.0.1\": 1,\n        }\n\n    def __repr__(self) -> str:\n        return \"SingleThreadedClient()\"\n\n    def __del__(self) -> None:\n        pass\n\n### Source File Dependency Files Content\n### Dependency File: dask.py\n\"\"\" Provides simplified 2 use cases of dask that we consider\n\n1. A UserDask is when a user supplies a dask client, in which case\nwe don't close this down and leave it up to the user to control its lifetime.\n2.  A LocalDask is one we use when no user dask is supplied. In this case\nwe make sure to spin up and close down clients as needed.\n\nBoth of these can be uniformly accessed as a context manager.\n\n.. code:: python\n\n    # Locally controlled dask client\n    local_dask = LocalDask(n_jobs=2)\n    with local_dask as client:\n        # Do stuff with client\n        ...\n\n    # `client` is shutdown properly\n\n    # ----------------\n\n    # User controlled dask client\n    user_dask = UserDask(user_client)\n\n    with user_dask as client:\n        # Do stuff with (client == user_client)\n        ...\n\n    # `user_client` is still open and up to the user to close\n\"\"\"\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import Any\n\nimport tempfile\n\nfrom dask.distributed import Client, LocalCluster\n\nfrom autosklearn.util.single_thread_client import SingleThreadedClient\n\n\nclass Dask(ABC):\n    @abstractmethod\n    def client(self) -> Client:\n        \"\"\"Should return a dask client\"\"\"\n        ...\n\n    @abstractmethod\n    def close(self) -> None:\n        \"\"\"Should close up any resources needed for the dask client\"\"\"\n        ...\n\n    def __enter__(self) -> Client:\n        return self.client()\n\n    def __exit__(self, *args: Any, **kwargs: Any) -> None:\n        self.close()\n\n    @abstractmethod\n    def __repr__(self) -> str:\n        ...\n\n\nclass UserDask(Dask):\n    \"\"\"A dask instance created by a user\"\"\"\n\n    def __init__(self, client: Client):\n        \"\"\"\n        Parameters\n        ----------\n        client : Client\n            The client they passed in\n        \"\"\"\n        self._client = client\n\n    def client(self) -> Client:\n        \"\"\"The dask client\"\"\"\n        return self._client\n\n    def close(self) -> None:\n        \"\"\"Close the dask client\"\"\"\n        # We do nothing, it's user provided\n        pass\n\n    def __repr__(self) -> str:\n        return \"UserDask(...)\"\n\n\nclass LocalDask(Dask):\n    def __init__(self, n_jobs: int | None = None) -> None:\n        self.n_jobs = n_jobs\n        self._client: Client | None = None\n        self._cluster: LocalCluster | None = None\n\n    def client(self) -> Client:\n        \"\"\"Creates a usable dask client or returns an existing one\n\n        If there is not current client, because it has been closed, create\n        a new one.\n        * If ``n_jobs == 1``, create a ``SingleThreadedClient``\n        * Else create a ``Client`` with a ``LocalCluster``\n        \"\"\"\n        if self._client is not None:\n            return self._client\n\n        if self.n_jobs == 1:\n            cluster = None\n            client = SingleThreadedClient()\n        else:\n            cluster = LocalCluster(\n                n_workers=self.n_jobs,\n                processes=False,\n                threads_per_worker=1,\n                # We use tmpdir to save the workers as deleting workers takes\n                # more time than deleting backend directories.\n                # This prevent an error saying that the worker file was deleted,\n                # so the client could not close the worker properly\n                local_directory=tempfile.gettempdir(),\n                # Memory is handled by the pynisher, not by the dask worker/nanny\n                memory_limit=0,\n            )\n            client = Client(cluster, heartbeat_interval=10000)  # 10s\n\n        self._client = client\n        self._cluster = cluster\n        return self._client\n\n    def close(self) -> None:\n        \"\"\"Closes any open dask client\"\"\"\n        if self._client is None:\n            return\n\n        self._client.close()\n        if self._cluster is not None:\n            self._cluster.close()\n\n        self._client = None\n        self._cluster = None\n\n    def __repr__(self) -> str:\n        return f\"LocalDask(n_jobs = {self.n_jobs})\"\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:43:24"}, {"role": "assistant", "content": "```python\nimport dask.distributed\nfrom distributed.utils_test import inc\n\nfrom autosklearn.util.single_thread_client import SingleThreadedClient\n\nimport pytest\n\n\ndef test_single_thread_client_like_dask_client():\n    single_thread_client = SingleThreadedClient()\n    assert isinstance(single_thread_client, dask.distributed.Client)\n    future = single_thread_client.submit(inc, 1)\n    assert isinstance(future, dask.distributed.Future)\n    assert future.done()\n    assert future.result() == 2\n    assert sum(single_thread_client.nthreads().values()) == 1\n    single_thread_client.close()\n    single_thread_client.shutdown()\n\n    # Client/Futures are printed, so make sure str works\n    # str calls __rpr__ which is the purpose of below check\n    assert str(future) != \"\"\n    assert str(single_thread_client) != \"\"\n\n    with pytest.raises(NotImplementedError):\n        single_thread_client.get_scheduler_logs()\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: auto-sklearn\nTest File Path: auto-sklearn\\test_stopwatch\\test_stopwatch.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nfrom typing import Iterator, Mapping, Optional, Tuple\n\nimport sys\nimport time\nfrom contextlib import contextmanager\nfrom itertools import repeat\n\nfrom typing_extensions import Literal\n\n\nclass TimingTask:\n    \"\"\"A task to start\"\"\"\n\n    def __init__(self, name: str):\n        \"\"\"\n        Parameters\n        ----------\n        name : str\n            The name of the task\n        \"\"\"\n        self.name = name\n        self.cpu_start: Optional[float] = None\n        self.cpu_end: Optional[float] = None\n        self.wall_start: Optional[float] = None\n        self.wall_end: Optional[float] = None\n\n    def start(self) -> None:\n        \"\"\"Start the task\"\"\"\n        if not self.started():\n            self.cpu_start = time.process_time()\n            self.wall_start = time.time()\n\n    def stop(self) -> None:\n        \"\"\"Stop the task\"\"\"\n        if not self.finished():\n            self.cpu_end = time.process_time()\n            self.wall_end = time.time()\n\n    def finished(self) -> bool:\n        \"\"\"Whether this task has been finished\"\"\"\n        return self.cpu_end is not None\n\n    def started(self) -> bool:\n        \"\"\"Whether this task has been started\"\"\"\n        return self.cpu_start is not None\n\n    @property\n    def cpu_duration(self) -> Optional[float]:\n        \"\"\"The duration according to cpu clock time\"\"\"\n        if self.cpu_start and self.cpu_end:\n            return self.cpu_end - self.cpu_start\n        else:\n            return None\n\n    @property\n    def wall_duration(self) -> Optional[float]:\n        \"\"\"The duration according to wall clock time\"\"\"\n        if self.wall_end and self.wall_start:\n            return self.wall_end - self.wall_start\n        else:\n            return None\n\n    @property\n    def durations(self) -> Tuple[Optional[float], Optional[float]]:\n        \"\"\"The durations (cpu, wall)\"\"\"\n        return self.cpu_duration, self.wall_duration\n\n\nclass StopWatch(Mapping[str, TimingTask]):\n    \"\"\"Class to collect timing tasks.\"\"\"\n\n    def __init__(self) -> None:\n        self.tasks = {\"_stopwatch_\": TimingTask(\"_stopwatch_\")}\n        self.tasks[\"_stopwatch_\"].start()\n\n    def __contains__(self, name: object) -> bool:\n        return name in self.tasks\n\n    def __getitem__(self, name: str) -> TimingTask:\n        return self.tasks[name]\n\n    def __len__(self) -> int:\n        return len(self.tasks)\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self.tasks)\n\n    def start(self, name: str) -> None:\n        \"\"\"Start a given task with a name\"\"\"\n        task = TimingTask(name)\n        self.tasks[name] = task\n        task.start()\n\n    def stop(self, name: str) -> None:\n        \"\"\"Stop a given task\"\"\"\n        if name in self.tasks:\n            self.tasks[name].stop()\n        else:\n            sys.stderr.write(f\"No task with name {name}\")\n\n    def total_cpu(self) -> float:\n        \"\"\"Return sum of CPU time for all so far finished tasks.\"\"\"\n        tasks = self.tasks.values()\n        return sum([t.cpu_duration for t in tasks if t.cpu_duration is not None])\n\n    def total_wall(self) -> float:\n        \"\"\"Return sum of wall clock time for all so far finished tasks.\"\"\"\n        tasks = self.tasks.values()\n        return sum([t.wall_duration for t in tasks if t.wall_duration is not None])\n\n    def time_since(\n        self,\n        name: str,\n        phase: Literal[\"start\", \"end\"] = \"start\",\n        default: float = 0.0,\n        raises: bool = False,\n    ) -> float:\n        \"\"\"The wall clock time since a task either began or ended\n\n        Parameters\n        ----------\n        name : str\n            The name of the task\n\n        phase : Literal[\"start\", \"end\"] = \"start\"\n            From which phase you want to know the time elapsed since\n\n        default: float = 0.0\n            If None (default) then an error is raised if an answer can't be given.\n\n        raises: bool = False\n            Whether the method should raise if it can't find a valid time\n\n        Returns\n        -------\n        float\n            The time elapsed\n\n        Raises\n        ------\n        ValueError\n            If no default is specified and\n            * the task has not been registered\n            * the \"start\" and the task never started\n            * the \"end\" and the task never started\n        \"\"\"\n        task = self.tasks.get(name, None)\n\n        if task is None:\n            if raises:\n                raise ValueError(f\"Task not listed in {list(self.tasks.keys())}\")\n            else:\n                return default\n\n        if phase == \"start\":\n            event_time = task.wall_start\n        elif phase == \"end\":\n            event_time = task.wall_end\n        else:\n            raise NotImplementedError()\n\n        if event_time is None:\n            if raises:\n                raise ValueError(f\"Task {task} has no time for {phase}\")\n            else:\n                return default\n\n        return time.time() - event_time\n\n    def wall_elapsed(self, name: str) -> float:\n        \"\"\"Get the currently elapsed wall time for a task\"\"\"\n        if name not in self.tasks:\n            return 0.0\n\n        task = self.tasks[name]\n        if task.wall_start is None:\n            return 0.0\n\n        if task.wall_duration is not None:\n            return task.wall_duration\n        else:\n            return time.time() - task.wall_start\n\n    def cpu_elapsed(self, name: str) -> float:\n        \"\"\"Get the currently elapsed cpu time for a task\"\"\"\n        if name not in self.tasks:\n            return 0.0\n\n        task = self.tasks[name]\n        if task.cpu_start is None:\n            return 0.0\n\n        if task.cpu_duration is not None:\n            return task.cpu_duration\n        else:\n            return time.time() - task.cpu_start\n\n    @contextmanager\n    def time(self, name: str) -> Iterator[TimingTask]:\n        \"\"\"Start timing a task\n\n        Parameters\n        ----------\n        name : str\n            The name of the task to measure\n        \"\"\"\n        task = TimingTask(name)\n        self.tasks[name] = task\n        task.start()\n        yield task\n        task.stop()\n\n    def __str__(self) -> str:\n        headers = [\n            \"Name\",\n            \"CPUStart\",\n            \"CPUEnd\",\n            \"CPUDur\",\n            \"WallStart\",\n            \"WallEnd\",\n            \"WallDur\",\n        ]\n        header = \"|\".join([f\"{h:10s}\" for h in headers])\n\n        sep = \"-\" * 12\n        seperator = \"+\" + \"+\".join(repeat(sep, len(headers))) + \"+\"\n\n        entries = []\n        for name, task in self.tasks.items():\n            entry = (\n                f\"{task.name:10s} | {task.cpu_start:10.5f} | {task.cpu_end: 10.5f} | \"\n                f\"{task.cpu_duration:10.5f} | {task.wall_start: 10s} | \"\n                f\"{task.wall_end:10s} | {task.wall_duration:10s} |\"\n            )\n            entries.append(entry)\n\n        return \"\\n\".join([header, seperator] + entries)\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:43:24"}, {"role": "assistant", "content": "```python\nimport time\n\nfrom autosklearn.util.stopwatch import StopWatch\n\n\ndef test_stopwatch_overhead() -> None:\n    wall_start = time.time()\n    cpu_start = time.process_time()\n\n    watch = StopWatch()\n    for i in range(1, 1000):\n        watch.start(\"task_%d\" % i)\n        watch.stop(\"task_%d\" % i)\n\n    cpu_end = time.process_time()\n    wall_end = time.time()\n\n    wall_duration = wall_end - wall_start\n    cpu_duration = cpu_end - cpu_start\n\n    cpu_overhead = cpu_duration - watch.total_cpu()\n    wall_overhead = wall_duration - watch.total_wall()\n\n    assert cpu_overhead < 1\n    assert wall_overhead < 1\n    assert watch.total_cpu() < 2 * watch.total_wall()\n\n\ndef test_contextmanager() -> None:\n    watch = StopWatch()\n\n    with watch.time(\"task\"):\n        assert watch[\"task\"].started()\n\n    assert \"task\" in watch\n    assert watch[\"task\"].finished()\n\n    assert watch[\"task\"].cpu_duration is not None\n    assert watch[\"task\"].wall_duration is not None\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: auto-sklearn\nTest File Path: auto-sklearn\\test_validation\\test_validation.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n# -*- encoding: utf-8 -*-\nfrom typing import List, Optional, Tuple, Union\n\nimport logging\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import spmatrix\nfrom sklearn.base import BaseEstimator\nfrom sklearn.exceptions import NotFittedError\n\nfrom autosklearn.data.feature_validator import SUPPORTED_FEAT_TYPES, FeatureValidator\nfrom autosklearn.data.target_validator import SUPPORTED_TARGET_TYPES, TargetValidator\nfrom autosklearn.util.logging_ import get_named_client_logger\n\n\ndef convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]:\n    \"\"\"If the labels `y` are sparse, it will convert it to its dense representation\n\n    Parameters\n    ----------\n    y: {array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_outputs)\n        The labels to 'densify' if sparse\n\n    Returns\n    -------\n    np.ndarray of shape (n_samples, ) or (n_samples, n_outputs)\n    \"\"\"\n    if isinstance(y, spmatrix):\n        y_ = y.toarray()\n\n        # For sparse one dimensional data, y.toarray will return [[1], [2], [3], ...]\n        # We need to flatten this before returning it\n        if y_.shape[0] == 1:\n            y_ = y_.flatten()\n    else:\n        y_ = y\n\n    return y_\n\n\nclass InputValidator(BaseEstimator):\n    \"\"\"\n    Makes sure the input data complies with Auto-sklearn requirements.\n    Categorical inputs are encoded via a Label Encoder, if the input\n    is a dataframe.\n\n    This class also perform checks for data integrity and flags the user\n    via informative errors.\n\n    Attributes\n    ----------\n    feat_type: Optional[List[str]] = None\n        In case the dataset is not a pandas DataFrame:\n            +   If provided, this list indicates which columns should be treated as\n                categorical it is internally transformed into a dictionary that\n                indicates a mapping from column index to categorical/numerical.\n            +   If not provided, by default all columns are treated as numerical\n\n        If the input dataset is of type pandas dataframe, this argument\n        must be none, as the column type will be inferred from the pandas dtypes.\n\n    is_classification: bool\n        For classification task, this flag indicates that the target data\n        should be encoded\n\n    feature_validator: FeatureValidator\n        A FeatureValidator instance used to validate and encode feature columns to match\n        sklearn expectations on the data\n\n    target_validator: TargetValidator\n        A TargetValidator instance used for classification to validate and encode the\n        target values\n    \"\"\"\n\n    def __init__(\n        self,\n        feat_type: Optional[List[str]] = None,\n        is_classification: bool = False,\n        logger_port: Optional[int] = None,\n        allow_string_features: bool = True,\n    ) -> None:\n        self.feat_type = feat_type\n        self.is_classification = is_classification\n        self.logger_port = logger_port\n        if self.logger_port is not None:\n            self.logger = get_named_client_logger(\n                name=\"Validation\",\n                port=self.logger_port,\n            )\n        else:\n            self.logger = logging.getLogger(\"Validation\")\n\n        self.allow_string_features = allow_string_features\n        self.feature_validator = FeatureValidator(\n            feat_type=self.feat_type,\n            logger=self.logger,\n            allow_string_features=self.allow_string_features,\n        )\n        self.target_validator = TargetValidator(\n            is_classification=self.is_classification, logger=self.logger\n        )\n        self._is_fitted = False\n\n    def fit(\n        self,\n        X_train: SUPPORTED_FEAT_TYPES,\n        y_train: SUPPORTED_TARGET_TYPES,\n        X_test: Optional[SUPPORTED_FEAT_TYPES] = None,\n        y_test: Optional[SUPPORTED_TARGET_TYPES] = None,\n    ) -> BaseEstimator:\n        \"\"\"\n        Validates and fit a categorical encoder (if needed) to the features, and\n        a encoder for targets in the case of classification. Specifically:\n\n        For features:\n        Valid data types are enforced (List, np.ndarray, pd.DataFrame, pd.Series, scipy\n        sparse) as well as dimensionality checks\n\n        If the provided data is a pandas DataFrame with categorical/boolean/int columns,\n        such columns will be encoded using an Ordinal Encoder\n\n        For targets:\n        * Checks for dimensionality as well as missing values are performed.\n        * If performing a classification task, the data is going to be encoded\n\n        Parameters\n        ----------\n        X_train: SUPPORTED_FEAT_TYPES\n            A set of features that are going to be validated (type and dimensionality\n            checks). If this data contains categorical columns, an encoder is going to\n            be instantiated and trained with this data.\n\n        y_train: SUPPORTED_TARGET_TYPES\n            A set of targets to encoded if the task is for classification.\n\n        X_test: Optional[SUPPORTED_FEAT_TYPES]\n            A hold out set of features used for checking\n\n        y_test: SUPPORTED_TARGET_TYPES\n            A hold out set of targets used for checking. Additionally, if the current\n            task is a classification task, this y_test categories are also going to be\n            used to fit a pre-processing encoding (to prevent errors on unseen classes).\n\n        Returns\n        -------\n        self\n        \"\"\"\n        # Check that the data is valid\n        if np.shape(X_train)[0] != np.shape(y_train)[0]:\n            raise ValueError(\n                \"Inconsistent number of train datapoints for features and targets,\"\n                \" {} for features and {} for targets\".format(\n                    np.shape(X_train)[0],\n                    np.shape(y_train)[0],\n                )\n            )\n        if X_test is not None and np.shape(X_test)[0] != np.shape(y_test)[0]:\n            raise ValueError(\n                \"Inconsistent number of test datapoints for features and targets,\"\n                \" {} for features and {} for targets\".format(\n                    np.shape(X_test)[0],\n                    np.shape(y_test)[0],\n                )\n            )\n\n        self.feature_validator.fit(X_train, X_test)\n        self.target_validator.fit(y_train, y_test)\n        self._is_fitted = True\n\n        return self\n\n    def transform(\n        self,\n        X: SUPPORTED_FEAT_TYPES,\n        y: Optional[Union[List, pd.Series, pd.DataFrame, np.ndarray]] = None,\n    ) -> Tuple[Union[np.ndarray, pd.DataFrame, spmatrix], Optional[np.ndarray]]:\n        \"\"\"\n        Transform the given target or features to a numpy array\n\n        Parameters\n        ----------\n            X: SUPPORTED_FEAT_TYPES\n                A set of features to transform\n            y: Optional[SUPPORTED_TARGET_TYPES]\n                A set of targets to transform\n\n        Return\n        ------\n            np.ndarray:\n                The transformed features array\n            np.ndarray:\n                The transformed targets array\n        \"\"\"\n        if not self._is_fitted:\n            raise NotFittedError(\n                \"Cannot call transform on a validator that is not fitted\"\n            )\n\n        X_transformed = self.feature_validator.transform(X)\n        if y is not None:\n            y_transformed = self.target_validator.transform(y)\n            return X_transformed, y_transformed\n        else:\n            return X_transformed, None\n\n### Source File Dependency Files Content\n### Dependency File: feature_validator.py\nfrom typing import Dict, List, Optional, Tuple, Union, cast\n\nimport logging\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.api.types import is_numeric_dtype, is_sparse\nfrom scipy.sparse import csr_matrix, spmatrix\nfrom sklearn.base import BaseEstimator\nfrom sklearn.exceptions import NotFittedError\n\nfrom autosklearn.util.logging_ import PickableLoggerAdapter\n\nSUPPORTED_FEAT_TYPES = Union[List, pd.DataFrame, np.ndarray, spmatrix]\n\n\nclass FeatureValidator(BaseEstimator):\n    \"\"\"\n    Checks the input data to Auto-Sklearn.\n\n    It also determines what columns are categorical and which ones are numerical,\n    so that the pre-processing pipeline can process this columns accordingly.\n\n    Attributes\n    ----------\n    feat_type: Optional[List[str]] = None\n        In case the dataset is not a pandas DataFrame:\n            +   If provided, this list indicates which columns should be treated as\n                categorical it is internally transformed into a dictionary that\n                indicates a mapping from column index to categorical/numerical.\n            +   If not provided, by default all columns are treated as numerical\n\n        If the input dataset is of type pandas dataframe, this argument\n        must be none, as the column type will be inferred from the pandas dtypes.\n\n    data_type:\n        Class name of the data type provided during fit.\n    \"\"\"\n\n    def __init__(\n        self,\n        feat_type: Optional[List[str]] = None,\n        logger: Optional[PickableLoggerAdapter] = None,\n        allow_string_features: bool = True,\n    ) -> None:\n        # If a dataframe was provided, we populate\n        # this attribute with a mapping from column to\n        # {numerical | categorical | string}\n        self.feat_type: Optional[Dict[Union[str, int], str]] = None\n        if feat_type is not None:\n            if isinstance(feat_type, dict):\n                self.feat_type = feat_type\n            elif not isinstance(feat_type, List):\n                raise ValueError(\n                    \"Auto-Sklearn expects a list of categorical/\"\n                    \"numerical/string feature types, yet a\"\n                    \" {} was provided\".format(type(feat_type))\n                )\n            else:\n\n                # Convert to a dictionary which will be passed to the ColumnTransformer\n                # Column Transformer supports strings or integer indexes\n                self.feat_type = {i: feat for i, feat in enumerate(feat_type)}\n\n        # Register types to detect unsupported data format changes\n        self.data_type = None  # type: Optional[type]\n        self.dtypes = {}  # type: Dict[str, str]\n\n        self.logger = logger if logger is not None else logging.getLogger(__name__)\n\n        self._is_fitted = False\n        self.allow_string_features = allow_string_features\n\n    def fit(\n        self,\n        X_train: SUPPORTED_FEAT_TYPES,\n        X_test: Optional[SUPPORTED_FEAT_TYPES] = None,\n    ) -> \"FeatureValidator\":\n        \"\"\"\n        Validates input data to Auto-Sklearn.\n        The supported data types are List, numpy arrays and pandas DataFrames.\n        CSR sparse data types are also supported\n\n        Parameters\n        ----------\n        X_train: SUPPORTED_FEAT_TYPES\n            A set of features that are going to be validated (type and dimensionality\n            checks) and a encoder fitted in the case the data needs encoding\n        X_test: Optional[SUPPORTED_FEAT_TYPES]\n            A hold out set of data used for checking\n        \"\"\"\n        # If a list was provided, it will be converted to pandas\n        if isinstance(X_train, List):\n            X_train, X_test = self.list_to_dataframe(X_train, X_test)\n\n        self._check_data(X_train)\n\n        # Handle categorical feature identification for the pipeline\n        if hasattr(X_train, \"iloc\"):\n            if self.feat_type is not None:\n                raise ValueError(\n                    \"When providing a DataFrame to Auto-Sklearn, we extract \"\n                    \"the feature types from the DataFrame.dtypes. That is, \"\n                    \"providing the option feat_type to the fit method is not \"\n                    \"supported when using a Dataframe. Please make sure that the \"\n                    \"type of each column in your DataFrame is properly set. \"\n                    \"More details about having the correct data type in your \"\n                    \"DataFrame can be seen in \"\n                    \"https://pandas.pydata.org/pandas-docs/stable/reference\"\n                    \"/api/pandas.DataFrame.astype.html\"\n                )\n            else:\n                self.feat_type = self.get_feat_type_from_columns(X_train)\n        else:\n            # Numpy array was provided\n            if self.feat_type is None:\n                # Assume numerical columns if a numpy array has no feature types\n                self.feat_type = {i: \"numerical\" for i in range(np.shape(X_train)[1])}\n            else:\n                # Check The feat type provided\n                if len(self.feat_type) != np.shape(X_train)[1]:\n                    raise ValueError(\n                        \"Array feat_type does not have same number of \"\n                        \"variables as X has features. %d vs %d.\"\n                        % (len(self.feat_type), np.shape(X_train)[1])\n                    )\n                if not all([isinstance(f, str) for f in self.feat_type.values()]):\n                    raise ValueError(\n                        \"feat_type must only contain strings: {}\".format(\n                            list(self.feat_type.values()),\n                        )\n                    )\n\n                for ft in self.feat_type.values():\n                    if ft.lower() not in [\"categorical\", \"numerical\", \"string\"]:\n                        raise ValueError(\n                            \"Only `Categorical`, `Numerical` and `String` are \"\n                            \"valid feature types\"\n                        )\n\n        if X_test is not None:\n            self._check_data(X_test)\n\n            if np.shape(X_train)[1] != np.shape(X_test)[1]:\n                raise ValueError(\n                    \"The feature dimensionality of the train and test \"\n                    \"data does not match train({}) != test({})\".format(\n                        np.shape(X_train)[1], np.shape(X_test)[1]\n                    )\n                )\n\n        self._is_fitted = True\n\n        return self\n\n    def transform(\n        self,\n        X: SUPPORTED_FEAT_TYPES,\n    ) -> Union[np.ndarray, spmatrix, pd.DataFrame]:\n        \"\"\"\n        Validates and fit a categorical encoder (if needed) to the features.\n        The supported data types are List, numpy arrays and pandas DataFrames.\n\n        Parameters\n        ----------\n        X_train: SUPPORTED_FEAT_TYPES\n            A set of features, whose categorical features are going to be\n            transformed\n\n        Return\n        ------\n        np.ndarray | spmatrix | pd.DataFrame:\n            The transformed array\n        \"\"\"\n        if not self._is_fitted:\n            raise NotFittedError(\n                \"Cannot call transform on a validator that is not fitted\"\n            )\n\n        # If a list was provided, it will be converted to pandas\n        if isinstance(X, List):\n            X_transformed, _ = self.list_to_dataframe(X)\n        else:\n            X_transformed = X\n\n        # Check the data here so we catch problems on new test data\n        self._check_data(X_transformed)\n\n        # Sparse related transformations\n        # Not all sparse format support index sorting\n        if isinstance(X_transformed, spmatrix):\n            if not isinstance(X_transformed, csr_matrix):\n                self.logger.warning(\n                    f\"Sparse data provided is of type {type(X_transformed)} \"\n                    \"yet Auto-Sklearn only support csr_matrix. Auto-sklearn \"\n                    \"will convert the provided data to the csr_matrix format.\"\n                )\n                X_transformed = X_transformed.tocsr(copy=False)\n\n            X_transformed.sort_indices()\n\n        return X_transformed\n\n    def _check_data(\n        self,\n        X: SUPPORTED_FEAT_TYPES,\n    ) -> None:\n        \"\"\"\n        Feature dimensionality and data type checks\n\n        Parameters\n        ----------\n        X: SUPPORTED_FEAT_TYPES\n            A set of features that are going to be validated (type and dimensionality)\n            and a encoder fitted in the case the data needs encoding\n        \"\"\"\n        # We consider columns that are all nan in a pandas frame as category\n        if hasattr(X, \"columns\"):\n            for column in cast(pd.DataFrame, X).columns:\n                if X[column].isna().all():\n                    X[column] = X[column].astype(\"category\")\n\n        if not isinstance(X, (np.ndarray, pd.DataFrame)) and not isinstance(\n            X, spmatrix\n        ):\n            raise ValueError(\n                \"Auto-sklearn only supports Numpy arrays, Pandas DataFrames,\"\n                \" scipy sparse and Python Lists, yet, the provided input is\"\n                \" of type {}\".format(type(X))\n            )\n\n        if self.data_type is None:\n            self.data_type = type(X)\n\n        if self.data_type != type(X):\n            self.logger.warning(\n                f\"Auto-sklearn previously received features of type {self.data_type} \"\n                f\"yet the current features have type {type(X)}. Changing the dtype \"\n                \"of inputs to an estimator might cause problems\"\n            )\n\n        # Do not support category/string numpy data. Only numbers\n        if hasattr(X, \"dtype\"):\n            if not np.issubdtype(X.dtype.type, np.number):  # type: ignore[union-attr]\n                raise ValueError(\n                    \"When providing a numpy array to Auto-sklearn, the only valid\"\n                    f\" dtypes are numerical ones. The provided data type {X.dtype.type}\"\n                    \" is not supported.\"\n                )\n\n        # Then for Pandas, we do not support Nan in categorical columns\n        if hasattr(X, \"iloc\"):\n            # If entered here, we have a pandas dataframe\n            X = cast(pd.DataFrame, X)\n\n            dtypes = {col: X[col].dtype.name.lower() for col in X.columns}\n            if len(self.dtypes) > 0:\n                if self.dtypes != dtypes:\n                    # To support list, we need to support object inference.\n                    # In extreme cases, the train column might be all integer,\n                    # and the test column might be float.\n                    self.logger.warning(\n                        \"Changing the dtype of the features after fit() is \"\n                        \"not recommended. Fit() method was called with \"\n                        \"{} whereas the new features have {} as type\".format(\n                            self.dtypes,\n                            dtypes,\n                        )\n                    )\n            else:\n                self.dtypes = dtypes\n\n    def get_feat_type_from_columns(\n        self,\n        X: pd.DataFrame,\n    ) -> Dict[Union[str, int], str]:\n        \"\"\"\n        Returns a dictionary that maps pandas dataframe columns to a feature type.\n        This feature type can be categorical, numerical or string\n\n        Parameters\n        ----------\n        X: pd.DataFrame\n            A set of features that are going to be validated (type and dimensionality\n            checks) and a encoder fitted in the case the data needs encoding\n\n        Returns\n        -------\n        feat_type:\n            dictionary with column to feature type mapping\n        \"\"\"\n        # Also, register the feature types for the estimator\n        feat_type = {}\n\n        # Make sure each column is a valid type\n        for i, column in enumerate(X.columns):\n            if is_sparse(X[column]):\n                raise ValueError(\n                    \"Auto-sklearn does not yet support sparse pandas Series.\"\n                    f\" Please convert {column} to a dense format.\"\n                )\n            elif X[column].dtype.name in [\"category\", \"bool\"]:\n                feat_type[column] = \"categorical\"\n            elif X[column].dtype.name == \"string\":\n                if self.allow_string_features:\n                    feat_type[column] = \"string\"\n                else:\n                    feat_type[column] = \"categorical\"\n                    warnings.warn(\n                        f\"you disabled text encoding column {column} will be \"\n                        f\"encoded as category\"\n                    )\n            # Move away from np.issubdtype as it causes\n            # TypeError: data type not understood in certain pandas types\n            elif not is_numeric_dtype(X[column]):\n                if X[column].dtype.name == \"object\":\n                    warnings.warn(\n                        f\"Input Column {column} has generic type object. \"\n                        f\"Autosklearn will treat this column as string. \"\n                        f\"Please ensure that this setting is suitable for your task.\",\n                        UserWarning,\n                    )\n                    if self.allow_string_features:\n                        feat_type[column] = \"string\"\n                    else:\n                        feat_type[column] = \"categorical\"\n                        warnings.warn(\n                            f\"you disabled text encoding column {column} will be\"\n                            f\"encoded as category\"\n                        )\n                elif pd.core.dtypes.common.is_datetime_or_timedelta_dtype(\n                    X[column].dtype\n                ):\n                    raise ValueError(\n                        \"Auto-sklearn does not support time and/or date datatype as \"\n                        f\"given in column {column}. Please convert the time \"\n                        \" information to a numerical value first. One example on how to\"\n                        \" do this can be found on \"\n                        \" https://stats.stackexchange.com/questions/311494/\"\n                    )\n                else:\n                    raise ValueError(\n                        \"Input Column {} has unsupported dtype {}. \"\n                        \"Supported column types are categorical/bool/numerical dtypes. \"\n                        \"Make sure your data is formatted in a correct way, \"\n                        \"before feeding it to Auto-Sklearn.\".format(\n                            column,\n                            X[column].dtype.name,\n                        )\n                    )\n            else:\n                feat_type[column] = \"numerical\"\n        return feat_type\n\n    def list_to_dataframe(\n        self,\n        X_train: SUPPORTED_FEAT_TYPES,\n        X_test: Optional[SUPPORTED_FEAT_TYPES] = None,\n    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n        \"\"\"\n        Converts a list to a DataFrame. In this process, column types are inferred.\n\n        If test data is provided, we proactively match it to train data\n\n        Parameters\n        ----------\n        X_train: SUPPORTED_FEAT_TYPES\n            A set of features that are going to be validated (type and dimensionality\n            checks) and a encoder fitted in the case the data needs encoding\n\n        X_test: Optional[SUPPORTED_FEAT_TYPES]\n            A hold out set of data used for checking\n\n        Returns\n        -------\n        Union[pd.DataFrame, pd.DataFrame]:\n            transformed (train, test) data from list to pandas DataFrame\n        \"\"\"\n        # If a list was provided, it will be converted to pandas\n        X_train = pd.DataFrame(data=X_train).convert_dtypes()\n\n        # Store the dtypes and use in case of re-fit\n        if len(self.dtypes) == 0:\n            self.dtypes = {\n                col: X_train[col].dtype.name.lower() for col in X_train.columns\n            }\n        else:\n            for col in X_train.columns:\n                # Try to convert to the original dtype used to fit the validator\n                # But also be robust to extreme cases (for example, the train data for a\n                # column was all np.int-like and the test data is np.float-type)\n                try:\n                    X_train[col] = X_train[col].astype(self.dtypes[col])\n                except Exception as e:\n                    self.logger.warning(\n                        f\"Failed to format column {col} as {self.dtypes[col]}: {e}\"\n                    )\n                    self.dtypes[col] = X_train[col].dtype.name.lower()\n\n        self.logger.warning(\n            \"The provided feature types to autosklearn are of type list.\"\n            \"Features have been interpreted as: {}\".format(\n                [(col, t) for col, t in zip(X_train.columns, X_train.dtypes)]\n            )\n        )\n        if X_test is not None:\n            if not isinstance(X_test, List):\n                self.logger.warning(\n                    \"Train features are a list while the provided test data\"\n                    \"is {}. X_test will be casted as DataFrame.\".format(type(X_test))\n                )\n            X_test = pd.DataFrame(data=X_test)\n            for col in X_test.columns:\n                try:\n                    X_test[col] = X_test[col].astype(self.dtypes[col])\n                except Exception as e:\n                    self.logger.warning(\n                        f\"Failed to format column {col} as {self.dtypes[col]}: {e}\"\n                    )\n                    self.dtypes[col] = X_test[col].dtype.name.lower()\n\n        return X_train, X_test\n\n\n### Dependency File: logging_.py\n\"\"\"\nFor accessing a logger, please default to using\n`get_named_client_logger(name, host, port)`\n\"\"\"\nfrom typing import Any, Dict, Iterator, Optional, TextIO, Type, cast\n\nimport logging\nimport logging.config\nimport logging.handlers\nimport multiprocessing\nimport os\nimport pickle\nimport random\nimport select\nimport socketserver\nimport struct\nimport threading\nimport warnings\nfrom contextlib import contextmanager\n\nimport yaml\n\n\ndef setup_logger(\n    output_dir: str,\n    filename: Optional[str] = None,\n    distributedlog_filename: Optional[str] = None,\n    logging_config: Optional[Dict] = None,\n) -> None:\n    # logging_config must be a dictionary object specifying the configuration\n    # for the loggers to be used in auto-sklearn.\n    if logging_config is None:\n        with open(os.path.join(os.path.dirname(__file__), \"logging.yaml\"), \"r\") as fh:\n            logging_config = yaml.safe_load(fh)\n\n    # Make sure we have a filename handler\n    if \"handlers\" not in logging_config:\n        logging_config[\"handlers\"] = {}\n    if \"file_handler\" not in logging_config[\"handlers\"]:\n        logging_config[\"handlers\"][\"file_handler\"] = {\n            \"class\": \"logging.FileHandler\",\n            \"level\": \"DEBUG\",\n            \"filename\": \"autosklearn.log\",\n        }\n    if \"distributed_logfile\" not in logging_config[\"handlers\"]:\n        # We have to create a file handler\n        logging_config[\"handlers\"][\"distributed_logfile\"] = {\n            \"class\": \"logging.FileHandler\",\n            \"level\": \"DEBUG\",\n            \"filename\": \"distributed.log\",\n        }\n\n    if filename is None:\n        filename = logging_config[\"handlers\"][\"file_handler\"][\"filename\"]\n\n    if distributedlog_filename is None:\n        distributedlog_filename = logging_config[\"handlers\"][\"distributed_logfile\"][\n            \"filename\"\n        ]\n\n    # Make path absolute only if required\n    # This is needed because this function might be called multiple times with the same\n    # dict, and we don't want /path/path/<name>.log but rather just /path/<name>.log\n    if os.path.sep not in logging_config[\"handlers\"][\"file_handler\"][\"filename\"]:\n        logging_config[\"handlers\"][\"file_handler\"][\"filename\"] = os.path.join(\n            output_dir, filename\n        )\n    if os.path.sep not in logging_config[\"handlers\"][\"distributed_logfile\"][\"filename\"]:\n        logging_config[\"handlers\"][\"distributed_logfile\"][\"filename\"] = os.path.join(\n            output_dir, distributedlog_filename\n        )\n    logging.config.dictConfig(logging_config)\n\n\ndef _create_logger(name: str) -> logging.Logger:\n    return logging.getLogger(name)\n\n\ndef get_logger(name: str) -> \"PickableLoggerAdapter\":\n    logger = PickableLoggerAdapter(name)\n    return logger\n\n\nclass PickableLoggerAdapter:\n    def __init__(self, name: str):\n        self.name = name\n        self.logger = _create_logger(name)\n\n    def __getstate__(self) -> Dict[str, Any]:\n        \"\"\"\n        Method is called when pickle dumps an object.\n\n        Returns\n        -------\n        Dictionary, representing the object state to be pickled. Ignores\n        the self.logger field and only returns the logger name.\n        \"\"\"\n        return {\"name\": self.name}\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        \"\"\"\n        Method is called when pickle loads an object. Retrieves the name and\n        creates a logger.\n\n        Parameters\n        ----------\n        state - dictionary, containing the logger name.\n\n        \"\"\"\n        self.name = state[\"name\"]\n        self.logger = _create_logger(self.name)\n\n    def debug(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.debug(msg, *args, **kwargs)\n\n    def info(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.info(msg, *args, **kwargs)\n\n    def warning(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.warning(msg, *args, **kwargs)\n\n    def error(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.error(msg, *args, **kwargs)\n\n    def exception(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.exception(msg, *args, **kwargs)\n\n    def critical(self, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.critical(msg, *args, **kwargs)\n\n    def log(self, level: int, msg: str, *args: Any, **kwargs: Any) -> None:\n        self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level: int) -> bool:\n        return self.logger.isEnabledFor(level)\n\n\ndef get_named_client_logger(\n    name: str,\n    host: str = \"localhost\",\n    port: int = logging.handlers.DEFAULT_TCP_LOGGING_PORT,\n) -> \"PicklableClientLogger\":\n    logger = PicklableClientLogger(name=name, host=host, port=port)\n    return logger\n\n\ndef _get_named_client_logger(\n    name: str,\n    host: str = \"localhost\",\n    port: int = logging.handlers.DEFAULT_TCP_LOGGING_PORT,\n) -> logging.Logger:\n    \"\"\"\n    When working with a logging server, clients are expected to create a logger using\n    this method. For example, the automl object will create a master that awaits\n    for records sent through tcp to localhost.\n\n    Ensemble builder will then instantiate a logger object that will submit records\n    via a socket handler to the server.\n\n    We do not need to use any format as the server will render the msg as it\n    needs to.\n\n    Parameters\n    ----------\n        name: (str)\n            the name of the logger, used to tag the messages in the main log\n        host: (str)\n            Address of where the server is gonna look for messages\n        port: (str)\n            Port used to communicate with the server\n\n    Returns\n    -------\n        local_loger: a logger object that has a socket handler\n    \"\"\"\n    # Setup the logger configuration\n    # We add client not only to identify that this is the client\n    # communication part of the logger, but to make sure we have\n    # a new singleton with the desired socket handlers\n    local_logger = _create_logger(\"Client-\" + name)\n    local_logger.propagate = False\n    local_logger.setLevel(logging.DEBUG)\n\n    try:\n        # Ignore mypy logging.handlers.SocketHandler has no attribute port\n        # This is not the case clearly, yet MyPy assumes this is not the case\n        # Even when using direct casting or getattr\n        ports = [\n            getattr(handler, \"port\", None) for handler in local_logger.handlers\n        ]  # type: ignore[attr-defined]\n    except AttributeError:\n        # We do not want to log twice but adding multiple times the same\n        # handler. So we check to what ports we communicate to\n        # We can prevent errors with streamers not having a port with this try\n        # block -- but it is a scenario that is unlikely to happen\n        ports = []\n\n    if port not in ports:\n        socketHandler = logging.handlers.SocketHandler(host, port)\n        local_logger.addHandler(socketHandler)\n\n    return local_logger\n\n\nclass PicklableClientLogger(PickableLoggerAdapter):\n    def __init__(self, name: str, host: str, port: int):\n        self.name = name\n        self.host = host\n        self.port = port\n        self.logger = _get_named_client_logger(name=name, host=host, port=port)\n\n    def __getstate__(self) -> Dict[str, Any]:\n        \"\"\"\n        Method is called when pickle dumps an object.\n\n        Returns\n        -------\n        Dictionary, representing the object state to be pickled. Ignores\n        the self.logger field and only returns the logger name.\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"host\": self.host,\n            \"port\": self.port,\n        }\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        \"\"\"\n        Method is called when pickle loads an object. Retrieves the name and\n        creates a logger.\n\n        Parameters\n        ----------\n        state - dictionary, containing the logger name.\n\n        \"\"\"\n        self.name = state[\"name\"]\n        self.host = state[\"host\"]\n        self.port = state[\"port\"]\n        self.logger = _get_named_client_logger(\n            name=self.name,\n            host=self.host,\n            port=self.port,\n        )\n\n\nclass LogRecordStreamHandler(socketserver.StreamRequestHandler):\n    \"\"\"Handler for a streaming logging request.\n\n    This basically logs the record using whatever logging policy is\n    configured locally.\n    \"\"\"\n\n    def handle(self) -> None:\n        \"\"\"\n        Handle multiple requests - each expected to be a 4-byte length,\n        followed by the LogRecord in pickle format. Logs the record\n        according to whatever policy is configured locally.\n        \"\"\"\n        while True:\n            chunk = self.connection.recv(4)  # type: ignore[attr-defined]\n            if len(chunk) < 4:\n                break\n            slen = struct.unpack(\">L\", chunk)[0]\n            chunk = self.connection.recv(slen)  # type: ignore[attr-defined]\n            while len(chunk) < slen:\n                chunk = chunk + self.connection.recv(slen - len(chunk))  # type: ignore[attr-defined]  # noqa: E501\n            obj = self.unPickle(chunk)\n            record = logging.makeLogRecord(obj)\n            self.handleLogRecord(record)\n\n    def unPickle(self, data: Any) -> Any:\n        return pickle.loads(data)\n\n    def handleLogRecord(self, record: logging.LogRecord) -> None:\n        # logname is define in LogRecordSocketReceiver\n        # Yet Mypy Cannot see this. This is needed so that we can\n        # re-use the logging setup for autosklearn into the recieved\n        # records\n        if self.server.logname is not None:  # type: ignore  # noqa\n            name = self.server.logname  # type: ignore  # noqa\n        else:\n            name = record.name\n        logger = logging.getLogger(name)\n        # N.B. EVERY record gets logged. This is because Logger.handle\n        # is normally called AFTER logger-level filtering. If you want\n        # to do filtering, do it at the client end to save wasting\n        # cycles and network bandwidth!\n        logger.handle(record)\n\n\ndef start_log_server(\n    host: str,\n    logname: str,\n    event: threading.Event,\n    port: multiprocessing.Value,\n    filename: str,\n    logging_config: Dict,\n    output_dir: str,\n) -> None:\n    setup_logger(\n        filename=filename, logging_config=logging_config, output_dir=output_dir\n    )\n\n    while True:\n        # Loop until we find a valid port\n        _port = random.randint(10000, 65535)\n        try:\n            receiver = LogRecordSocketReceiver(\n                host=host,\n                port=_port,\n                logname=logname,\n                event=event,\n            )\n            with port.get_lock():\n                port.value = _port\n            receiver.serve_until_stopped()\n            break\n        except OSError:\n            continue\n\n\nclass LogRecordSocketReceiver(socketserver.ThreadingTCPServer):\n    \"\"\"\n    This class implement a entity that receives tcp messages on a given address\n    For further information, please check\n    https://docs.python.org/3/howto/logging-cookbook.html#configuration-server-example\n    \"\"\"\n\n    allow_reuse_address = True\n\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = logging.handlers.DEFAULT_TCP_LOGGING_PORT,\n        handler: Type[LogRecordStreamHandler] = LogRecordStreamHandler,\n        logname: Optional[str] = None,\n        event: threading.Event = None,\n    ):\n        socketserver.ThreadingTCPServer.__init__(self, (host, port), handler)\n        self.timeout = 1\n        self.logname = logname\n        self.event = event\n\n    def serve_until_stopped(self) -> None:\n        while True:\n            rd, wr, ex = select.select([self.socket.fileno()], [], [], self.timeout)\n            if rd:\n                self.handle_request()\n            if self.event is not None and self.event.is_set():\n                break\n\n\n@contextmanager\ndef warnings_to(logger: Optional[PicklableClientLogger] = None) -> Iterator[None]:\n    \"\"\"A context manager to catch warnings and send them to the logger\n\n    If no logger is passed, warnings propogate as they normally would.\n\n    Parameters\n    ----------\n    logger: Optional[PicklableClientLogger] = None\n        The logger to send the warnings to\n    \"\"\"\n    # Send to a logger if one exists\n    if logger:\n\n        with warnings.catch_warnings():\n\n            def to_log(\n                logger: PicklableClientLogger,\n                message: str,\n                category: Type[Warning],  # Weird but that's what it is\n                filename: str,\n                lineno: int,\n                file: Optional[TextIO] = None,\n                line: Optional[str] = None,\n            ) -> None:\n                logger.warning(f\"{filename}:{lineno} {category.__name__}:{message}\")\n\n            # Mypy was complaining that logger didn't exist in `to_log` see here:\n            # https://mypy.readthedocs.io/en/stable/common_issues.html#narrowing-and-inner-functions\n            # we explicitly pass it in and have to force it's type with `cast`\n            warnings.showwarning = lambda *args: to_log(\n                cast(PicklableClientLogger, logger), *args\n            )\n\n            yield\n    # Else do nothing, warnings go to wherever they would without this context\n    else:\n        yield\n\n\n### Dependency File: target_validator.py\nfrom typing import List, Optional, Type, Union, cast\n\nimport logging\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.utils\nfrom pandas.api.types import is_numeric_dtype\nfrom pandas.core.dtypes.base import ExtensionDtype\nfrom scipy.sparse import spmatrix\nfrom sklearn.base import BaseEstimator\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.utils.multiclass import type_of_target\n\nfrom autosklearn.util.logging_ import PickableLoggerAdapter\n\nSUPPORTED_TARGET_TYPES = Union[List, pd.Series, pd.DataFrame, np.ndarray, spmatrix]\nSUPPORTED_XDATA_TYPES = Union[pd.Series, pd.DataFrame, np.ndarray, spmatrix]\n\n\nclass TargetValidator(BaseEstimator):\n    \"\"\"A class to pre-process targets.\n\n    It validates the data provided during fit to make sure it matches Sklearn\n    expectation as well as encoding the targets in case of classification\n\n    Attributes\n    ----------\n    is_classification: bool\n        A bool that indicates if the validator should operate in classification mode.\n        During classification, the targets are encoded.\n\n    encoder: Optional[BaseEstimator]\n        Host a encoder object if the data requires transformation (for example, if\n        provided a categorical column in a pandas DataFrame).\n\n    enc_columns: List[str]\n        List of columns that where encoded\n    \"\"\"\n\n    def __init__(\n        self,\n        is_classification: bool = False,\n        logger: Optional[PickableLoggerAdapter] = None,\n    ) -> None:\n        self.is_classification = is_classification\n\n        self.data_type = None  # type: Optional[type]\n\n        # NOTE: type update\n        #   Encoders don't seems to have a unified base class for\n        #   the methods 'transform'. Could make a `prototype` class that\n        #   duck types for the 'transform', 'fit_transform' methods\n        #   This is however fine while we only use ordinal encoder\n        self.encoder = None  # type: Optional[OrdinalEncoder]\n\n        self.out_dimensionality = None  # type: Optional[int]\n        self.type_of_target = None  # type: Optional[str]\n\n        self.logger = logger if logger is not None else logging.getLogger(__name__)\n\n        # Store the dtype for remapping to correct type\n        self.dtype = None  # type: Optional[Type]\n\n        self._is_fitted = False\n\n    def fit(\n        self,\n        y_train: Union[List, np.ndarray, pd.Series, pd.DataFrame],\n        y_test: Optional[Union[List, np.ndarray, pd.Series, pd.DataFrame]] = None,\n    ) -> \"TargetValidator\":\n        \"\"\"\n        Validates and fit a categorical encoder (if needed) to the targets\n        The supported data types are List, numpy arrays and pandas DataFrames.\n\n        Parameters\n        ----------\n            y_train: SUPPORTED_TARGET_TYPES\n                A set of targets set aside for training\n            y_test: Union[SUPPORTED_TARGET_TYPES]\n                A hold out set of data used of the targets. It is also used to fit the\n                categories of the encoder.\n        \"\"\"\n        # Check that the data is valid\n        self._check_data(y_train)\n\n        shape = np.shape(y_train)\n        if y_test is not None:\n            self._check_data(y_test)\n\n            if len(shape) != len(np.shape(y_test)) or (\n                len(shape) > 1 and (shape[1] != np.shape(y_test)[1])\n            ):\n                raise ValueError(\n                    \"The dimensionality of the train and test\"\n                    \" targets do not match\"\n                    f\" train {np.shape(y_train)}\"\n                    f\" != test {np.shape(y_test)}\"\n                )\n\n            if isinstance(y_train, pd.DataFrame):\n                if not isinstance(y_test, pd.DataFrame):\n                    y_test = pd.DataFrame(y_test)\n\n                if y_train.columns.tolist() != y_test.columns.tolist():\n                    raise ValueError(\n                        \"Train and test targets must both have the\"\n                        f\" same columns, yet y={y_train.columns}\"\n                        f\" and y_test={y_test.columns}\"\n                    )\n\n                if list(y_train.dtypes) != list(y_test.dtypes):\n                    raise ValueError(\n                        \"Train and test targets must both have the same dtypes\"\n                    )\n\n        if self.out_dimensionality is None:\n            self.out_dimensionality = 1 if len(shape) == 1 else shape[1]\n        else:\n            _n_outputs = 1 if len(shape) == 1 else shape[1]\n            if self.out_dimensionality != _n_outputs:\n                raise ValueError(\n                    \"Number of outputs changed from %d to %d!\"\n                    % (self.out_dimensionality, _n_outputs)\n                )\n\n        # Fit on the training data\n        self._fit(y_train, y_test)\n\n        self._is_fitted = True\n\n        return self\n\n    def _fit(\n        self,\n        y_train: Union[List, np.ndarray, pd.Series, pd.DataFrame],\n        y_test: Optional[Union[List, np.ndarray, pd.Series, pd.DataFrame]] = None,\n    ) -> \"TargetValidator\":\n        \"\"\"\n        If dealing with classification, this utility encodes the targets.\n\n        It does so by also using the classes from the test data, to prevent encoding\n        errors\n\n        Parameters\n        ----------\n            y_train: SUPPORTED_TARGET_TYPES\n                The labels of the current task. They are going to be encoded in case\n                of classification\n            y_test: Optional[SUPPORTED_TARGET_TYPES]\n                A holdout set of labels\n        \"\"\"\n        if not self.is_classification or self.type_of_target == \"multilabel-indicator\":\n            # Only fit an encoder for classification tasks\n            # Also, encoding multilabel indicator data makes the data multiclass\n            # Let the user employ a MultiLabelBinarizer if needed\n            self.encoder = None\n            return self\n\n        # Validate the shape of the input\n        shape = np.shape(y_train)\n        ndim = len(shape)\n        if ndim > 1 and shape[1] != 1:\n            # We should not reach this if statement, we check for type of targets before\n            raise ValueError(\n                \"Multi-dimensional classification is not yet\"\n                \" supported. Encoding multidimensional data\"\n                \" converts multiple columns to a 1 dimensional encoding.\"\n                f\" Data involved = {shape}/{self.type_of_target}\"\n            )\n\n        # Create the encoder\n        self.encoder = OrdinalEncoder(handle_unknown=\"error\")\n\n        # Clear typing to just numpy arrays and pandas\n        if isinstance(y_train, List):\n            y = np.asarray(y_train)\n        else:\n            y = y_train\n\n        # Attempt to store dtype if it's numeric, we use this in\n        # inverse_transform to try corretly restore it's dtype\n        if isinstance(y, pd.Series):\n            if isinstance(y.dtype, ExtensionDtype):\n                warnings.warn(\n                    \"Fitting transformer with a pandas series which\"\n                    f\" has the dtype {y.dtype}. Inverse transform\"\n                    \" may not be able preserve dtype when converting\"\n                    \" to np.ndarray\"\n                )\n            if is_numeric_dtype(y.dtype):\n                self.dtype = y.dtype\n        elif isinstance(y, pd.DataFrame):\n            if is_numeric_dtype(y.dtypes[0]):\n                self.dtype = y.dtypes[0]\n        else:\n            if is_numeric_dtype(y.dtype):\n                self.dtype = y.dtype\n\n        # Merge y_test and y_train for encoding\n        if y_test is not None:\n            if isinstance(y, (pd.Series, pd.DataFrame)):\n                if isinstance(y, pd.Series):\n                    y_test = pd.Series(y_test)\n                else:\n                    y_test = pd.DataFrame(y_test)\n\n                y = pd.concat([y, y_test], ignore_index=True, sort=False)\n            else:\n                y = np.concatenate([y, y_test])\n\n        # If one dimensional, we need to reshape it\n        # [1, 2, 3] -> [[1], [2], [3]]\n        if y.ndim == 1:\n            if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n                y = y.to_numpy()\n\n            y = y.reshape(-1, 1)\n\n        # Fit encoder\n        self.encoder.fit(y)\n\n        return self\n\n    def transform(\n        self,\n        y: Union[List, pd.Series, pd.DataFrame, np.ndarray],\n    ) -> np.ndarray:\n        \"\"\"\n        Validates and fit a categorical encoder (if needed) to the features.\n        The supported data types are List, numpy arrays and pandas DataFrames.\n\n        Parameters\n        ----------\n            y: SUPPORTED_TARGET_TYPES\n                A set of targets that are going to be encoded if the current task\n                is classification\n        Return\n        ------\n            np.ndarray:\n                The transformed array\n        \"\"\"\n        if not self._is_fitted:\n            raise NotFittedError(\"TargetValidator must have fit() called first\")\n\n        # Check the data here so we catch problems on new test data\n        self._check_data(y)\n\n        # Clear the types List and DataFrame off of y\n        if isinstance(y, List):\n            y_transformed = np.asarray(y)\n        elif isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            y_transformed = y.to_numpy()\n        else:\n            y_transformed = y\n\n        if self.encoder is None:\n            return y_transformed\n\n        # The Ordinal encoder expects a 2 dimensional input.\n        # The targets are 1 dimensional, so reshape to match the expected shape\n        if y_transformed.ndim == 1:\n            y_transformed = y_transformed.reshape(-1, 1)\n            y_transformed = self.encoder.transform(y_transformed).reshape(-1)\n        else:\n            y_transformed = self.encoder.transform(y_transformed)\n\n        # check_array ensures correct numerical features for array\n        y_transformed = sklearn.utils.check_array(\n            y_transformed,\n            force_all_finite=True,\n            accept_sparse=\"csr\",\n            ensure_2d=False,\n        )\n\n        # Ensure we flatten any arrays [[1], [2], [1], [1], ...] to [1,2,1,1,...]\n        if y_transformed.ndim == 2 and y_transformed.shape[1] == 1:\n            y_transformed = y_transformed.ravel()\n\n        return y_transformed\n\n    def inverse_transform(\n        self,\n        y: Union[List, pd.Series, pd.DataFrame, np.ndarray],\n    ) -> np.ndarray:\n        \"\"\"Revert any encoding transformation done on a target array.\n\n        Parameters\n        ----------\n        y: Union[List, np.ndarray, pd.DataFrame, pd.Series]\n            Target array to be transformed back to original form before encoding\n\n        Return\n        ------\n        np.ndarray:\n            The transformed array\n        \"\"\"\n        if not self._is_fitted:\n            raise NotFittedError(\"TargetValidator must have fit() called first\")\n\n        # If there's no encoder, just return it as the expected ndarray\n        if self.encoder is None:\n            return np.asarray(y)\n\n        # If it's one dimensional, we need to convert it\n        # [1,2,3] -> [[1], [2], [3]]\n        shape = np.shape(y)\n        if len(shape) == 1:\n\n            if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n                y = y.to_numpy()\n            elif isinstance(y, List):\n                y = np.asarray(y)\n\n            # Perform the transformation and reshape it\n            y = y.reshape(-1, 1)\n            y_inv = self.encoder.inverse_transform(y)\n            y_inv = y_inv.reshape(-1)\n\n        else:\n            # Otherwise we can can just invert it as expected\n            y_inv = self.encoder.inverse_transform(y)\n\n        # Inverse transform returns a numpy array of type object\n        # This breaks certain metrics as accuracy, which makes type_of_target be unknown\n        # If while fit a dtype was observed, we try to honor that dtype\n        if self.dtype is not None:\n            y_inv = y_inv.astype(self.dtype)\n\n        return y_inv\n\n    def is_single_column_target(self) -> bool:\n        \"\"\"Output is encoded with a single column encoding\"\"\"\n        return self.out_dimensionality == 1\n\n    def _check_data(\n        self,\n        y: SUPPORTED_TARGET_TYPES,\n    ) -> None:\n        \"\"\"\n        Perform dimensionality and data type checks on the targets\n\n        Parameters\n        ----------\n        y: Union[np.ndarray, pd.DataFrame, pd.Series]\n            A set of features whose dimensionality and data type is going to be checked\n        \"\"\"\n        if not isinstance(\n            y, (np.ndarray, pd.DataFrame, List, pd.Series)\n        ) and not isinstance(y, spmatrix):\n            raise ValueError(\n                \"Auto-sklearn only supports Numpy arrays, Pandas DataFrames,\"\n                \" pd.Series, sparse data and Python Lists as targets, yet, \"\n                \"the provided input is of type {}\".format(type(y))\n            )\n\n        if isinstance(y, spmatrix) and not np.issubdtype(y.dtype.type, np.number):\n            raise ValueError(\n                \"When providing a sparse matrix as targets, the only supported \"\n                \"values are numerical. Please consider using a dense\"\n                \" instead.\"\n            )\n\n        if self.data_type is None:\n            self.data_type = type(y)\n        if self.data_type != type(y):\n            self.logger.warning(\n                \"Auto-sklearn previously received targets of type %s \"\n                \"yet the current features have type %s. Changing the dtype \"\n                \"of inputs to an estimator might cause problems\"\n                % (\n                    str(self.data_type),\n                    str(type(y)),\n                ),\n            )\n\n        # No Nan is supported\n        has_nan_values = False\n        if hasattr(y, \"iloc\"):\n            has_nan_values = cast(pd.DataFrame, y).isnull().values.any()\n\n        if isinstance(y, spmatrix):\n            has_nan_values = not np.array_equal(y.data, y.data)\n        else:\n            # List and array like values are considered here\n            # np.isnan cannot work on strings, so we have to check for every element\n            # but NaN, are not equal to themselves:\n            has_nan_values = not np.array_equal(y, y)\n        if has_nan_values:\n            raise ValueError(\n                \"Target values cannot contain missing/NaN values. \"\n                \"This is not supported by scikit-learn. \"\n            )\n\n        # Pandas Series is not supported for multi-label indicator\n        # This format checks are done by type of target\n        try:\n            self.type_of_target = type_of_target(y)\n        except Exception as e:\n            raise ValueError(\n                \"The provided data could not be interpreted by Sklearn. \"\n                \"While determining the type of the targets via type_of_target \"\n                \"run into exception: {}.\".format(e)\n            )\n\n        supported_output_types = (\n            \"binary\",\n            \"continuous\",\n            \"continuous-multioutput\",\n            \"multiclass\",\n            \"multilabel-indicator\",\n            # Notice unknown/multiclass-multioutput are not supported\n            # This can only happen during testing only as estimators\n            # should filter out unsupported types.\n        )\n        if self.type_of_target not in supported_output_types:\n            raise ValueError(\n                \"Provided targets are not supported by Auto-Sklearn. \"\n                \"Provided type is {} whereas supported types are {}.\".format(\n                    self.type_of_target, supported_output_types\n                )\n            )\n\n    @property\n    def classes_(self) -> np.ndarray:\n        \"\"\"\n        Complies with scikit learn classes_ attribute,\n        which consist of a ndarray of shape (n_classes,)\n        where n_classes are the number of classes seen while fitting\n        a encoder to the targets.\n\n        Returns\n        -------\n        classes_: np.ndarray\n            The unique classes seen during encoding of a classifier\n        \"\"\"\n        if self.encoder is None:\n            return np.array([])\n        else:\n            return self.encoder.categories_[0]\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:43:24"}, {"role": "assistant", "content": "```python\nimport numpy as np\nimport pandas as pd\nimport sklearn.datasets\nimport sklearn.model_selection\nfrom scipy import sparse\n\nfrom autosklearn.data.validation import InputValidator\n\nimport pytest\n\n\n@pytest.mark.parametrize(\"openmlid\", [2, 40975, 40984])\n@pytest.mark.parametrize(\"as_frame\", [True, False])\ndef test_data_validation_for_classification(openmlid, as_frame):\n    x, y = sklearn.datasets.fetch_openml(\n        data_id=openmlid, return_X_y=True, as_frame=as_frame\n    )\n    validator = InputValidator(is_classification=True)\n\n    if as_frame:\n        # NaN is not supported in categories, so\n        # drop columns with them.\n        nan_cols = [i for i in x.columns if x[i].isnull().any()]\n        cat_cols = [i for i in x.columns if x[i].dtype.name in [\"category\", \"bool\"]]\n        unsupported_columns = list(set(nan_cols) & set(cat_cols))\n        if len(unsupported_columns) > 0:\n            x.drop(unsupported_columns, axis=1, inplace=True)\n\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n        x, y, test_size=0.33, random_state=0\n    )\n\n    validator.fit(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n\n    X_train_t, y_train_t = validator.transform(X_train, y_train)\n    assert np.shape(X_train) == np.shape(X_train_t)\n\n    # Leave columns that are complete NaN\n    # The sklearn pipeline will handle that\n    if as_frame and np.any(pd.isnull(X_train).values.all(axis=0)):\n        assert np.any(pd.isnull(X_train_t).values.all(axis=0))\n    elif not as_frame and np.any(pd.isnull(X_train).all(axis=0)):\n        assert np.any(pd.isnull(X_train_t).all(axis=0))\n\n    # make sure everything was encoded to number\n    assert np.issubdtype(y_train_t.dtype, np.number)\n\n    # Make sure we created a feat type\n    validator.feature_validator.feat_type is not None\n\n\n@pytest.mark.parametrize(\"openmlid\", [505, 546, 531])\n@pytest.mark.parametrize(\"as_frame\", [True, False])\ndef test_data_validation_for_regression(openmlid, as_frame):\n    x, y = sklearn.datasets.fetch_openml(\n        data_id=openmlid, return_X_y=True, as_frame=as_frame\n    )\n    validator = InputValidator(is_classification=False)\n\n    if as_frame:\n        # NaN is not supported in categories, so\n        # drop columns with them.\n        nan_cols = [i for i in x.columns if x[i].isnull().any()]\n        cat_cols = [i for i in x.columns if x[i].dtype.name in [\"category\", \"bool\"]]\n        unsupported_columns = list(set(nan_cols) & set(cat_cols))\n        if len(unsupported_columns) > 0:\n            x.drop(unsupported_columns, axis=1, inplace=True)\n\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n        x, y, test_size=0.33, random_state=0\n    )\n\n    validator.fit(X_train=X_train, y_train=y_train)\n\n    X_train_t, y_train_t = validator.transform(X_train, y_train)\n    assert np.shape(X_train) == np.shape(X_train_t)\n\n    # Leave columns that are complete NaN\n    # The sklearn pipeline will handle that\n    if as_frame and np.any(pd.isnull(X_train).values.all(axis=0)):\n        assert np.any(pd.isnull(X_train_t).values.all(axis=0))\n    elif not as_frame and np.any(pd.isnull(X_train).all(axis=0)):\n        assert np.any(pd.isnull(X_train_t).all(axis=0))\n\n    validator.feature_validator.feat_type is not None\n\n\ndef test_sparse_data_validation_for_regression():\n    X, y = sklearn.datasets.make_regression(\n        n_samples=100, n_features=50, random_state=0\n    )\n    X_sp = sparse.coo_matrix(X)\n    validator = InputValidator(is_classification=False)\n\n    validator.fit(X_train=X_sp, y_train=y)\n\n    X_t, y_t = validator.transform(X, y)\n    assert np.shape(X) == np.shape(X_t)\n\n    # make sure everything was encoded to number\n    assert np.issubdtype(X_t.dtype, np.number)\n    assert np.issubdtype(y_t.dtype, np.number)\n\n    # Make sure we can change the sparse format\n    X_t, y_t = validator.transform(sparse.csr_matrix(X), y)\n\n\ndef test_validation_unsupported():\n    \"\"\"\n    Makes sure we raise a proper message to the user,\n    when providing not supported data input\n    \"\"\"\n    validator = InputValidator()\n    with pytest.raises(ValueError, match=r\"Inconsistent number of train datapoints.*\"):\n        validator.fit(\n            X_train=np.array([[0, 1, 0], [0, 1, 1]]),\n            y_train=np.array([0, 1, 0, 0, 0, 0]),\n        )\n    with pytest.raises(ValueError, match=r\"Inconsistent number of test datapoints.*\"):\n        validator.fit(\n            X_train=np.array([[0, 1, 0], [0, 1, 1]]),\n            y_train=np.array([0, 1]),\n            X_test=np.array([[0, 1, 0], [0, 1, 1]]),\n            y_test=np.array([0, 1, 0, 0, 0, 0]),\n        )\n    with pytest.raises(\n        ValueError, match=r\"Cannot call transform on a validator .*fitted\"\n    ):\n        validator.transform(\n            X=np.array([[0, 1, 0], [0, 1, 1]]),\n            y=np.array([0, 1]),\n        )\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: scrapy\nTest File Path: scrapy\\test_command_check\\test_command_check.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nimport argparse\nimport time\nfrom collections import defaultdict\nfrom unittest import TextTestResult as _TextTestResult\nfrom unittest import TextTestRunner\n\nfrom scrapy.commands import ScrapyCommand\nfrom scrapy.contracts import ContractsManager\nfrom scrapy.utils.conf import build_component_list\nfrom scrapy.utils.misc import load_object, set_environ\n\n\nclass TextTestResult(_TextTestResult):\n    def printSummary(self, start: float, stop: float) -> None:\n        write = self.stream.write\n        writeln = self.stream.writeln\n\n        run = self.testsRun\n        plural = \"s\" if run != 1 else \"\"\n\n        writeln(self.separator2)\n        writeln(f\"Ran {run} contract{plural} in {stop - start:.3f}s\")\n        writeln()\n\n        infos = []\n        if not self.wasSuccessful():\n            write(\"FAILED\")\n            failed, errored = map(len, (self.failures, self.errors))\n            if failed:\n                infos.append(f\"failures={failed}\")\n            if errored:\n                infos.append(f\"errors={errored}\")\n        else:\n            write(\"OK\")\n\n        if infos:\n            writeln(f\" ({', '.join(infos)})\")\n        else:\n            write(\"\\n\")\n\n\nclass Command(ScrapyCommand):\n    requires_project = True\n    default_settings = {\"LOG_ENABLED\": False}\n\n    def syntax(self) -> str:\n        return \"[options] <spider>\"\n\n    def short_desc(self) -> str:\n        return \"Check spider contracts\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-l\",\n            \"--list\",\n            dest=\"list\",\n            action=\"store_true\",\n            help=\"only list contracts, without checking them\",\n        )\n        parser.add_argument(\n            \"-v\",\n            \"--verbose\",\n            dest=\"verbose\",\n            default=False,\n            action=\"store_true\",\n            help=\"print contract tests for all spiders\",\n        )\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        # load contracts\n        contracts = build_component_list(self.settings.getwithbase(\"SPIDER_CONTRACTS\"))\n        conman = ContractsManager(load_object(c) for c in contracts)\n        runner = TextTestRunner(verbosity=2 if opts.verbose else 1)\n        result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)\n\n        # contract requests\n        contract_reqs = defaultdict(list)\n\n        assert self.crawler_process\n        spider_loader = self.crawler_process.spider_loader\n\n        with set_environ(SCRAPY_CHECK=\"true\"):\n            for spidername in args or spider_loader.list():\n                spidercls = spider_loader.load(spidername)\n                spidercls.start_requests = lambda s: conman.from_spider(s, result)  # type: ignore[assignment,method-assign,return-value]\n\n                tested_methods = conman.tested_methods_from_spidercls(spidercls)\n                if opts.list:\n                    for method in tested_methods:\n                        contract_reqs[spidercls.name].append(method)\n                elif tested_methods:\n                    self.crawler_process.crawl(spidercls)\n\n            # start checks\n            if opts.list:\n                for spider, methods in sorted(contract_reqs.items()):\n                    if not methods and not opts.verbose:\n                        continue\n                    print(spider)\n                    for method in sorted(methods):\n                        print(f\"  * {method}\")\n            else:\n                start = time.time()\n                self.crawler_process.start()\n                stop = time.time()\n\n                result.printErrors()\n                result.printSummary(start, stop)\n                self.exitcode = int(not result.wasSuccessful())\n\n### Source File Dependency Files Content\n### Dependency File: conf.py\nfrom __future__ import annotations\n\nimport numbers\nimport os\nimport sys\nfrom configparser import ConfigParser\nfrom operator import itemgetter\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Callable, cast\n\nfrom scrapy.exceptions import UsageError\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.deprecate import update_classpath\nfrom scrapy.utils.python import without_none_values\n\nif TYPE_CHECKING:\n    from collections.abc import Collection, Iterable, Mapping, MutableMapping\n\n\ndef build_component_list(\n    compdict: MutableMapping[Any, Any],\n    *,\n    convert: Callable[[Any], Any] = update_classpath,\n) -> list[Any]:\n    \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n\n    def _check_components(complist: Collection[Any]) -> None:\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError(\n                f\"Some paths in {complist!r} convert to the same object, \"\n                \"please update your settings\"\n            )\n\n    def _map_keys(compdict: Mapping[Any, Any]) -> BaseSettings | dict[Any, Any]:\n        if isinstance(compdict, BaseSettings):\n            compbs = BaseSettings()\n            for k, v in compdict.items():\n                prio = compdict.getpriority(k)\n                assert prio is not None\n                if compbs.getpriority(convert(k)) == prio:\n                    raise ValueError(\n                        f\"Some paths in {list(compdict.keys())!r} \"\n                        \"convert to the same \"\n                        \"object, please update your settings\"\n                    )\n                compbs.set(convert(k), v, priority=prio)\n            return compbs\n        _check_components(compdict)\n        return {convert(k): v for k, v in compdict.items()}\n\n    def _validate_values(compdict: Mapping[Any, Any]) -> None:\n        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in compdict.items():\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError(\n                    f\"Invalid value {value} for component {name}, \"\n                    \"please provide a real number or None instead\"\n                )\n\n    _validate_values(compdict)\n    compdict = without_none_values(_map_keys(compdict))\n    return [k for k, v in sorted(compdict.items(), key=itemgetter(1))]\n\n\ndef arglist_to_dict(arglist: list[str]) -> dict[str, str]:\n    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n    dict\n    \"\"\"\n    return dict(x.split(\"=\", 1) for x in arglist)\n\n\ndef closest_scrapy_cfg(\n    path: str | os.PathLike = \".\",\n    prevpath: str | os.PathLike | None = None,\n) -> str:\n    \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n    directory and its parents\n    \"\"\"\n    if prevpath is not None and str(path) == str(prevpath):\n        return \"\"\n    path = Path(path).resolve()\n    cfgfile = path / \"scrapy.cfg\"\n    if cfgfile.exists():\n        return str(cfgfile)\n    return closest_scrapy_cfg(path.parent, path)\n\n\ndef init_env(project: str = \"default\", set_syspath: bool = True) -> None:\n    \"\"\"Initialize environment to use command-line tool from inside a project\n    dir. This sets the Scrapy settings module and modifies the Python path to\n    be able to locate the project module.\n    \"\"\"\n    cfg = get_config()\n    if cfg.has_option(\"settings\", project):\n        os.environ[\"SCRAPY_SETTINGS_MODULE\"] = cfg.get(\"settings\", project)\n    closest = closest_scrapy_cfg()\n    if closest:\n        projdir = str(Path(closest).parent)\n        if set_syspath and projdir not in sys.path:\n            sys.path.append(projdir)\n\n\ndef get_config(use_closest: bool = True) -> ConfigParser:\n    \"\"\"Get Scrapy config file as a ConfigParser\"\"\"\n    sources = get_sources(use_closest)\n    cfg = ConfigParser()\n    cfg.read(sources)\n    return cfg\n\n\ndef get_sources(use_closest: bool = True) -> list[str]:\n    xdg_config_home = (\n        os.environ.get(\"XDG_CONFIG_HOME\") or Path(\"~/.config\").expanduser()\n    )\n    sources = [\n        \"/etc/scrapy.cfg\",\n        r\"c:\\scrapy\\scrapy.cfg\",\n        str(Path(xdg_config_home) / \"scrapy.cfg\"),\n        str(Path(\"~/.scrapy.cfg\").expanduser()),\n    ]\n    if use_closest:\n        sources.append(closest_scrapy_cfg())\n    return sources\n\n\ndef feed_complete_default_values_from_settings(\n    feed: dict[str, Any], settings: BaseSettings\n) -> dict[str, Any]:\n    out = feed.copy()\n    out.setdefault(\"batch_item_count\", settings.getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\"))\n    out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n    out.setdefault(\"fields\", settings.getdictorlist(\"FEED_EXPORT_FIELDS\") or None)\n    out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n    out.setdefault(\"uri_params\", settings[\"FEED_URI_PARAMS\"])\n    out.setdefault(\"item_export_kwargs\", {})\n    if settings[\"FEED_EXPORT_INDENT\"] is None:\n        out.setdefault(\"indent\", None)\n    else:\n        out.setdefault(\"indent\", settings.getint(\"FEED_EXPORT_INDENT\"))\n    return out\n\n\ndef feed_process_params_from_cli(\n    settings: BaseSettings,\n    output: list[str],\n    *,\n    overwrite_output: list[str] | None = None,\n) -> dict[str, dict[str, Any]]:\n    \"\"\"\n    Receives feed export params (from the 'crawl' or 'runspider' commands),\n    checks for inconsistencies in their quantities and returns a dictionary\n    suitable to be used as the FEEDS setting.\n    \"\"\"\n    valid_output_formats: Iterable[str] = without_none_values(\n        cast(dict[str, str], settings.getwithbase(\"FEED_EXPORTERS\"))\n    ).keys()\n\n    def check_valid_format(output_format: str) -> None:\n        if output_format not in valid_output_formats:\n            raise UsageError(\n                f\"Unrecognized output format '{output_format}'. \"\n                f\"Set a supported one ({tuple(valid_output_formats)}) \"\n                \"after a colon at the end of the output URI (i.e. -o/-O \"\n                \"<URI>:<FORMAT>) or as a file extension.\"\n            )\n\n    overwrite = False\n    if overwrite_output:\n        if output:\n            raise UsageError(\n                \"Please use only one of -o/--output and -O/--overwrite-output\"\n            )\n        output = overwrite_output\n        overwrite = True\n\n    result: dict[str, dict[str, Any]] = {}\n    for element in output:\n        try:\n            feed_uri, feed_format = element.rsplit(\":\", 1)\n            check_valid_format(feed_format)\n        except (ValueError, UsageError):\n            feed_uri = element\n            feed_format = Path(element).suffix.replace(\".\", \"\")\n        else:\n            if feed_uri == \"-\":\n                feed_uri = \"stdout:\"\n        check_valid_format(feed_format)\n        result[feed_uri] = {\"format\": feed_format}\n        if overwrite:\n            result[feed_uri][\"overwrite\"] = True\n\n    # FEEDS setting should take precedence over the matching CLI options\n    result.update(settings.getdict(\"FEEDS\"))\n\n    return result\n\n\n### Dependency File: misc.py\n\"\"\"Helper functions which don't fit anywhere else\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nimport hashlib\nimport inspect\nimport os\nimport re\nimport warnings\nfrom collections import deque\nfrom collections.abc import Iterable\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom importlib import import_module\nfrom pkgutil import iter_modules\nfrom typing import IO, TYPE_CHECKING, Any, TypeVar, cast\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.item import Item\nfrom scrapy.utils.datatypes import LocalWeakReferencedCache\n\nif TYPE_CHECKING:\n    from collections.abc import Callable, Iterator\n    from types import ModuleType\n\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n\n\n_ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\nT = TypeVar(\"T\")\n\n\ndef arg_to_iter(arg: Any) -> Iterable[Any]:\n    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n    value, or an iterable.\n\n    Exception: if arg is a dict, [arg] will be returned\n    \"\"\"\n    if arg is None:\n        return []\n    if not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, \"__iter__\"):\n        return cast(Iterable[Any], arg)\n    return [arg]\n\n\ndef load_object(path: str | Callable[..., Any]) -> Any:\n    \"\"\"Load an object given its absolute object path, and return it.\n\n    The object can be the import path of a class, function, variable or an\n    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.\n\n    If ``path`` is not a string, but is a callable object, such as a class or\n    a function, then return it as is.\n    \"\"\"\n\n    if not isinstance(path, str):\n        if callable(path):\n            return path\n        raise TypeError(\n            f\"Unexpected argument type, expected string or object, got: {type(path)}\"\n        )\n\n    try:\n        dot = path.rindex(\".\")\n    except ValueError:\n        raise ValueError(f\"Error loading object '{path}': not a full path\")\n\n    module, name = path[:dot], path[dot + 1 :]\n    mod = import_module(module)\n\n    try:\n        obj = getattr(mod, name)\n    except AttributeError:\n        raise NameError(f\"Module '{module}' doesn't define any object named '{name}'\")\n\n    return obj\n\n\ndef walk_modules(path: str) -> list[ModuleType]:\n    \"\"\"Loads a module and all its submodules from the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    For example: walk_modules('scrapy.utils')\n    \"\"\"\n\n    mods: list[ModuleType] = []\n    mod = import_module(path)\n    mods.append(mod)\n    if hasattr(mod, \"__path__\"):\n        for _, subpath, ispkg in iter_modules(mod.__path__):\n            fullpath = path + \".\" + subpath\n            if ispkg:\n                mods += walk_modules(fullpath)\n            else:\n                submod = import_module(fullpath)\n                mods.append(submod)\n    return mods\n\n\ndef md5sum(file: IO[bytes]) -> str:\n    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n    whole content in memory.\n\n    >>> from io import BytesIO\n    >>> md5sum(BytesIO(b'file content to hash'))\n    '784406af91dd5a54fbb9c84c2236595a'\n    \"\"\"\n    warnings.warn(\n        (\n            \"The scrapy.utils.misc.md5sum function is deprecated and will be \"\n            \"removed in a future version of Scrapy.\"\n        ),\n        ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    m = hashlib.md5()  # noqa: S324\n    while True:\n        d = file.read(8096)\n        if not d:\n            break\n        m.update(d)\n    return m.hexdigest()\n\n\ndef rel_has_nofollow(rel: str | None) -> bool:\n    \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n    return rel is not None and \"nofollow\" in rel.replace(\",\", \" \").split()\n\n\ndef create_instance(objcls, settings, crawler, *args, **kwargs):\n    \"\"\"Construct a class instance using its ``from_crawler`` or\n    ``from_settings`` constructors, if available.\n\n    At least one of ``settings`` and ``crawler`` needs to be different from\n    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.\n    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be\n    tried.\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructors.\n\n    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n\n    .. versionchanged:: 2.2\n       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n       extension has not been implemented correctly).\n    \"\"\"\n    warnings.warn(\n        \"The create_instance() function is deprecated. \"\n        \"Please use build_from_crawler() instead.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n\n    if settings is None:\n        if crawler is None:\n            raise ValueError(\"Specify at least one of settings and crawler.\")\n        settings = crawler.settings\n    if crawler and hasattr(objcls, \"from_crawler\"):\n        instance = objcls.from_crawler(crawler, *args, **kwargs)\n        method_name = \"from_crawler\"\n    elif hasattr(objcls, \"from_settings\"):\n        instance = objcls.from_settings(settings, *args, **kwargs)\n        method_name = \"from_settings\"\n    else:\n        instance = objcls(*args, **kwargs)\n        method_name = \"__new__\"\n    if instance is None:\n        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n    return instance\n\n\ndef build_from_crawler(\n    objcls: type[T], crawler: Crawler, /, *args: Any, **kwargs: Any\n) -> T:\n    \"\"\"Construct a class instance using its ``from_crawler`` or ``from_settings`` constructor.\n\n    .. versionadded:: 2.12\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n\n    Raises ``TypeError`` if the resulting instance is ``None``.\n    \"\"\"\n    if hasattr(objcls, \"from_crawler\"):\n        instance = objcls.from_crawler(crawler, *args, **kwargs)  # type: ignore[attr-defined]\n        method_name = \"from_crawler\"\n    elif hasattr(objcls, \"from_settings\"):\n        warnings.warn(\n            f\"{objcls.__qualname__} has from_settings() but not from_crawler().\"\n            \" This is deprecated and calling from_settings() will be removed in a future\"\n            \" Scrapy version. You can implement a simple from_crawler() that calls\"\n            \" from_settings() with crawler.settings.\",\n            category=ScrapyDeprecationWarning,\n            stacklevel=2,\n        )\n        instance = objcls.from_settings(crawler.settings, *args, **kwargs)  # type: ignore[attr-defined]\n        method_name = \"from_settings\"\n    else:\n        instance = objcls(*args, **kwargs)\n        method_name = \"__new__\"\n    if instance is None:\n        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n    return cast(T, instance)\n\n\n@contextmanager\ndef set_environ(**kwargs: str) -> Iterator[None]:\n    \"\"\"Temporarily set environment variables inside the context manager and\n    fully restore previous environment afterwards\n    \"\"\"\n\n    original_env = {k: os.environ.get(k) for k in kwargs}\n    os.environ.update(kwargs)\n    try:\n        yield\n    finally:\n        for k, v in original_env.items():\n            if v is None:\n                del os.environ[k]\n            else:\n                os.environ[k] = v\n\n\ndef walk_callable(node: ast.AST) -> Iterable[ast.AST]:\n    \"\"\"Similar to ``ast.walk``, but walks only function body and skips nested\n    functions defined within the node.\n    \"\"\"\n    todo: deque[ast.AST] = deque([node])\n    walked_func_def = False\n    while todo:\n        node = todo.popleft()\n        if isinstance(node, ast.FunctionDef):\n            if walked_func_def:\n                continue\n            walked_func_def = True\n        todo.extend(ast.iter_child_nodes(node))\n        yield node\n\n\n_generator_callbacks_cache = LocalWeakReferencedCache(limit=128)\n\n\ndef is_generator_with_return_value(callable: Callable[..., Any]) -> bool:\n    \"\"\"\n    Returns True if a callable is a generator function which includes a\n    'return' statement with a value different than None, False otherwise\n    \"\"\"\n    if callable in _generator_callbacks_cache:\n        return bool(_generator_callbacks_cache[callable])\n\n    def returns_none(return_node: ast.Return) -> bool:\n        value = return_node.value\n        return value is None or (\n            isinstance(value, ast.Constant) and value.value is None\n        )\n\n    if inspect.isgeneratorfunction(callable):\n        func = callable\n        while isinstance(func, partial):\n            func = func.func\n\n        src = inspect.getsource(func)\n        pattern = re.compile(r\"(^[\\t ]+)\")\n        code = pattern.sub(\"\", src)\n\n        match = pattern.match(src)  # finds indentation\n        if match:\n            code = re.sub(f\"\\n{match.group(0)}\", \"\\n\", code)  # remove indentation\n\n        tree = ast.parse(code)\n        for node in walk_callable(tree):\n            if isinstance(node, ast.Return) and not returns_none(node):\n                _generator_callbacks_cache[callable] = True\n                return bool(_generator_callbacks_cache[callable])\n\n    _generator_callbacks_cache[callable] = False\n    return bool(_generator_callbacks_cache[callable])\n\n\ndef warn_on_generator_with_return_value(\n    spider: Spider, callable: Callable[..., Any]\n) -> None:\n    \"\"\"\n    Logs a warning if a callable is a generator function and includes\n    a 'return' statement with a value different than None\n    \"\"\"\n    try:\n        if is_generator_with_return_value(callable):\n            warnings.warn(\n                f'The \"{spider.__class__.__name__}.{callable.__name__}\" method is '\n                'a generator and includes a \"return\" statement with a value '\n                \"different than None. This could lead to unexpected behaviour. Please see \"\n                \"https://docs.python.org/3/reference/simple_stmts.html#the-return-statement \"\n                'for details about the semantics of the \"return\" statement within generators',\n                stacklevel=2,\n            )\n    except IndentationError:\n        callable_name = spider.__class__.__name__ + \".\" + callable.__name__\n        warnings.warn(\n            f'Unable to determine whether or not \"{callable_name}\" is a generator with a return value. '\n            \"This will not prevent your code from working, but it prevents Scrapy from detecting \"\n            f'potential issues in your implementation of \"{callable_name}\". Please, report this in the '\n            \"Scrapy issue tracker (https://github.com/scrapy/scrapy/issues), \"\n            f'including the code of \"{callable_name}\"',\n            stacklevel=2,\n        )\n\n\n### Dependency File: __init__.py\n\"\"\"\nBase class for Scrapy commands\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport builtins\nimport os\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any\n\nfrom twisted.python import failure\n\nfrom scrapy.exceptions import UsageError\nfrom scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    from scrapy.crawler import Crawler, CrawlerProcess\n\n\nclass ScrapyCommand:\n    requires_project: bool = False\n    crawler_process: CrawlerProcess | None = None\n\n    # default settings to be used for this command instead of global defaults\n    default_settings: dict[str, Any] = {}\n\n    exitcode: int = 0\n\n    def __init__(self) -> None:\n        self.settings: Any = None  # set in scrapy.cmdline\n\n    def set_crawler(self, crawler: Crawler) -> None:\n        if hasattr(self, \"_crawler\"):\n            raise RuntimeError(\"crawler already set\")\n        self._crawler: Crawler = crawler\n\n    def syntax(self) -> str:\n        \"\"\"\n        Command syntax (preferably one-line). Do not include command name.\n        \"\"\"\n        return \"\"\n\n    def short_desc(self) -> str:\n        \"\"\"\n        A short description of the command\n        \"\"\"\n        return \"\"\n\n    def long_desc(self) -> str:\n        \"\"\"A long description of the command. Return short description when not\n        available. It cannot contain newlines since contents will be formatted\n        by optparser which removes newlines and wraps text.\n        \"\"\"\n        return self.short_desc()\n\n    def help(self) -> str:\n        \"\"\"An extensive help for the command. It will be shown when using the\n        \"help\" command. It can contain newlines since no post-formatting will\n        be applied to its contents.\n        \"\"\"\n        return self.long_desc()\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        \"\"\"\n        Populate option parse with options available for this command\n        \"\"\"\n        group = parser.add_argument_group(title=\"Global Options\")\n        group.add_argument(\n            \"--logfile\", metavar=\"FILE\", help=\"log file. if omitted stderr will be used\"\n        )\n        group.add_argument(\n            \"-L\",\n            \"--loglevel\",\n            metavar=\"LEVEL\",\n            default=None,\n            help=f\"log level (default: {self.settings['LOG_LEVEL']})\",\n        )\n        group.add_argument(\n            \"--nolog\", action=\"store_true\", help=\"disable logging completely\"\n        )\n        group.add_argument(\n            \"--profile\",\n            metavar=\"FILE\",\n            default=None,\n            help=\"write python cProfile stats to FILE\",\n        )\n        group.add_argument(\"--pidfile\", metavar=\"FILE\", help=\"write process ID to FILE\")\n        group.add_argument(\n            \"-s\",\n            \"--set\",\n            action=\"append\",\n            default=[],\n            metavar=\"NAME=VALUE\",\n            help=\"set/override setting (may be repeated)\",\n        )\n        group.add_argument(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n\n    def process_options(self, args: list[str], opts: argparse.Namespace) -> None:\n        try:\n            self.settings.setdict(arglist_to_dict(opts.set), priority=\"cmdline\")\n        except ValueError:\n            raise UsageError(\"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n\n        if opts.logfile:\n            self.settings.set(\"LOG_ENABLED\", True, priority=\"cmdline\")\n            self.settings.set(\"LOG_FILE\", opts.logfile, priority=\"cmdline\")\n\n        if opts.loglevel:\n            self.settings.set(\"LOG_ENABLED\", True, priority=\"cmdline\")\n            self.settings.set(\"LOG_LEVEL\", opts.loglevel, priority=\"cmdline\")\n\n        if opts.nolog:\n            self.settings.set(\"LOG_ENABLED\", False, priority=\"cmdline\")\n\n        if opts.pidfile:\n            Path(opts.pidfile).write_text(\n                str(os.getpid()) + os.linesep, encoding=\"utf-8\"\n            )\n\n        if opts.pdb:\n            failure.startDebugMode()\n\n    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n        \"\"\"\n        Entry point for running commands\n        \"\"\"\n        raise NotImplementedError\n\n\nclass BaseRunSpiderCommand(ScrapyCommand):\n    \"\"\"\n    Common class used to share functionality between the crawl, parse and runspider commands\n    \"\"\"\n\n    def add_options(self, parser: argparse.ArgumentParser) -> None:\n        super().add_options(parser)\n        parser.add_argument(\n            \"-a\",\n            dest=\"spargs\",\n            action=\"append\",\n            default=[],\n            metavar=\"NAME=VALUE\",\n            help=\"set spider argument (may be repeated)\",\n        )\n        parser.add_argument(\n            \"-o\",\n            \"--output\",\n            metavar=\"FILE\",\n            action=\"append\",\n            help=\"append scraped items to the end of FILE (use - for stdout),\"\n            \" to define format set a colon at the end of the output URI (i.e. -o FILE:FORMAT)\",\n        )\n        parser.add_argument(\n            \"-O\",\n            \"--overwrite-output\",\n            metavar=\"FILE\",\n            action=\"append\",\n            help=\"dump scraped items into FILE, overwriting any existing file,\"\n            \" to define format set a colon at the end of the output URI (i.e. -O FILE:FORMAT)\",\n        )\n\n    def process_options(self, args: list[str], opts: argparse.Namespace) -> None:\n        super().process_options(args, opts)\n        try:\n            opts.spargs = arglist_to_dict(opts.spargs)\n        except ValueError:\n            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n        if opts.output or opts.overwrite_output:\n            feeds = feed_process_params_from_cli(\n                self.settings,\n                opts.output,\n                overwrite_output=opts.overwrite_output,\n            )\n            self.settings.set(\"FEEDS\", feeds, priority=\"cmdline\")\n\n\nclass ScrapyHelpFormatter(argparse.HelpFormatter):\n    \"\"\"\n    Help Formatter for scrapy command line help messages.\n    \"\"\"\n\n    def __init__(\n        self,\n        prog: str,\n        indent_increment: int = 2,\n        max_help_position: int = 24,\n        width: int | None = None,\n    ):\n        super().__init__(\n            prog,\n            indent_increment=indent_increment,\n            max_help_position=max_help_position,\n            width=width,\n        )\n\n    def _join_parts(self, part_strings: Iterable[str]) -> str:\n        # scrapy.commands.list shadows builtins.list\n        parts = self.format_part_strings(builtins.list(part_strings))\n        return super()._join_parts(parts)\n\n    def format_part_strings(self, part_strings: list[str]) -> list[str]:\n        \"\"\"\n        Underline and title case command line help message headers.\n        \"\"\"\n        if part_strings and part_strings[0].startswith(\"usage: \"):\n            part_strings[0] = \"Usage\\n=====\\n  \" + part_strings[0][len(\"usage: \") :]\n        headings = [\n            i for i in range(len(part_strings)) if part_strings[i].endswith(\":\\n\")\n        ]\n        for index in headings[::-1]:\n            char = \"-\" if \"Global Options\" in part_strings[index] else \"=\"\n            part_strings[index] = part_strings[index][:-2].title()\n            underline = \"\".join([\"\\n\", (char * len(part_strings[index])), \"\\n\"])\n            part_strings.insert(index + 1, underline)\n        return part_strings\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:46:39"}, {"role": "assistant", "content": "```python\nimport sys\nfrom io import StringIO\nfrom unittest.mock import Mock, PropertyMock, call, patch\n\nfrom scrapy.commands.check import Command, TextTestResult\nfrom tests.test_commands import CommandTest\n\n\nclass CheckCommandTest(CommandTest):\n    command = \"check\"\n\n    def setUp(self):\n        super().setUp()\n        self.spider_name = \"check_spider\"\n        self.spider = (self.proj_mod_path / \"spiders\" / \"checkspider.py\").resolve()\n\n    def _write_contract(self, contracts, parse_def):\n        self.spider.write_text(\n            f\"\"\"\nimport scrapy\n\nclass CheckSpider(scrapy.Spider):\n    name = '{self.spider_name}'\n    start_urls = ['data:,']\n\n    def parse(self, response, **cb_kwargs):\n        \\\"\\\"\\\"\n        @url data:,\n        {contracts}\n        \\\"\\\"\\\"\n        {parse_def}\n        \"\"\",\n            encoding=\"utf-8\",\n        )\n\n    def _test_contract(self, contracts=\"\", parse_def=\"pass\"):\n        self._write_contract(contracts, parse_def)\n        p, out, err = self.proc(\"check\")\n        self.assertNotIn(\"F\", out)\n        self.assertIn(\"OK\", err)\n        self.assertEqual(p.returncode, 0)\n\n    def test_check_returns_requests_contract(self):\n        contracts = \"\"\"\n        @returns requests 1\n        \"\"\"\n        parse_def = \"\"\"\n        yield scrapy.Request(url='http://next-url.com')\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_check_returns_items_contract(self):\n        contracts = \"\"\"\n        @returns items 1\n        \"\"\"\n        parse_def = \"\"\"\n        yield {'key1': 'val1', 'key2': 'val2'}\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_check_cb_kwargs_contract(self):\n        contracts = \"\"\"\n        @cb_kwargs {\"arg1\": \"val1\", \"arg2\": \"val2\"}\n        \"\"\"\n        parse_def = \"\"\"\n        if len(cb_kwargs.items()) == 0:\n            raise Exception(\"Callback args not set\")\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_check_scrapes_contract(self):\n        contracts = \"\"\"\n        @scrapes key1 key2\n        \"\"\"\n        parse_def = \"\"\"\n        yield {'key1': 'val1', 'key2': 'val2'}\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_check_all_default_contracts(self):\n        contracts = \"\"\"\n        @returns items 1\n        @returns requests 1\n        @scrapes key1 key2\n        @cb_kwargs {\"arg1\": \"val1\", \"arg2\": \"val2\"}\n        \"\"\"\n        parse_def = \"\"\"\n        yield {'key1': 'val1', 'key2': 'val2'}\n        yield scrapy.Request(url='http://next-url.com')\n        if len(cb_kwargs.items()) == 0:\n            raise Exception(\"Callback args not set\")\n        \"\"\"\n        self._test_contract(contracts, parse_def)\n\n    def test_SCRAPY_CHECK_set(self):\n        parse_def = \"\"\"\n        import os\n        if not os.environ.get('SCRAPY_CHECK'):\n            raise Exception('SCRAPY_CHECK not set')\n        \"\"\"\n        self._test_contract(parse_def=parse_def)\n\n    def test_printSummary_with_unsuccessful_test_result_without_errors_and_without_failures(\n        self,\n    ):\n        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n        start_time = 1.0\n        stop_time = 2.0\n        result.testsRun = 5\n        result.failures = []\n        result.errors = []\n        result.unexpectedSuccesses = [\"a\", \"b\"]\n        with patch.object(result.stream, \"write\") as mock_write:\n            result.printSummary(start_time, stop_time)\n            mock_write.assert_has_calls([call(\"FAILED\"), call(\"\\n\")])\n\n    def test_printSummary_with_unsuccessful_test_result_with_only_failures(self):\n        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n        start_time = 1.0\n        stop_time = 2.0\n        result.testsRun = 5\n        result.failures = [(self, \"failure\")]\n        result.errors = []\n        with patch.object(result.stream, \"writeln\") as mock_write:\n            result.printSummary(start_time, stop_time)\n            mock_write.assert_called_with(\" (failures=1)\")\n\n    def test_printSummary_with_unsuccessful_test_result_with_only_errors(self):\n        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n        start_time = 1.0\n        stop_time = 2.0\n        result.testsRun = 5\n        result.failures = []\n        result.errors = [(self, \"error\")]\n        with patch.object(result.stream, \"writeln\") as mock_write:\n            result.printSummary(start_time, stop_time)\n            mock_write.assert_called_with(\" (errors=1)\")\n\n    def test_printSummary_with_unsuccessful_test_result_with_both_failures_and_errors(\n        self,\n    ):\n        result = TextTestResult(Mock(), descriptions=False, verbosity=1)\n        start_time = 1.0\n        stop_time = 2.0\n        result.testsRun = 5\n        result.failures = [(self, \"failure\")]\n        result.errors = [(self, \"error\")]\n        with patch.object(result.stream, \"writeln\") as mock_write:\n            result.printSummary(start_time, stop_time)\n            mock_write.assert_called_with(\" (failures=1, errors=1)\")\n\n    @patch(\"scrapy.commands.check.ContractsManager\")\n    def test_run_with_opts_list_prints_spider(self, cm_cls_mock):\n        output = StringIO()\n        sys.stdout = output\n        cmd = Command()\n        cmd.settings = Mock(getwithbase=Mock(return_value={}))\n        cm_cls_mock.return_value = cm_mock = Mock()\n        spider_loader_mock = Mock()\n        cmd.crawler_process = Mock(spider_loader=spider_loader_mock)\n        spider_name = \"FakeSpider\"\n        spider_cls_mock = Mock()\n        type(spider_cls_mock).name = PropertyMock(return_value=spider_name)\n        spider_loader_mock.load.side_effect = lambda x: {spider_name: spider_cls_mock}[\n            x\n        ]\n        tested_methods = [\"fakeMethod1\", \"fakeMethod2\"]\n        cm_mock.tested_methods_from_spidercls.side_effect = lambda x: {\n            spider_cls_mock: tested_methods\n        }[x]\n\n        cmd.run([spider_name], Mock(list=True))\n\n        self.assertEqual(\n            \"FakeSpider\\n  * fakeMethod1\\n  * fakeMethod2\\n\", output.getvalue()\n        )\n        sys.stdout = sys.__stdout__\n\n    @patch(\"scrapy.commands.check.ContractsManager\")\n    def test_run_without_opts_list_does_not_crawl_spider_with_no_tested_methods(\n        self, cm_cls_mock\n    ):\n        cmd = Command()\n        cmd.settings = Mock(getwithbase=Mock(return_value={}))\n        cm_cls_mock.return_value = cm_mock = Mock()\n        spider_loader_mock = Mock()\n        cmd.crawler_process = Mock(spider_loader=spider_loader_mock)\n        spider_name = \"FakeSpider\"\n        spider_cls_mock = Mock()\n        spider_loader_mock.load.side_effect = lambda x: {spider_name: spider_cls_mock}[\n            x\n        ]\n        tested_methods = []\n        cm_mock.tested_methods_from_spidercls.side_effect = lambda x: {\n            spider_cls_mock: tested_methods\n        }[x]\n\n        cmd.run([spider_name], Mock(list=False))\n\n        cmd.crawler_process.crawl.assert_not_called()\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: scrapy\nTest File Path: scrapy\\test_http_cookies\\test_http_cookies.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport re\nimport time\nfrom http.cookiejar import Cookie, CookiePolicy, DefaultCookiePolicy\nfrom http.cookiejar import CookieJar as _CookieJar\nfrom typing import TYPE_CHECKING, Any, cast\n\nfrom scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    from collections.abc import Iterator, Sequence\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n    from scrapy import Request\n    from scrapy.http import Response\n\n\n# Defined in the http.cookiejar module, but undocumented:\n# https://github.com/python/cpython/blob/v3.9.0/Lib/http/cookiejar.py#L527\nIPV4_RE = re.compile(r\"\\.\\d+$\", re.ASCII)\n\n\nclass CookieJar:\n    def __init__(\n        self,\n        policy: CookiePolicy | None = None,\n        check_expired_frequency: int = 10000,\n    ):\n        self.policy: CookiePolicy = policy or DefaultCookiePolicy()\n        self.jar: _CookieJar = _CookieJar(self.policy)\n        self.jar._cookies_lock = _DummyLock()  # type: ignore[attr-defined]\n        self.check_expired_frequency: int = check_expired_frequency\n        self.processed: int = 0\n\n    def extract_cookies(self, response: Response, request: Request) -> None:\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        self.jar.extract_cookies(wrsp, wreq)  # type: ignore[arg-type]\n\n    def add_cookie_header(self, request: Request) -> None:\n        wreq = WrappedRequest(request)\n        self.policy._now = self.jar._now = int(time.time())  # type: ignore[attr-defined]\n\n        # the cookiejar implementation iterates through all domains\n        # instead we restrict to potential matches on the domain\n        req_host = urlparse_cached(request).hostname\n        if not req_host:\n            return\n\n        if not IPV4_RE.search(req_host):\n            hosts = potential_domain_matches(req_host)\n            if \".\" not in req_host:\n                hosts += [req_host + \".local\"]\n        else:\n            hosts = [req_host]\n\n        cookies = []\n        for host in hosts:\n            if host in self.jar._cookies:  # type: ignore[attr-defined]\n                cookies += self.jar._cookies_for_domain(host, wreq)  # type: ignore[attr-defined]\n\n        attrs = self.jar._cookie_attrs(cookies)  # type: ignore[attr-defined]\n        if attrs and not wreq.has_header(\"Cookie\"):\n            wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n\n        self.processed += 1\n        if self.processed % self.check_expired_frequency == 0:\n            # This is still quite inefficient for large number of cookies\n            self.jar.clear_expired_cookies()\n\n    @property\n    def _cookies(self) -> dict[str, dict[str, dict[str, Cookie]]]:\n        return self.jar._cookies  # type: ignore[attr-defined,no-any-return]\n\n    def clear_session_cookies(self) -> None:\n        return self.jar.clear_session_cookies()\n\n    def clear(\n        self,\n        domain: str | None = None,\n        path: str | None = None,\n        name: str | None = None,\n    ) -> None:\n        self.jar.clear(domain, path, name)\n\n    def __iter__(self) -> Iterator[Cookie]:\n        return iter(self.jar)\n\n    def __len__(self) -> int:\n        return len(self.jar)\n\n    def set_policy(self, pol: CookiePolicy) -> None:\n        self.jar.set_policy(pol)\n\n    def make_cookies(self, response: Response, request: Request) -> Sequence[Cookie]:\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.make_cookies(wrsp, wreq)  # type: ignore[arg-type]\n\n    def set_cookie(self, cookie: Cookie) -> None:\n        self.jar.set_cookie(cookie)\n\n    def set_cookie_if_ok(self, cookie: Cookie, request: Request) -> None:\n        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))  # type: ignore[arg-type]\n\n\ndef potential_domain_matches(domain: str) -> list[str]:\n    \"\"\"Potential domain matches for a cookie\n\n    >>> potential_domain_matches('www.example.com')\n    ['www.example.com', 'example.com', '.www.example.com', '.example.com']\n\n    \"\"\"\n    matches = [domain]\n    try:\n        start = domain.index(\".\") + 1\n        end = domain.rindex(\".\")\n        while start < end:\n            matches.append(domain[start:])\n            start = domain.index(\".\", start) + 1\n    except ValueError:\n        pass\n    return matches + [\".\" + d for d in matches]\n\n\nclass _DummyLock:\n    def acquire(self) -> None:\n        pass\n\n    def release(self) -> None:\n        pass\n\n\nclass WrappedRequest:\n    \"\"\"Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class\n\n    see http://docs.python.org/library/urllib2.html#urllib2.Request\n    \"\"\"\n\n    def __init__(self, request: Request):\n        self.request = request\n\n    def get_full_url(self) -> str:\n        return self.request.url\n\n    def get_host(self) -> str:\n        return urlparse_cached(self.request).netloc\n\n    def get_type(self) -> str:\n        return urlparse_cached(self.request).scheme\n\n    def is_unverifiable(self) -> bool:\n        \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n\n        It defaults to False. An unverifiable request is one whose URL the user did not have the\n        option to approve. For example, if the request is for an image in an\n        HTML document, and the user had no option to approve the automatic\n        fetching of the image, this should be true.\n        \"\"\"\n        return cast(bool, self.request.meta.get(\"is_unverifiable\", False))\n\n    @property\n    def full_url(self) -> str:\n        return self.get_full_url()\n\n    @property\n    def host(self) -> str:\n        return self.get_host()\n\n    @property\n    def type(self) -> str:\n        return self.get_type()\n\n    @property\n    def unverifiable(self) -> bool:\n        return self.is_unverifiable()\n\n    @property\n    def origin_req_host(self) -> str:\n        return cast(str, urlparse_cached(self.request).hostname)\n\n    def has_header(self, name: str) -> bool:\n        return name in self.request.headers\n\n    def get_header(self, name: str, default: str | None = None) -> str | None:\n        value = self.request.headers.get(name, default)\n        return to_unicode(value, errors=\"replace\") if value is not None else None\n\n    def header_items(self) -> list[tuple[str, list[str]]]:\n        return [\n            (\n                to_unicode(k, errors=\"replace\"),\n                [to_unicode(x, errors=\"replace\") for x in v],\n            )\n            for k, v in self.request.headers.items()\n        ]\n\n    def add_unredirected_header(self, name: str, value: str) -> None:\n        self.request.headers.appendlist(name, value)\n\n\nclass WrappedResponse:\n    def __init__(self, response: Response):\n        self.response = response\n\n    def info(self) -> Self:\n        return self\n\n    def get_all(self, name: str, default: Any = None) -> list[str]:\n        return [\n            to_unicode(v, errors=\"replace\") for v in self.response.headers.getlist(name)\n        ]\n\n### Source File Dependency Files Content\n### Dependency File: httpobj.py\n\"\"\"Helper functions for scrapy.http objects (Request, Response)\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\nfrom urllib.parse import ParseResult, urlparse\nfrom weakref import WeakKeyDictionary\n\nif TYPE_CHECKING:\n    from scrapy.http import Request, Response\n\n\n_urlparse_cache: WeakKeyDictionary[Request | Response, ParseResult] = (\n    WeakKeyDictionary()\n)\n\n\ndef urlparse_cached(request_or_response: Request | Response) -> ParseResult:\n    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n    Request or Response object\n    \"\"\"\n    if request_or_response not in _urlparse_cache:\n        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n    return _urlparse_cache[request_or_response]\n\n\n### Dependency File: python.py\n\"\"\"\nThis module contains essential stuff that should've come with Python itself ;)\n\"\"\"\n\nfrom __future__ import annotations\n\nimport gc\nimport inspect\nimport re\nimport sys\nimport warnings\nimport weakref\nfrom collections.abc import AsyncIterable, Iterable, Mapping\nfrom functools import partial, wraps\nfrom itertools import chain\nfrom typing import TYPE_CHECKING, Any, TypeVar, overload\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.asyncgen import as_async_generator\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncIterator, Callable, Iterator\n    from re import Pattern\n\n    # typing.Concatenate and typing.ParamSpec require Python 3.10\n    from typing_extensions import Concatenate, ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n_T = TypeVar(\"_T\")\n_KT = TypeVar(\"_KT\")\n_VT = TypeVar(\"_VT\")\n\n\ndef flatten(x: Iterable[Any]) -> list[Any]:\n    \"\"\"flatten(sequence) -> list\n\n    Returns a single, flat list which contains all elements retrieved\n    from the sequence and all recursively contained sub-sequences\n    (iterables).\n\n    Examples:\n    >>> [1, 2, [3,4], (5,6)]\n    [1, 2, [3, 4], (5, 6)]\n    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n    >>> flatten([\"foo\", \"bar\"])\n    ['foo', 'bar']\n    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n    ['foo', 'baz', 42, 'bar']\n    \"\"\"\n    warnings.warn(\n        \"The flatten function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    return list(iflatten(x))\n\n\ndef iflatten(x: Iterable[Any]) -> Iterable[Any]:\n    \"\"\"iflatten(sequence) -> iterator\n\n    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n    warnings.warn(\n        \"The iflatten function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    for el in x:\n        if is_listlike(el):\n            yield from iflatten(el)\n        else:\n            yield el\n\n\ndef is_listlike(x: Any) -> bool:\n    \"\"\"\n    >>> is_listlike(\"foo\")\n    False\n    >>> is_listlike(5)\n    False\n    >>> is_listlike(b\"foo\")\n    False\n    >>> is_listlike([b\"foo\"])\n    True\n    >>> is_listlike((b\"foo\",))\n    True\n    >>> is_listlike({})\n    True\n    >>> is_listlike(set())\n    True\n    >>> is_listlike((x for x in range(3)))\n    True\n    >>> is_listlike(range(5))\n    True\n    \"\"\"\n    return hasattr(x, \"__iter__\") and not isinstance(x, (str, bytes))\n\n\ndef unique(list_: Iterable[_T], key: Callable[[_T], Any] = lambda x: x) -> list[_T]:\n    \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n    seen = set()\n    result: list[_T] = []\n    for item in list_:\n        seenkey = key(item)\n        if seenkey in seen:\n            continue\n        seen.add(seenkey)\n        result.append(item)\n    return result\n\n\ndef to_unicode(\n    text: str | bytes, encoding: str | None = None, errors: str = \"strict\"\n) -> str:\n    \"\"\"Return the unicode representation of a bytes object ``text``. If\n    ``text`` is already an unicode object, return it as-is.\"\"\"\n    if isinstance(text, str):\n        return text\n    if not isinstance(text, (bytes, str)):\n        raise TypeError(\n            \"to_unicode must receive a bytes or str \"\n            f\"object, got {type(text).__name__}\"\n        )\n    if encoding is None:\n        encoding = \"utf-8\"\n    return text.decode(encoding, errors)\n\n\ndef to_bytes(\n    text: str | bytes, encoding: str | None = None, errors: str = \"strict\"\n) -> bytes:\n    \"\"\"Return the binary representation of ``text``. If ``text``\n    is already a bytes object, return it as-is.\"\"\"\n    if isinstance(text, bytes):\n        return text\n    if not isinstance(text, str):\n        raise TypeError(\n            f\"to_bytes must receive a str or bytes object, got {type(text).__name__}\"\n        )\n    if encoding is None:\n        encoding = \"utf-8\"\n    return text.encode(encoding, errors)\n\n\ndef re_rsearch(\n    pattern: str | Pattern[str], text: str, chunk_size: int = 1024\n) -> tuple[int, int] | None:\n    \"\"\"\n    This function does a reverse search in a text using a regular expression\n    given in the attribute 'pattern'.\n    Since the re module does not provide this functionality, we have to find for\n    the expression into chunks of text extracted from the end (for the sake of efficiency).\n    At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for\n    the pattern. If the pattern is not found, another chunk is extracted, and another\n    search is performed.\n    This process continues until a match is found, or until the whole file is read.\n    In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing\n    the start position of the match, and the ending (regarding the entire text).\n    \"\"\"\n\n    def _chunk_iter() -> Iterable[tuple[str, int]]:\n        offset = len(text)\n        while True:\n            offset -= chunk_size * 1024\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)\n\n    if isinstance(pattern, str):\n        pattern = re.compile(pattern)\n\n    for chunk, offset in _chunk_iter():\n        matches = list(pattern.finditer(chunk))\n        if matches:\n            start, end = matches[-1].span()\n            return offset + start, offset + end\n    return None\n\n\n_SelfT = TypeVar(\"_SelfT\")\n\n\ndef memoizemethod_noargs(\n    method: Callable[Concatenate[_SelfT, _P], _T]\n) -> Callable[Concatenate[_SelfT, _P], _T]:\n    \"\"\"Decorator to cache the result of a method (without arguments) using a\n    weak reference to its object\n    \"\"\"\n    cache: weakref.WeakKeyDictionary[_SelfT, _T] = weakref.WeakKeyDictionary()\n\n    @wraps(method)\n    def new_method(self: _SelfT, *args: _P.args, **kwargs: _P.kwargs) -> _T:\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]\n\n    return new_method\n\n\n_BINARYCHARS = {\n    i for i in range(32) if to_bytes(chr(i)) not in {b\"\\0\", b\"\\t\", b\"\\n\", b\"\\r\"}\n}\n\n\ndef binary_is_text(data: bytes) -> bool:\n    \"\"\"Returns ``True`` if the given ``data`` argument (a ``bytes`` object)\n    does not contain unprintable control characters.\n    \"\"\"\n    if not isinstance(data, bytes):\n        raise TypeError(f\"data must be bytes, got '{type(data).__name__}'\")\n    return all(c not in _BINARYCHARS for c in data)\n\n\ndef get_func_args(func: Callable[..., Any], stripself: bool = False) -> list[str]:\n    \"\"\"Return the argument name list of a callable object\"\"\"\n    if not callable(func):\n        raise TypeError(f\"func must be callable, got '{type(func).__name__}'\")\n\n    args: list[str] = []\n    try:\n        sig = inspect.signature(func)\n    except ValueError:\n        return args\n\n    if isinstance(func, partial):\n        partial_args = func.args\n        partial_kw = func.keywords\n\n        for name, param in sig.parameters.items():\n            if param.name in partial_args:\n                continue\n            if partial_kw and param.name in partial_kw:\n                continue\n            args.append(name)\n    else:\n        args = list(sig.parameters)\n\n    if stripself and args and args[0] == \"self\":\n        args = args[1:]\n    return args\n\n\ndef get_spec(func: Callable[..., Any]) -> tuple[list[str], dict[str, Any]]:\n    \"\"\"Returns (args, kwargs) tuple for a function\n    >>> import re\n    >>> get_spec(re.match)\n    (['pattern', 'string'], {'flags': 0})\n\n    >>> class Test:\n    ...     def __call__(self, val):\n    ...         pass\n    ...     def method(self, val, flags=0):\n    ...         pass\n\n    >>> get_spec(Test)\n    (['self', 'val'], {})\n\n    >>> get_spec(Test.method)\n    (['self', 'val'], {'flags': 0})\n\n    >>> get_spec(Test().method)\n    (['self', 'val'], {'flags': 0})\n    \"\"\"\n\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        spec = inspect.getfullargspec(func)\n    elif hasattr(func, \"__call__\"):  # noqa: B004\n        spec = inspect.getfullargspec(func.__call__)\n    else:\n        raise TypeError(f\"{type(func)} is not callable\")\n\n    defaults: tuple[Any, ...] = spec.defaults or ()\n\n    firstdefault = len(spec.args) - len(defaults)\n    args = spec.args[:firstdefault]\n    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n    return args, kwargs\n\n\ndef equal_attributes(\n    obj1: Any, obj2: Any, attributes: list[str | Callable[[Any], Any]] | None\n) -> bool:\n    \"\"\"Compare two objects attributes\"\"\"\n    warnings.warn(\n        \"The equal_attributes function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    # not attributes given return False by default\n    if not attributes:\n        return False\n\n    temp1, temp2 = object(), object()\n    for attr in attributes:\n        # support callables like itemgetter\n        if callable(attr):\n            if attr(obj1) != attr(obj2):\n                return False\n        elif getattr(obj1, attr, temp1) != getattr(obj2, attr, temp2):\n            return False\n    # all attributes equal\n    return True\n\n\n@overload\ndef without_none_values(iterable: Mapping[_KT, _VT]) -> dict[_KT, _VT]: ...\n\n\n@overload\ndef without_none_values(iterable: Iterable[_KT]) -> Iterable[_KT]: ...\n\n\ndef without_none_values(\n    iterable: Mapping[_KT, _VT] | Iterable[_KT]\n) -> dict[_KT, _VT] | Iterable[_KT]:\n    \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n\n    If ``iterable`` is a mapping, return a dictionary where all pairs that have\n    value ``None`` have been removed.\n    \"\"\"\n    if isinstance(iterable, Mapping):\n        return {k: v for k, v in iterable.items() if v is not None}\n    # the iterable __init__ must take another iterable\n    return type(iterable)(v for v in iterable if v is not None)  # type: ignore[call-arg]\n\n\ndef global_object_name(obj: Any) -> str:\n    \"\"\"Return the full import path of the given class.\n\n    >>> from scrapy import Request\n    >>> global_object_name(Request)\n    'scrapy.http.request.Request'\n    \"\"\"\n    return f\"{obj.__module__}.{obj.__qualname__}\"\n\n\nif hasattr(sys, \"pypy_version_info\"):\n\n    def garbage_collect() -> None:\n        # Collecting weakreferences can take two collections on PyPy.\n        gc.collect()\n        gc.collect()\n\nelse:\n\n    def garbage_collect() -> None:\n        gc.collect()\n\n\nclass MutableChain(Iterable[_T]):\n    \"\"\"\n    Thin wrapper around itertools.chain, allowing to add iterables \"in-place\"\n    \"\"\"\n\n    def __init__(self, *args: Iterable[_T]):\n        self.data: Iterator[_T] = chain.from_iterable(args)\n\n    def extend(self, *iterables: Iterable[_T]) -> None:\n        self.data = chain(self.data, chain.from_iterable(iterables))\n\n    def __iter__(self) -> Iterator[_T]:\n        return self\n\n    def __next__(self) -> _T:\n        return next(self.data)\n\n\nasync def _async_chain(\n    *iterables: Iterable[_T] | AsyncIterable[_T],\n) -> AsyncIterator[_T]:\n    for it in iterables:\n        async for o in as_async_generator(it):\n            yield o\n\n\nclass MutableAsyncChain(AsyncIterable[_T]):\n    \"\"\"\n    Similar to MutableChain but for async iterables\n    \"\"\"\n\n    def __init__(self, *args: Iterable[_T] | AsyncIterable[_T]):\n        self.data: AsyncIterator[_T] = _async_chain(*args)\n\n    def extend(self, *iterables: Iterable[_T] | AsyncIterable[_T]) -> None:\n        self.data = _async_chain(self.data, _async_chain(*iterables))\n\n    def __aiter__(self) -> AsyncIterator[_T]:\n        return self\n\n    async def __anext__(self) -> _T:\n        return await self.data.__anext__()\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:46:39"}, {"role": "assistant", "content": "```python\nfrom unittest import TestCase\n\nfrom scrapy.http import Request, Response\nfrom scrapy.http.cookies import WrappedRequest, WrappedResponse\nfrom scrapy.utils.httpobj import urlparse_cached\n\n\nclass WrappedRequestTest(TestCase):\n    def setUp(self):\n        self.request = Request(\n            \"http://www.example.com/page.html\", headers={\"Content-Type\": \"text/html\"}\n        )\n        self.wrapped = WrappedRequest(self.request)\n\n    def test_get_full_url(self):\n        self.assertEqual(self.wrapped.get_full_url(), self.request.url)\n        self.assertEqual(self.wrapped.full_url, self.request.url)\n\n    def test_get_host(self):\n        self.assertEqual(self.wrapped.get_host(), urlparse_cached(self.request).netloc)\n        self.assertEqual(self.wrapped.host, urlparse_cached(self.request).netloc)\n\n    def test_get_type(self):\n        self.assertEqual(self.wrapped.get_type(), urlparse_cached(self.request).scheme)\n        self.assertEqual(self.wrapped.type, urlparse_cached(self.request).scheme)\n\n    def test_is_unverifiable(self):\n        self.assertFalse(self.wrapped.is_unverifiable())\n        self.assertFalse(self.wrapped.unverifiable)\n\n    def test_is_unverifiable2(self):\n        self.request.meta[\"is_unverifiable\"] = True\n        self.assertTrue(self.wrapped.is_unverifiable())\n        self.assertTrue(self.wrapped.unverifiable)\n\n    def test_get_origin_req_host(self):\n        self.assertEqual(self.wrapped.origin_req_host, \"www.example.com\")\n\n    def test_has_header(self):\n        self.assertTrue(self.wrapped.has_header(\"content-type\"))\n        self.assertFalse(self.wrapped.has_header(\"xxxxx\"))\n\n    def test_get_header(self):\n        self.assertEqual(self.wrapped.get_header(\"content-type\"), \"text/html\")\n        self.assertEqual(self.wrapped.get_header(\"xxxxx\", \"def\"), \"def\")\n        self.assertEqual(self.wrapped.get_header(\"xxxxx\"), None)\n        wrapped = WrappedRequest(\n            Request(\n                \"http://www.example.com/page.html\", headers={\"empty-binary-header\": b\"\"}\n            )\n        )\n        self.assertEqual(wrapped.get_header(\"empty-binary-header\"), \"\")\n\n    def test_header_items(self):\n        self.assertEqual(self.wrapped.header_items(), [(\"Content-Type\", [\"text/html\"])])\n\n    def test_add_unredirected_header(self):\n        self.wrapped.add_unredirected_header(\"hello\", \"world\")\n        self.assertEqual(self.request.headers[\"hello\"], b\"world\")\n\n\nclass WrappedResponseTest(TestCase):\n    def setUp(self):\n        self.response = Response(\n            \"http://www.example.com/page.html\", headers={\"Content-TYpe\": \"text/html\"}\n        )\n        self.wrapped = WrappedResponse(self.response)\n\n    def test_info(self):\n        self.assertIs(self.wrapped.info(), self.wrapped)\n\n    def test_get_all(self):\n        # get_all result must be native string\n        self.assertEqual(self.wrapped.get_all(\"content-type\"), [\"text/html\"])\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: scrapy\nTest File Path: scrapy\\test_http_headers\\test_http_headers.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections.abc import Mapping\nfrom typing import TYPE_CHECKING, Any, AnyStr, Union, cast\n\nfrom w3lib.http import headers_dict_to_raw\n\nfrom scrapy.utils.datatypes import CaseInsensitiveDict, CaselessDict\nfrom scrapy.utils.python import to_unicode\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\n_RawValueT = Union[bytes, str, int]\n\n\n# isn't fully compatible typing-wise with either dict or CaselessDict,\n# but it needs refactoring anyway, see also https://github.com/scrapy/scrapy/pull/5146\nclass Headers(CaselessDict):\n    \"\"\"Case insensitive http headers dictionary\"\"\"\n\n    def __init__(\n        self,\n        seq: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n        encoding: str = \"utf-8\",\n    ):\n        self.encoding: str = encoding\n        super().__init__(seq)\n\n    def update(  # type: ignore[override]\n        self, seq: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]]\n    ) -> None:\n        seq = seq.items() if isinstance(seq, Mapping) else seq\n        iseq: dict[bytes, list[bytes]] = {}\n        for k, v in seq:\n            iseq.setdefault(self.normkey(k), []).extend(self.normvalue(v))\n        super().update(iseq)\n\n    def normkey(self, key: AnyStr) -> bytes:  # type: ignore[override]\n        \"\"\"Normalize key to bytes\"\"\"\n        return self._tobytes(key.title())\n\n    def normvalue(self, value: _RawValueT | Iterable[_RawValueT]) -> list[bytes]:\n        \"\"\"Normalize values to bytes\"\"\"\n        _value: Iterable[_RawValueT]\n        if value is None:\n            _value = []\n        elif isinstance(value, (str, bytes)):\n            _value = [value]\n        elif hasattr(value, \"__iter__\"):\n            _value = value\n        else:\n            _value = [value]\n\n        return [self._tobytes(x) for x in _value]\n\n    def _tobytes(self, x: _RawValueT) -> bytes:\n        if isinstance(x, bytes):\n            return x\n        if isinstance(x, str):\n            return x.encode(self.encoding)\n        if isinstance(x, int):\n            return str(x).encode(self.encoding)\n        raise TypeError(f\"Unsupported value type: {type(x)}\")\n\n    def __getitem__(self, key: AnyStr) -> bytes | None:\n        try:\n            return cast(list[bytes], super().__getitem__(key))[-1]\n        except IndexError:\n            return None\n\n    def get(self, key: AnyStr, def_val: Any = None) -> bytes | None:\n        try:\n            return cast(list[bytes], super().get(key, def_val))[-1]\n        except IndexError:\n            return None\n\n    def getlist(self, key: AnyStr, def_val: Any = None) -> list[bytes]:\n        try:\n            return cast(list[bytes], super().__getitem__(key))\n        except KeyError:\n            if def_val is not None:\n                return self.normvalue(def_val)\n            return []\n\n    def setlist(self, key: AnyStr, list_: Iterable[_RawValueT]) -> None:\n        self[key] = list_\n\n    def setlistdefault(\n        self, key: AnyStr, default_list: Iterable[_RawValueT] = ()\n    ) -> Any:\n        return self.setdefault(key, default_list)\n\n    def appendlist(self, key: AnyStr, value: Iterable[_RawValueT]) -> None:\n        lst = self.getlist(key)\n        lst.extend(self.normvalue(value))\n        self[key] = lst\n\n    def items(self) -> Iterable[tuple[bytes, list[bytes]]]:  # type: ignore[override]\n        return ((k, self.getlist(k)) for k in self.keys())\n\n    def values(self) -> list[bytes | None]:  # type: ignore[override]\n        return [\n            self[k] for k in self.keys()  # pylint: disable=consider-using-dict-items\n        ]\n\n    def to_string(self) -> bytes:\n        return headers_dict_to_raw(self)\n\n    def to_unicode_dict(self) -> CaseInsensitiveDict:\n        \"\"\"Return headers as a CaseInsensitiveDict with str keys\n        and str values. Multiple values are joined with ','.\n        \"\"\"\n        return CaseInsensitiveDict(\n            (\n                to_unicode(key, encoding=self.encoding),\n                to_unicode(b\",\".join(value), encoding=self.encoding),\n            )\n            for key, value in self.items()\n        )\n\n    def __copy__(self) -> Self:\n        return self.__class__(self)\n\n    copy = __copy__\n\n### Source File Dependency Files Content\n### Dependency File: datatypes.py\n\"\"\"\nThis module contains data types used by Scrapy which are not included in the\nPython Standard Library.\n\nThis module must not depend on any module outside the Standard Library.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport collections\nimport contextlib\nimport warnings\nimport weakref\nfrom collections import OrderedDict\nfrom collections.abc import Mapping\nfrom typing import TYPE_CHECKING, Any, AnyStr, TypeVar\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable, Sequence\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\n_KT = TypeVar(\"_KT\")\n_VT = TypeVar(\"_VT\")\n\n\nclass CaselessDict(dict):\n    __slots__ = ()\n\n    def __new__(cls, *args: Any, **kwargs: Any) -> Self:\n        from scrapy.http.headers import Headers\n\n        if issubclass(cls, CaselessDict) and not issubclass(cls, Headers):\n            warnings.warn(\n                \"scrapy.utils.datatypes.CaselessDict is deprecated,\"\n                \" please use scrapy.utils.datatypes.CaseInsensitiveDict instead\",\n                category=ScrapyDeprecationWarning,\n                stacklevel=2,\n            )\n        return super().__new__(cls, *args, **kwargs)\n\n    def __init__(\n        self,\n        seq: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]] | None = None,\n    ):\n        super().__init__()\n        if seq:\n            self.update(seq)\n\n    def __getitem__(self, key: AnyStr) -> Any:\n        return dict.__getitem__(self, self.normkey(key))\n\n    def __setitem__(self, key: AnyStr, value: Any) -> None:\n        dict.__setitem__(self, self.normkey(key), self.normvalue(value))\n\n    def __delitem__(self, key: AnyStr) -> None:\n        dict.__delitem__(self, self.normkey(key))\n\n    def __contains__(self, key: AnyStr) -> bool:  # type: ignore[override]\n        return dict.__contains__(self, self.normkey(key))\n\n    has_key = __contains__\n\n    def __copy__(self) -> Self:\n        return self.__class__(self)\n\n    copy = __copy__\n\n    def normkey(self, key: AnyStr) -> AnyStr:\n        \"\"\"Method to normalize dictionary key access\"\"\"\n        return key.lower()\n\n    def normvalue(self, value: Any) -> Any:\n        \"\"\"Method to normalize values prior to be set\"\"\"\n        return value\n\n    def get(self, key: AnyStr, def_val: Any = None) -> Any:\n        return dict.get(self, self.normkey(key), self.normvalue(def_val))\n\n    def setdefault(self, key: AnyStr, def_val: Any = None) -> Any:\n        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))  # type: ignore[arg-type]\n\n    # doesn't fully implement MutableMapping.update()\n    def update(self, seq: Mapping[AnyStr, Any] | Iterable[tuple[AnyStr, Any]]) -> None:  # type: ignore[override]\n        seq = seq.items() if isinstance(seq, Mapping) else seq\n        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n        super().update(iseq)\n\n    @classmethod\n    def fromkeys(cls, keys: Iterable[AnyStr], value: Any = None) -> Self:  # type: ignore[override]\n        return cls((k, value) for k in keys)  # type: ignore[misc]\n\n    def pop(self, key: AnyStr, *args: Any) -> Any:\n        return dict.pop(self, self.normkey(key), *args)\n\n\nclass CaseInsensitiveDict(collections.UserDict):\n    \"\"\"A dict-like structure that accepts strings or bytes\n    as keys and allows case-insensitive lookups.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        self._keys: dict = {}\n        super().__init__(*args, **kwargs)\n\n    def __getitem__(self, key: AnyStr) -> Any:\n        normalized_key = self._normkey(key)\n        return super().__getitem__(self._keys[normalized_key.lower()])\n\n    def __setitem__(self, key: AnyStr, value: Any) -> None:\n        normalized_key = self._normkey(key)\n        try:\n            lower_key = self._keys[normalized_key.lower()]\n            del self[lower_key]\n        except KeyError:\n            pass\n        super().__setitem__(normalized_key, self._normvalue(value))\n        self._keys[normalized_key.lower()] = normalized_key\n\n    def __delitem__(self, key: AnyStr) -> None:\n        normalized_key = self._normkey(key)\n        stored_key = self._keys.pop(normalized_key.lower())\n        super().__delitem__(stored_key)\n\n    def __contains__(self, key: AnyStr) -> bool:  # type: ignore[override]\n        normalized_key = self._normkey(key)\n        return normalized_key.lower() in self._keys\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__}: {super().__repr__()}>\"\n\n    def _normkey(self, key: AnyStr) -> AnyStr:\n        return key\n\n    def _normvalue(self, value: Any) -> Any:\n        return value\n\n\nclass LocalCache(OrderedDict[_KT, _VT]):\n    \"\"\"Dictionary with a finite number of keys.\n\n    Older items expires first.\n    \"\"\"\n\n    def __init__(self, limit: int | None = None):\n        super().__init__()\n        self.limit: int | None = limit\n\n    def __setitem__(self, key: _KT, value: _VT) -> None:\n        if self.limit:\n            while len(self) >= self.limit:\n                self.popitem(last=False)\n        super().__setitem__(key, value)\n\n\nclass LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n    \"\"\"\n    A weakref.WeakKeyDictionary implementation that uses LocalCache as its\n    underlying data structure, making it ordered and capable of being size-limited.\n\n    Useful for memoization, while avoiding keeping received\n    arguments in memory only because of the cached references.\n\n    Note: like LocalCache and unlike weakref.WeakKeyDictionary,\n    it cannot be instantiated with an initial dictionary.\n    \"\"\"\n\n    def __init__(self, limit: int | None = None):\n        super().__init__()\n        self.data: LocalCache = LocalCache(limit=limit)\n\n    def __setitem__(self, key: _KT, value: _VT) -> None:\n        # if raised, key is not weak-referenceable, skip caching\n        with contextlib.suppress(TypeError):\n            super().__setitem__(key, value)\n\n    def __getitem__(self, key: _KT) -> _VT | None:  # type: ignore[override]\n        try:\n            return super().__getitem__(key)\n        except (TypeError, KeyError):\n            return None  # key is either not weak-referenceable or not cached\n\n\nclass SequenceExclude:\n    \"\"\"Object to test if an item is NOT within some sequence.\"\"\"\n\n    def __init__(self, seq: Sequence[Any]):\n        self.seq: Sequence[Any] = seq\n\n    def __contains__(self, item: Any) -> bool:\n        return item not in self.seq\n\n\n### Dependency File: python.py\n\"\"\"\nThis module contains essential stuff that should've come with Python itself ;)\n\"\"\"\n\nfrom __future__ import annotations\n\nimport gc\nimport inspect\nimport re\nimport sys\nimport warnings\nimport weakref\nfrom collections.abc import AsyncIterable, Iterable, Mapping\nfrom functools import partial, wraps\nfrom itertools import chain\nfrom typing import TYPE_CHECKING, Any, TypeVar, overload\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.asyncgen import as_async_generator\n\nif TYPE_CHECKING:\n    from collections.abc import AsyncIterator, Callable, Iterator\n    from re import Pattern\n\n    # typing.Concatenate and typing.ParamSpec require Python 3.10\n    from typing_extensions import Concatenate, ParamSpec\n\n    _P = ParamSpec(\"_P\")\n\n_T = TypeVar(\"_T\")\n_KT = TypeVar(\"_KT\")\n_VT = TypeVar(\"_VT\")\n\n\ndef flatten(x: Iterable[Any]) -> list[Any]:\n    \"\"\"flatten(sequence) -> list\n\n    Returns a single, flat list which contains all elements retrieved\n    from the sequence and all recursively contained sub-sequences\n    (iterables).\n\n    Examples:\n    >>> [1, 2, [3,4], (5,6)]\n    [1, 2, [3, 4], (5, 6)]\n    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n    >>> flatten([\"foo\", \"bar\"])\n    ['foo', 'bar']\n    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n    ['foo', 'baz', 42, 'bar']\n    \"\"\"\n    warnings.warn(\n        \"The flatten function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    return list(iflatten(x))\n\n\ndef iflatten(x: Iterable[Any]) -> Iterable[Any]:\n    \"\"\"iflatten(sequence) -> iterator\n\n    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n    warnings.warn(\n        \"The iflatten function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    for el in x:\n        if is_listlike(el):\n            yield from iflatten(el)\n        else:\n            yield el\n\n\ndef is_listlike(x: Any) -> bool:\n    \"\"\"\n    >>> is_listlike(\"foo\")\n    False\n    >>> is_listlike(5)\n    False\n    >>> is_listlike(b\"foo\")\n    False\n    >>> is_listlike([b\"foo\"])\n    True\n    >>> is_listlike((b\"foo\",))\n    True\n    >>> is_listlike({})\n    True\n    >>> is_listlike(set())\n    True\n    >>> is_listlike((x for x in range(3)))\n    True\n    >>> is_listlike(range(5))\n    True\n    \"\"\"\n    return hasattr(x, \"__iter__\") and not isinstance(x, (str, bytes))\n\n\ndef unique(list_: Iterable[_T], key: Callable[[_T], Any] = lambda x: x) -> list[_T]:\n    \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n    seen = set()\n    result: list[_T] = []\n    for item in list_:\n        seenkey = key(item)\n        if seenkey in seen:\n            continue\n        seen.add(seenkey)\n        result.append(item)\n    return result\n\n\ndef to_unicode(\n    text: str | bytes, encoding: str | None = None, errors: str = \"strict\"\n) -> str:\n    \"\"\"Return the unicode representation of a bytes object ``text``. If\n    ``text`` is already an unicode object, return it as-is.\"\"\"\n    if isinstance(text, str):\n        return text\n    if not isinstance(text, (bytes, str)):\n        raise TypeError(\n            \"to_unicode must receive a bytes or str \"\n            f\"object, got {type(text).__name__}\"\n        )\n    if encoding is None:\n        encoding = \"utf-8\"\n    return text.decode(encoding, errors)\n\n\ndef to_bytes(\n    text: str | bytes, encoding: str | None = None, errors: str = \"strict\"\n) -> bytes:\n    \"\"\"Return the binary representation of ``text``. If ``text``\n    is already a bytes object, return it as-is.\"\"\"\n    if isinstance(text, bytes):\n        return text\n    if not isinstance(text, str):\n        raise TypeError(\n            f\"to_bytes must receive a str or bytes object, got {type(text).__name__}\"\n        )\n    if encoding is None:\n        encoding = \"utf-8\"\n    return text.encode(encoding, errors)\n\n\ndef re_rsearch(\n    pattern: str | Pattern[str], text: str, chunk_size: int = 1024\n) -> tuple[int, int] | None:\n    \"\"\"\n    This function does a reverse search in a text using a regular expression\n    given in the attribute 'pattern'.\n    Since the re module does not provide this functionality, we have to find for\n    the expression into chunks of text extracted from the end (for the sake of efficiency).\n    At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for\n    the pattern. If the pattern is not found, another chunk is extracted, and another\n    search is performed.\n    This process continues until a match is found, or until the whole file is read.\n    In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing\n    the start position of the match, and the ending (regarding the entire text).\n    \"\"\"\n\n    def _chunk_iter() -> Iterable[tuple[str, int]]:\n        offset = len(text)\n        while True:\n            offset -= chunk_size * 1024\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)\n\n    if isinstance(pattern, str):\n        pattern = re.compile(pattern)\n\n    for chunk, offset in _chunk_iter():\n        matches = list(pattern.finditer(chunk))\n        if matches:\n            start, end = matches[-1].span()\n            return offset + start, offset + end\n    return None\n\n\n_SelfT = TypeVar(\"_SelfT\")\n\n\ndef memoizemethod_noargs(\n    method: Callable[Concatenate[_SelfT, _P], _T]\n) -> Callable[Concatenate[_SelfT, _P], _T]:\n    \"\"\"Decorator to cache the result of a method (without arguments) using a\n    weak reference to its object\n    \"\"\"\n    cache: weakref.WeakKeyDictionary[_SelfT, _T] = weakref.WeakKeyDictionary()\n\n    @wraps(method)\n    def new_method(self: _SelfT, *args: _P.args, **kwargs: _P.kwargs) -> _T:\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]\n\n    return new_method\n\n\n_BINARYCHARS = {\n    i for i in range(32) if to_bytes(chr(i)) not in {b\"\\0\", b\"\\t\", b\"\\n\", b\"\\r\"}\n}\n\n\ndef binary_is_text(data: bytes) -> bool:\n    \"\"\"Returns ``True`` if the given ``data`` argument (a ``bytes`` object)\n    does not contain unprintable control characters.\n    \"\"\"\n    if not isinstance(data, bytes):\n        raise TypeError(f\"data must be bytes, got '{type(data).__name__}'\")\n    return all(c not in _BINARYCHARS for c in data)\n\n\ndef get_func_args(func: Callable[..., Any], stripself: bool = False) -> list[str]:\n    \"\"\"Return the argument name list of a callable object\"\"\"\n    if not callable(func):\n        raise TypeError(f\"func must be callable, got '{type(func).__name__}'\")\n\n    args: list[str] = []\n    try:\n        sig = inspect.signature(func)\n    except ValueError:\n        return args\n\n    if isinstance(func, partial):\n        partial_args = func.args\n        partial_kw = func.keywords\n\n        for name, param in sig.parameters.items():\n            if param.name in partial_args:\n                continue\n            if partial_kw and param.name in partial_kw:\n                continue\n            args.append(name)\n    else:\n        args = list(sig.parameters)\n\n    if stripself and args and args[0] == \"self\":\n        args = args[1:]\n    return args\n\n\ndef get_spec(func: Callable[..., Any]) -> tuple[list[str], dict[str, Any]]:\n    \"\"\"Returns (args, kwargs) tuple for a function\n    >>> import re\n    >>> get_spec(re.match)\n    (['pattern', 'string'], {'flags': 0})\n\n    >>> class Test:\n    ...     def __call__(self, val):\n    ...         pass\n    ...     def method(self, val, flags=0):\n    ...         pass\n\n    >>> get_spec(Test)\n    (['self', 'val'], {})\n\n    >>> get_spec(Test.method)\n    (['self', 'val'], {'flags': 0})\n\n    >>> get_spec(Test().method)\n    (['self', 'val'], {'flags': 0})\n    \"\"\"\n\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        spec = inspect.getfullargspec(func)\n    elif hasattr(func, \"__call__\"):  # noqa: B004\n        spec = inspect.getfullargspec(func.__call__)\n    else:\n        raise TypeError(f\"{type(func)} is not callable\")\n\n    defaults: tuple[Any, ...] = spec.defaults or ()\n\n    firstdefault = len(spec.args) - len(defaults)\n    args = spec.args[:firstdefault]\n    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n    return args, kwargs\n\n\ndef equal_attributes(\n    obj1: Any, obj2: Any, attributes: list[str | Callable[[Any], Any]] | None\n) -> bool:\n    \"\"\"Compare two objects attributes\"\"\"\n    warnings.warn(\n        \"The equal_attributes function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    # not attributes given return False by default\n    if not attributes:\n        return False\n\n    temp1, temp2 = object(), object()\n    for attr in attributes:\n        # support callables like itemgetter\n        if callable(attr):\n            if attr(obj1) != attr(obj2):\n                return False\n        elif getattr(obj1, attr, temp1) != getattr(obj2, attr, temp2):\n            return False\n    # all attributes equal\n    return True\n\n\n@overload\ndef without_none_values(iterable: Mapping[_KT, _VT]) -> dict[_KT, _VT]: ...\n\n\n@overload\ndef without_none_values(iterable: Iterable[_KT]) -> Iterable[_KT]: ...\n\n\ndef without_none_values(\n    iterable: Mapping[_KT, _VT] | Iterable[_KT]\n) -> dict[_KT, _VT] | Iterable[_KT]:\n    \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n\n    If ``iterable`` is a mapping, return a dictionary where all pairs that have\n    value ``None`` have been removed.\n    \"\"\"\n    if isinstance(iterable, Mapping):\n        return {k: v for k, v in iterable.items() if v is not None}\n    # the iterable __init__ must take another iterable\n    return type(iterable)(v for v in iterable if v is not None)  # type: ignore[call-arg]\n\n\ndef global_object_name(obj: Any) -> str:\n    \"\"\"Return the full import path of the given class.\n\n    >>> from scrapy import Request\n    >>> global_object_name(Request)\n    'scrapy.http.request.Request'\n    \"\"\"\n    return f\"{obj.__module__}.{obj.__qualname__}\"\n\n\nif hasattr(sys, \"pypy_version_info\"):\n\n    def garbage_collect() -> None:\n        # Collecting weakreferences can take two collections on PyPy.\n        gc.collect()\n        gc.collect()\n\nelse:\n\n    def garbage_collect() -> None:\n        gc.collect()\n\n\nclass MutableChain(Iterable[_T]):\n    \"\"\"\n    Thin wrapper around itertools.chain, allowing to add iterables \"in-place\"\n    \"\"\"\n\n    def __init__(self, *args: Iterable[_T]):\n        self.data: Iterator[_T] = chain.from_iterable(args)\n\n    def extend(self, *iterables: Iterable[_T]) -> None:\n        self.data = chain(self.data, chain.from_iterable(iterables))\n\n    def __iter__(self) -> Iterator[_T]:\n        return self\n\n    def __next__(self) -> _T:\n        return next(self.data)\n\n\nasync def _async_chain(\n    *iterables: Iterable[_T] | AsyncIterable[_T],\n) -> AsyncIterator[_T]:\n    for it in iterables:\n        async for o in as_async_generator(it):\n            yield o\n\n\nclass MutableAsyncChain(AsyncIterable[_T]):\n    \"\"\"\n    Similar to MutableChain but for async iterables\n    \"\"\"\n\n    def __init__(self, *args: Iterable[_T] | AsyncIterable[_T]):\n        self.data: AsyncIterator[_T] = _async_chain(*args)\n\n    def extend(self, *iterables: Iterable[_T] | AsyncIterable[_T]) -> None:\n        self.data = _async_chain(self.data, _async_chain(*iterables))\n\n    def __aiter__(self) -> AsyncIterator[_T]:\n        return self\n\n    async def __anext__(self) -> _T:\n        return await self.data.__anext__()\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:46:39"}, {"role": "assistant", "content": "```python\nimport copy\nimport unittest\n\nfrom scrapy.http import Headers\n\n\nclass HeadersTest(unittest.TestCase):\n    def assertSortedEqual(self, first, second, msg=None):\n        return self.assertEqual(sorted(first), sorted(second), msg)\n\n    def test_basics(self):\n        h = Headers({\"Content-Type\": \"text/html\", \"Content-Length\": 1234})\n        assert h[\"Content-Type\"]\n        assert h[\"Content-Length\"]\n\n        self.assertRaises(KeyError, h.__getitem__, \"Accept\")\n        self.assertEqual(h.get(\"Accept\"), None)\n        self.assertEqual(h.getlist(\"Accept\"), [])\n\n        self.assertEqual(h.get(\"Accept\", \"*/*\"), b\"*/*\")\n        self.assertEqual(h.getlist(\"Accept\", \"*/*\"), [b\"*/*\"])\n        self.assertEqual(\n            h.getlist(\"Accept\", [\"text/html\", \"images/jpeg\"]),\n            [b\"text/html\", b\"images/jpeg\"],\n        )\n\n    def test_single_value(self):\n        h = Headers()\n        h[\"Content-Type\"] = \"text/html\"\n        self.assertEqual(h[\"Content-Type\"], b\"text/html\")\n        self.assertEqual(h.get(\"Content-Type\"), b\"text/html\")\n        self.assertEqual(h.getlist(\"Content-Type\"), [b\"text/html\"])\n\n    def test_multivalue(self):\n        h = Headers()\n        h[\"X-Forwarded-For\"] = hlist = [\"ip1\", \"ip2\"]\n        self.assertEqual(h[\"X-Forwarded-For\"], b\"ip2\")\n        self.assertEqual(h.get(\"X-Forwarded-For\"), b\"ip2\")\n        self.assertEqual(h.getlist(\"X-Forwarded-For\"), [b\"ip1\", b\"ip2\"])\n        assert h.getlist(\"X-Forwarded-For\") is not hlist\n\n    def test_multivalue_for_one_header(self):\n        h = Headers(((\"a\", \"b\"), (\"a\", \"c\")))\n        self.assertEqual(h[\"a\"], b\"c\")\n        self.assertEqual(h.get(\"a\"), b\"c\")\n        self.assertEqual(h.getlist(\"a\"), [b\"b\", b\"c\"])\n\n    def test_encode_utf8(self):\n        h = Headers({\"key\": \"\\xa3\"}, encoding=\"utf-8\")\n        key, val = dict(h).popitem()\n        assert isinstance(key, bytes), key\n        assert isinstance(val[0], bytes), val[0]\n        self.assertEqual(val[0], b\"\\xc2\\xa3\")\n\n    def test_encode_latin1(self):\n        h = Headers({\"key\": \"\\xa3\"}, encoding=\"latin1\")\n        key, val = dict(h).popitem()\n        self.assertEqual(val[0], b\"\\xa3\")\n\n    def test_encode_multiple(self):\n        h = Headers({\"key\": [\"\\xa3\"]}, encoding=\"utf-8\")\n        key, val = dict(h).popitem()\n        self.assertEqual(val[0], b\"\\xc2\\xa3\")\n\n    def test_delete_and_contains(self):\n        h = Headers()\n        h[\"Content-Type\"] = \"text/html\"\n        assert \"Content-Type\" in h\n        del h[\"Content-Type\"]\n        assert \"Content-Type\" not in h\n\n    def test_setdefault(self):\n        h = Headers()\n        hlist = [\"ip1\", \"ip2\"]\n        olist = h.setdefault(\"X-Forwarded-For\", hlist)\n        assert h.getlist(\"X-Forwarded-For\") is not hlist\n        assert h.getlist(\"X-Forwarded-For\") is olist\n\n        h = Headers()\n        olist = h.setdefault(\"X-Forwarded-For\", \"ip1\")\n        self.assertEqual(h.getlist(\"X-Forwarded-For\"), [b\"ip1\"])\n        assert h.getlist(\"X-Forwarded-For\") is olist\n\n    def test_iterables(self):\n        idict = {\"Content-Type\": \"text/html\", \"X-Forwarded-For\": [\"ip1\", \"ip2\"]}\n\n        h = Headers(idict)\n        self.assertDictEqual(\n            dict(h),\n            {b\"Content-Type\": [b\"text/html\"], b\"X-Forwarded-For\": [b\"ip1\", b\"ip2\"]},\n        )\n        self.assertSortedEqual(h.keys(), [b\"X-Forwarded-For\", b\"Content-Type\"])\n        self.assertSortedEqual(\n            h.items(),\n            [(b\"X-Forwarded-For\", [b\"ip1\", b\"ip2\"]), (b\"Content-Type\", [b\"text/html\"])],\n        )\n        self.assertSortedEqual(h.values(), [b\"ip2\", b\"text/html\"])\n\n    def test_update(self):\n        h = Headers()\n        h.update({\"Content-Type\": \"text/html\", \"X-Forwarded-For\": [\"ip1\", \"ip2\"]})\n        self.assertEqual(h.getlist(\"Content-Type\"), [b\"text/html\"])\n        self.assertEqual(h.getlist(\"X-Forwarded-For\"), [b\"ip1\", b\"ip2\"])\n\n    def test_copy(self):\n        h1 = Headers({\"header1\": [\"value1\", \"value2\"]})\n        h2 = copy.copy(h1)\n        self.assertEqual(h1, h2)\n        self.assertEqual(h1.getlist(\"header1\"), h2.getlist(\"header1\"))\n        assert h1.getlist(\"header1\") is not h2.getlist(\"header1\")\n        assert isinstance(h2, Headers)\n\n    def test_appendlist(self):\n        h1 = Headers({\"header1\": \"value1\"})\n        h1.appendlist(\"header1\", \"value3\")\n        self.assertEqual(h1.getlist(\"header1\"), [b\"value1\", b\"value3\"])\n\n        h1 = Headers()\n        h1.appendlist(\"header1\", \"value1\")\n        h1.appendlist(\"header1\", \"value3\")\n        self.assertEqual(h1.getlist(\"header1\"), [b\"value1\", b\"value3\"])\n\n    def test_setlist(self):\n        h1 = Headers({\"header1\": \"value1\"})\n        self.assertEqual(h1.getlist(\"header1\"), [b\"value1\"])\n        h1.setlist(\"header1\", [b\"value2\", b\"value3\"])\n        self.assertEqual(h1.getlist(\"header1\"), [b\"value2\", b\"value3\"])\n\n    def test_setlistdefault(self):\n        h1 = Headers({\"header1\": \"value1\"})\n        h1.setlistdefault(\"header1\", [\"value2\", \"value3\"])\n        h1.setlistdefault(\"header2\", [\"value2\", \"value3\"])\n        self.assertEqual(h1.getlist(\"header1\"), [b\"value1\"])\n        self.assertEqual(h1.getlist(\"header2\"), [b\"value2\", b\"value3\"])\n\n    def test_none_value(self):\n        h1 = Headers()\n        h1[\"foo\"] = \"bar\"\n        h1[\"foo\"] = None\n        h1.setdefault(\"foo\", \"bar\")\n        self.assertEqual(h1.get(\"foo\"), None)\n        self.assertEqual(h1.getlist(\"foo\"), [])\n\n    def test_int_value(self):\n        h1 = Headers({\"hey\": 5})\n        h1[\"foo\"] = 1\n        h1.setdefault(\"bar\", 2)\n        h1.setlist(\"buz\", [1, \"dos\", 3])\n        self.assertEqual(h1.getlist(\"foo\"), [b\"1\"])\n        self.assertEqual(h1.getlist(\"bar\"), [b\"2\"])\n        self.assertEqual(h1.getlist(\"buz\"), [b\"1\", b\"dos\", b\"3\"])\n        self.assertEqual(h1.getlist(\"hey\"), [b\"5\"])\n\n    def test_invalid_value(self):\n        self.assertRaisesRegex(\n            TypeError, \"Unsupported value type\", Headers, {\"foo\": object()}\n        )\n        self.assertRaisesRegex(\n            TypeError, \"Unsupported value type\", Headers().__setitem__, \"foo\", object()\n        )\n        self.assertRaisesRegex(\n            TypeError, \"Unsupported value type\", Headers().setdefault, \"foo\", object()\n        )\n        self.assertRaisesRegex(\n            TypeError, \"Unsupported value type\", Headers().setlist, \"foo\", [object()]\n        )\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: scrapy\nTest File Path: scrapy\\test_item\\test_item.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n\n### Source File Dependency Files Content\n### Dependency File: trackref.py\n\"\"\"This module provides some functions and classes to record and report\nreferences to live object instances.\n\nIf you want live objects for a particular class to be tracked, you only have to\nsubclass from object_ref (instead of object).\n\nAbout performance: This library has a minimal performance impact when enabled,\nand no performance penalty at all when disabled (as object_ref becomes just an\nalias to object in that case).\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom operator import itemgetter\nfrom time import time\nfrom typing import TYPE_CHECKING, Any\nfrom weakref import WeakKeyDictionary\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nNoneType = type(None)\nlive_refs: defaultdict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)\n\n\nclass object_ref:\n    \"\"\"Inherit from this class to a keep a record of live instances\"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, *args: Any, **kwargs: Any) -> Self:\n        obj = object.__new__(cls)\n        live_refs[cls][obj] = time()\n        return obj\n\n\n# using Any as it's hard to type type(None)\ndef format_live_refs(ignore: Any = NoneType) -> str:\n    \"\"\"Return a tabular representation of tracked objects\"\"\"\n    s = \"Live References\\n\\n\"\n    now = time()\n    for cls, wdict in sorted(live_refs.items(), key=lambda x: x[0].__name__):\n        if not wdict:\n            continue\n        if issubclass(cls, ignore):\n            continue\n        oldest = min(wdict.values())\n        s += f\"{cls.__name__:<30} {len(wdict):6}   oldest: {int(now - oldest)}s ago\\n\"\n    return s\n\n\ndef print_live_refs(*a: Any, **kw: Any) -> None:\n    \"\"\"Print tracked objects\"\"\"\n    print(format_live_refs(*a, **kw))\n\n\ndef get_oldest(class_name: str) -> Any:\n    \"\"\"Get the oldest object for a specific class name\"\"\"\n    for cls, wdict in live_refs.items():\n        if cls.__name__ == class_name:\n            if not wdict:\n                break\n            return min(wdict.items(), key=itemgetter(1))[0]\n    return None\n\n\ndef iter_all(class_name: str) -> Iterable[Any]:\n    \"\"\"Iterate over all objects of the same class by its class name\"\"\"\n    for cls, wdict in live_refs.items():\n        if cls.__name__ == class_name:\n            return wdict.keys()\n    return []\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:46:39"}, {"role": "assistant", "content": "```python\nimport unittest\nfrom abc import ABCMeta\nfrom unittest import mock\n\nfrom scrapy.item import Field, Item, ItemMeta\n\n\nclass ItemTest(unittest.TestCase):\n    def assertSortedEqual(self, first, second, msg=None):\n        return self.assertEqual(sorted(first), sorted(second), msg)\n\n    def test_simple(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        i[\"name\"] = \"name\"\n        self.assertEqual(i[\"name\"], \"name\")\n\n    def test_init(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        self.assertRaises(KeyError, i.__getitem__, \"name\")\n\n        i2 = TestItem(name=\"john doe\")\n        self.assertEqual(i2[\"name\"], \"john doe\")\n\n        i3 = TestItem({\"name\": \"john doe\"})\n        self.assertEqual(i3[\"name\"], \"john doe\")\n\n        i4 = TestItem(i3)\n        self.assertEqual(i4[\"name\"], \"john doe\")\n\n        self.assertRaises(KeyError, TestItem, {\"name\": \"john doe\", \"other\": \"foo\"})\n\n    def test_invalid_field(self):\n        class TestItem(Item):\n            pass\n\n        i = TestItem()\n        self.assertRaises(KeyError, i.__setitem__, \"field\", \"text\")\n        self.assertRaises(KeyError, i.__getitem__, \"field\")\n\n    def test_repr(self):\n        class TestItem(Item):\n            name = Field()\n            number = Field()\n\n        i = TestItem()\n        i[\"name\"] = \"John Doe\"\n        i[\"number\"] = 123\n        itemrepr = repr(i)\n\n        self.assertEqual(itemrepr, \"{'name': 'John Doe', 'number': 123}\")\n\n        i2 = eval(itemrepr)  # pylint: disable=eval-used\n        self.assertEqual(i2[\"name\"], \"John Doe\")\n        self.assertEqual(i2[\"number\"], 123)\n\n    def test_private_attr(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        i._private = \"test\"\n        self.assertEqual(i._private, \"test\")\n\n    def test_raise_getattr(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        self.assertRaises(AttributeError, getattr, i, \"name\")\n\n    def test_raise_setattr(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        self.assertRaises(AttributeError, setattr, i, \"name\", \"john\")\n\n    def test_custom_methods(self):\n        class TestItem(Item):\n            name = Field()\n\n            def get_name(self):\n                return self[\"name\"]\n\n            def change_name(self, name):\n                self[\"name\"] = name\n\n        i = TestItem()\n        self.assertRaises(KeyError, i.get_name)\n        i[\"name\"] = \"lala\"\n        self.assertEqual(i.get_name(), \"lala\")\n        i.change_name(\"other\")\n        self.assertEqual(i.get_name(), \"other\")\n\n    def test_metaclass(self):\n        class TestItem(Item):\n            name = Field()\n            keys = Field()\n            values = Field()\n\n        i = TestItem()\n        i[\"name\"] = \"John\"\n        self.assertEqual(list(i.keys()), [\"name\"])\n        self.assertEqual(list(i.values()), [\"John\"])\n\n        i[\"keys\"] = \"Keys\"\n        i[\"values\"] = \"Values\"\n        self.assertSortedEqual(list(i.keys()), [\"keys\", \"values\", \"name\"])\n        self.assertSortedEqual(list(i.values()), [\"Keys\", \"Values\", \"John\"])\n\n    def test_metaclass_with_fields_attribute(self):\n        class TestItem(Item):\n            fields = {\"new\": Field(default=\"X\")}\n\n        item = TestItem(new=\"New\")\n        self.assertSortedEqual(list(item.keys()), [\"new\"])\n        self.assertSortedEqual(list(item.values()), [\"New\"])\n\n    def test_metaclass_inheritance(self):\n        class ParentItem(Item):\n            name = Field()\n            keys = Field()\n            values = Field()\n\n        class TestItem(ParentItem):\n            keys = Field()\n\n        i = TestItem()\n        i[\"keys\"] = 3\n        self.assertEqual(list(i.keys()), [\"keys\"])\n        self.assertEqual(list(i.values()), [3])\n\n    def test_metaclass_multiple_inheritance_simple(self):\n        class A(Item):\n            fields = {\"load\": Field(default=\"A\")}\n            save = Field(default=\"A\")\n\n        class B(A):\n            pass\n\n        class C(Item):\n            fields = {\"load\": Field(default=\"C\")}\n            save = Field(default=\"C\")\n\n        class D(B, C):\n            pass\n\n        item = D(save=\"X\", load=\"Y\")\n        self.assertEqual(item[\"save\"], \"X\")\n        self.assertEqual(item[\"load\"], \"Y\")\n        self.assertEqual(D.fields, {\"load\": {\"default\": \"A\"}, \"save\": {\"default\": \"A\"}})\n\n        # D class inverted\n        class E(C, B):\n            pass\n\n        self.assertEqual(E(save=\"X\")[\"save\"], \"X\")\n        self.assertEqual(E(load=\"X\")[\"load\"], \"X\")\n        self.assertEqual(E.fields, {\"load\": {\"default\": \"C\"}, \"save\": {\"default\": \"C\"}})\n\n    def test_metaclass_multiple_inheritance_diamond(self):\n        class A(Item):\n            fields = {\"update\": Field(default=\"A\")}\n            save = Field(default=\"A\")\n            load = Field(default=\"A\")\n\n        class B(A):\n            pass\n\n        class C(A):\n            fields = {\"update\": Field(default=\"C\")}\n            save = Field(default=\"C\")\n\n        class D(B, C):\n            fields = {\"update\": Field(default=\"D\")}\n            load = Field(default=\"D\")\n\n        self.assertEqual(D(save=\"X\")[\"save\"], \"X\")\n        self.assertEqual(D(load=\"X\")[\"load\"], \"X\")\n        self.assertEqual(\n            D.fields,\n            {\n                \"save\": {\"default\": \"C\"},\n                \"load\": {\"default\": \"D\"},\n                \"update\": {\"default\": \"D\"},\n            },\n        )\n\n        # D class inverted\n        class E(C, B):\n            load = Field(default=\"E\")\n\n        self.assertEqual(E(save=\"X\")[\"save\"], \"X\")\n        self.assertEqual(E(load=\"X\")[\"load\"], \"X\")\n        self.assertEqual(\n            E.fields,\n            {\n                \"save\": {\"default\": \"C\"},\n                \"load\": {\"default\": \"E\"},\n                \"update\": {\"default\": \"C\"},\n            },\n        )\n\n    def test_metaclass_multiple_inheritance_without_metaclass(self):\n        class A(Item):\n            fields = {\"load\": Field(default=\"A\")}\n            save = Field(default=\"A\")\n\n        class B(A):\n            pass\n\n        class C:\n            fields = {\"load\": Field(default=\"C\")}\n            not_allowed = Field(default=\"not_allowed\")\n            save = Field(default=\"C\")\n\n        class D(B, C):\n            pass\n\n        self.assertRaises(KeyError, D, not_allowed=\"value\")\n        self.assertEqual(D(save=\"X\")[\"save\"], \"X\")\n        self.assertEqual(D.fields, {\"save\": {\"default\": \"A\"}, \"load\": {\"default\": \"A\"}})\n\n        # D class inverted\n        class E(C, B):\n            pass\n\n        self.assertRaises(KeyError, E, not_allowed=\"value\")\n        self.assertEqual(E(save=\"X\")[\"save\"], \"X\")\n        self.assertEqual(E.fields, {\"save\": {\"default\": \"A\"}, \"load\": {\"default\": \"A\"}})\n\n    def test_to_dict(self):\n        class TestItem(Item):\n            name = Field()\n\n        i = TestItem()\n        i[\"name\"] = \"John\"\n        self.assertEqual(dict(i), {\"name\": \"John\"})\n\n    def test_copy(self):\n        class TestItem(Item):\n            name = Field()\n\n        item = TestItem({\"name\": \"lower\"})\n        copied_item = item.copy()\n        self.assertNotEqual(id(item), id(copied_item))\n        copied_item[\"name\"] = copied_item[\"name\"].upper()\n        self.assertNotEqual(item[\"name\"], copied_item[\"name\"])\n\n    def test_deepcopy(self):\n        class TestItem(Item):\n            tags = Field()\n\n        item = TestItem({\"tags\": [\"tag1\"]})\n        copied_item = item.deepcopy()\n        item[\"tags\"].append(\"tag2\")\n        assert item[\"tags\"] != copied_item[\"tags\"]\n\n\nclass ItemMetaTest(unittest.TestCase):\n    def test_new_method_propagates_classcell(self):\n        new_mock = mock.Mock(side_effect=ABCMeta.__new__)\n        base = ItemMeta.__bases__[0]\n\n        with mock.patch.object(base, \"__new__\", new_mock):\n\n            class MyItem(Item):\n                def f(self):\n                    # For rationale of this see:\n                    # https://github.com/python/cpython/blob/ee1a81b77444c6715cbe610e951c655b6adab88b/Lib/test/test_super.py#L222\n                    return __class__\n\n            MyItem()\n\n        (first_call, second_call) = new_mock.call_args_list[-2:]\n\n        mcs, class_name, bases, attrs = first_call[0]\n        assert \"__classcell__\" not in attrs\n        mcs, class_name, bases, attrs = second_call[0]\n        assert \"__classcell__\" in attrs\n\n\nclass ItemMetaClassCellRegression(unittest.TestCase):\n    def test_item_meta_classcell_regression(self):\n        class MyItem(Item, metaclass=ItemMeta):\n            def __init__(\n                self, *args, **kwargs\n            ):  # pylint: disable=useless-parent-delegation\n                # This call to super() trigger the __classcell__ propagation\n                # requirement. When not done properly raises an error:\n                # TypeError: __class__ set to <class '__main__.MyItem'>\n                # defining 'MyItem' as <class '__main__.MyItem'>\n                super().__init__(*args, **kwargs)\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: scrapy\nTest File Path: scrapy\\test_link\\test_link.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:46:39"}, {"role": "assistant", "content": "```python\nimport unittest\n\nfrom scrapy.link import Link\n\n\nclass LinkTest(unittest.TestCase):\n    def _assert_same_links(self, link1, link2):\n        self.assertEqual(link1, link2)\n        self.assertEqual(hash(link1), hash(link2))\n\n    def _assert_different_links(self, link1, link2):\n        self.assertNotEqual(link1, link2)\n        self.assertNotEqual(hash(link1), hash(link2))\n\n    def test_eq_and_hash(self):\n        l1 = Link(\"http://www.example.com\")\n        l2 = Link(\"http://www.example.com/other\")\n        l3 = Link(\"http://www.example.com\")\n\n        self._assert_same_links(l1, l1)\n        self._assert_different_links(l1, l2)\n        self._assert_same_links(l1, l3)\n\n        l4 = Link(\"http://www.example.com\", text=\"test\")\n        l5 = Link(\"http://www.example.com\", text=\"test2\")\n        l6 = Link(\"http://www.example.com\", text=\"test\")\n\n        self._assert_same_links(l4, l4)\n        self._assert_different_links(l4, l5)\n        self._assert_same_links(l4, l6)\n\n        l7 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=False\n        )\n        l8 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=False\n        )\n        l9 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=True\n        )\n        l10 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"other\", nofollow=False\n        )\n        self._assert_same_links(l7, l8)\n        self._assert_different_links(l7, l9)\n        self._assert_different_links(l7, l10)\n\n    def test_repr(self):\n        l1 = Link(\n            \"http://www.example.com\", text=\"test\", fragment=\"something\", nofollow=True\n        )\n        l2 = eval(repr(l1))  # pylint: disable=eval-used\n        self._assert_same_links(l1, l2)\n\n    def test_bytes_url(self):\n        with self.assertRaises(TypeError):\n            Link(b\"http://www.example.com/\\xc2\\xa3\")\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: requests\nTest File Path: requests\\test_adapters\\test_adapters.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"\nrequests.adapters\n~~~~~~~~~~~~~~~~~\n\nThis module contains the transport adapters that Requests uses to define\nand maintain connections.\n\"\"\"\n\nimport os.path\nimport socket  # noqa: F401\nimport typing\nimport warnings\n\nfrom urllib3.exceptions import ClosedPoolError, ConnectTimeoutError\nfrom urllib3.exceptions import HTTPError as _HTTPError\nfrom urllib3.exceptions import InvalidHeader as _InvalidHeader\nfrom urllib3.exceptions import (\n    LocationValueError,\n    MaxRetryError,\n    NewConnectionError,\n    ProtocolError,\n)\nfrom urllib3.exceptions import ProxyError as _ProxyError\nfrom urllib3.exceptions import ReadTimeoutError, ResponseError\nfrom urllib3.exceptions import SSLError as _SSLError\nfrom urllib3.poolmanager import PoolManager, proxy_from_url\nfrom urllib3.util import Timeout as TimeoutSauce\nfrom urllib3.util import parse_url\nfrom urllib3.util.retry import Retry\nfrom urllib3.util.ssl_ import create_urllib3_context\n\nfrom .auth import _basic_auth_str\nfrom .compat import basestring, urlparse\nfrom .cookies import extract_cookies_to_jar\nfrom .exceptions import (\n    ConnectionError,\n    ConnectTimeout,\n    InvalidHeader,\n    InvalidProxyURL,\n    InvalidSchema,\n    InvalidURL,\n    ProxyError,\n    ReadTimeout,\n    RetryError,\n    SSLError,\n)\nfrom .models import Response\nfrom .structures import CaseInsensitiveDict\nfrom .utils import (\n    DEFAULT_CA_BUNDLE_PATH,\n    extract_zipped_paths,\n    get_auth_from_url,\n    get_encoding_from_headers,\n    prepend_scheme_if_needed,\n    select_proxy,\n    urldefragauth,\n)\n\ntry:\n    from urllib3.contrib.socks import SOCKSProxyManager\nexcept ImportError:\n\n    def SOCKSProxyManager(*args, **kwargs):\n        raise InvalidSchema(\"Missing dependencies for SOCKS support.\")\n\n\nif typing.TYPE_CHECKING:\n    from .models import PreparedRequest\n\n\nDEFAULT_POOLBLOCK = False\nDEFAULT_POOLSIZE = 10\nDEFAULT_RETRIES = 0\nDEFAULT_POOL_TIMEOUT = None\n\n\ntry:\n    import ssl  # noqa: F401\n\n    _preloaded_ssl_context = create_urllib3_context()\n    _preloaded_ssl_context.load_verify_locations(\n        extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)\n    )\nexcept ImportError:\n    # Bypass default SSLContext creation when Python\n    # interpreter isn't built with the ssl module.\n    _preloaded_ssl_context = None\n\n\ndef _urllib3_request_context(\n    request: \"PreparedRequest\",\n    verify: \"bool | str | None\",\n    client_cert: \"typing.Tuple[str, str] | str | None\",\n    poolmanager: \"PoolManager\",\n) -> \"(typing.Dict[str, typing.Any], typing.Dict[str, typing.Any])\":\n    host_params = {}\n    pool_kwargs = {}\n    parsed_request_url = urlparse(request.url)\n    scheme = parsed_request_url.scheme.lower()\n    port = parsed_request_url.port\n\n    # Determine if we have and should use our default SSLContext\n    # to optimize performance on standard requests.\n    poolmanager_kwargs = getattr(poolmanager, \"connection_pool_kw\", {})\n    has_poolmanager_ssl_context = poolmanager_kwargs.get(\"ssl_context\")\n    should_use_default_ssl_context = (\n        _preloaded_ssl_context is not None and not has_poolmanager_ssl_context\n    )\n\n    cert_reqs = \"CERT_REQUIRED\"\n    if verify is False:\n        cert_reqs = \"CERT_NONE\"\n    elif verify is True and should_use_default_ssl_context:\n        pool_kwargs[\"ssl_context\"] = _preloaded_ssl_context\n    elif isinstance(verify, str):\n        if not os.path.isdir(verify):\n            pool_kwargs[\"ca_certs\"] = verify\n        else:\n            pool_kwargs[\"ca_cert_dir\"] = verify\n    pool_kwargs[\"cert_reqs\"] = cert_reqs\n    if client_cert is not None:\n        if isinstance(client_cert, tuple) and len(client_cert) == 2:\n            pool_kwargs[\"cert_file\"] = client_cert[0]\n            pool_kwargs[\"key_file\"] = client_cert[1]\n        else:\n            # According to our docs, we allow users to specify just the client\n            # cert path\n            pool_kwargs[\"cert_file\"] = client_cert\n    host_params = {\n        \"scheme\": scheme,\n        \"host\": parsed_request_url.hostname,\n        \"port\": port,\n    }\n    return host_params, pool_kwargs\n\n\nclass BaseAdapter:\n    \"\"\"The Base Transport Adapter\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def send(\n        self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None\n    ):\n        \"\"\"Sends PreparedRequest object. Returns Response object.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param stream: (optional) Whether to stream the request content.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a :ref:`(connect timeout,\n            read timeout) <timeouts>` tuple.\n        :type timeout: float or tuple\n        :param verify: (optional) Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a string, in which case it must be a path\n            to a CA bundle to use\n        :param cert: (optional) Any user-provided SSL certificate to be trusted.\n        :param proxies: (optional) The proxies dictionary to apply to the request.\n        \"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Cleans up adapter specific items.\"\"\"\n        raise NotImplementedError\n\n\nclass HTTPAdapter(BaseAdapter):\n    \"\"\"The built-in HTTP Adapter for urllib3.\n\n    Provides a general-case interface for Requests sessions to contact HTTP and\n    HTTPS urls by implementing the Transport Adapter interface. This class will\n    usually be created by the :class:`Session <Session>` class under the\n    covers.\n\n    :param pool_connections: The number of urllib3 connection pools to cache.\n    :param pool_maxsize: The maximum number of connections to save in the pool.\n    :param max_retries: The maximum number of retries each connection\n        should attempt. Note, this applies only to failed DNS lookups, socket\n        connections and connection timeouts, never to requests where data has\n        made it to the server. By default, Requests does not retry failed\n        connections. If you need granular control over the conditions under\n        which we retry a request, import urllib3's ``Retry`` class and pass\n        that instead.\n    :param pool_block: Whether the connection pool should block for connections.\n\n    Usage::\n\n      >>> import requests\n      >>> s = requests.Session()\n      >>> a = requests.adapters.HTTPAdapter(max_retries=3)\n      >>> s.mount('http://', a)\n    \"\"\"\n\n    __attrs__ = [\n        \"max_retries\",\n        \"config\",\n        \"_pool_connections\",\n        \"_pool_maxsize\",\n        \"_pool_block\",\n    ]\n\n    def __init__(\n        self,\n        pool_connections=DEFAULT_POOLSIZE,\n        pool_maxsize=DEFAULT_POOLSIZE,\n        max_retries=DEFAULT_RETRIES,\n        pool_block=DEFAULT_POOLBLOCK,\n    ):\n        if max_retries == DEFAULT_RETRIES:\n            self.max_retries = Retry(0, read=False)\n        else:\n            self.max_retries = Retry.from_int(max_retries)\n        self.config = {}\n        self.proxy_manager = {}\n\n        super().__init__()\n\n        self._pool_connections = pool_connections\n        self._pool_maxsize = pool_maxsize\n        self._pool_block = pool_block\n\n        self.init_poolmanager(pool_connections, pool_maxsize, block=pool_block)\n\n    def __getstate__(self):\n        return {attr: getattr(self, attr, None) for attr in self.__attrs__}\n\n    def __setstate__(self, state):\n        # Can't handle by adding 'proxy_manager' to self.__attrs__ because\n        # self.poolmanager uses a lambda function, which isn't pickleable.\n        self.proxy_manager = {}\n        self.config = {}\n\n        for attr, value in state.items():\n            setattr(self, attr, value)\n\n        self.init_poolmanager(\n            self._pool_connections, self._pool_maxsize, block=self._pool_block\n        )\n\n    def init_poolmanager(\n        self, connections, maxsize, block=DEFAULT_POOLBLOCK, **pool_kwargs\n    ):\n        \"\"\"Initializes a urllib3 PoolManager.\n\n        This method should not be called from user code, and is only\n        exposed for use when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param connections: The number of urllib3 connection pools to cache.\n        :param maxsize: The maximum number of connections to save in the pool.\n        :param block: Block when no free connections are available.\n        :param pool_kwargs: Extra keyword arguments used to initialize the Pool Manager.\n        \"\"\"\n        # save these values for pickling\n        self._pool_connections = connections\n        self._pool_maxsize = maxsize\n        self._pool_block = block\n\n        self.poolmanager = PoolManager(\n            num_pools=connections,\n            maxsize=maxsize,\n            block=block,\n            **pool_kwargs,\n        )\n\n    def proxy_manager_for(self, proxy, **proxy_kwargs):\n        \"\"\"Return urllib3 ProxyManager for the given proxy.\n\n        This method should not be called from user code, and is only\n        exposed for use when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param proxy: The proxy to return a urllib3 ProxyManager for.\n        :param proxy_kwargs: Extra keyword arguments used to configure the Proxy Manager.\n        :returns: ProxyManager\n        :rtype: urllib3.ProxyManager\n        \"\"\"\n        if proxy in self.proxy_manager:\n            manager = self.proxy_manager[proxy]\n        elif proxy.lower().startswith(\"socks\"):\n            username, password = get_auth_from_url(proxy)\n            manager = self.proxy_manager[proxy] = SOCKSProxyManager(\n                proxy,\n                username=username,\n                password=password,\n                num_pools=self._pool_connections,\n                maxsize=self._pool_maxsize,\n                block=self._pool_block,\n                **proxy_kwargs,\n            )\n        else:\n            proxy_headers = self.proxy_headers(proxy)\n            manager = self.proxy_manager[proxy] = proxy_from_url(\n                proxy,\n                proxy_headers=proxy_headers,\n                num_pools=self._pool_connections,\n                maxsize=self._pool_maxsize,\n                block=self._pool_block,\n                **proxy_kwargs,\n            )\n\n        return manager\n\n    def cert_verify(self, conn, url, verify, cert):\n        \"\"\"Verify a SSL certificate. This method should not be called from user\n        code, and is only exposed for use when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param conn: The urllib3 connection object associated with the cert.\n        :param url: The requested URL.\n        :param verify: Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a string, in which case it must be a path\n            to a CA bundle to use\n        :param cert: The SSL certificate to verify.\n        \"\"\"\n        if url.lower().startswith(\"https\") and verify:\n            conn.cert_reqs = \"CERT_REQUIRED\"\n\n            # Only load the CA certificates if 'verify' is a string indicating the CA bundle to use.\n            # Otherwise, if verify is a boolean, we don't load anything since\n            # the connection will be using a context with the default certificates already loaded,\n            # and this avoids a call to the slow load_verify_locations()\n            if verify is not True:\n                # `verify` must be a str with a path then\n                cert_loc = verify\n\n                if not os.path.exists(cert_loc):\n                    raise OSError(\n                        f\"Could not find a suitable TLS CA certificate bundle, \"\n                        f\"invalid path: {cert_loc}\"\n                    )\n\n                if not os.path.isdir(cert_loc):\n                    conn.ca_certs = cert_loc\n                else:\n                    conn.ca_cert_dir = cert_loc\n        else:\n            conn.cert_reqs = \"CERT_NONE\"\n            conn.ca_certs = None\n            conn.ca_cert_dir = None\n\n        if cert:\n            if not isinstance(cert, basestring):\n                conn.cert_file = cert[0]\n                conn.key_file = cert[1]\n            else:\n                conn.cert_file = cert\n                conn.key_file = None\n            if conn.cert_file and not os.path.exists(conn.cert_file):\n                raise OSError(\n                    f\"Could not find the TLS certificate file, \"\n                    f\"invalid path: {conn.cert_file}\"\n                )\n            if conn.key_file and not os.path.exists(conn.key_file):\n                raise OSError(\n                    f\"Could not find the TLS key file, invalid path: {conn.key_file}\"\n                )\n\n    def build_response(self, req, resp):\n        \"\"\"Builds a :class:`Response <requests.Response>` object from a urllib3\n        response. This should not be called from user code, and is only exposed\n        for use when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`\n\n        :param req: The :class:`PreparedRequest <PreparedRequest>` used to generate the response.\n        :param resp: The urllib3 response object.\n        :rtype: requests.Response\n        \"\"\"\n        response = Response()\n\n        # Fallback to None if there's no status_code, for whatever reason.\n        response.status_code = getattr(resp, \"status\", None)\n\n        # Make headers case-insensitive.\n        response.headers = CaseInsensitiveDict(getattr(resp, \"headers\", {}))\n\n        # Set encoding.\n        response.encoding = get_encoding_from_headers(response.headers)\n        response.raw = resp\n        response.reason = response.raw.reason\n\n        if isinstance(req.url, bytes):\n            response.url = req.url.decode(\"utf-8\")\n        else:\n            response.url = req.url\n\n        # Add new cookies from the server.\n        extract_cookies_to_jar(response.cookies, req, resp)\n\n        # Give the Response some context.\n        response.request = req\n        response.connection = self\n\n        return response\n\n    def build_connection_pool_key_attributes(self, request, verify, cert=None):\n        \"\"\"Build the PoolKey attributes used by urllib3 to return a connection.\n\n        This looks at the PreparedRequest, the user-specified verify value,\n        and the value of the cert parameter to determine what PoolKey values\n        to use to select a connection from a given urllib3 Connection Pool.\n\n        The SSL related pool key arguments are not consistently set. As of\n        this writing, use the following to determine what keys may be in that\n        dictionary:\n\n        * If ``verify`` is ``True``, ``\"ssl_context\"`` will be set and will be the\n          default Requests SSL Context\n        * If ``verify`` is ``False``, ``\"ssl_context\"`` will not be set but\n          ``\"cert_reqs\"`` will be set\n        * If ``verify`` is a string, (i.e., it is a user-specified trust bundle)\n          ``\"ca_certs\"`` will be set if the string is not a directory recognized\n          by :py:func:`os.path.isdir`, otherwise ``\"ca_certs_dir\"`` will be\n          set.\n        * If ``\"cert\"`` is specified, ``\"cert_file\"`` will always be set. If\n          ``\"cert\"`` is a tuple with a second item, ``\"key_file\"`` will also\n          be present\n\n        To override these settings, one may subclass this class, call this\n        method and use the above logic to change parameters as desired. For\n        example, if one wishes to use a custom :py:class:`ssl.SSLContext` one\n        must both set ``\"ssl_context\"`` and based on what else they require,\n        alter the other keys to ensure the desired behaviour.\n\n        :param request:\n            The PreparedReqest being sent over the connection.\n        :type request:\n            :class:`~requests.models.PreparedRequest`\n        :param verify:\n            Either a boolean, in which case it controls whether\n            we verify the server's TLS certificate, or a string, in which case it\n            must be a path to a CA bundle to use.\n        :param cert:\n            (optional) Any user-provided SSL certificate for client\n            authentication (a.k.a., mTLS). This may be a string (i.e., just\n            the path to a file which holds both certificate and key) or a\n            tuple of length 2 with the certificate file path and key file\n            path.\n        :returns:\n            A tuple of two dictionaries. The first is the \"host parameters\"\n            portion of the Pool Key including scheme, hostname, and port. The\n            second is a dictionary of SSLContext related parameters.\n        \"\"\"\n        return _urllib3_request_context(request, verify, cert, self.poolmanager)\n\n    def get_connection_with_tls_context(self, request, verify, proxies=None, cert=None):\n        \"\"\"Returns a urllib3 connection for the given request and TLS settings.\n        This should not be called from user code, and is only exposed for use\n        when subclassing the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param request:\n            The :class:`PreparedRequest <PreparedRequest>` object to be sent\n            over the connection.\n        :param verify:\n            Either a boolean, in which case it controls whether we verify the\n            server's TLS certificate, or a string, in which case it must be a\n            path to a CA bundle to use.\n        :param proxies:\n            (optional) The proxies dictionary to apply to the request.\n        :param cert:\n            (optional) Any user-provided SSL certificate to be used for client\n            authentication (a.k.a., mTLS).\n        :rtype:\n            urllib3.ConnectionPool\n        \"\"\"\n        proxy = select_proxy(request.url, proxies)\n        try:\n            host_params, pool_kwargs = self.build_connection_pool_key_attributes(\n                request,\n                verify,\n                cert,\n            )\n        except ValueError as e:\n            raise InvalidURL(e, request=request)\n        if proxy:\n            proxy = prepend_scheme_if_needed(proxy, \"http\")\n            proxy_url = parse_url(proxy)\n            if not proxy_url.host:\n                raise InvalidProxyURL(\n                    \"Please check proxy URL. It is malformed \"\n                    \"and could be missing the host.\"\n                )\n            proxy_manager = self.proxy_manager_for(proxy)\n            conn = proxy_manager.connection_from_host(\n                **host_params, pool_kwargs=pool_kwargs\n            )\n        else:\n            # Only scheme should be lower case\n            conn = self.poolmanager.connection_from_host(\n                **host_params, pool_kwargs=pool_kwargs\n            )\n\n        return conn\n\n    def get_connection(self, url, proxies=None):\n        \"\"\"DEPRECATED: Users should move to `get_connection_with_tls_context`\n        for all subclasses of HTTPAdapter using Requests>=2.32.2.\n\n        Returns a urllib3 connection for the given URL. This should not be\n        called from user code, and is only exposed for use when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param url: The URL to connect to.\n        :param proxies: (optional) A Requests-style dictionary of proxies used on this request.\n        :rtype: urllib3.ConnectionPool\n        \"\"\"\n        warnings.warn(\n            (\n                \"`get_connection` has been deprecated in favor of \"\n                \"`get_connection_with_tls_context`. Custom HTTPAdapter subclasses \"\n                \"will need to migrate for Requests>=2.32.2. Please see \"\n                \"https://github.com/psf/requests/pull/6710 for more details.\"\n            ),\n            DeprecationWarning,\n        )\n        proxy = select_proxy(url, proxies)\n\n        if proxy:\n            proxy = prepend_scheme_if_needed(proxy, \"http\")\n            proxy_url = parse_url(proxy)\n            if not proxy_url.host:\n                raise InvalidProxyURL(\n                    \"Please check proxy URL. It is malformed \"\n                    \"and could be missing the host.\"\n                )\n            proxy_manager = self.proxy_manager_for(proxy)\n            conn = proxy_manager.connection_from_url(url)\n        else:\n            # Only scheme should be lower case\n            parsed = urlparse(url)\n            url = parsed.geturl()\n            conn = self.poolmanager.connection_from_url(url)\n\n        return conn\n\n    def close(self):\n        \"\"\"Disposes of any internal state.\n\n        Currently, this closes the PoolManager and any active ProxyManager,\n        which closes any pooled connections.\n        \"\"\"\n        self.poolmanager.clear()\n        for proxy in self.proxy_manager.values():\n            proxy.clear()\n\n    def request_url(self, request, proxies):\n        \"\"\"Obtain the url to use when making the final request.\n\n        If the message is being sent through a HTTP proxy, the full URL has to\n        be used. Otherwise, we should only use the path portion of the URL.\n\n        This should not be called from user code, and is only exposed for use\n        when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs.\n        :rtype: str\n        \"\"\"\n        proxy = select_proxy(request.url, proxies)\n        scheme = urlparse(request.url).scheme\n\n        is_proxied_http_request = proxy and scheme != \"https\"\n        using_socks_proxy = False\n        if proxy:\n            proxy_scheme = urlparse(proxy).scheme.lower()\n            using_socks_proxy = proxy_scheme.startswith(\"socks\")\n\n        url = request.path_url\n        if url.startswith(\"//\"):  # Don't confuse urllib3\n            url = f\"/{url.lstrip('/')}\"\n\n        if is_proxied_http_request and not using_socks_proxy:\n            url = urldefragauth(request.url)\n\n        return url\n\n    def add_headers(self, request, **kwargs):\n        \"\"\"Add any headers needed by the connection. As of v2.0 this does\n        nothing by default, but is left for overriding by users that subclass\n        the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        This should not be called from user code, and is only exposed for use\n        when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` to add headers to.\n        :param kwargs: The keyword arguments from the call to send().\n        \"\"\"\n        pass\n\n    def proxy_headers(self, proxy):\n        \"\"\"Returns a dictionary of the headers to add to any request sent\n        through a proxy. This works with urllib3 magic to ensure that they are\n        correctly sent to the proxy, rather than in a tunnelled request if\n        CONNECT is being used.\n\n        This should not be called from user code, and is only exposed for use\n        when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param proxy: The url of the proxy being used for this request.\n        :rtype: dict\n        \"\"\"\n        headers = {}\n        username, password = get_auth_from_url(proxy)\n\n        if username:\n            headers[\"Proxy-Authorization\"] = _basic_auth_str(username, password)\n\n        return headers\n\n    def send(\n        self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None\n    ):\n        \"\"\"Sends PreparedRequest object. Returns Response object.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param stream: (optional) Whether to stream the request content.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a :ref:`(connect timeout,\n            read timeout) <timeouts>` tuple.\n        :type timeout: float or tuple or urllib3 Timeout object\n        :param verify: (optional) Either a boolean, in which case it controls whether\n            we verify the server's TLS certificate, or a string, in which case it\n            must be a path to a CA bundle to use\n        :param cert: (optional) Any user-provided SSL certificate to be trusted.\n        :param proxies: (optional) The proxies dictionary to apply to the request.\n        :rtype: requests.Response\n        \"\"\"\n\n        try:\n            conn = self.get_connection_with_tls_context(\n                request, verify, proxies=proxies, cert=cert\n            )\n        except LocationValueError as e:\n            raise InvalidURL(e, request=request)\n\n        self.cert_verify(conn, request.url, verify, cert)\n        url = self.request_url(request, proxies)\n        self.add_headers(\n            request,\n            stream=stream,\n            timeout=timeout,\n            verify=verify,\n            cert=cert,\n            proxies=proxies,\n        )\n\n        chunked = not (request.body is None or \"Content-Length\" in request.headers)\n\n        if isinstance(timeout, tuple):\n            try:\n                connect, read = timeout\n                timeout = TimeoutSauce(connect=connect, read=read)\n            except ValueError:\n                raise ValueError(\n                    f\"Invalid timeout {timeout}. Pass a (connect, read) timeout tuple, \"\n                    f\"or a single float to set both timeouts to the same value.\"\n                )\n        elif isinstance(timeout, TimeoutSauce):\n            pass\n        else:\n            timeout = TimeoutSauce(connect=timeout, read=timeout)\n\n        try:\n            resp = conn.urlopen(\n                method=request.method,\n                url=url,\n                body=request.body,\n                headers=request.headers,\n                redirect=False,\n                assert_same_host=False,\n                preload_content=False,\n                decode_content=False,\n                retries=self.max_retries,\n                timeout=timeout,\n                chunked=chunked,\n            )\n\n        except (ProtocolError, OSError) as err:\n            raise ConnectionError(err, request=request)\n\n        except MaxRetryError as e:\n            if isinstance(e.reason, ConnectTimeoutError):\n                # TODO: Remove this in 3.0.0: see #2811\n                if not isinstance(e.reason, NewConnectionError):\n                    raise ConnectTimeout(e, request=request)\n\n            if isinstance(e.reason, ResponseError):\n                raise RetryError(e, request=request)\n\n            if isinstance(e.reason, _ProxyError):\n                raise ProxyError(e, request=request)\n\n            if isinstance(e.reason, _SSLError):\n                # This branch is for urllib3 v1.22 and later.\n                raise SSLError(e, request=request)\n\n            raise ConnectionError(e, request=request)\n\n        except ClosedPoolError as e:\n            raise ConnectionError(e, request=request)\n\n        except _ProxyError as e:\n            raise ProxyError(e)\n\n        except (_SSLError, _HTTPError) as e:\n            if isinstance(e, _SSLError):\n                # This branch is for urllib3 versions earlier than v1.22\n                raise SSLError(e, request=request)\n            elif isinstance(e, ReadTimeoutError):\n                raise ReadTimeout(e, request=request)\n            elif isinstance(e, _InvalidHeader):\n                raise InvalidHeader(e, request=request)\n            else:\n                raise\n\n        return self.build_response(request, resp)\n\n### Source File Dependency Files Content\n### Dependency File: auth.py\n\"\"\"\nrequests.auth\n~~~~~~~~~~~~~\n\nThis module contains the authentication handlers for Requests.\n\"\"\"\n\nimport hashlib\nimport os\nimport re\nimport threading\nimport time\nimport warnings\nfrom base64 import b64encode\n\nfrom ._internal_utils import to_native_string\nfrom .compat import basestring, str, urlparse\nfrom .cookies import extract_cookies_to_jar\nfrom .utils import parse_dict_header\n\nCONTENT_TYPE_FORM_URLENCODED = \"application/x-www-form-urlencoded\"\nCONTENT_TYPE_MULTI_PART = \"multipart/form-data\"\n\n\ndef _basic_auth_str(username, password):\n    \"\"\"Returns a Basic Auth string.\"\"\"\n\n    # \"I want us to put a big-ol' comment on top of it that\n    # says that this behaviour is dumb but we need to preserve\n    # it because people are relying on it.\"\n    #    - Lukasa\n    #\n    # These are here solely to maintain backwards compatibility\n    # for things like ints. This will be removed in 3.0.0.\n    if not isinstance(username, basestring):\n        warnings.warn(\n            \"Non-string usernames will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(username),\n            category=DeprecationWarning,\n        )\n        username = str(username)\n\n    if not isinstance(password, basestring):\n        warnings.warn(\n            \"Non-string passwords will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(type(password)),\n            category=DeprecationWarning,\n        )\n        password = str(password)\n    # -- End Removal --\n\n    if isinstance(username, str):\n        username = username.encode(\"latin1\")\n\n    if isinstance(password, str):\n        password = password.encode(\"latin1\")\n\n    authstr = \"Basic \" + to_native_string(\n        b64encode(b\":\".join((username, password))).strip()\n    )\n\n    return authstr\n\n\nclass AuthBase:\n    \"\"\"Base class that all auth implementations derive from\"\"\"\n\n    def __call__(self, r):\n        raise NotImplementedError(\"Auth hooks must be callable.\")\n\n\nclass HTTPBasicAuth(AuthBase):\n    \"\"\"Attaches HTTP Basic Authentication to the given Request object.\"\"\"\n\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\n    def __eq__(self, other):\n        return all(\n            [\n                self.username == getattr(other, \"username\", None),\n                self.password == getattr(other, \"password\", None),\n            ]\n        )\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __call__(self, r):\n        r.headers[\"Authorization\"] = _basic_auth_str(self.username, self.password)\n        return r\n\n\nclass HTTPProxyAuth(HTTPBasicAuth):\n    \"\"\"Attaches HTTP Proxy Authentication to a given Request object.\"\"\"\n\n    def __call__(self, r):\n        r.headers[\"Proxy-Authorization\"] = _basic_auth_str(self.username, self.password)\n        return r\n\n\nclass HTTPDigestAuth(AuthBase):\n    \"\"\"Attaches HTTP Digest Authentication to the given Request object.\"\"\"\n\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n        # Keep state in per-thread local storage\n        self._thread_local = threading.local()\n\n    def init_per_thread_state(self):\n        # Ensure state is initialized just once per-thread\n        if not hasattr(self._thread_local, \"init\"):\n            self._thread_local.init = True\n            self._thread_local.last_nonce = \"\"\n            self._thread_local.nonce_count = 0\n            self._thread_local.chal = {}\n            self._thread_local.pos = None\n            self._thread_local.num_401_calls = None\n\n    def build_digest_header(self, method, url):\n        \"\"\"\n        :rtype: str\n        \"\"\"\n\n        realm = self._thread_local.chal[\"realm\"]\n        nonce = self._thread_local.chal[\"nonce\"]\n        qop = self._thread_local.chal.get(\"qop\")\n        algorithm = self._thread_local.chal.get(\"algorithm\")\n        opaque = self._thread_local.chal.get(\"opaque\")\n        hash_utf8 = None\n\n        if algorithm is None:\n            _algorithm = \"MD5\"\n        else:\n            _algorithm = algorithm.upper()\n        # lambdas assume digest modules are imported at the top level\n        if _algorithm == \"MD5\" or _algorithm == \"MD5-SESS\":\n\n            def md5_utf8(x):\n                if isinstance(x, str):\n                    x = x.encode(\"utf-8\")\n                return hashlib.md5(x).hexdigest()\n\n            hash_utf8 = md5_utf8\n        elif _algorithm == \"SHA\":\n\n            def sha_utf8(x):\n                if isinstance(x, str):\n                    x = x.encode(\"utf-8\")\n                return hashlib.sha1(x).hexdigest()\n\n            hash_utf8 = sha_utf8\n        elif _algorithm == \"SHA-256\":\n\n            def sha256_utf8(x):\n                if isinstance(x, str):\n                    x = x.encode(\"utf-8\")\n                return hashlib.sha256(x).hexdigest()\n\n            hash_utf8 = sha256_utf8\n        elif _algorithm == \"SHA-512\":\n\n            def sha512_utf8(x):\n                if isinstance(x, str):\n                    x = x.encode(\"utf-8\")\n                return hashlib.sha512(x).hexdigest()\n\n            hash_utf8 = sha512_utf8\n\n        KD = lambda s, d: hash_utf8(f\"{s}:{d}\")  # noqa:E731\n\n        if hash_utf8 is None:\n            return None\n\n        # XXX not implemented yet\n        entdig = None\n        p_parsed = urlparse(url)\n        #: path is request-uri defined in RFC 2616 which should not be empty\n        path = p_parsed.path or \"/\"\n        if p_parsed.query:\n            path += f\"?{p_parsed.query}\"\n\n        A1 = f\"{self.username}:{realm}:{self.password}\"\n        A2 = f\"{method}:{path}\"\n\n        HA1 = hash_utf8(A1)\n        HA2 = hash_utf8(A2)\n\n        if nonce == self._thread_local.last_nonce:\n            self._thread_local.nonce_count += 1\n        else:\n            self._thread_local.nonce_count = 1\n        ncvalue = f\"{self._thread_local.nonce_count:08x}\"\n        s = str(self._thread_local.nonce_count).encode(\"utf-8\")\n        s += nonce.encode(\"utf-8\")\n        s += time.ctime().encode(\"utf-8\")\n        s += os.urandom(8)\n\n        cnonce = hashlib.sha1(s).hexdigest()[:16]\n        if _algorithm == \"MD5-SESS\":\n            HA1 = hash_utf8(f\"{HA1}:{nonce}:{cnonce}\")\n\n        if not qop:\n            respdig = KD(HA1, f\"{nonce}:{HA2}\")\n        elif qop == \"auth\" or \"auth\" in qop.split(\",\"):\n            noncebit = f\"{nonce}:{ncvalue}:{cnonce}:auth:{HA2}\"\n            respdig = KD(HA1, noncebit)\n        else:\n            # XXX handle auth-int.\n            return None\n\n        self._thread_local.last_nonce = nonce\n\n        # XXX should the partial digests be encoded too?\n        base = (\n            f'username=\"{self.username}\", realm=\"{realm}\", nonce=\"{nonce}\", '\n            f'uri=\"{path}\", response=\"{respdig}\"'\n        )\n        if opaque:\n            base += f', opaque=\"{opaque}\"'\n        if algorithm:\n            base += f', algorithm=\"{algorithm}\"'\n        if entdig:\n            base += f', digest=\"{entdig}\"'\n        if qop:\n            base += f', qop=\"auth\", nc={ncvalue}, cnonce=\"{cnonce}\"'\n\n        return f\"Digest {base}\"\n\n    def handle_redirect(self, r, **kwargs):\n        \"\"\"Reset num_401_calls counter on redirects.\"\"\"\n        if r.is_redirect:\n            self._thread_local.num_401_calls = 1\n\n    def handle_401(self, r, **kwargs):\n        \"\"\"\n        Takes the given response and tries digest-auth, if needed.\n\n        :rtype: requests.Response\n        \"\"\"\n\n        # If response is not 4xx, do not auth\n        # See https://github.com/psf/requests/issues/3772\n        if not 400 <= r.status_code < 500:\n            self._thread_local.num_401_calls = 1\n            return r\n\n        if self._thread_local.pos is not None:\n            # Rewind the file position indicator of the body to where\n            # it was to resend the request.\n            r.request.body.seek(self._thread_local.pos)\n        s_auth = r.headers.get(\"www-authenticate\", \"\")\n\n        if \"digest\" in s_auth.lower() and self._thread_local.num_401_calls < 2:\n            self._thread_local.num_401_calls += 1\n            pat = re.compile(r\"digest \", flags=re.IGNORECASE)\n            self._thread_local.chal = parse_dict_header(pat.sub(\"\", s_auth, count=1))\n\n            # Consume content and release the original connection\n            # to allow our new request to reuse the same one.\n            r.content\n            r.close()\n            prep = r.request.copy()\n            extract_cookies_to_jar(prep._cookies, r.request, r.raw)\n            prep.prepare_cookies(prep._cookies)\n\n            prep.headers[\"Authorization\"] = self.build_digest_header(\n                prep.method, prep.url\n            )\n            _r = r.connection.send(prep, **kwargs)\n            _r.history.append(r)\n            _r.request = prep\n\n            return _r\n\n        self._thread_local.num_401_calls = 1\n        return r\n\n    def __call__(self, r):\n        # Initialize per-thread state, if needed\n        self.init_per_thread_state()\n        # If we have a saved nonce, skip the 401\n        if self._thread_local.last_nonce:\n            r.headers[\"Authorization\"] = self.build_digest_header(r.method, r.url)\n        try:\n            self._thread_local.pos = r.body.tell()\n        except AttributeError:\n            # In the case of HTTPDigestAuth being reused and the body of\n            # the previous request was a file-like object, pos has the\n            # file position of the previous body. Ensure it's set to\n            # None.\n            self._thread_local.pos = None\n        r.register_hook(\"response\", self.handle_401)\n        r.register_hook(\"response\", self.handle_redirect)\n        self._thread_local.num_401_calls = 1\n\n        return r\n\n    def __eq__(self, other):\n        return all(\n            [\n                self.username == getattr(other, \"username\", None),\n                self.password == getattr(other, \"password\", None),\n            ]\n        )\n\n    def __ne__(self, other):\n        return not self == other\n\n\n### Dependency File: compat.py\n\"\"\"\nrequests.compat\n~~~~~~~~~~~~~~~\n\nThis module previously handled import compatibility issues\nbetween Python 2 and Python 3. It remains for backwards\ncompatibility until the next major version.\n\"\"\"\n\nimport importlib\nimport sys\n\n# -------\n# urllib3\n# -------\nfrom urllib3 import __version__ as urllib3_version\n\n# Detect which major version of urllib3 is being used.\ntry:\n    is_urllib3_1 = int(urllib3_version.split(\".\")[0]) == 1\nexcept (TypeError, AttributeError):\n    # If we can't discern a version, prefer old functionality.\n    is_urllib3_1 = True\n\n# -------------------\n# Character Detection\n# -------------------\n\n\ndef _resolve_char_detection():\n    \"\"\"Find supported character detection libraries.\"\"\"\n    chardet = None\n    for lib in (\"chardet\", \"charset_normalizer\"):\n        if chardet is None:\n            try:\n                chardet = importlib.import_module(lib)\n            except ImportError:\n                pass\n    return chardet\n\n\nchardet = _resolve_char_detection()\n\n# -------\n# Pythons\n# -------\n\n# Syntax sugar.\n_ver = sys.version_info\n\n#: Python 2.x?\nis_py2 = _ver[0] == 2\n\n#: Python 3.x?\nis_py3 = _ver[0] == 3\n\n# json/simplejson module import resolution\nhas_simplejson = False\ntry:\n    import simplejson as json\n\n    has_simplejson = True\nexcept ImportError:\n    import json\n\nif has_simplejson:\n    from simplejson import JSONDecodeError\nelse:\n    from json import JSONDecodeError\n\n# Keep OrderedDict for backwards compatibility.\nfrom collections import OrderedDict\nfrom collections.abc import Callable, Mapping, MutableMapping\nfrom http import cookiejar as cookielib\nfrom http.cookies import Morsel\nfrom io import StringIO\n\n# --------------\n# Legacy Imports\n# --------------\nfrom urllib.parse import (\n    quote,\n    quote_plus,\n    unquote,\n    unquote_plus,\n    urldefrag,\n    urlencode,\n    urljoin,\n    urlparse,\n    urlsplit,\n    urlunparse,\n)\nfrom urllib.request import (\n    getproxies,\n    getproxies_environment,\n    parse_http_list,\n    proxy_bypass,\n    proxy_bypass_environment,\n)\n\nbuiltin_str = str\nstr = str\nbytes = bytes\nbasestring = (str, bytes)\nnumeric_types = (int, float)\ninteger_types = (int,)\n\n\n### Dependency File: cookies.py\n\"\"\"\nrequests.cookies\n~~~~~~~~~~~~~~~~\n\nCompatibility code to be able to use `http.cookiejar.CookieJar` with requests.\n\nrequests.utils imports from here, so be careful with imports.\n\"\"\"\n\nimport calendar\nimport copy\nimport time\n\nfrom ._internal_utils import to_native_string\nfrom .compat import Morsel, MutableMapping, cookielib, urlparse, urlunparse\n\ntry:\n    import threading\nexcept ImportError:\n    import dummy_threading as threading\n\n\nclass MockRequest:\n    \"\"\"Wraps a `requests.Request` to mimic a `urllib2.Request`.\n\n    The code in `http.cookiejar.CookieJar` expects this interface in order to correctly\n    manage cookie policies, i.e., determine whether a cookie can be set, given the\n    domains of the request and the cookie.\n\n    The original request object is read-only. The client is responsible for collecting\n    the new headers via `get_new_headers()` and interpreting them appropriately. You\n    probably want `get_cookie_header`, defined below.\n    \"\"\"\n\n    def __init__(self, request):\n        self._r = request\n        self._new_headers = {}\n        self.type = urlparse(self._r.url).scheme\n\n    def get_type(self):\n        return self.type\n\n    def get_host(self):\n        return urlparse(self._r.url).netloc\n\n    def get_origin_req_host(self):\n        return self.get_host()\n\n    def get_full_url(self):\n        # Only return the response's URL if the user hadn't set the Host\n        # header\n        if not self._r.headers.get(\"Host\"):\n            return self._r.url\n        # If they did set it, retrieve it and reconstruct the expected domain\n        host = to_native_string(self._r.headers[\"Host\"], encoding=\"utf-8\")\n        parsed = urlparse(self._r.url)\n        # Reconstruct the URL as we expect it\n        return urlunparse(\n            [\n                parsed.scheme,\n                host,\n                parsed.path,\n                parsed.params,\n                parsed.query,\n                parsed.fragment,\n            ]\n        )\n\n    def is_unverifiable(self):\n        return True\n\n    def has_header(self, name):\n        return name in self._r.headers or name in self._new_headers\n\n    def get_header(self, name, default=None):\n        return self._r.headers.get(name, self._new_headers.get(name, default))\n\n    def add_header(self, key, val):\n        \"\"\"cookiejar has no legitimate use for this method; add it back if you find one.\"\"\"\n        raise NotImplementedError(\n            \"Cookie headers should be added with add_unredirected_header()\"\n        )\n\n    def add_unredirected_header(self, name, value):\n        self._new_headers[name] = value\n\n    def get_new_headers(self):\n        return self._new_headers\n\n    @property\n    def unverifiable(self):\n        return self.is_unverifiable()\n\n    @property\n    def origin_req_host(self):\n        return self.get_origin_req_host()\n\n    @property\n    def host(self):\n        return self.get_host()\n\n\nclass MockResponse:\n    \"\"\"Wraps a `httplib.HTTPMessage` to mimic a `urllib.addinfourl`.\n\n    ...what? Basically, expose the parsed HTTP headers from the server response\n    the way `http.cookiejar` expects to see them.\n    \"\"\"\n\n    def __init__(self, headers):\n        \"\"\"Make a MockResponse for `cookiejar` to read.\n\n        :param headers: a httplib.HTTPMessage or analogous carrying the headers\n        \"\"\"\n        self._headers = headers\n\n    def info(self):\n        return self._headers\n\n    def getheaders(self, name):\n        self._headers.getheaders(name)\n\n\ndef extract_cookies_to_jar(jar, request, response):\n    \"\"\"Extract the cookies from the response into a CookieJar.\n\n    :param jar: http.cookiejar.CookieJar (not necessarily a RequestsCookieJar)\n    :param request: our own requests.Request object\n    :param response: urllib3.HTTPResponse object\n    \"\"\"\n    if not (hasattr(response, \"_original_response\") and response._original_response):\n        return\n    # the _original_response field is the wrapped httplib.HTTPResponse object,\n    req = MockRequest(request)\n    # pull out the HTTPMessage with the headers and put it in the mock:\n    res = MockResponse(response._original_response.msg)\n    jar.extract_cookies(res, req)\n\n\ndef get_cookie_header(jar, request):\n    \"\"\"\n    Produce an appropriate Cookie header string to be sent with `request`, or None.\n\n    :rtype: str\n    \"\"\"\n    r = MockRequest(request)\n    jar.add_cookie_header(r)\n    return r.get_new_headers().get(\"Cookie\")\n\n\ndef remove_cookie_by_name(cookiejar, name, domain=None, path=None):\n    \"\"\"Unsets a cookie by name, by default over all domains and paths.\n\n    Wraps CookieJar.clear(), is O(n).\n    \"\"\"\n    clearables = []\n    for cookie in cookiejar:\n        if cookie.name != name:\n            continue\n        if domain is not None and domain != cookie.domain:\n            continue\n        if path is not None and path != cookie.path:\n            continue\n        clearables.append((cookie.domain, cookie.path, cookie.name))\n\n    for domain, path, name in clearables:\n        cookiejar.clear(domain, path, name)\n\n\nclass CookieConflictError(RuntimeError):\n    \"\"\"There are two cookies that meet the criteria specified in the cookie jar.\n    Use .get and .set and include domain and path args in order to be more specific.\n    \"\"\"\n\n\nclass RequestsCookieJar(cookielib.CookieJar, MutableMapping):\n    \"\"\"Compatibility class; is a http.cookiejar.CookieJar, but exposes a dict\n    interface.\n\n    This is the CookieJar we create by default for requests and sessions that\n    don't specify one, since some clients may expect response.cookies and\n    session.cookies to support dict operations.\n\n    Requests does not use the dict interface internally; it's just for\n    compatibility with external client code. All requests code should work\n    out of the box with externally provided instances of ``CookieJar``, e.g.\n    ``LWPCookieJar`` and ``FileCookieJar``.\n\n    Unlike a regular CookieJar, this class is pickleable.\n\n    .. warning:: dictionary operations that are normally O(1) may be O(n).\n    \"\"\"\n\n    def get(self, name, default=None, domain=None, path=None):\n        \"\"\"Dict-like get() that also supports optional domain and path args in\n        order to resolve naming collisions from using one cookie jar over\n        multiple domains.\n\n        .. warning:: operation is O(n), not O(1).\n        \"\"\"\n        try:\n            return self._find_no_duplicates(name, domain, path)\n        except KeyError:\n            return default\n\n    def set(self, name, value, **kwargs):\n        \"\"\"Dict-like set() that also supports optional domain and path args in\n        order to resolve naming collisions from using one cookie jar over\n        multiple domains.\n        \"\"\"\n        # support client code that unsets cookies by assignment of a None value:\n        if value is None:\n            remove_cookie_by_name(\n                self, name, domain=kwargs.get(\"domain\"), path=kwargs.get(\"path\")\n            )\n            return\n\n        if isinstance(value, Morsel):\n            c = morsel_to_cookie(value)\n        else:\n            c = create_cookie(name, value, **kwargs)\n        self.set_cookie(c)\n        return c\n\n    def iterkeys(self):\n        \"\"\"Dict-like iterkeys() that returns an iterator of names of cookies\n        from the jar.\n\n        .. seealso:: itervalues() and iteritems().\n        \"\"\"\n        for cookie in iter(self):\n            yield cookie.name\n\n    def keys(self):\n        \"\"\"Dict-like keys() that returns a list of names of cookies from the\n        jar.\n\n        .. seealso:: values() and items().\n        \"\"\"\n        return list(self.iterkeys())\n\n    def itervalues(self):\n        \"\"\"Dict-like itervalues() that returns an iterator of values of cookies\n        from the jar.\n\n        .. seealso:: iterkeys() and iteritems().\n        \"\"\"\n        for cookie in iter(self):\n            yield cookie.value\n\n    def values(self):\n        \"\"\"Dict-like values() that returns a list of values of cookies from the\n        jar.\n\n        .. seealso:: keys() and items().\n        \"\"\"\n        return list(self.itervalues())\n\n    def iteritems(self):\n        \"\"\"Dict-like iteritems() that returns an iterator of name-value tuples\n        from the jar.\n\n        .. seealso:: iterkeys() and itervalues().\n        \"\"\"\n        for cookie in iter(self):\n            yield cookie.name, cookie.value\n\n    def items(self):\n        \"\"\"Dict-like items() that returns a list of name-value tuples from the\n        jar. Allows client-code to call ``dict(RequestsCookieJar)`` and get a\n        vanilla python dict of key value pairs.\n\n        .. seealso:: keys() and values().\n        \"\"\"\n        return list(self.iteritems())\n\n    def list_domains(self):\n        \"\"\"Utility method to list all the domains in the jar.\"\"\"\n        domains = []\n        for cookie in iter(self):\n            if cookie.domain not in domains:\n                domains.append(cookie.domain)\n        return domains\n\n    def list_paths(self):\n        \"\"\"Utility method to list all the paths in the jar.\"\"\"\n        paths = []\n        for cookie in iter(self):\n            if cookie.path not in paths:\n                paths.append(cookie.path)\n        return paths\n\n    def multiple_domains(self):\n        \"\"\"Returns True if there are multiple domains in the jar.\n        Returns False otherwise.\n\n        :rtype: bool\n        \"\"\"\n        domains = []\n        for cookie in iter(self):\n            if cookie.domain is not None and cookie.domain in domains:\n                return True\n            domains.append(cookie.domain)\n        return False  # there is only one domain in jar\n\n    def get_dict(self, domain=None, path=None):\n        \"\"\"Takes as an argument an optional domain and path and returns a plain\n        old Python dict of name-value pairs of cookies that meet the\n        requirements.\n\n        :rtype: dict\n        \"\"\"\n        dictionary = {}\n        for cookie in iter(self):\n            if (domain is None or cookie.domain == domain) and (\n                path is None or cookie.path == path\n            ):\n                dictionary[cookie.name] = cookie.value\n        return dictionary\n\n    def __contains__(self, name):\n        try:\n            return super().__contains__(name)\n        except CookieConflictError:\n            return True\n\n    def __getitem__(self, name):\n        \"\"\"Dict-like __getitem__() for compatibility with client code. Throws\n        exception if there are more than one cookie with name. In that case,\n        use the more explicit get() method instead.\n\n        .. warning:: operation is O(n), not O(1).\n        \"\"\"\n        return self._find_no_duplicates(name)\n\n    def __setitem__(self, name, value):\n        \"\"\"Dict-like __setitem__ for compatibility with client code. Throws\n        exception if there is already a cookie of that name in the jar. In that\n        case, use the more explicit set() method instead.\n        \"\"\"\n        self.set(name, value)\n\n    def __delitem__(self, name):\n        \"\"\"Deletes a cookie given a name. Wraps ``http.cookiejar.CookieJar``'s\n        ``remove_cookie_by_name()``.\n        \"\"\"\n        remove_cookie_by_name(self, name)\n\n    def set_cookie(self, cookie, *args, **kwargs):\n        if (\n            hasattr(cookie.value, \"startswith\")\n            and cookie.value.startswith('\"')\n            and cookie.value.endswith('\"')\n        ):\n            cookie.value = cookie.value.replace('\\\\\"', \"\")\n        return super().set_cookie(cookie, *args, **kwargs)\n\n    def update(self, other):\n        \"\"\"Updates this jar with cookies from another CookieJar or dict-like\"\"\"\n        if isinstance(other, cookielib.CookieJar):\n            for cookie in other:\n                self.set_cookie(copy.copy(cookie))\n        else:\n            super().update(other)\n\n    def _find(self, name, domain=None, path=None):\n        \"\"\"Requests uses this method internally to get cookie values.\n\n        If there are conflicting cookies, _find arbitrarily chooses one.\n        See _find_no_duplicates if you want an exception thrown if there are\n        conflicting cookies.\n\n        :param name: a string containing name of cookie\n        :param domain: (optional) string containing domain of cookie\n        :param path: (optional) string containing path of cookie\n        :return: cookie.value\n        \"\"\"\n        for cookie in iter(self):\n            if cookie.name == name:\n                if domain is None or cookie.domain == domain:\n                    if path is None or cookie.path == path:\n                        return cookie.value\n\n        raise KeyError(f\"name={name!r}, domain={domain!r}, path={path!r}\")\n\n    def _find_no_duplicates(self, name, domain=None, path=None):\n        \"\"\"Both ``__get_item__`` and ``get`` call this function: it's never\n        used elsewhere in Requests.\n\n        :param name: a string containing name of cookie\n        :param domain: (optional) string containing domain of cookie\n        :param path: (optional) string containing path of cookie\n        :raises KeyError: if cookie is not found\n        :raises CookieConflictError: if there are multiple cookies\n            that match name and optionally domain and path\n        :return: cookie.value\n        \"\"\"\n        toReturn = None\n        for cookie in iter(self):\n            if cookie.name == name:\n                if domain is None or cookie.domain == domain:\n                    if path is None or cookie.path == path:\n                        if toReturn is not None:\n                            # if there are multiple cookies that meet passed in criteria\n                            raise CookieConflictError(\n                                f\"There are multiple cookies with name, {name!r}\"\n                            )\n                        # we will eventually return this as long as no cookie conflict\n                        toReturn = cookie.value\n\n        if toReturn:\n            return toReturn\n        raise KeyError(f\"name={name!r}, domain={domain!r}, path={path!r}\")\n\n    def __getstate__(self):\n        \"\"\"Unlike a normal CookieJar, this class is pickleable.\"\"\"\n        state = self.__dict__.copy()\n        # remove the unpickleable RLock object\n        state.pop(\"_cookies_lock\")\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Unlike a normal CookieJar, this class is pickleable.\"\"\"\n        self.__dict__.update(state)\n        if \"_cookies_lock\" not in self.__dict__:\n            self._cookies_lock = threading.RLock()\n\n    def copy(self):\n        \"\"\"Return a copy of this RequestsCookieJar.\"\"\"\n        new_cj = RequestsCookieJar()\n        new_cj.set_policy(self.get_policy())\n        new_cj.update(self)\n        return new_cj\n\n    def get_policy(self):\n        \"\"\"Return the CookiePolicy instance used.\"\"\"\n        return self._policy\n\n\ndef _copy_cookie_jar(jar):\n    if jar is None:\n        return None\n\n    if hasattr(jar, \"copy\"):\n        # We're dealing with an instance of RequestsCookieJar\n        return jar.copy()\n    # We're dealing with a generic CookieJar instance\n    new_jar = copy.copy(jar)\n    new_jar.clear()\n    for cookie in jar:\n        new_jar.set_cookie(copy.copy(cookie))\n    return new_jar\n\n\ndef create_cookie(name, value, **kwargs):\n    \"\"\"Make a cookie from underspecified parameters.\n\n    By default, the pair of `name` and `value` will be set for the domain ''\n    and sent on every request (this is sometimes called a \"supercookie\").\n    \"\"\"\n    result = {\n        \"version\": 0,\n        \"name\": name,\n        \"value\": value,\n        \"port\": None,\n        \"domain\": \"\",\n        \"path\": \"/\",\n        \"secure\": False,\n        \"expires\": None,\n        \"discard\": True,\n        \"comment\": None,\n        \"comment_url\": None,\n        \"rest\": {\"HttpOnly\": None},\n        \"rfc2109\": False,\n    }\n\n    badargs = set(kwargs) - set(result)\n    if badargs:\n        raise TypeError(\n            f\"create_cookie() got unexpected keyword arguments: {list(badargs)}\"\n        )\n\n    result.update(kwargs)\n    result[\"port_specified\"] = bool(result[\"port\"])\n    result[\"domain_specified\"] = bool(result[\"domain\"])\n    result[\"domain_initial_dot\"] = result[\"domain\"].startswith(\".\")\n    result[\"path_specified\"] = bool(result[\"path\"])\n\n    return cookielib.Cookie(**result)\n\n\ndef morsel_to_cookie(morsel):\n    \"\"\"Convert a Morsel object into a Cookie containing the one k/v pair.\"\"\"\n\n    expires = None\n    if morsel[\"max-age\"]:\n        try:\n            expires = int(time.time() + int(morsel[\"max-age\"]))\n        except ValueError:\n            raise TypeError(f\"max-age: {morsel['max-age']} must be integer\")\n    elif morsel[\"expires\"]:\n        time_template = \"%a, %d-%b-%Y %H:%M:%S GMT\"\n        expires = calendar.timegm(time.strptime(morsel[\"expires\"], time_template))\n    return create_cookie(\n        comment=morsel[\"comment\"],\n        comment_url=bool(morsel[\"comment\"]),\n        discard=False,\n        domain=morsel[\"domain\"],\n        expires=expires,\n        name=morsel.key,\n        path=morsel[\"path\"],\n        port=None,\n        rest={\"HttpOnly\": morsel[\"httponly\"]},\n        rfc2109=False,\n        secure=bool(morsel[\"secure\"]),\n        value=morsel.value,\n        version=morsel[\"version\"] or 0,\n    )\n\n\ndef cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):\n    \"\"\"Returns a CookieJar from a key/value dictionary.\n\n    :param cookie_dict: Dict of key/values to insert into CookieJar.\n    :param cookiejar: (optional) A cookiejar to add the cookies to.\n    :param overwrite: (optional) If False, will not replace cookies\n        already in the jar with new ones.\n    :rtype: CookieJar\n    \"\"\"\n    if cookiejar is None:\n        cookiejar = RequestsCookieJar()\n\n    if cookie_dict is not None:\n        names_from_jar = [cookie.name for cookie in cookiejar]\n        for name in cookie_dict:\n            if overwrite or (name not in names_from_jar):\n                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))\n\n    return cookiejar\n\n\ndef merge_cookies(cookiejar, cookies):\n    \"\"\"Add cookies to cookiejar and returns a merged CookieJar.\n\n    :param cookiejar: CookieJar object to add the cookies to.\n    :param cookies: Dictionary or CookieJar object to be added.\n    :rtype: CookieJar\n    \"\"\"\n    if not isinstance(cookiejar, cookielib.CookieJar):\n        raise ValueError(\"You can only merge into CookieJar\")\n\n    if isinstance(cookies, dict):\n        cookiejar = cookiejar_from_dict(cookies, cookiejar=cookiejar, overwrite=False)\n    elif isinstance(cookies, cookielib.CookieJar):\n        try:\n            cookiejar.update(cookies)\n        except AttributeError:\n            for cookie_in_jar in cookies:\n                cookiejar.set_cookie(cookie_in_jar)\n\n    return cookiejar\n\n\n### Dependency File: exceptions.py\n\"\"\"\nrequests.exceptions\n~~~~~~~~~~~~~~~~~~~\n\nThis module contains the set of Requests' exceptions.\n\"\"\"\nfrom urllib3.exceptions import HTTPError as BaseHTTPError\n\nfrom .compat import JSONDecodeError as CompatJSONDecodeError\n\n\nclass RequestException(IOError):\n    \"\"\"There was an ambiguous exception that occurred while handling your\n    request.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize RequestException with `request` and `response` objects.\"\"\"\n        response = kwargs.pop(\"response\", None)\n        self.response = response\n        self.request = kwargs.pop(\"request\", None)\n        if response is not None and not self.request and hasattr(response, \"request\"):\n            self.request = self.response.request\n        super().__init__(*args, **kwargs)\n\n\nclass InvalidJSONError(RequestException):\n    \"\"\"A JSON error occurred.\"\"\"\n\n\nclass JSONDecodeError(InvalidJSONError, CompatJSONDecodeError):\n    \"\"\"Couldn't decode the text into json\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Construct the JSONDecodeError instance first with all\n        args. Then use it's args to construct the IOError so that\n        the json specific args aren't used as IOError specific args\n        and the error message from JSONDecodeError is preserved.\n        \"\"\"\n        CompatJSONDecodeError.__init__(self, *args)\n        InvalidJSONError.__init__(self, *self.args, **kwargs)\n\n    def __reduce__(self):\n        \"\"\"\n        The __reduce__ method called when pickling the object must\n        be the one from the JSONDecodeError (be it json/simplejson)\n        as it expects all the arguments for instantiation, not just\n        one like the IOError, and the MRO would by default call the\n        __reduce__ method from the IOError due to the inheritance order.\n        \"\"\"\n        return CompatJSONDecodeError.__reduce__(self)\n\n\nclass HTTPError(RequestException):\n    \"\"\"An HTTP error occurred.\"\"\"\n\n\nclass ConnectionError(RequestException):\n    \"\"\"A Connection error occurred.\"\"\"\n\n\nclass ProxyError(ConnectionError):\n    \"\"\"A proxy error occurred.\"\"\"\n\n\nclass SSLError(ConnectionError):\n    \"\"\"An SSL error occurred.\"\"\"\n\n\nclass Timeout(RequestException):\n    \"\"\"The request timed out.\n\n    Catching this error will catch both\n    :exc:`~requests.exceptions.ConnectTimeout` and\n    :exc:`~requests.exceptions.ReadTimeout` errors.\n    \"\"\"\n\n\nclass ConnectTimeout(ConnectionError, Timeout):\n    \"\"\"The request timed out while trying to connect to the remote server.\n\n    Requests that produced this error are safe to retry.\n    \"\"\"\n\n\nclass ReadTimeout(Timeout):\n    \"\"\"The server did not send any data in the allotted amount of time.\"\"\"\n\n\nclass URLRequired(RequestException):\n    \"\"\"A valid URL is required to make a request.\"\"\"\n\n\nclass TooManyRedirects(RequestException):\n    \"\"\"Too many redirects.\"\"\"\n\n\nclass MissingSchema(RequestException, ValueError):\n    \"\"\"The URL scheme (e.g. http or https) is missing.\"\"\"\n\n\nclass InvalidSchema(RequestException, ValueError):\n    \"\"\"The URL scheme provided is either invalid or unsupported.\"\"\"\n\n\nclass InvalidURL(RequestException, ValueError):\n    \"\"\"The URL provided was somehow invalid.\"\"\"\n\n\nclass InvalidHeader(RequestException, ValueError):\n    \"\"\"The header value provided was somehow invalid.\"\"\"\n\n\nclass InvalidProxyURL(InvalidURL):\n    \"\"\"The proxy URL provided is invalid.\"\"\"\n\n\nclass ChunkedEncodingError(RequestException):\n    \"\"\"The server declared chunked encoding but sent an invalid chunk.\"\"\"\n\n\nclass ContentDecodingError(RequestException, BaseHTTPError):\n    \"\"\"Failed to decode response content.\"\"\"\n\n\nclass StreamConsumedError(RequestException, TypeError):\n    \"\"\"The content for this response was already consumed.\"\"\"\n\n\nclass RetryError(RequestException):\n    \"\"\"Custom retries logic failed\"\"\"\n\n\nclass UnrewindableBodyError(RequestException):\n    \"\"\"Requests encountered an error when trying to rewind a body.\"\"\"\n\n\n# Warnings\n\n\nclass RequestsWarning(Warning):\n    \"\"\"Base warning for Requests.\"\"\"\n\n\nclass FileModeWarning(RequestsWarning, DeprecationWarning):\n    \"\"\"A file was opened in text mode, but Requests determined its binary length.\"\"\"\n\n\nclass RequestsDependencyWarning(RequestsWarning):\n    \"\"\"An imported dependency doesn't match the expected version range.\"\"\"\n\n\n### Dependency File: models.py\n\"\"\"\nrequests.models\n~~~~~~~~~~~~~~~\n\nThis module contains the primary objects that power Requests.\n\"\"\"\n\nimport datetime\n\n# Import encoding now, to avoid implicit import later.\n# Implicit import within threads may cause LookupError when standard library is in a ZIP,\n# such as in Embedded Python. See https://github.com/psf/requests/issues/3578.\nimport encodings.idna  # noqa: F401\nfrom io import UnsupportedOperation\n\nfrom urllib3.exceptions import (\n    DecodeError,\n    LocationParseError,\n    ProtocolError,\n    ReadTimeoutError,\n    SSLError,\n)\nfrom urllib3.fields import RequestField\nfrom urllib3.filepost import encode_multipart_formdata\nfrom urllib3.util import parse_url\n\nfrom ._internal_utils import to_native_string, unicode_is_ascii\nfrom .auth import HTTPBasicAuth\nfrom .compat import (\n    Callable,\n    JSONDecodeError,\n    Mapping,\n    basestring,\n    builtin_str,\n    chardet,\n    cookielib,\n)\nfrom .compat import json as complexjson\nfrom .compat import urlencode, urlsplit, urlunparse\nfrom .cookies import _copy_cookie_jar, cookiejar_from_dict, get_cookie_header\nfrom .exceptions import (\n    ChunkedEncodingError,\n    ConnectionError,\n    ContentDecodingError,\n    HTTPError,\n    InvalidJSONError,\n    InvalidURL,\n)\nfrom .exceptions import JSONDecodeError as RequestsJSONDecodeError\nfrom .exceptions import MissingSchema\nfrom .exceptions import SSLError as RequestsSSLError\nfrom .exceptions import StreamConsumedError\nfrom .hooks import default_hooks\nfrom .status_codes import codes\nfrom .structures import CaseInsensitiveDict\nfrom .utils import (\n    check_header_validity,\n    get_auth_from_url,\n    guess_filename,\n    guess_json_utf,\n    iter_slices,\n    parse_header_links,\n    requote_uri,\n    stream_decode_response_unicode,\n    super_len,\n    to_key_val_list,\n)\n\n#: The set of HTTP status codes that indicate an automatically\n#: processable redirect.\nREDIRECT_STATI = (\n    codes.moved,  # 301\n    codes.found,  # 302\n    codes.other,  # 303\n    codes.temporary_redirect,  # 307\n    codes.permanent_redirect,  # 308\n)\n\nDEFAULT_REDIRECT_LIMIT = 30\nCONTENT_CHUNK_SIZE = 10 * 1024\nITER_CHUNK_SIZE = 512\n\n\nclass RequestEncodingMixin:\n    @property\n    def path_url(self):\n        \"\"\"Build the path URL to use.\"\"\"\n\n        url = []\n\n        p = urlsplit(self.url)\n\n        path = p.path\n        if not path:\n            path = \"/\"\n\n        url.append(path)\n\n        query = p.query\n        if query:\n            url.append(\"?\")\n            url.append(query)\n\n        return \"\".join(url)\n\n    @staticmethod\n    def _encode_params(data):\n        \"\"\"Encode parameters in a piece of data.\n\n        Will successfully encode parameters when passed as a dict or a list of\n        2-tuples. Order is retained if data is a list of 2-tuples but arbitrary\n        if parameters are supplied as a dict.\n        \"\"\"\n\n        if isinstance(data, (str, bytes)):\n            return data\n        elif hasattr(data, \"read\"):\n            return data\n        elif hasattr(data, \"__iter__\"):\n            result = []\n            for k, vs in to_key_val_list(data):\n                if isinstance(vs, basestring) or not hasattr(vs, \"__iter__\"):\n                    vs = [vs]\n                for v in vs:\n                    if v is not None:\n                        result.append(\n                            (\n                                k.encode(\"utf-8\") if isinstance(k, str) else k,\n                                v.encode(\"utf-8\") if isinstance(v, str) else v,\n                            )\n                        )\n            return urlencode(result, doseq=True)\n        else:\n            return data\n\n    @staticmethod\n    def _encode_files(files, data):\n        \"\"\"Build the body for a multipart/form-data request.\n\n        Will successfully encode files when passed as a dict or a list of\n        tuples. Order is retained if data is a list of tuples but arbitrary\n        if parameters are supplied as a dict.\n        The tuples may be 2-tuples (filename, fileobj), 3-tuples (filename, fileobj, contentype)\n        or 4-tuples (filename, fileobj, contentype, custom_headers).\n        \"\"\"\n        if not files:\n            raise ValueError(\"Files must be provided.\")\n        elif isinstance(data, basestring):\n            raise ValueError(\"Data must not be a string.\")\n\n        new_fields = []\n        fields = to_key_val_list(data or {})\n        files = to_key_val_list(files or {})\n\n        for field, val in fields:\n            if isinstance(val, basestring) or not hasattr(val, \"__iter__\"):\n                val = [val]\n            for v in val:\n                if v is not None:\n                    # Don't call str() on bytestrings: in Py3 it all goes wrong.\n                    if not isinstance(v, bytes):\n                        v = str(v)\n\n                    new_fields.append(\n                        (\n                            field.decode(\"utf-8\")\n                            if isinstance(field, bytes)\n                            else field,\n                            v.encode(\"utf-8\") if isinstance(v, str) else v,\n                        )\n                    )\n\n        for k, v in files:\n            # support for explicit filename\n            ft = None\n            fh = None\n            if isinstance(v, (tuple, list)):\n                if len(v) == 2:\n                    fn, fp = v\n                elif len(v) == 3:\n                    fn, fp, ft = v\n                else:\n                    fn, fp, ft, fh = v\n            else:\n                fn = guess_filename(v) or k\n                fp = v\n\n            if isinstance(fp, (str, bytes, bytearray)):\n                fdata = fp\n            elif hasattr(fp, \"read\"):\n                fdata = fp.read()\n            elif fp is None:\n                continue\n            else:\n                fdata = fp\n\n            rf = RequestField(name=k, data=fdata, filename=fn, headers=fh)\n            rf.make_multipart(content_type=ft)\n            new_fields.append(rf)\n\n        body, content_type = encode_multipart_formdata(new_fields)\n\n        return body, content_type\n\n\nclass RequestHooksMixin:\n    def register_hook(self, event, hook):\n        \"\"\"Properly register a hook.\"\"\"\n\n        if event not in self.hooks:\n            raise ValueError(f'Unsupported event specified, with event name \"{event}\"')\n\n        if isinstance(hook, Callable):\n            self.hooks[event].append(hook)\n        elif hasattr(hook, \"__iter__\"):\n            self.hooks[event].extend(h for h in hook if isinstance(h, Callable))\n\n    def deregister_hook(self, event, hook):\n        \"\"\"Deregister a previously registered hook.\n        Returns True if the hook existed, False if not.\n        \"\"\"\n\n        try:\n            self.hooks[event].remove(hook)\n            return True\n        except ValueError:\n            return False\n\n\nclass Request(RequestHooksMixin):\n    \"\"\"A user-created :class:`Request <Request>` object.\n\n    Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.\n\n    :param method: HTTP method to use.\n    :param url: URL to send.\n    :param headers: dictionary of headers to send.\n    :param files: dictionary of {filename: fileobject} files to multipart upload.\n    :param data: the body to attach to the request. If a dictionary or\n        list of tuples ``[(key, value)]`` is provided, form-encoding will\n        take place.\n    :param json: json for the body to attach to the request (if files or data is not specified).\n    :param params: URL parameters to append to the URL. If a dictionary or\n        list of tuples ``[(key, value)]`` is provided, form-encoding will\n        take place.\n    :param auth: Auth handler or (user, pass) tuple.\n    :param cookies: dictionary or CookieJar of cookies to attach to this request.\n    :param hooks: dictionary of callback hooks, for internal usage.\n\n    Usage::\n\n      >>> import requests\n      >>> req = requests.Request('GET', 'https://httpbin.org/get')\n      >>> req.prepare()\n      <PreparedRequest [GET]>\n    \"\"\"\n\n    def __init__(\n        self,\n        method=None,\n        url=None,\n        headers=None,\n        files=None,\n        data=None,\n        params=None,\n        auth=None,\n        cookies=None,\n        hooks=None,\n        json=None,\n    ):\n        # Default empty dicts for dict params.\n        data = [] if data is None else data\n        files = [] if files is None else files\n        headers = {} if headers is None else headers\n        params = {} if params is None else params\n        hooks = {} if hooks is None else hooks\n\n        self.hooks = default_hooks()\n        for k, v in list(hooks.items()):\n            self.register_hook(event=k, hook=v)\n\n        self.method = method\n        self.url = url\n        self.headers = headers\n        self.files = files\n        self.data = data\n        self.json = json\n        self.params = params\n        self.auth = auth\n        self.cookies = cookies\n\n    def __repr__(self):\n        return f\"<Request [{self.method}]>\"\n\n    def prepare(self):\n        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it.\"\"\"\n        p = PreparedRequest()\n        p.prepare(\n            method=self.method,\n            url=self.url,\n            headers=self.headers,\n            files=self.files,\n            data=self.data,\n            json=self.json,\n            params=self.params,\n            auth=self.auth,\n            cookies=self.cookies,\n            hooks=self.hooks,\n        )\n        return p\n\n\nclass PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n    \"\"\"The fully mutable :class:`PreparedRequest <PreparedRequest>` object,\n    containing the exact bytes that will be sent to the server.\n\n    Instances are generated from a :class:`Request <Request>` object, and\n    should not be instantiated manually; doing so may produce undesirable\n    effects.\n\n    Usage::\n\n      >>> import requests\n      >>> req = requests.Request('GET', 'https://httpbin.org/get')\n      >>> r = req.prepare()\n      >>> r\n      <PreparedRequest [GET]>\n\n      >>> s = requests.Session()\n      >>> s.send(r)\n      <Response [200]>\n    \"\"\"\n\n    def __init__(self):\n        #: HTTP verb to send to the server.\n        self.method = None\n        #: HTTP URL to send the request to.\n        self.url = None\n        #: dictionary of HTTP headers.\n        self.headers = None\n        # The `CookieJar` used to create the Cookie header will be stored here\n        # after prepare_cookies is called\n        self._cookies = None\n        #: request body to send to the server.\n        self.body = None\n        #: dictionary of callback hooks, for internal usage.\n        self.hooks = default_hooks()\n        #: integer denoting starting position of a readable file-like body.\n        self._body_position = None\n\n    def prepare(\n        self,\n        method=None,\n        url=None,\n        headers=None,\n        files=None,\n        data=None,\n        params=None,\n        auth=None,\n        cookies=None,\n        hooks=None,\n        json=None,\n    ):\n        \"\"\"Prepares the entire request with the given parameters.\"\"\"\n\n        self.prepare_method(method)\n        self.prepare_url(url, params)\n        self.prepare_headers(headers)\n        self.prepare_cookies(cookies)\n        self.prepare_body(data, files, json)\n        self.prepare_auth(auth, url)\n\n        # Note that prepare_auth must be last to enable authentication schemes\n        # such as OAuth to work on a fully prepared request.\n\n        # This MUST go after prepare_auth. Authenticators could add a hook\n        self.prepare_hooks(hooks)\n\n    def __repr__(self):\n        return f\"<PreparedRequest [{self.method}]>\"\n\n    def copy(self):\n        p = PreparedRequest()\n        p.method = self.method\n        p.url = self.url\n        p.headers = self.headers.copy() if self.headers is not None else None\n        p._cookies = _copy_cookie_jar(self._cookies)\n        p.body = self.body\n        p.hooks = self.hooks\n        p._body_position = self._body_position\n        return p\n\n    def prepare_method(self, method):\n        \"\"\"Prepares the given HTTP method.\"\"\"\n        self.method = method\n        if self.method is not None:\n            self.method = to_native_string(self.method.upper())\n\n    @staticmethod\n    def _get_idna_encoded_host(host):\n        import idna\n\n        try:\n            host = idna.encode(host, uts46=True).decode(\"utf-8\")\n        except idna.IDNAError:\n            raise UnicodeError\n        return host\n\n    def prepare_url(self, url, params):\n        \"\"\"Prepares the given HTTP URL.\"\"\"\n        #: Accept objects that have string representations.\n        #: We're unable to blindly call unicode/str functions\n        #: as this will include the bytestring indicator (b'')\n        #: on python 3.x.\n        #: https://github.com/psf/requests/pull/2238\n        if isinstance(url, bytes):\n            url = url.decode(\"utf8\")\n        else:\n            url = str(url)\n\n        # Remove leading whitespaces from url\n        url = url.lstrip()\n\n        # Don't do any URL preparation for non-HTTP schemes like `mailto`,\n        # `data` etc to work around exceptions from `url_parse`, which\n        # handles RFC 3986 only.\n        if \":\" in url and not url.lower().startswith(\"http\"):\n            self.url = url\n            return\n\n        # Support for unicode domain names and paths.\n        try:\n            scheme, auth, host, port, path, query, fragment = parse_url(url)\n        except LocationParseError as e:\n            raise InvalidURL(*e.args)\n\n        if not scheme:\n            raise MissingSchema(\n                f\"Invalid URL {url!r}: No scheme supplied. \"\n                f\"Perhaps you meant https://{url}?\"\n            )\n\n        if not host:\n            raise InvalidURL(f\"Invalid URL {url!r}: No host supplied\")\n\n        # In general, we want to try IDNA encoding the hostname if the string contains\n        # non-ASCII characters. This allows users to automatically get the correct IDNA\n        # behaviour. For strings containing only ASCII characters, we need to also verify\n        # it doesn't start with a wildcard (*), before allowing the unencoded hostname.\n        if not unicode_is_ascii(host):\n            try:\n                host = self._get_idna_encoded_host(host)\n            except UnicodeError:\n                raise InvalidURL(\"URL has an invalid label.\")\n        elif host.startswith((\"*\", \".\")):\n            raise InvalidURL(\"URL has an invalid label.\")\n\n        # Carefully reconstruct the network location\n        netloc = auth or \"\"\n        if netloc:\n            netloc += \"@\"\n        netloc += host\n        if port:\n            netloc += f\":{port}\"\n\n        # Bare domains aren't valid URLs.\n        if not path:\n            path = \"/\"\n\n        if isinstance(params, (str, bytes)):\n            params = to_native_string(params)\n\n        enc_params = self._encode_params(params)\n        if enc_params:\n            if query:\n                query = f\"{query}&{enc_params}\"\n            else:\n                query = enc_params\n\n        url = requote_uri(urlunparse([scheme, netloc, path, None, query, fragment]))\n        self.url = url\n\n    def prepare_headers(self, headers):\n        \"\"\"Prepares the given HTTP headers.\"\"\"\n\n        self.headers = CaseInsensitiveDict()\n        if headers:\n            for header in headers.items():\n                # Raise exception on invalid header value.\n                check_header_validity(header)\n                name, value = header\n                self.headers[to_native_string(name)] = value\n\n    def prepare_body(self, data, files, json=None):\n        \"\"\"Prepares the given HTTP body data.\"\"\"\n\n        # Check if file, fo, generator, iterator.\n        # If not, run through normal process.\n\n        # Nottin' on you.\n        body = None\n        content_type = None\n\n        if not data and json is not None:\n            # urllib3 requires a bytes-like body. Python 2's json.dumps\n            # provides this natively, but Python 3 gives a Unicode string.\n            content_type = \"application/json\"\n\n            try:\n                body = complexjson.dumps(json, allow_nan=False)\n            except ValueError as ve:\n                raise InvalidJSONError(ve, request=self)\n\n            if not isinstance(body, bytes):\n                body = body.encode(\"utf-8\")\n\n        is_stream = all(\n            [\n                hasattr(data, \"__iter__\"),\n                not isinstance(data, (basestring, list, tuple, Mapping)),\n            ]\n        )\n\n        if is_stream:\n            try:\n                length = super_len(data)\n            except (TypeError, AttributeError, UnsupportedOperation):\n                length = None\n\n            body = data\n\n            if getattr(body, \"tell\", None) is not None:\n                # Record the current file position before reading.\n                # This will allow us to rewind a file in the event\n                # of a redirect.\n                try:\n                    self._body_position = body.tell()\n                except OSError:\n                    # This differentiates from None, allowing us to catch\n                    # a failed `tell()` later when trying to rewind the body\n                    self._body_position = object()\n\n            if files:\n                raise NotImplementedError(\n                    \"Streamed bodies and files are mutually exclusive.\"\n                )\n\n            if length:\n                self.headers[\"Content-Length\"] = builtin_str(length)\n            else:\n                self.headers[\"Transfer-Encoding\"] = \"chunked\"\n        else:\n            # Multi-part file uploads.\n            if files:\n                (body, content_type) = self._encode_files(files, data)\n            else:\n                if data:\n                    body = self._encode_params(data)\n                    if isinstance(data, basestring) or hasattr(data, \"read\"):\n                        content_type = None\n                    else:\n                        content_type = \"application/x-www-form-urlencoded\"\n\n            self.prepare_content_length(body)\n\n            # Add content-type if it wasn't explicitly provided.\n            if content_type and (\"content-type\" not in self.headers):\n                self.headers[\"Content-Type\"] = content_type\n\n        self.body = body\n\n    def prepare_content_length(self, body):\n        \"\"\"Prepare Content-Length header based on request method and body\"\"\"\n        if body is not None:\n            length = super_len(body)\n            if length:\n                # If length exists, set it. Otherwise, we fallback\n                # to Transfer-Encoding: chunked.\n                self.headers[\"Content-Length\"] = builtin_str(length)\n        elif (\n            self.method not in (\"GET\", \"HEAD\")\n            and self.headers.get(\"Content-Length\") is None\n        ):\n            # Set Content-Length to 0 for methods that can have a body\n            # but don't provide one. (i.e. not GET or HEAD)\n            self.headers[\"Content-Length\"] = \"0\"\n\n    def prepare_auth(self, auth, url=\"\"):\n        \"\"\"Prepares the given HTTP auth data.\"\"\"\n\n        # If no Auth is explicitly provided, extract it from the URL first.\n        if auth is None:\n            url_auth = get_auth_from_url(self.url)\n            auth = url_auth if any(url_auth) else None\n\n        if auth:\n            if isinstance(auth, tuple) and len(auth) == 2:\n                # special-case basic HTTP auth\n                auth = HTTPBasicAuth(*auth)\n\n            # Allow auth to make its changes.\n            r = auth(self)\n\n            # Update self to reflect the auth changes.\n            self.__dict__.update(r.__dict__)\n\n            # Recompute Content-Length\n            self.prepare_content_length(self.body)\n\n    def prepare_cookies(self, cookies):\n        \"\"\"Prepares the given HTTP cookie data.\n\n        This function eventually generates a ``Cookie`` header from the\n        given cookies using cookielib. Due to cookielib's design, the header\n        will not be regenerated if it already exists, meaning this function\n        can only be called once for the life of the\n        :class:`PreparedRequest <PreparedRequest>` object. Any subsequent calls\n        to ``prepare_cookies`` will have no actual effect, unless the \"Cookie\"\n        header is removed beforehand.\n        \"\"\"\n        if isinstance(cookies, cookielib.CookieJar):\n            self._cookies = cookies\n        else:\n            self._cookies = cookiejar_from_dict(cookies)\n\n        cookie_header = get_cookie_header(self._cookies, self)\n        if cookie_header is not None:\n            self.headers[\"Cookie\"] = cookie_header\n\n    def prepare_hooks(self, hooks):\n        \"\"\"Prepares the given hooks.\"\"\"\n        # hooks can be passed as None to the prepare method and to this\n        # method. To prevent iterating over None, simply use an empty list\n        # if hooks is False-y\n        hooks = hooks or []\n        for event in hooks:\n            self.register_hook(event, hooks[event])\n\n\nclass Response:\n    \"\"\"The :class:`Response <Response>` object, which contains a\n    server's response to an HTTP request.\n    \"\"\"\n\n    __attrs__ = [\n        \"_content\",\n        \"status_code\",\n        \"headers\",\n        \"url\",\n        \"history\",\n        \"encoding\",\n        \"reason\",\n        \"cookies\",\n        \"elapsed\",\n        \"request\",\n    ]\n\n    def __init__(self):\n        self._content = False\n        self._content_consumed = False\n        self._next = None\n\n        #: Integer Code of responded HTTP Status, e.g. 404 or 200.\n        self.status_code = None\n\n        #: Case-insensitive Dictionary of Response Headers.\n        #: For example, ``headers['content-encoding']`` will return the\n        #: value of a ``'Content-Encoding'`` response header.\n        self.headers = CaseInsensitiveDict()\n\n        #: File-like object representation of response (for advanced usage).\n        #: Use of ``raw`` requires that ``stream=True`` be set on the request.\n        #: This requirement does not apply for use internally to Requests.\n        self.raw = None\n\n        #: Final URL location of Response.\n        self.url = None\n\n        #: Encoding to decode with when accessing r.text.\n        self.encoding = None\n\n        #: A list of :class:`Response <Response>` objects from\n        #: the history of the Request. Any redirect responses will end\n        #: up here. The list is sorted from the oldest to the most recent request.\n        self.history = []\n\n        #: Textual reason of responded HTTP Status, e.g. \"Not Found\" or \"OK\".\n        self.reason = None\n\n        #: A CookieJar of Cookies the server sent back.\n        self.cookies = cookiejar_from_dict({})\n\n        #: The amount of time elapsed between sending the request\n        #: and the arrival of the response (as a timedelta).\n        #: This property specifically measures the time taken between sending\n        #: the first byte of the request and finishing parsing the headers. It\n        #: is therefore unaffected by consuming the response content or the\n        #: value of the ``stream`` keyword argument.\n        self.elapsed = datetime.timedelta(0)\n\n        #: The :class:`PreparedRequest <PreparedRequest>` object to which this\n        #: is a response.\n        self.request = None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n    def __getstate__(self):\n        # Consume everything; accessing the content attribute makes\n        # sure the content has been fully read.\n        if not self._content_consumed:\n            self.content\n\n        return {attr: getattr(self, attr, None) for attr in self.__attrs__}\n\n    def __setstate__(self, state):\n        for name, value in state.items():\n            setattr(self, name, value)\n\n        # pickled objects do not have .raw\n        setattr(self, \"_content_consumed\", True)\n        setattr(self, \"raw\", None)\n\n    def __repr__(self):\n        return f\"<Response [{self.status_code}]>\"\n\n    def __bool__(self):\n        \"\"\"Returns True if :attr:`status_code` is less than 400.\n\n        This attribute checks if the status code of the response is between\n        400 and 600 to see if there was a client error or a server error. If\n        the status code, is between 200 and 400, this will return True. This\n        is **not** a check to see if the response code is ``200 OK``.\n        \"\"\"\n        return self.ok\n\n    def __nonzero__(self):\n        \"\"\"Returns True if :attr:`status_code` is less than 400.\n\n        This attribute checks if the status code of the response is between\n        400 and 600 to see if there was a client error or a server error. If\n        the status code, is between 200 and 400, this will return True. This\n        is **not** a check to see if the response code is ``200 OK``.\n        \"\"\"\n        return self.ok\n\n    def __iter__(self):\n        \"\"\"Allows you to use a response as an iterator.\"\"\"\n        return self.iter_content(128)\n\n    @property\n    def ok(self):\n        \"\"\"Returns True if :attr:`status_code` is less than 400, False if not.\n\n        This attribute checks if the status code of the response is between\n        400 and 600 to see if there was a client error or a server error. If\n        the status code is between 200 and 400, this will return True. This\n        is **not** a check to see if the response code is ``200 OK``.\n        \"\"\"\n        try:\n            self.raise_for_status()\n        except HTTPError:\n            return False\n        return True\n\n    @property\n    def is_redirect(self):\n        \"\"\"True if this Response is a well-formed HTTP redirect that could have\n        been processed automatically (by :meth:`Session.resolve_redirects`).\n        \"\"\"\n        return \"location\" in self.headers and self.status_code in REDIRECT_STATI\n\n    @property\n    def is_permanent_redirect(self):\n        \"\"\"True if this Response one of the permanent versions of redirect.\"\"\"\n        return \"location\" in self.headers and self.status_code in (\n            codes.moved_permanently,\n            codes.permanent_redirect,\n        )\n\n    @property\n    def next(self):\n        \"\"\"Returns a PreparedRequest for the next request in a redirect chain, if there is one.\"\"\"\n        return self._next\n\n    @property\n    def apparent_encoding(self):\n        \"\"\"The apparent encoding, provided by the charset_normalizer or chardet libraries.\"\"\"\n        if chardet is not None:\n            return chardet.detect(self.content)[\"encoding\"]\n        else:\n            # If no character detection library is available, we'll fall back\n            # to a standard Python utf-8 str.\n            return \"utf-8\"\n\n    def iter_content(self, chunk_size=1, decode_unicode=False):\n        \"\"\"Iterates over the response data.  When stream=True is set on the\n        request, this avoids reading the content at once into memory for\n        large responses.  The chunk size is the number of bytes it should\n        read into memory.  This is not necessarily the length of each item\n        returned as decoding can take place.\n\n        chunk_size must be of type int or None. A value of None will\n        function differently depending on the value of `stream`.\n        stream=True will read data as it arrives in whatever size the\n        chunks are received. If stream=False, data is returned as\n        a single chunk.\n\n        If decode_unicode is True, content will be decoded using the best\n        available encoding based on the response.\n        \"\"\"\n\n        def generate():\n            # Special case for urllib3.\n            if hasattr(self.raw, \"stream\"):\n                try:\n                    yield from self.raw.stream(chunk_size, decode_content=True)\n                except ProtocolError as e:\n                    raise ChunkedEncodingError(e)\n                except DecodeError as e:\n                    raise ContentDecodingError(e)\n                except ReadTimeoutError as e:\n                    raise ConnectionError(e)\n                except SSLError as e:\n                    raise RequestsSSLError(e)\n            else:\n                # Standard file-like object.\n                while True:\n                    chunk = self.raw.read(chunk_size)\n                    if not chunk:\n                        break\n                    yield chunk\n\n            self._content_consumed = True\n\n        if self._content_consumed and isinstance(self._content, bool):\n            raise StreamConsumedError()\n        elif chunk_size is not None and not isinstance(chunk_size, int):\n            raise TypeError(\n                f\"chunk_size must be an int, it is instead a {type(chunk_size)}.\"\n            )\n        # simulate reading small chunks of the content\n        reused_chunks = iter_slices(self._content, chunk_size)\n\n        stream_chunks = generate()\n\n        chunks = reused_chunks if self._content_consumed else stream_chunks\n\n        if decode_unicode:\n            chunks = stream_decode_response_unicode(chunks, self)\n\n        return chunks\n\n    def iter_lines(\n        self, chunk_size=ITER_CHUNK_SIZE, decode_unicode=False, delimiter=None\n    ):\n        \"\"\"Iterates over the response data, one line at a time.  When\n        stream=True is set on the request, this avoids reading the\n        content at once into memory for large responses.\n\n        .. note:: This method is not reentrant safe.\n        \"\"\"\n\n        pending = None\n\n        for chunk in self.iter_content(\n            chunk_size=chunk_size, decode_unicode=decode_unicode\n        ):\n            if pending is not None:\n                chunk = pending + chunk\n\n            if delimiter:\n                lines = chunk.split(delimiter)\n            else:\n                lines = chunk.splitlines()\n\n            if lines and lines[-1] and chunk and lines[-1][-1] == chunk[-1]:\n                pending = lines.pop()\n            else:\n                pending = None\n\n            yield from lines\n\n        if pending is not None:\n            yield pending\n\n    @property\n    def content(self):\n        \"\"\"Content of the response, in bytes.\"\"\"\n\n        if self._content is False:\n            # Read the contents.\n            if self._content_consumed:\n                raise RuntimeError(\"The content for this response was already consumed\")\n\n            if self.status_code == 0 or self.raw is None:\n                self._content = None\n            else:\n                self._content = b\"\".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b\"\"\n\n        self._content_consumed = True\n        # don't need to release the connection; that's been handled by urllib3\n        # since we exhausted the data.\n        return self._content\n\n    @property\n    def text(self):\n        \"\"\"Content of the response, in unicode.\n\n        If Response.encoding is None, encoding will be guessed using\n        ``charset_normalizer`` or ``chardet``.\n\n        The encoding of the response content is determined based solely on HTTP\n        headers, following RFC 2616 to the letter. If you can take advantage of\n        non-HTTP knowledge to make a better guess at the encoding, you should\n        set ``r.encoding`` appropriately before accessing this property.\n        \"\"\"\n\n        # Try charset from content-type\n        content = None\n        encoding = self.encoding\n\n        if not self.content:\n            return \"\"\n\n        # Fallback to auto-detected encoding.\n        if self.encoding is None:\n            encoding = self.apparent_encoding\n\n        # Decode unicode from given encoding.\n        try:\n            content = str(self.content, encoding, errors=\"replace\")\n        except (LookupError, TypeError):\n            # A LookupError is raised if the encoding was not found which could\n            # indicate a misspelling or similar mistake.\n            #\n            # A TypeError can be raised if encoding is None\n            #\n            # So we try blindly encoding.\n            content = str(self.content, errors=\"replace\")\n\n        return content\n\n    def json(self, **kwargs):\n        r\"\"\"Returns the json-encoded content of a response, if any.\n\n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        :raises requests.exceptions.JSONDecodeError: If the response body does not\n            contain valid json.\n        \"\"\"\n\n        if not self.encoding and self.content and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using charset_normalizer to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return complexjson.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n                except JSONDecodeError as e:\n                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n\n        try:\n            return complexjson.loads(self.text, **kwargs)\n        except JSONDecodeError as e:\n            # Catch JSON-related errors and raise as requests.JSONDecodeError\n            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n            raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n\n    @property\n    def links(self):\n        \"\"\"Returns the parsed header links of the response, if any.\"\"\"\n\n        header = self.headers.get(\"link\")\n\n        resolved_links = {}\n\n        if header:\n            links = parse_header_links(header)\n\n            for link in links:\n                key = link.get(\"rel\") or link.get(\"url\")\n                resolved_links[key] = link\n\n        return resolved_links\n\n    def raise_for_status(self):\n        \"\"\"Raises :class:`HTTPError`, if one occurred.\"\"\"\n\n        http_error_msg = \"\"\n        if isinstance(self.reason, bytes):\n            # We attempt to decode utf-8 first because some servers\n            # choose to localize their reason strings. If the string\n            # isn't utf-8, we fall back to iso-8859-1 for all other\n            # encodings. (See PR #3538)\n            try:\n                reason = self.reason.decode(\"utf-8\")\n            except UnicodeDecodeError:\n                reason = self.reason.decode(\"iso-8859-1\")\n        else:\n            reason = self.reason\n\n        if 400 <= self.status_code < 500:\n            http_error_msg = (\n                f\"{self.status_code} Client Error: {reason} for url: {self.url}\"\n            )\n\n        elif 500 <= self.status_code < 600:\n            http_error_msg = (\n                f\"{self.status_code} Server Error: {reason} for url: {self.url}\"\n            )\n\n        if http_error_msg:\n            raise HTTPError(http_error_msg, response=self)\n\n    def close(self):\n        \"\"\"Releases the connection back to the pool. Once this method has been\n        called the underlying ``raw`` object must not be accessed again.\n\n        *Note: Should not normally need to be called explicitly.*\n        \"\"\"\n        if not self._content_consumed:\n            self.raw.close()\n\n        release_conn = getattr(self.raw, \"release_conn\", None)\n        if release_conn is not None:\n            release_conn()\n\n\n### Dependency File: structures.py\n\"\"\"\nrequests.structures\n~~~~~~~~~~~~~~~~~~~\n\nData structures that power Requests.\n\"\"\"\n\nfrom collections import OrderedDict\n\nfrom .compat import Mapping, MutableMapping\n\n\nclass CaseInsensitiveDict(MutableMapping):\n    \"\"\"A case-insensitive ``dict``-like object.\n\n    Implements all methods and operations of\n    ``MutableMapping`` as well as dict's ``copy``. Also\n    provides ``lower_items``.\n\n    All keys are expected to be strings. The structure remembers the\n    case of the last key to be set, and ``iter(instance)``,\n    ``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``\n    will contain case-sensitive keys. However, querying and contains\n    testing is case insensitive::\n\n        cid = CaseInsensitiveDict()\n        cid['Accept'] = 'application/json'\n        cid['aCCEPT'] == 'application/json'  # True\n        list(cid) == ['Accept']  # True\n\n    For example, ``headers['content-encoding']`` will return the\n    value of a ``'Content-Encoding'`` response header, regardless\n    of how the header name was originally stored.\n\n    If the constructor, ``.update``, or equality comparison\n    operations are given keys that have equal ``.lower()``s, the\n    behavior is undefined.\n    \"\"\"\n\n    def __init__(self, data=None, **kwargs):\n        self._store = OrderedDict()\n        if data is None:\n            data = {}\n        self.update(data, **kwargs)\n\n    def __setitem__(self, key, value):\n        # Use the lowercased key for lookups, but store the actual\n        # key alongside the value.\n        self._store[key.lower()] = (key, value)\n\n    def __getitem__(self, key):\n        return self._store[key.lower()][1]\n\n    def __delitem__(self, key):\n        del self._store[key.lower()]\n\n    def __iter__(self):\n        return (casedkey for casedkey, mappedvalue in self._store.values())\n\n    def __len__(self):\n        return len(self._store)\n\n    def lower_items(self):\n        \"\"\"Like iteritems(), but with all lowercase keys.\"\"\"\n        return ((lowerkey, keyval[1]) for (lowerkey, keyval) in self._store.items())\n\n    def __eq__(self, other):\n        if isinstance(other, Mapping):\n            other = CaseInsensitiveDict(other)\n        else:\n            return NotImplemented\n        # Compare insensitively\n        return dict(self.lower_items()) == dict(other.lower_items())\n\n    # Copy is required\n    def copy(self):\n        return CaseInsensitiveDict(self._store.values())\n\n    def __repr__(self):\n        return str(dict(self.items()))\n\n\nclass LookupDict(dict):\n    \"\"\"Dictionary lookup object.\"\"\"\n\n    def __init__(self, name=None):\n        self.name = name\n        super().__init__()\n\n    def __repr__(self):\n        return f\"<lookup '{self.name}'>\"\n\n    def __getitem__(self, key):\n        # We allow fall-through here, so values default to None\n\n        return self.__dict__.get(key, None)\n\n    def get(self, key, default=None):\n        return self.__dict__.get(key, default)\n\n\n### Dependency File: utils.py\n\"\"\"\nrequests.utils\n~~~~~~~~~~~~~~\n\nThis module provides utility functions that are used within Requests\nthat are also useful for external consumption.\n\"\"\"\n\nimport codecs\nimport contextlib\nimport io\nimport os\nimport re\nimport socket\nimport struct\nimport sys\nimport tempfile\nimport warnings\nimport zipfile\nfrom collections import OrderedDict\n\nfrom urllib3.util import make_headers, parse_url\n\nfrom . import certs\nfrom .__version__ import __version__\n\n# to_native_string is unused here, but imported here for backwards compatibility\nfrom ._internal_utils import (  # noqa: F401\n    _HEADER_VALIDATORS_BYTE,\n    _HEADER_VALIDATORS_STR,\n    HEADER_VALIDATORS,\n    to_native_string,\n)\nfrom .compat import (\n    Mapping,\n    basestring,\n    bytes,\n    getproxies,\n    getproxies_environment,\n    integer_types,\n    is_urllib3_1,\n)\nfrom .compat import parse_http_list as _parse_list_header\nfrom .compat import (\n    proxy_bypass,\n    proxy_bypass_environment,\n    quote,\n    str,\n    unquote,\n    urlparse,\n    urlunparse,\n)\nfrom .cookies import cookiejar_from_dict\nfrom .exceptions import (\n    FileModeWarning,\n    InvalidHeader,\n    InvalidURL,\n    UnrewindableBodyError,\n)\nfrom .structures import CaseInsensitiveDict\n\nNETRC_FILES = (\".netrc\", \"_netrc\")\n\nDEFAULT_CA_BUNDLE_PATH = certs.where()\n\nDEFAULT_PORTS = {\"http\": 80, \"https\": 443}\n\n# Ensure that ', ' is used to preserve previous delimiter behavior.\nDEFAULT_ACCEPT_ENCODING = \", \".join(\n    re.split(r\",\\s*\", make_headers(accept_encoding=True)[\"accept-encoding\"])\n)\n\n\nif sys.platform == \"win32\":\n    # provide a proxy_bypass version on Windows without DNS lookups\n\n    def proxy_bypass_registry(host):\n        try:\n            import winreg\n        except ImportError:\n            return False\n\n        try:\n            internetSettings = winreg.OpenKey(\n                winreg.HKEY_CURRENT_USER,\n                r\"Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\",\n            )\n            # ProxyEnable could be REG_SZ or REG_DWORD, normalizing it\n            proxyEnable = int(winreg.QueryValueEx(internetSettings, \"ProxyEnable\")[0])\n            # ProxyOverride is almost always a string\n            proxyOverride = winreg.QueryValueEx(internetSettings, \"ProxyOverride\")[0]\n        except (OSError, ValueError):\n            return False\n        if not proxyEnable or not proxyOverride:\n            return False\n\n        # make a check value list from the registry entry: replace the\n        # '<local>' string by the localhost entry and the corresponding\n        # canonical entry.\n        proxyOverride = proxyOverride.split(\";\")\n        # filter out empty strings to avoid re.match return true in the following code.\n        proxyOverride = filter(None, proxyOverride)\n        # now check if we match one of the registry values.\n        for test in proxyOverride:\n            if test == \"<local>\":\n                if \".\" not in host:\n                    return True\n            test = test.replace(\".\", r\"\\.\")  # mask dots\n            test = test.replace(\"*\", r\".*\")  # change glob sequence\n            test = test.replace(\"?\", r\".\")  # change glob char\n            if re.match(test, host, re.I):\n                return True\n        return False\n\n    def proxy_bypass(host):  # noqa\n        \"\"\"Return True, if the host should be bypassed.\n\n        Checks proxy settings gathered from the environment, if specified,\n        or the registry.\n        \"\"\"\n        if getproxies_environment():\n            return proxy_bypass_environment(host)\n        else:\n            return proxy_bypass_registry(host)\n\n\ndef dict_to_sequence(d):\n    \"\"\"Returns an internal sequence dictionary update.\"\"\"\n\n    if hasattr(d, \"items\"):\n        d = d.items()\n\n    return d\n\n\ndef super_len(o):\n    total_length = None\n    current_position = 0\n\n    if not is_urllib3_1 and isinstance(o, str):\n        # urllib3 2.x+ treats all strings as utf-8 instead\n        # of latin-1 (iso-8859-1) like http.client.\n        o = o.encode(\"utf-8\")\n\n    if hasattr(o, \"__len__\"):\n        total_length = len(o)\n\n    elif hasattr(o, \"len\"):\n        total_length = o.len\n\n    elif hasattr(o, \"fileno\"):\n        try:\n            fileno = o.fileno()\n        except (io.UnsupportedOperation, AttributeError):\n            # AttributeError is a surprising exception, seeing as how we've just checked\n            # that `hasattr(o, 'fileno')`.  It happens for objects obtained via\n            # `Tarfile.extractfile()`, per issue 5229.\n            pass\n        else:\n            total_length = os.fstat(fileno).st_size\n\n            # Having used fstat to determine the file length, we need to\n            # confirm that this file was opened up in binary mode.\n            if \"b\" not in o.mode:\n                warnings.warn(\n                    (\n                        \"Requests has determined the content-length for this \"\n                        \"request using the binary size of the file: however, the \"\n                        \"file has been opened in text mode (i.e. without the 'b' \"\n                        \"flag in the mode). This may lead to an incorrect \"\n                        \"content-length. In Requests 3.0, support will be removed \"\n                        \"for files in text mode.\"\n                    ),\n                    FileModeWarning,\n                )\n\n    if hasattr(o, \"tell\"):\n        try:\n            current_position = o.tell()\n        except OSError:\n            # This can happen in some weird situations, such as when the file\n            # is actually a special file descriptor like stdin. In this\n            # instance, we don't know what the length is, so set it to zero and\n            # let requests chunk it instead.\n            if total_length is not None:\n                current_position = total_length\n        else:\n            if hasattr(o, \"seek\") and total_length is None:\n                # StringIO and BytesIO have seek but no usable fileno\n                try:\n                    # seek to end of file\n                    o.seek(0, 2)\n                    total_length = o.tell()\n\n                    # seek back to current position to support\n                    # partially read file-like objects\n                    o.seek(current_position or 0)\n                except OSError:\n                    total_length = 0\n\n    if total_length is None:\n        total_length = 0\n\n    return max(0, total_length - current_position)\n\n\ndef get_netrc_auth(url, raise_errors=False):\n    \"\"\"Returns the Requests tuple auth for a given url from netrc.\"\"\"\n\n    netrc_file = os.environ.get(\"NETRC\")\n    if netrc_file is not None:\n        netrc_locations = (netrc_file,)\n    else:\n        netrc_locations = (f\"~/{f}\" for f in NETRC_FILES)\n\n    try:\n        from netrc import NetrcParseError, netrc\n\n        netrc_path = None\n\n        for f in netrc_locations:\n            try:\n                loc = os.path.expanduser(f)\n            except KeyError:\n                # os.path.expanduser can fail when $HOME is undefined and\n                # getpwuid fails. See https://bugs.python.org/issue20164 &\n                # https://github.com/psf/requests/issues/1846\n                return\n\n            if os.path.exists(loc):\n                netrc_path = loc\n                break\n\n        # Abort early if there isn't one.\n        if netrc_path is None:\n            return\n\n        ri = urlparse(url)\n\n        # Strip port numbers from netloc. This weird `if...encode`` dance is\n        # used for Python 3.2, which doesn't support unicode literals.\n        splitstr = b\":\"\n        if isinstance(url, str):\n            splitstr = splitstr.decode(\"ascii\")\n        host = ri.netloc.split(splitstr)[0]\n\n        try:\n            _netrc = netrc(netrc_path).authenticators(host)\n            if _netrc:\n                # Return with login / password\n                login_i = 0 if _netrc[0] else 1\n                return (_netrc[login_i], _netrc[2])\n        except (NetrcParseError, OSError):\n            # If there was a parsing error or a permissions issue reading the file,\n            # we'll just skip netrc auth unless explicitly asked to raise errors.\n            if raise_errors:\n                raise\n\n    # App Engine hackiness.\n    except (ImportError, AttributeError):\n        pass\n\n\ndef guess_filename(obj):\n    \"\"\"Tries to guess the filename of the given object.\"\"\"\n    name = getattr(obj, \"name\", None)\n    if name and isinstance(name, basestring) and name[0] != \"<\" and name[-1] != \">\":\n        return os.path.basename(name)\n\n\ndef extract_zipped_paths(path):\n    \"\"\"Replace nonexistent paths that look like they refer to a member of a zip\n    archive with the location of an extracted copy of the target, or else\n    just return the provided path unchanged.\n    \"\"\"\n    if os.path.exists(path):\n        # this is already a valid path, no need to do anything further\n        return path\n\n    # find the first valid part of the provided path and treat that as a zip archive\n    # assume the rest of the path is the name of a member in the archive\n    archive, member = os.path.split(path)\n    while archive and not os.path.exists(archive):\n        archive, prefix = os.path.split(archive)\n        if not prefix:\n            # If we don't check for an empty prefix after the split (in other words, archive remains unchanged after the split),\n            # we _can_ end up in an infinite loop on a rare corner case affecting a small number of users\n            break\n        member = \"/\".join([prefix, member])\n\n    if not zipfile.is_zipfile(archive):\n        return path\n\n    zip_file = zipfile.ZipFile(archive)\n    if member not in zip_file.namelist():\n        return path\n\n    # we have a valid zip archive and a valid member of that archive\n    tmp = tempfile.gettempdir()\n    extracted_path = os.path.join(tmp, member.split(\"/\")[-1])\n    if not os.path.exists(extracted_path):\n        # use read + write to avoid the creating nested folders, we only want the file, avoids mkdir racing condition\n        with atomic_open(extracted_path) as file_handler:\n            file_handler.write(zip_file.read(member))\n    return extracted_path\n\n\n@contextlib.contextmanager\ndef atomic_open(filename):\n    \"\"\"Write a file to the disk in an atomic fashion\"\"\"\n    tmp_descriptor, tmp_name = tempfile.mkstemp(dir=os.path.dirname(filename))\n    try:\n        with os.fdopen(tmp_descriptor, \"wb\") as tmp_handler:\n            yield tmp_handler\n        os.replace(tmp_name, filename)\n    except BaseException:\n        os.remove(tmp_name)\n        raise\n\n\ndef from_key_val_list(value):\n    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. Unless it can not be represented as such, return an\n    OrderedDict, e.g.,\n\n    ::\n\n        >>> from_key_val_list([('key', 'val')])\n        OrderedDict([('key', 'val')])\n        >>> from_key_val_list('string')\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot encode objects that are not 2-tuples\n        >>> from_key_val_list({'key': 'val'})\n        OrderedDict([('key', 'val')])\n\n    :rtype: OrderedDict\n    \"\"\"\n    if value is None:\n        return None\n\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError(\"cannot encode objects that are not 2-tuples\")\n\n    return OrderedDict(value)\n\n\ndef to_key_val_list(value):\n    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. If it can be, return a list of tuples, e.g.,\n\n    ::\n\n        >>> to_key_val_list([('key', 'val')])\n        [('key', 'val')]\n        >>> to_key_val_list({'key': 'val'})\n        [('key', 'val')]\n        >>> to_key_val_list('string')\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot encode objects that are not 2-tuples\n\n    :rtype: list\n    \"\"\"\n    if value is None:\n        return None\n\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError(\"cannot encode objects that are not 2-tuples\")\n\n    if isinstance(value, Mapping):\n        value = value.items()\n\n    return list(value)\n\n\n# From mitsuhiko/werkzeug (used with permission).\ndef parse_list_header(value):\n    \"\"\"Parse lists as described by RFC 2068 Section 2.\n\n    In particular, parse comma-separated lists where the elements of\n    the list may include quoted-strings.  A quoted-string could\n    contain a comma.  A non-quoted string could have quotes in the\n    middle.  Quotes are removed automatically after parsing.\n\n    It basically works like :func:`parse_set_header` just that items\n    may appear multiple times and case sensitivity is preserved.\n\n    The return value is a standard :class:`list`:\n\n    >>> parse_list_header('token, \"quoted value\"')\n    ['token', 'quoted value']\n\n    To create a header from the :class:`list` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a list header.\n    :return: :class:`list`\n    :rtype: list\n    \"\"\"\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result\n\n\n# From mitsuhiko/werkzeug (used with permission).\ndef parse_dict_header(value):\n    \"\"\"Parse lists of key, value pairs as described by RFC 2068 Section 2 and\n    convert them into a python dict:\n\n    >>> d = parse_dict_header('foo=\"is a fish\", bar=\"as well\"')\n    >>> type(d) is dict\n    True\n    >>> sorted(d.items())\n    [('bar', 'as well'), ('foo', 'is a fish')]\n\n    If there is no value for a key it will be `None`:\n\n    >>> parse_dict_header('key_without_value')\n    {'key_without_value': None}\n\n    To create a header from the :class:`dict` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a dict header.\n    :return: :class:`dict`\n    :rtype: dict\n    \"\"\"\n    result = {}\n    for item in _parse_list_header(value):\n        if \"=\" not in item:\n            result[item] = None\n            continue\n        name, value = item.split(\"=\", 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result\n\n\n# From mitsuhiko/werkzeug (used with permission).\ndef unquote_header_value(value, is_filename=False):\n    r\"\"\"Unquotes a header value.  (Reversal of :func:`quote_header_value`).\n    This does not use the real unquoting but what browsers are actually\n    using for quoting.\n\n    :param value: the header value to unquote.\n    :rtype: str\n    \"\"\"\n    if value and value[0] == value[-1] == '\"':\n        # this is not the real unquoting, but fixing this so that the\n        # RFC is met will result in bugs with internet explorer and\n        # probably some other browsers as well.  IE for example is\n        # uploading files with \"C:\\foo\\bar.txt\" as filename\n        value = value[1:-1]\n\n        # if this is a filename and the starting characters look like\n        # a UNC path, then just return the value without quotes.  Using the\n        # replace sequence below on a UNC path has the effect of turning\n        # the leading double slash into a single slash and then\n        # _fix_ie_filename() doesn't work correctly.  See #458.\n        if not is_filename or value[:2] != \"\\\\\\\\\":\n            return value.replace(\"\\\\\\\\\", \"\\\\\").replace('\\\\\"', '\"')\n    return value\n\n\ndef dict_from_cookiejar(cj):\n    \"\"\"Returns a key/value dictionary from a CookieJar.\n\n    :param cj: CookieJar object to extract cookies from.\n    :rtype: dict\n    \"\"\"\n\n    cookie_dict = {cookie.name: cookie.value for cookie in cj}\n    return cookie_dict\n\n\ndef add_dict_to_cookiejar(cj, cookie_dict):\n    \"\"\"Returns a CookieJar from a key/value dictionary.\n\n    :param cj: CookieJar to insert cookies into.\n    :param cookie_dict: Dict of key/values to insert into CookieJar.\n    :rtype: CookieJar\n    \"\"\"\n\n    return cookiejar_from_dict(cookie_dict, cj)\n\n\ndef get_encodings_from_content(content):\n    \"\"\"Returns encodings from given content string.\n\n    :param content: bytestring to extract encodings from.\n    \"\"\"\n    warnings.warn(\n        (\n            \"In requests 3.0, get_encodings_from_content will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n\n    charset_re = re.compile(r'<meta.*?charset=[\"\\']*(.+?)[\"\\'>]', flags=re.I)\n    pragma_re = re.compile(r'<meta.*?content=[\"\\']*;?charset=(.+?)[\"\\'>]', flags=re.I)\n    xml_re = re.compile(r'^<\\?xml.*?encoding=[\"\\']*(.+?)[\"\\'>]')\n\n    return (\n        charset_re.findall(content)\n        + pragma_re.findall(content)\n        + xml_re.findall(content)\n    )\n\n\ndef _parse_content_type_header(header):\n    \"\"\"Returns content type and parameters from given header\n\n    :param header: string\n    :return: tuple containing content type and dictionary of\n         parameters\n    \"\"\"\n\n    tokens = header.split(\";\")\n    content_type, params = tokens[0].strip(), tokens[1:]\n    params_dict = {}\n    items_to_strip = \"\\\"' \"\n\n    for param in params:\n        param = param.strip()\n        if param:\n            key, value = param, True\n            index_of_equals = param.find(\"=\")\n            if index_of_equals != -1:\n                key = param[:index_of_equals].strip(items_to_strip)\n                value = param[index_of_equals + 1 :].strip(items_to_strip)\n            params_dict[key.lower()] = value\n    return content_type, params_dict\n\n\ndef get_encoding_from_headers(headers):\n    \"\"\"Returns encodings from given HTTP Header Dict.\n\n    :param headers: dictionary to extract encoding from.\n    :rtype: str\n    \"\"\"\n\n    content_type = headers.get(\"content-type\")\n\n    if not content_type:\n        return None\n\n    content_type, params = _parse_content_type_header(content_type)\n\n    if \"charset\" in params:\n        return params[\"charset\"].strip(\"'\\\"\")\n\n    if \"text\" in content_type:\n        return \"ISO-8859-1\"\n\n    if \"application/json\" in content_type:\n        # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset\n        return \"utf-8\"\n\n\ndef stream_decode_response_unicode(iterator, r):\n    \"\"\"Stream decodes an iterator.\"\"\"\n\n    if r.encoding is None:\n        yield from iterator\n        return\n\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors=\"replace\")\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b\"\", final=True)\n    if rv:\n        yield rv\n\n\ndef iter_slices(string, slice_length):\n    \"\"\"Iterate over slices of a string.\"\"\"\n    pos = 0\n    if slice_length is None or slice_length <= 0:\n        slice_length = len(string)\n    while pos < len(string):\n        yield string[pos : pos + slice_length]\n        pos += slice_length\n\n\ndef get_unicode_from_response(r):\n    \"\"\"Returns the requested content back in unicode.\n\n    :param r: Response object to get unicode content from.\n\n    Tried:\n\n    1. charset from content-type\n    2. fall back and replace all unicode characters\n\n    :rtype: str\n    \"\"\"\n    warnings.warn(\n        (\n            \"In requests 3.0, get_unicode_from_response will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n\n    tried_encodings = []\n\n    # Try charset from content-type\n    encoding = get_encoding_from_headers(r.headers)\n\n    if encoding:\n        try:\n            return str(r.content, encoding)\n        except UnicodeError:\n            tried_encodings.append(encoding)\n\n    # Fall back:\n    try:\n        return str(r.content, encoding, errors=\"replace\")\n    except TypeError:\n        return r.content\n\n\n# The unreserved URI characters (RFC 3986)\nUNRESERVED_SET = frozenset(\n    \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\" + \"0123456789-._~\"\n)\n\n\ndef unquote_unreserved(uri):\n    \"\"\"Un-escape any percent-escape sequences in a URI that are unreserved\n    characters. This leaves all reserved, illegal and non-ASCII bytes encoded.\n\n    :rtype: str\n    \"\"\"\n    parts = uri.split(\"%\")\n    for i in range(1, len(parts)):\n        h = parts[i][0:2]\n        if len(h) == 2 and h.isalnum():\n            try:\n                c = chr(int(h, 16))\n            except ValueError:\n                raise InvalidURL(f\"Invalid percent-escape sequence: '{h}'\")\n\n            if c in UNRESERVED_SET:\n                parts[i] = c + parts[i][2:]\n            else:\n                parts[i] = f\"%{parts[i]}\"\n        else:\n            parts[i] = f\"%{parts[i]}\"\n    return \"\".join(parts)\n\n\ndef requote_uri(uri):\n    \"\"\"Re-quote the given URI.\n\n    This function passes the given URI through an unquote/quote cycle to\n    ensure that it is fully and consistently quoted.\n\n    :rtype: str\n    \"\"\"\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        # Unquote only the unreserved characters\n        # Then quote only illegal characters (do not quote reserved,\n        # unreserved, or '%')\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        # We couldn't unquote the given URI, so let's try quoting it, but\n        # there may be unquoted '%'s in the URI. We need to make sure they're\n        # properly quoted so they do not cause issues elsewhere.\n        return quote(uri, safe=safe_without_percent)\n\n\ndef address_in_network(ip, net):\n    \"\"\"This function allows you to check if an IP belongs to a network subnet\n\n    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24\n             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24\n\n    :rtype: bool\n    \"\"\"\n    ipaddr = struct.unpack(\"=L\", socket.inet_aton(ip))[0]\n    netaddr, bits = net.split(\"/\")\n    netmask = struct.unpack(\"=L\", socket.inet_aton(dotted_netmask(int(bits))))[0]\n    network = struct.unpack(\"=L\", socket.inet_aton(netaddr))[0] & netmask\n    return (ipaddr & netmask) == (network & netmask)\n\n\ndef dotted_netmask(mask):\n    \"\"\"Converts mask from /xx format to xxx.xxx.xxx.xxx\n\n    Example: if mask is 24 function returns 255.255.255.0\n\n    :rtype: str\n    \"\"\"\n    bits = 0xFFFFFFFF ^ (1 << 32 - mask) - 1\n    return socket.inet_ntoa(struct.pack(\">I\", bits))\n\n\ndef is_ipv4_address(string_ip):\n    \"\"\"\n    :rtype: bool\n    \"\"\"\n    try:\n        socket.inet_aton(string_ip)\n    except OSError:\n        return False\n    return True\n\n\ndef is_valid_cidr(string_network):\n    \"\"\"\n    Very simple check of the cidr format in no_proxy variable.\n\n    :rtype: bool\n    \"\"\"\n    if string_network.count(\"/\") == 1:\n        try:\n            mask = int(string_network.split(\"/\")[1])\n        except ValueError:\n            return False\n\n        if mask < 1 or mask > 32:\n            return False\n\n        try:\n            socket.inet_aton(string_network.split(\"/\")[0])\n        except OSError:\n            return False\n    else:\n        return False\n    return True\n\n\n@contextlib.contextmanager\ndef set_environ(env_name, value):\n    \"\"\"Set the environment variable 'env_name' to 'value'\n\n    Save previous value, yield, and then restore the previous value stored in\n    the environment variable 'env_name'.\n\n    If 'value' is None, do nothing\"\"\"\n    value_changed = value is not None\n    if value_changed:\n        old_value = os.environ.get(env_name)\n        os.environ[env_name] = value\n    try:\n        yield\n    finally:\n        if value_changed:\n            if old_value is None:\n                del os.environ[env_name]\n            else:\n                os.environ[env_name] = old_value\n\n\ndef should_bypass_proxies(url, no_proxy):\n    \"\"\"\n    Returns whether we should bypass proxies or not.\n\n    :rtype: bool\n    \"\"\"\n\n    # Prioritize lowercase environment variables over uppercase\n    # to keep a consistent behaviour with other http projects (curl, wget).\n    def get_proxy(key):\n        return os.environ.get(key) or os.environ.get(key.upper())\n\n    # First check whether no_proxy is defined. If it is, check that the URL\n    # we're getting isn't in the no_proxy list.\n    no_proxy_arg = no_proxy\n    if no_proxy is None:\n        no_proxy = get_proxy(\"no_proxy\")\n    parsed = urlparse(url)\n\n    if parsed.hostname is None:\n        # URLs don't always have hostnames, e.g. file:/// urls.\n        return True\n\n    if no_proxy:\n        # We need to check whether we match here. We need to see if we match\n        # the end of the hostname, both with and without the port.\n        no_proxy = (host for host in no_proxy.replace(\" \", \"\").split(\",\") if host)\n\n        if is_ipv4_address(parsed.hostname):\n            for proxy_ip in no_proxy:\n                if is_valid_cidr(proxy_ip):\n                    if address_in_network(parsed.hostname, proxy_ip):\n                        return True\n                elif parsed.hostname == proxy_ip:\n                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &\n                    # matches the IP of the index\n                    return True\n        else:\n            host_with_port = parsed.hostname\n            if parsed.port:\n                host_with_port += f\":{parsed.port}\"\n\n            for host in no_proxy:\n                if parsed.hostname.endswith(host) or host_with_port.endswith(host):\n                    # The URL does match something in no_proxy, so we don't want\n                    # to apply the proxies on this URL.\n                    return True\n\n    with set_environ(\"no_proxy\", no_proxy_arg):\n        # parsed.hostname can be `None` in cases such as a file URI.\n        try:\n            bypass = proxy_bypass(parsed.hostname)\n        except (TypeError, socket.gaierror):\n            bypass = False\n\n    if bypass:\n        return True\n\n    return False\n\n\ndef get_environ_proxies(url, no_proxy=None):\n    \"\"\"\n    Return a dict of environment proxies.\n\n    :rtype: dict\n    \"\"\"\n    if should_bypass_proxies(url, no_proxy=no_proxy):\n        return {}\n    else:\n        return getproxies()\n\n\ndef select_proxy(url, proxies):\n    \"\"\"Select a proxy for the url, if applicable.\n\n    :param url: The url being for the request\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    \"\"\"\n    proxies = proxies or {}\n    urlparts = urlparse(url)\n    if urlparts.hostname is None:\n        return proxies.get(urlparts.scheme, proxies.get(\"all\"))\n\n    proxy_keys = [\n        urlparts.scheme + \"://\" + urlparts.hostname,\n        urlparts.scheme,\n        \"all://\" + urlparts.hostname,\n        \"all\",\n    ]\n    proxy = None\n    for proxy_key in proxy_keys:\n        if proxy_key in proxies:\n            proxy = proxies[proxy_key]\n            break\n\n    return proxy\n\n\ndef resolve_proxies(request, proxies, trust_env=True):\n    \"\"\"This method takes proxy information from a request and configuration\n    input to resolve a mapping of target proxies. This will consider settings\n    such as NO_PROXY to strip proxy configurations.\n\n    :param request: Request or PreparedRequest\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    :param trust_env: Boolean declaring whether to trust environment configs\n\n    :rtype: dict\n    \"\"\"\n    proxies = proxies if proxies is not None else {}\n    url = request.url\n    scheme = urlparse(url).scheme\n    no_proxy = proxies.get(\"no_proxy\")\n    new_proxies = proxies.copy()\n\n    if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):\n        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n\n        proxy = environ_proxies.get(scheme, environ_proxies.get(\"all\"))\n\n        if proxy:\n            new_proxies.setdefault(scheme, proxy)\n    return new_proxies\n\n\ndef default_user_agent(name=\"python-requests\"):\n    \"\"\"\n    Return a string representing the default user agent.\n\n    :rtype: str\n    \"\"\"\n    return f\"{name}/{__version__}\"\n\n\ndef default_headers():\n    \"\"\"\n    :rtype: requests.structures.CaseInsensitiveDict\n    \"\"\"\n    return CaseInsensitiveDict(\n        {\n            \"User-Agent\": default_user_agent(),\n            \"Accept-Encoding\": DEFAULT_ACCEPT_ENCODING,\n            \"Accept\": \"*/*\",\n            \"Connection\": \"keep-alive\",\n        }\n    )\n\n\ndef parse_header_links(value):\n    \"\"\"Return a list of parsed link headers proxies.\n\n    i.e. Link: <http:/.../front.jpeg>; rel=front; type=\"image/jpeg\",<http://.../back.jpeg>; rel=back;type=\"image/jpeg\"\n\n    :rtype: list\n    \"\"\"\n\n    links = []\n\n    replace_chars = \" '\\\"\"\n\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n\n    for val in re.split(\", *<\", value):\n        try:\n            url, params = val.split(\";\", 1)\n        except ValueError:\n            url, params = val, \"\"\n\n        link = {\"url\": url.strip(\"<> '\\\"\")}\n\n        for param in params.split(\";\"):\n            try:\n                key, value = param.split(\"=\")\n            except ValueError:\n                break\n\n            link[key.strip(replace_chars)] = value.strip(replace_chars)\n\n        links.append(link)\n\n    return links\n\n\n# Null bytes; no need to recreate these on each call to guess_json_utf\n_null = \"\\x00\".encode(\"ascii\")  # encoding to ASCII for Python 3\n_null2 = _null * 2\n_null3 = _null * 3\n\n\ndef guess_json_utf(data):\n    \"\"\"\n    :rtype: str\n    \"\"\"\n    # JSON always starts with two ASCII characters, so detection is as\n    # easy as counting the nulls and from their location and count\n    # determine the encoding. Also detect a BOM, if present.\n    sample = data[:4]\n    if sample in (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):\n        return \"utf-32\"  # BOM included\n    if sample[:3] == codecs.BOM_UTF8:\n        return \"utf-8-sig\"  # BOM included, MS style (discouraged)\n    if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):\n        return \"utf-16\"  # BOM included\n    nullcount = sample.count(_null)\n    if nullcount == 0:\n        return \"utf-8\"\n    if nullcount == 2:\n        if sample[::2] == _null2:  # 1st and 3rd are null\n            return \"utf-16-be\"\n        if sample[1::2] == _null2:  # 2nd and 4th are null\n            return \"utf-16-le\"\n        # Did not detect 2 valid UTF-16 ascii-range characters\n    if nullcount == 3:\n        if sample[:3] == _null3:\n            return \"utf-32-be\"\n        if sample[1:] == _null3:\n            return \"utf-32-le\"\n        # Did not detect a valid UTF-32 ascii-range character\n    return None\n\n\ndef prepend_scheme_if_needed(url, new_scheme):\n    \"\"\"Given a URL that may or may not have a scheme, prepend the given scheme.\n    Does not replace a present scheme with the one provided as an argument.\n\n    :rtype: str\n    \"\"\"\n    parsed = parse_url(url)\n    scheme, auth, host, port, path, query, fragment = parsed\n\n    # A defect in urlparse determines that there isn't a netloc present in some\n    # urls. We previously assumed parsing was overly cautious, and swapped the\n    # netloc and path. Due to a lack of tests on the original defect, this is\n    # maintained with parse_url for backwards compatibility.\n    netloc = parsed.netloc\n    if not netloc:\n        netloc, path = path, netloc\n\n    if auth:\n        # parse_url doesn't provide the netloc with auth\n        # so we'll add it ourselves.\n        netloc = \"@\".join([auth, netloc])\n    if scheme is None:\n        scheme = new_scheme\n    if path is None:\n        path = \"\"\n\n    return urlunparse((scheme, netloc, path, \"\", query, fragment))\n\n\ndef get_auth_from_url(url):\n    \"\"\"Given a url with authentication components, extract them into a tuple of\n    username,password.\n\n    :rtype: (str,str)\n    \"\"\"\n    parsed = urlparse(url)\n\n    try:\n        auth = (unquote(parsed.username), unquote(parsed.password))\n    except (AttributeError, TypeError):\n        auth = (\"\", \"\")\n\n    return auth\n\n\ndef check_header_validity(header):\n    \"\"\"Verifies that header parts don't contain leading whitespace\n    reserved characters, or return characters.\n\n    :param header: tuple, in the format (name, value).\n    \"\"\"\n    name, value = header\n    _validate_header_part(header, name, 0)\n    _validate_header_part(header, value, 1)\n\n\ndef _validate_header_part(header, header_part, header_validator_index):\n    if isinstance(header_part, str):\n        validator = _HEADER_VALIDATORS_STR[header_validator_index]\n    elif isinstance(header_part, bytes):\n        validator = _HEADER_VALIDATORS_BYTE[header_validator_index]\n    else:\n        raise InvalidHeader(\n            f\"Header part ({header_part!r}) from {header} \"\n            f\"must be of type str or bytes, not {type(header_part)}\"\n        )\n\n    if not validator.match(header_part):\n        header_kind = \"name\" if header_validator_index == 0 else \"value\"\n        raise InvalidHeader(\n            f\"Invalid leading whitespace, reserved character(s), or return \"\n            f\"character(s) in header {header_kind}: {header_part!r}\"\n        )\n\n\ndef urldefragauth(url):\n    \"\"\"\n    Given a url remove the fragment and the authentication part.\n\n    :rtype: str\n    \"\"\"\n    scheme, netloc, path, params, query, fragment = urlparse(url)\n\n    # see func:`prepend_scheme_if_needed`\n    if not netloc:\n        netloc, path = path, netloc\n\n    netloc = netloc.rsplit(\"@\", 1)[-1]\n\n    return urlunparse((scheme, netloc, path, params, query, \"\"))\n\n\ndef rewind_body(prepared_request):\n    \"\"\"Move file pointer back to its recorded starting position\n    so it can be read again on redirect.\n    \"\"\"\n    body_seek = getattr(prepared_request.body, \"seek\", None)\n    if body_seek is not None and isinstance(\n        prepared_request._body_position, integer_types\n    ):\n        try:\n            body_seek(prepared_request._body_position)\n        except OSError:\n            raise UnrewindableBodyError(\n                \"An error occurred when rewinding request body for redirect.\"\n            )\n    else:\n        raise UnrewindableBodyError(\"Unable to rewind request body for redirect.\")\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:13:32"}, {"role": "assistant", "content": "```python\nimport requests.adapters\n\n\ndef test_request_url_trims_leading_path_separators():\n    \"\"\"See also https://github.com/psf/requests/issues/6643.\"\"\"\n    a = requests.adapters.HTTPAdapter()\n    p = requests.Request(method=\"GET\", url=\"http://127.0.0.1:10000//v:h\").prepare()\n    assert \"/v:h\" == a.request_url(p, {})\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: requests\nTest File Path: requests\\test_help\\test_help.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"Module containing bug report helper(s).\"\"\"\n\nimport json\nimport platform\nimport ssl\nimport sys\n\nimport idna\nimport urllib3\n\nfrom . import __version__ as requests_version\n\ntry:\n    import charset_normalizer\nexcept ImportError:\n    charset_normalizer = None\n\ntry:\n    import chardet\nexcept ImportError:\n    chardet = None\n\ntry:\n    from urllib3.contrib import pyopenssl\nexcept ImportError:\n    pyopenssl = None\n    OpenSSL = None\n    cryptography = None\nelse:\n    import cryptography\n    import OpenSSL\n\n\ndef _implementation():\n    \"\"\"Return a dict with the Python implementation and version.\n\n    Provide both the name and the version of the Python implementation\n    currently running. For example, on CPython 3.10.3 it will return\n    {'name': 'CPython', 'version': '3.10.3'}.\n\n    This function works best on CPython and PyPy: in particular, it probably\n    doesn't work for Jython or IronPython. Future investigation should be done\n    to work out the correct shape of the code for those platforms.\n    \"\"\"\n    implementation = platform.python_implementation()\n\n    if implementation == \"CPython\":\n        implementation_version = platform.python_version()\n    elif implementation == \"PyPy\":\n        implementation_version = \"{}.{}.{}\".format(\n            sys.pypy_version_info.major,\n            sys.pypy_version_info.minor,\n            sys.pypy_version_info.micro,\n        )\n        if sys.pypy_version_info.releaselevel != \"final\":\n            implementation_version = \"\".join(\n                [implementation_version, sys.pypy_version_info.releaselevel]\n            )\n    elif implementation == \"Jython\":\n        implementation_version = platform.python_version()  # Complete Guess\n    elif implementation == \"IronPython\":\n        implementation_version = platform.python_version()  # Complete Guess\n    else:\n        implementation_version = \"Unknown\"\n\n    return {\"name\": implementation, \"version\": implementation_version}\n\n\ndef info():\n    \"\"\"Generate information for a bug report.\"\"\"\n    try:\n        platform_info = {\n            \"system\": platform.system(),\n            \"release\": platform.release(),\n        }\n    except OSError:\n        platform_info = {\n            \"system\": \"Unknown\",\n            \"release\": \"Unknown\",\n        }\n\n    implementation_info = _implementation()\n    urllib3_info = {\"version\": urllib3.__version__}\n    charset_normalizer_info = {\"version\": None}\n    chardet_info = {\"version\": None}\n    if charset_normalizer:\n        charset_normalizer_info = {\"version\": charset_normalizer.__version__}\n    if chardet:\n        chardet_info = {\"version\": chardet.__version__}\n\n    pyopenssl_info = {\n        \"version\": None,\n        \"openssl_version\": \"\",\n    }\n    if OpenSSL:\n        pyopenssl_info = {\n            \"version\": OpenSSL.__version__,\n            \"openssl_version\": f\"{OpenSSL.SSL.OPENSSL_VERSION_NUMBER:x}\",\n        }\n    cryptography_info = {\n        \"version\": getattr(cryptography, \"__version__\", \"\"),\n    }\n    idna_info = {\n        \"version\": getattr(idna, \"__version__\", \"\"),\n    }\n\n    system_ssl = ssl.OPENSSL_VERSION_NUMBER\n    system_ssl_info = {\"version\": f\"{system_ssl:x}\" if system_ssl is not None else \"\"}\n\n    return {\n        \"platform\": platform_info,\n        \"implementation\": implementation_info,\n        \"system_ssl\": system_ssl_info,\n        \"using_pyopenssl\": pyopenssl is not None,\n        \"using_charset_normalizer\": chardet is None,\n        \"pyOpenSSL\": pyopenssl_info,\n        \"urllib3\": urllib3_info,\n        \"chardet\": chardet_info,\n        \"charset_normalizer\": charset_normalizer_info,\n        \"cryptography\": cryptography_info,\n        \"idna\": idna_info,\n        \"requests\": {\n            \"version\": requests_version,\n        },\n    }\n\n\ndef main():\n    \"\"\"Pretty-print the bug information as JSON.\"\"\"\n    print(json.dumps(info(), sort_keys=True, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n\n### Source File Dependency Files Content\n### Dependency File: __version__.py\n# .-. .-. .-. . . .-. .-. .-. .-.\n# |(  |-  |.| | | |-  `-.  |  `-.\n# ' ' `-' `-`.`-' `-' `-'  '  `-'\n\n__title__ = \"requests\"\n__description__ = \"Python HTTP for Humans.\"\n__url__ = \"https://requests.readthedocs.io\"\n__version__ = \"2.32.3\"\n__build__ = 0x023203\n__author__ = \"Kenneth Reitz\"\n__author_email__ = \"me@kennethreitz.org\"\n__license__ = \"Apache-2.0\"\n__copyright__ = \"Copyright Kenneth Reitz\"\n__cake__ = \"\\u2728 \\U0001f370 \\u2728\"\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:13:32"}, {"role": "assistant", "content": "```python\nfrom unittest import mock\n\nfrom requests.help import info\n\n\ndef test_system_ssl():\n    \"\"\"Verify we're actually setting system_ssl when it should be available.\"\"\"\n    assert info()[\"system_ssl\"][\"version\"] != \"\"\n\n\nclass VersionedPackage:\n    def __init__(self, version):\n        self.__version__ = version\n\n\ndef test_idna_without_version_attribute():\n    \"\"\"Older versions of IDNA don't provide a __version__ attribute, verify\n    that if we have such a package, we don't blow up.\n    \"\"\"\n    with mock.patch(\"requests.help.idna\", new=None):\n        assert info()[\"idna\"] == {\"version\": \"\"}\n\n\ndef test_idna_with_version_attribute():\n    \"\"\"Verify we're actually setting idna version when it should be available.\"\"\"\n    with mock.patch(\"requests.help.idna\", new=VersionedPackage(\"2.6\")):\n        assert info()[\"idna\"] == {\"version\": \"2.6\"}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: requests\nTest File Path: requests\\test_hooks\\test_hooks.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"\nrequests.hooks\n~~~~~~~~~~~~~~\n\nThis module provides the capabilities for the Requests hooks system.\n\nAvailable hooks:\n\n``response``:\n    The response generated from a Request.\n\"\"\"\nHOOKS = [\"response\"]\n\n\ndef default_hooks():\n    return {event: [] for event in HOOKS}\n\n\n# TODO: response is the only one\n\n\ndef dispatch_hook(key, hooks, hook_data, **kwargs):\n    \"\"\"Dispatches a hook dictionary on a given piece of data.\"\"\"\n    hooks = hooks or {}\n    hooks = hooks.get(key)\n    if hooks:\n        if hasattr(hooks, \"__call__\"):\n            hooks = [hooks]\n        for hook in hooks:\n            _hook_data = hook(hook_data, **kwargs)\n            if _hook_data is not None:\n                hook_data = _hook_data\n    return hook_data\n\n### Source File Dependency Files Content\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:13:32"}, {"role": "assistant", "content": "```python\nimport pytest\n\nfrom requests import hooks\n\n\ndef hook(value):\n    return value[1:]\n\n\n@pytest.mark.parametrize(\n    \"hooks_list, result\",\n    (\n        (hook, \"ata\"),\n        ([hook, lambda x: None, hook], \"ta\"),\n    ),\n)\ndef test_hooks(hooks_list, result):\n    assert hooks.dispatch_hook(\"response\", {\"response\": hooks_list}, \"Data\") == result\n\n\ndef test_default_hooks():\n    assert hooks.default_hooks() == {\"response\": []}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: requests\nTest File Path: requests\\test_structures\\test_structures.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"\nrequests.structures\n~~~~~~~~~~~~~~~~~~~\n\nData structures that power Requests.\n\"\"\"\n\nfrom collections import OrderedDict\n\nfrom .compat import Mapping, MutableMapping\n\n\nclass CaseInsensitiveDict(MutableMapping):\n    \"\"\"A case-insensitive ``dict``-like object.\n\n    Implements all methods and operations of\n    ``MutableMapping`` as well as dict's ``copy``. Also\n    provides ``lower_items``.\n\n    All keys are expected to be strings. The structure remembers the\n    case of the last key to be set, and ``iter(instance)``,\n    ``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``\n    will contain case-sensitive keys. However, querying and contains\n    testing is case insensitive::\n\n        cid = CaseInsensitiveDict()\n        cid['Accept'] = 'application/json'\n        cid['aCCEPT'] == 'application/json'  # True\n        list(cid) == ['Accept']  # True\n\n    For example, ``headers['content-encoding']`` will return the\n    value of a ``'Content-Encoding'`` response header, regardless\n    of how the header name was originally stored.\n\n    If the constructor, ``.update``, or equality comparison\n    operations are given keys that have equal ``.lower()``s, the\n    behavior is undefined.\n    \"\"\"\n\n    def __init__(self, data=None, **kwargs):\n        self._store = OrderedDict()\n        if data is None:\n            data = {}\n        self.update(data, **kwargs)\n\n    def __setitem__(self, key, value):\n        # Use the lowercased key for lookups, but store the actual\n        # key alongside the value.\n        self._store[key.lower()] = (key, value)\n\n    def __getitem__(self, key):\n        return self._store[key.lower()][1]\n\n    def __delitem__(self, key):\n        del self._store[key.lower()]\n\n    def __iter__(self):\n        return (casedkey for casedkey, mappedvalue in self._store.values())\n\n    def __len__(self):\n        return len(self._store)\n\n    def lower_items(self):\n        \"\"\"Like iteritems(), but with all lowercase keys.\"\"\"\n        return ((lowerkey, keyval[1]) for (lowerkey, keyval) in self._store.items())\n\n    def __eq__(self, other):\n        if isinstance(other, Mapping):\n            other = CaseInsensitiveDict(other)\n        else:\n            return NotImplemented\n        # Compare insensitively\n        return dict(self.lower_items()) == dict(other.lower_items())\n\n    # Copy is required\n    def copy(self):\n        return CaseInsensitiveDict(self._store.values())\n\n    def __repr__(self):\n        return str(dict(self.items()))\n\n\nclass LookupDict(dict):\n    \"\"\"Dictionary lookup object.\"\"\"\n\n    def __init__(self, name=None):\n        self.name = name\n        super().__init__()\n\n    def __repr__(self):\n        return f\"<lookup '{self.name}'>\"\n\n    def __getitem__(self, key):\n        # We allow fall-through here, so values default to None\n\n        return self.__dict__.get(key, None)\n\n    def get(self, key, default=None):\n        return self.__dict__.get(key, default)\n\n### Source File Dependency Files Content\n### Dependency File: compat.py\n\"\"\"\nrequests.compat\n~~~~~~~~~~~~~~~\n\nThis module previously handled import compatibility issues\nbetween Python 2 and Python 3. It remains for backwards\ncompatibility until the next major version.\n\"\"\"\n\nimport importlib\nimport sys\n\n# -------\n# urllib3\n# -------\nfrom urllib3 import __version__ as urllib3_version\n\n# Detect which major version of urllib3 is being used.\ntry:\n    is_urllib3_1 = int(urllib3_version.split(\".\")[0]) == 1\nexcept (TypeError, AttributeError):\n    # If we can't discern a version, prefer old functionality.\n    is_urllib3_1 = True\n\n# -------------------\n# Character Detection\n# -------------------\n\n\ndef _resolve_char_detection():\n    \"\"\"Find supported character detection libraries.\"\"\"\n    chardet = None\n    for lib in (\"chardet\", \"charset_normalizer\"):\n        if chardet is None:\n            try:\n                chardet = importlib.import_module(lib)\n            except ImportError:\n                pass\n    return chardet\n\n\nchardet = _resolve_char_detection()\n\n# -------\n# Pythons\n# -------\n\n# Syntax sugar.\n_ver = sys.version_info\n\n#: Python 2.x?\nis_py2 = _ver[0] == 2\n\n#: Python 3.x?\nis_py3 = _ver[0] == 3\n\n# json/simplejson module import resolution\nhas_simplejson = False\ntry:\n    import simplejson as json\n\n    has_simplejson = True\nexcept ImportError:\n    import json\n\nif has_simplejson:\n    from simplejson import JSONDecodeError\nelse:\n    from json import JSONDecodeError\n\n# Keep OrderedDict for backwards compatibility.\nfrom collections import OrderedDict\nfrom collections.abc import Callable, Mapping, MutableMapping\nfrom http import cookiejar as cookielib\nfrom http.cookies import Morsel\nfrom io import StringIO\n\n# --------------\n# Legacy Imports\n# --------------\nfrom urllib.parse import (\n    quote,\n    quote_plus,\n    unquote,\n    unquote_plus,\n    urldefrag,\n    urlencode,\n    urljoin,\n    urlparse,\n    urlsplit,\n    urlunparse,\n)\nfrom urllib.request import (\n    getproxies,\n    getproxies_environment,\n    parse_http_list,\n    proxy_bypass,\n    proxy_bypass_environment,\n)\n\nbuiltin_str = str\nstr = str\nbytes = bytes\nbasestring = (str, bytes)\nnumeric_types = (int, float)\ninteger_types = (int,)\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:13:32"}, {"role": "assistant", "content": "```python\nimport pytest\n\nfrom requests.structures import CaseInsensitiveDict, LookupDict\n\n\nclass TestCaseInsensitiveDict:\n    @pytest.fixture(autouse=True)\n    def setup(self):\n        \"\"\"CaseInsensitiveDict instance with \"Accept\" header.\"\"\"\n        self.case_insensitive_dict = CaseInsensitiveDict()\n        self.case_insensitive_dict[\"Accept\"] = \"application/json\"\n\n    def test_list(self):\n        assert list(self.case_insensitive_dict) == [\"Accept\"]\n\n    possible_keys = pytest.mark.parametrize(\n        \"key\", (\"accept\", \"ACCEPT\", \"aCcEpT\", \"Accept\")\n    )\n\n    @possible_keys\n    def test_getitem(self, key):\n        assert self.case_insensitive_dict[key] == \"application/json\"\n\n    @possible_keys\n    def test_delitem(self, key):\n        del self.case_insensitive_dict[key]\n        assert key not in self.case_insensitive_dict\n\n    def test_lower_items(self):\n        assert list(self.case_insensitive_dict.lower_items()) == [\n            (\"accept\", \"application/json\")\n        ]\n\n    def test_repr(self):\n        assert repr(self.case_insensitive_dict) == \"{'Accept': 'application/json'}\"\n\n    def test_copy(self):\n        copy = self.case_insensitive_dict.copy()\n        assert copy is not self.case_insensitive_dict\n        assert copy == self.case_insensitive_dict\n\n    @pytest.mark.parametrize(\n        \"other, result\",\n        (\n            ({\"AccePT\": \"application/json\"}, True),\n            ({}, False),\n            (None, False),\n        ),\n    )\n    def test_instance_equality(self, other, result):\n        assert (self.case_insensitive_dict == other) is result\n\n\nclass TestLookupDict:\n    @pytest.fixture(autouse=True)\n    def setup(self):\n        \"\"\"LookupDict instance with \"bad_gateway\" attribute.\"\"\"\n        self.lookup_dict = LookupDict(\"test\")\n        self.lookup_dict.bad_gateway = 502\n\n    def test_repr(self):\n        assert repr(self.lookup_dict) == \"<lookup 'test'>\"\n\n    get_item_parameters = pytest.mark.parametrize(\n        \"key, value\",\n        (\n            (\"bad_gateway\", 502),\n            (\"not_a_key\", None),\n        ),\n    )\n\n    @get_item_parameters\n    def test_getitem(self, key, value):\n        assert self.lookup_dict[key] == value\n\n    @get_item_parameters\n    def test_get(self, key, value):\n        assert self.lookup_dict.get(key) == value\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: requests\nTest File Path: requests\\test_testserver\\test_testserver.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nimport select\nimport socket\nimport ssl\nimport threading\n\n\ndef consume_socket_content(sock, timeout=0.5):\n    chunks = 65536\n    content = b\"\"\n\n    while True:\n        more_to_read = select.select([sock], [], [], timeout)[0]\n        if not more_to_read:\n            break\n\n        new_content = sock.recv(chunks)\n        if not new_content:\n            break\n\n        content += new_content\n\n    return content\n\n\nclass Server(threading.Thread):\n    \"\"\"Dummy server using for unit testing\"\"\"\n\n    WAIT_EVENT_TIMEOUT = 5\n\n    def __init__(\n        self,\n        handler=None,\n        host=\"localhost\",\n        port=0,\n        requests_to_handle=1,\n        wait_to_close_event=None,\n    ):\n        super().__init__()\n\n        self.handler = handler or consume_socket_content\n        self.handler_results = []\n\n        self.host = host\n        self.port = port\n        self.requests_to_handle = requests_to_handle\n\n        self.wait_to_close_event = wait_to_close_event\n        self.ready_event = threading.Event()\n        self.stop_event = threading.Event()\n\n    @classmethod\n    def text_response_server(cls, text, request_timeout=0.5, **kwargs):\n        def text_response_handler(sock):\n            request_content = consume_socket_content(sock, timeout=request_timeout)\n            sock.send(text.encode(\"utf-8\"))\n\n            return request_content\n\n        return Server(text_response_handler, **kwargs)\n\n    @classmethod\n    def basic_response_server(cls, **kwargs):\n        return cls.text_response_server(\n            \"HTTP/1.1 200 OK\\r\\n\" + \"Content-Length: 0\\r\\n\\r\\n\", **kwargs\n        )\n\n    def run(self):\n        try:\n            self.server_sock = self._create_socket_and_bind()\n            # in case self.port = 0\n            self.port = self.server_sock.getsockname()[1]\n            self.ready_event.set()\n            self._handle_requests()\n\n            if self.wait_to_close_event:\n                self.wait_to_close_event.wait(self.WAIT_EVENT_TIMEOUT)\n        finally:\n            self.ready_event.set()  # just in case of exception\n            self._close_server_sock_ignore_errors()\n            self.stop_event.set()\n\n    def _create_socket_and_bind(self):\n        sock = socket.socket()\n        sock.bind((self.host, self.port))\n        sock.listen()\n        return sock\n\n    def _close_server_sock_ignore_errors(self):\n        try:\n            self.server_sock.close()\n        except OSError:\n            pass\n\n    def _handle_requests(self):\n        for _ in range(self.requests_to_handle):\n            sock = self._accept_connection()\n            if not sock:\n                break\n\n            handler_result = self.handler(sock)\n\n            self.handler_results.append(handler_result)\n            sock.close()\n\n    def _accept_connection(self):\n        try:\n            ready, _, _ = select.select(\n                [self.server_sock], [], [], self.WAIT_EVENT_TIMEOUT\n            )\n            if not ready:\n                return None\n\n            return self.server_sock.accept()[0]\n        except OSError:\n            return None\n\n    def __enter__(self):\n        self.start()\n        if not self.ready_event.wait(self.WAIT_EVENT_TIMEOUT):\n            raise RuntimeError(\"Timeout waiting for server to be ready.\")\n        return self.host, self.port\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if exc_type is None:\n            self.stop_event.wait(self.WAIT_EVENT_TIMEOUT)\n        else:\n            if self.wait_to_close_event:\n                # avoid server from waiting for event timeouts\n                # if an exception is found in the main thread\n                self.wait_to_close_event.set()\n\n        # ensure server thread doesn't get stuck waiting for connections\n        self._close_server_sock_ignore_errors()\n        self.join()\n        return False  # allow exceptions to propagate\n\n\nclass TLSServer(Server):\n    def __init__(\n        self,\n        *,\n        handler=None,\n        host=\"localhost\",\n        port=0,\n        requests_to_handle=1,\n        wait_to_close_event=None,\n        cert_chain=None,\n        keyfile=None,\n        mutual_tls=False,\n        cacert=None,\n    ):\n        super().__init__(\n            handler=handler,\n            host=host,\n            port=port,\n            requests_to_handle=requests_to_handle,\n            wait_to_close_event=wait_to_close_event,\n        )\n        self.cert_chain = cert_chain\n        self.keyfile = keyfile\n        self.ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n        self.ssl_context.load_cert_chain(self.cert_chain, keyfile=self.keyfile)\n        self.mutual_tls = mutual_tls\n        self.cacert = cacert\n        if mutual_tls:\n            # For simplicity, we're going to assume that the client cert is\n            # issued by the same CA as our Server certificate\n            self.ssl_context.verify_mode = ssl.CERT_OPTIONAL\n            self.ssl_context.load_verify_locations(self.cacert)\n\n    def _create_socket_and_bind(self):\n        sock = socket.socket()\n        sock = self.ssl_context.wrap_socket(sock, server_side=True)\n        sock.bind((self.host, self.port))\n        sock.listen()\n        return sock\n\n### Source File Dependency Files Content\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:13:32"}, {"role": "assistant", "content": "```python\nimport socket\nimport threading\nimport time\n\nimport pytest\nfrom tests.testserver.server import Server\n\nimport requests\n\n\nclass TestTestServer:\n    def test_basic(self):\n        \"\"\"messages are sent and received properly\"\"\"\n        question = b\"success?\"\n        answer = b\"yeah, success\"\n\n        def handler(sock):\n            text = sock.recv(1000)\n            assert text == question\n            sock.sendall(answer)\n\n        with Server(handler) as (host, port):\n            sock = socket.socket()\n            sock.connect((host, port))\n            sock.sendall(question)\n            text = sock.recv(1000)\n            assert text == answer\n            sock.close()\n\n    def test_server_closes(self):\n        \"\"\"the server closes when leaving the context manager\"\"\"\n        with Server.basic_response_server() as (host, port):\n            sock = socket.socket()\n            sock.connect((host, port))\n\n            sock.close()\n\n        with pytest.raises(socket.error):\n            new_sock = socket.socket()\n            new_sock.connect((host, port))\n\n    def test_text_response(self):\n        \"\"\"the text_response_server sends the given text\"\"\"\n        server = Server.text_response_server(\n            \"HTTP/1.1 200 OK\\r\\n\" \"Content-Length: 6\\r\\n\" \"\\r\\nroflol\"\n        )\n\n        with server as (host, port):\n            r = requests.get(f\"http://{host}:{port}\")\n\n            assert r.status_code == 200\n            assert r.text == \"roflol\"\n            assert r.headers[\"Content-Length\"] == \"6\"\n\n    def test_basic_response(self):\n        \"\"\"the basic response server returns an empty http response\"\"\"\n        with Server.basic_response_server() as (host, port):\n            r = requests.get(f\"http://{host}:{port}\")\n            assert r.status_code == 200\n            assert r.text == \"\"\n            assert r.headers[\"Content-Length\"] == \"0\"\n\n    def test_basic_waiting_server(self):\n        \"\"\"the server waits for the block_server event to be set before closing\"\"\"\n        block_server = threading.Event()\n\n        with Server.basic_response_server(wait_to_close_event=block_server) as (\n            host,\n            port,\n        ):\n            sock = socket.socket()\n            sock.connect((host, port))\n            sock.sendall(b\"send something\")\n            time.sleep(2.5)\n            sock.sendall(b\"still alive\")\n            block_server.set()  # release server block\n\n    def test_multiple_requests(self):\n        \"\"\"multiple requests can be served\"\"\"\n        requests_to_handle = 5\n\n        server = Server.basic_response_server(requests_to_handle=requests_to_handle)\n\n        with server as (host, port):\n            server_url = f\"http://{host}:{port}\"\n            for _ in range(requests_to_handle):\n                r = requests.get(server_url)\n                assert r.status_code == 200\n\n            # the (n+1)th request fails\n            with pytest.raises(requests.exceptions.ConnectionError):\n                r = requests.get(server_url)\n\n    @pytest.mark.skip(reason=\"this fails non-deterministically under pytest-xdist\")\n    def test_request_recovery(self):\n        \"\"\"can check the requests content\"\"\"\n        # TODO: figure out why this sometimes fails when using pytest-xdist.\n        server = Server.basic_response_server(requests_to_handle=2)\n        first_request = b\"put your hands up in the air\"\n        second_request = b\"put your hand down in the floor\"\n\n        with server as address:\n            sock1 = socket.socket()\n            sock2 = socket.socket()\n\n            sock1.connect(address)\n            sock1.sendall(first_request)\n            sock1.close()\n\n            sock2.connect(address)\n            sock2.sendall(second_request)\n            sock2.close()\n\n        assert server.handler_results[0] == first_request\n        assert server.handler_results[1] == second_request\n\n    def test_requests_after_timeout_are_not_received(self):\n        \"\"\"the basic response handler times out when receiving requests\"\"\"\n        server = Server.basic_response_server(request_timeout=1)\n\n        with server as address:\n            sock = socket.socket()\n            sock.connect(address)\n            time.sleep(1.5)\n            sock.sendall(b\"hehehe, not received\")\n            sock.close()\n\n        assert server.handler_results[0] == b\"\"\n\n    def test_request_recovery_with_bigger_timeout(self):\n        \"\"\"a biggest timeout can be specified\"\"\"\n        server = Server.basic_response_server(request_timeout=3)\n        data = b\"bananadine\"\n\n        with server as address:\n            sock = socket.socket()\n            sock.connect(address)\n            time.sleep(1.5)\n            sock.sendall(data)\n            sock.close()\n\n        assert server.handler_results[0] == data\n\n    def test_server_finishes_on_error(self):\n        \"\"\"the server thread exits even if an exception exits the context manager\"\"\"\n        server = Server.basic_response_server()\n        with pytest.raises(Exception):\n            with server:\n                raise Exception()\n\n        assert len(server.handler_results) == 0\n\n        # if the server thread fails to finish, the test suite will hang\n        # and get killed by the jenkins timeout.\n\n    def test_server_finishes_when_no_connections(self):\n        \"\"\"the server thread exits even if there are no connections\"\"\"\n        server = Server.basic_response_server()\n        with server:\n            pass\n\n        assert len(server.handler_results) == 0\n\n        # if the server thread fails to finish, the test suite will hang\n        # and get killed by the jenkins timeout.\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: requests\nTest File Path: requests\\test_utils\\test_utils.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"\nrequests.utils\n~~~~~~~~~~~~~~\n\nThis module provides utility functions that are used within Requests\nthat are also useful for external consumption.\n\"\"\"\n\nimport codecs\nimport contextlib\nimport io\nimport os\nimport re\nimport socket\nimport struct\nimport sys\nimport tempfile\nimport warnings\nimport zipfile\nfrom collections import OrderedDict\n\nfrom urllib3.util import make_headers, parse_url\n\nfrom . import certs\nfrom .__version__ import __version__\n\n# to_native_string is unused here, but imported here for backwards compatibility\nfrom ._internal_utils import (  # noqa: F401\n    _HEADER_VALIDATORS_BYTE,\n    _HEADER_VALIDATORS_STR,\n    HEADER_VALIDATORS,\n    to_native_string,\n)\nfrom .compat import (\n    Mapping,\n    basestring,\n    bytes,\n    getproxies,\n    getproxies_environment,\n    integer_types,\n    is_urllib3_1,\n)\nfrom .compat import parse_http_list as _parse_list_header\nfrom .compat import (\n    proxy_bypass,\n    proxy_bypass_environment,\n    quote,\n    str,\n    unquote,\n    urlparse,\n    urlunparse,\n)\nfrom .cookies import cookiejar_from_dict\nfrom .exceptions import (\n    FileModeWarning,\n    InvalidHeader,\n    InvalidURL,\n    UnrewindableBodyError,\n)\nfrom .structures import CaseInsensitiveDict\n\nNETRC_FILES = (\".netrc\", \"_netrc\")\n\nDEFAULT_CA_BUNDLE_PATH = certs.where()\n\nDEFAULT_PORTS = {\"http\": 80, \"https\": 443}\n\n# Ensure that ', ' is used to preserve previous delimiter behavior.\nDEFAULT_ACCEPT_ENCODING = \", \".join(\n    re.split(r\",\\s*\", make_headers(accept_encoding=True)[\"accept-encoding\"])\n)\n\n\nif sys.platform == \"win32\":\n    # provide a proxy_bypass version on Windows without DNS lookups\n\n    def proxy_bypass_registry(host):\n        try:\n            import winreg\n        except ImportError:\n            return False\n\n        try:\n            internetSettings = winreg.OpenKey(\n                winreg.HKEY_CURRENT_USER,\n                r\"Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\",\n            )\n            # ProxyEnable could be REG_SZ or REG_DWORD, normalizing it\n            proxyEnable = int(winreg.QueryValueEx(internetSettings, \"ProxyEnable\")[0])\n            # ProxyOverride is almost always a string\n            proxyOverride = winreg.QueryValueEx(internetSettings, \"ProxyOverride\")[0]\n        except (OSError, ValueError):\n            return False\n        if not proxyEnable or not proxyOverride:\n            return False\n\n        # make a check value list from the registry entry: replace the\n        # '<local>' string by the localhost entry and the corresponding\n        # canonical entry.\n        proxyOverride = proxyOverride.split(\";\")\n        # filter out empty strings to avoid re.match return true in the following code.\n        proxyOverride = filter(None, proxyOverride)\n        # now check if we match one of the registry values.\n        for test in proxyOverride:\n            if test == \"<local>\":\n                if \".\" not in host:\n                    return True\n            test = test.replace(\".\", r\"\\.\")  # mask dots\n            test = test.replace(\"*\", r\".*\")  # change glob sequence\n            test = test.replace(\"?\", r\".\")  # change glob char\n            if re.match(test, host, re.I):\n                return True\n        return False\n\n    def proxy_bypass(host):  # noqa\n        \"\"\"Return True, if the host should be bypassed.\n\n        Checks proxy settings gathered from the environment, if specified,\n        or the registry.\n        \"\"\"\n        if getproxies_environment():\n            return proxy_bypass_environment(host)\n        else:\n            return proxy_bypass_registry(host)\n\n\ndef dict_to_sequence(d):\n    \"\"\"Returns an internal sequence dictionary update.\"\"\"\n\n    if hasattr(d, \"items\"):\n        d = d.items()\n\n    return d\n\n\ndef super_len(o):\n    total_length = None\n    current_position = 0\n\n    if not is_urllib3_1 and isinstance(o, str):\n        # urllib3 2.x+ treats all strings as utf-8 instead\n        # of latin-1 (iso-8859-1) like http.client.\n        o = o.encode(\"utf-8\")\n\n    if hasattr(o, \"__len__\"):\n        total_length = len(o)\n\n    elif hasattr(o, \"len\"):\n        total_length = o.len\n\n    elif hasattr(o, \"fileno\"):\n        try:\n            fileno = o.fileno()\n        except (io.UnsupportedOperation, AttributeError):\n            # AttributeError is a surprising exception, seeing as how we've just checked\n            # that `hasattr(o, 'fileno')`.  It happens for objects obtained via\n            # `Tarfile.extractfile()`, per issue 5229.\n            pass\n        else:\n            total_length = os.fstat(fileno).st_size\n\n            # Having used fstat to determine the file length, we need to\n            # confirm that this file was opened up in binary mode.\n            if \"b\" not in o.mode:\n                warnings.warn(\n                    (\n                        \"Requests has determined the content-length for this \"\n                        \"request using the binary size of the file: however, the \"\n                        \"file has been opened in text mode (i.e. without the 'b' \"\n                        \"flag in the mode). This may lead to an incorrect \"\n                        \"content-length. In Requests 3.0, support will be removed \"\n                        \"for files in text mode.\"\n                    ),\n                    FileModeWarning,\n                )\n\n    if hasattr(o, \"tell\"):\n        try:\n            current_position = o.tell()\n        except OSError:\n            # This can happen in some weird situations, such as when the file\n            # is actually a special file descriptor like stdin. In this\n            # instance, we don't know what the length is, so set it to zero and\n            # let requests chunk it instead.\n            if total_length is not None:\n                current_position = total_length\n        else:\n            if hasattr(o, \"seek\") and total_length is None:\n                # StringIO and BytesIO have seek but no usable fileno\n                try:\n                    # seek to end of file\n                    o.seek(0, 2)\n                    total_length = o.tell()\n\n                    # seek back to current position to support\n                    # partially read file-like objects\n                    o.seek(current_position or 0)\n                except OSError:\n                    total_length = 0\n\n    if total_length is None:\n        total_length = 0\n\n    return max(0, total_length - current_position)\n\n\ndef get_netrc_auth(url, raise_errors=False):\n    \"\"\"Returns the Requests tuple auth for a given url from netrc.\"\"\"\n\n    netrc_file = os.environ.get(\"NETRC\")\n    if netrc_file is not None:\n        netrc_locations = (netrc_file,)\n    else:\n        netrc_locations = (f\"~/{f}\" for f in NETRC_FILES)\n\n    try:\n        from netrc import NetrcParseError, netrc\n\n        netrc_path = None\n\n        for f in netrc_locations:\n            try:\n                loc = os.path.expanduser(f)\n            except KeyError:\n                # os.path.expanduser can fail when $HOME is undefined and\n                # getpwuid fails. See https://bugs.python.org/issue20164 &\n                # https://github.com/psf/requests/issues/1846\n                return\n\n            if os.path.exists(loc):\n                netrc_path = loc\n                break\n\n        # Abort early if there isn't one.\n        if netrc_path is None:\n            return\n\n        ri = urlparse(url)\n\n        # Strip port numbers from netloc. This weird `if...encode`` dance is\n        # used for Python 3.2, which doesn't support unicode literals.\n        splitstr = b\":\"\n        if isinstance(url, str):\n            splitstr = splitstr.decode(\"ascii\")\n        host = ri.netloc.split(splitstr)[0]\n\n        try:\n            _netrc = netrc(netrc_path).authenticators(host)\n            if _netrc:\n                # Return with login / password\n                login_i = 0 if _netrc[0] else 1\n                return (_netrc[login_i], _netrc[2])\n        except (NetrcParseError, OSError):\n            # If there was a parsing error or a permissions issue reading the file,\n            # we'll just skip netrc auth unless explicitly asked to raise errors.\n            if raise_errors:\n                raise\n\n    # App Engine hackiness.\n    except (ImportError, AttributeError):\n        pass\n\n\ndef guess_filename(obj):\n    \"\"\"Tries to guess the filename of the given object.\"\"\"\n    name = getattr(obj, \"name\", None)\n    if name and isinstance(name, basestring) and name[0] != \"<\" and name[-1] != \">\":\n        return os.path.basename(name)\n\n\ndef extract_zipped_paths(path):\n    \"\"\"Replace nonexistent paths that look like they refer to a member of a zip\n    archive with the location of an extracted copy of the target, or else\n    just return the provided path unchanged.\n    \"\"\"\n    if os.path.exists(path):\n        # this is already a valid path, no need to do anything further\n        return path\n\n    # find the first valid part of the provided path and treat that as a zip archive\n    # assume the rest of the path is the name of a member in the archive\n    archive, member = os.path.split(path)\n    while archive and not os.path.exists(archive):\n        archive, prefix = os.path.split(archive)\n        if not prefix:\n            # If we don't check for an empty prefix after the split (in other words, archive remains unchanged after the split),\n            # we _can_ end up in an infinite loop on a rare corner case affecting a small number of users\n            break\n        member = \"/\".join([prefix, member])\n\n    if not zipfile.is_zipfile(archive):\n        return path\n\n    zip_file = zipfile.ZipFile(archive)\n    if member not in zip_file.namelist():\n        return path\n\n    # we have a valid zip archive and a valid member of that archive\n    tmp = tempfile.gettempdir()\n    extracted_path = os.path.join(tmp, member.split(\"/\")[-1])\n    if not os.path.exists(extracted_path):\n        # use read + write to avoid the creating nested folders, we only want the file, avoids mkdir racing condition\n        with atomic_open(extracted_path) as file_handler:\n            file_handler.write(zip_file.read(member))\n    return extracted_path\n\n\n@contextlib.contextmanager\ndef atomic_open(filename):\n    \"\"\"Write a file to the disk in an atomic fashion\"\"\"\n    tmp_descriptor, tmp_name = tempfile.mkstemp(dir=os.path.dirname(filename))\n    try:\n        with os.fdopen(tmp_descriptor, \"wb\") as tmp_handler:\n            yield tmp_handler\n        os.replace(tmp_name, filename)\n    except BaseException:\n        os.remove(tmp_name)\n        raise\n\n\ndef from_key_val_list(value):\n    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. Unless it can not be represented as such, return an\n    OrderedDict, e.g.,\n\n    ::\n\n        >>> from_key_val_list([('key', 'val')])\n        OrderedDict([('key', 'val')])\n        >>> from_key_val_list('string')\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot encode objects that are not 2-tuples\n        >>> from_key_val_list({'key': 'val'})\n        OrderedDict([('key', 'val')])\n\n    :rtype: OrderedDict\n    \"\"\"\n    if value is None:\n        return None\n\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError(\"cannot encode objects that are not 2-tuples\")\n\n    return OrderedDict(value)\n\n\ndef to_key_val_list(value):\n    \"\"\"Take an object and test to see if it can be represented as a\n    dictionary. If it can be, return a list of tuples, e.g.,\n\n    ::\n\n        >>> to_key_val_list([('key', 'val')])\n        [('key', 'val')]\n        >>> to_key_val_list({'key': 'val'})\n        [('key', 'val')]\n        >>> to_key_val_list('string')\n        Traceback (most recent call last):\n        ...\n        ValueError: cannot encode objects that are not 2-tuples\n\n    :rtype: list\n    \"\"\"\n    if value is None:\n        return None\n\n    if isinstance(value, (str, bytes, bool, int)):\n        raise ValueError(\"cannot encode objects that are not 2-tuples\")\n\n    if isinstance(value, Mapping):\n        value = value.items()\n\n    return list(value)\n\n\n# From mitsuhiko/werkzeug (used with permission).\ndef parse_list_header(value):\n    \"\"\"Parse lists as described by RFC 2068 Section 2.\n\n    In particular, parse comma-separated lists where the elements of\n    the list may include quoted-strings.  A quoted-string could\n    contain a comma.  A non-quoted string could have quotes in the\n    middle.  Quotes are removed automatically after parsing.\n\n    It basically works like :func:`parse_set_header` just that items\n    may appear multiple times and case sensitivity is preserved.\n\n    The return value is a standard :class:`list`:\n\n    >>> parse_list_header('token, \"quoted value\"')\n    ['token', 'quoted value']\n\n    To create a header from the :class:`list` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a list header.\n    :return: :class:`list`\n    :rtype: list\n    \"\"\"\n    result = []\n    for item in _parse_list_header(value):\n        if item[:1] == item[-1:] == '\"':\n            item = unquote_header_value(item[1:-1])\n        result.append(item)\n    return result\n\n\n# From mitsuhiko/werkzeug (used with permission).\ndef parse_dict_header(value):\n    \"\"\"Parse lists of key, value pairs as described by RFC 2068 Section 2 and\n    convert them into a python dict:\n\n    >>> d = parse_dict_header('foo=\"is a fish\", bar=\"as well\"')\n    >>> type(d) is dict\n    True\n    >>> sorted(d.items())\n    [('bar', 'as well'), ('foo', 'is a fish')]\n\n    If there is no value for a key it will be `None`:\n\n    >>> parse_dict_header('key_without_value')\n    {'key_without_value': None}\n\n    To create a header from the :class:`dict` again, use the\n    :func:`dump_header` function.\n\n    :param value: a string with a dict header.\n    :return: :class:`dict`\n    :rtype: dict\n    \"\"\"\n    result = {}\n    for item in _parse_list_header(value):\n        if \"=\" not in item:\n            result[item] = None\n            continue\n        name, value = item.split(\"=\", 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result\n\n\n# From mitsuhiko/werkzeug (used with permission).\ndef unquote_header_value(value, is_filename=False):\n    r\"\"\"Unquotes a header value.  (Reversal of :func:`quote_header_value`).\n    This does not use the real unquoting but what browsers are actually\n    using for quoting.\n\n    :param value: the header value to unquote.\n    :rtype: str\n    \"\"\"\n    if value and value[0] == value[-1] == '\"':\n        # this is not the real unquoting, but fixing this so that the\n        # RFC is met will result in bugs with internet explorer and\n        # probably some other browsers as well.  IE for example is\n        # uploading files with \"C:\\foo\\bar.txt\" as filename\n        value = value[1:-1]\n\n        # if this is a filename and the starting characters look like\n        # a UNC path, then just return the value without quotes.  Using the\n        # replace sequence below on a UNC path has the effect of turning\n        # the leading double slash into a single slash and then\n        # _fix_ie_filename() doesn't work correctly.  See #458.\n        if not is_filename or value[:2] != \"\\\\\\\\\":\n            return value.replace(\"\\\\\\\\\", \"\\\\\").replace('\\\\\"', '\"')\n    return value\n\n\ndef dict_from_cookiejar(cj):\n    \"\"\"Returns a key/value dictionary from a CookieJar.\n\n    :param cj: CookieJar object to extract cookies from.\n    :rtype: dict\n    \"\"\"\n\n    cookie_dict = {cookie.name: cookie.value for cookie in cj}\n    return cookie_dict\n\n\ndef add_dict_to_cookiejar(cj, cookie_dict):\n    \"\"\"Returns a CookieJar from a key/value dictionary.\n\n    :param cj: CookieJar to insert cookies into.\n    :param cookie_dict: Dict of key/values to insert into CookieJar.\n    :rtype: CookieJar\n    \"\"\"\n\n    return cookiejar_from_dict(cookie_dict, cj)\n\n\ndef get_encodings_from_content(content):\n    \"\"\"Returns encodings from given content string.\n\n    :param content: bytestring to extract encodings from.\n    \"\"\"\n    warnings.warn(\n        (\n            \"In requests 3.0, get_encodings_from_content will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n\n    charset_re = re.compile(r'<meta.*?charset=[\"\\']*(.+?)[\"\\'>]', flags=re.I)\n    pragma_re = re.compile(r'<meta.*?content=[\"\\']*;?charset=(.+?)[\"\\'>]', flags=re.I)\n    xml_re = re.compile(r'^<\\?xml.*?encoding=[\"\\']*(.+?)[\"\\'>]')\n\n    return (\n        charset_re.findall(content)\n        + pragma_re.findall(content)\n        + xml_re.findall(content)\n    )\n\n\ndef _parse_content_type_header(header):\n    \"\"\"Returns content type and parameters from given header\n\n    :param header: string\n    :return: tuple containing content type and dictionary of\n         parameters\n    \"\"\"\n\n    tokens = header.split(\";\")\n    content_type, params = tokens[0].strip(), tokens[1:]\n    params_dict = {}\n    items_to_strip = \"\\\"' \"\n\n    for param in params:\n        param = param.strip()\n        if param:\n            key, value = param, True\n            index_of_equals = param.find(\"=\")\n            if index_of_equals != -1:\n                key = param[:index_of_equals].strip(items_to_strip)\n                value = param[index_of_equals + 1 :].strip(items_to_strip)\n            params_dict[key.lower()] = value\n    return content_type, params_dict\n\n\ndef get_encoding_from_headers(headers):\n    \"\"\"Returns encodings from given HTTP Header Dict.\n\n    :param headers: dictionary to extract encoding from.\n    :rtype: str\n    \"\"\"\n\n    content_type = headers.get(\"content-type\")\n\n    if not content_type:\n        return None\n\n    content_type, params = _parse_content_type_header(content_type)\n\n    if \"charset\" in params:\n        return params[\"charset\"].strip(\"'\\\"\")\n\n    if \"text\" in content_type:\n        return \"ISO-8859-1\"\n\n    if \"application/json\" in content_type:\n        # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset\n        return \"utf-8\"\n\n\ndef stream_decode_response_unicode(iterator, r):\n    \"\"\"Stream decodes an iterator.\"\"\"\n\n    if r.encoding is None:\n        yield from iterator\n        return\n\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors=\"replace\")\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b\"\", final=True)\n    if rv:\n        yield rv\n\n\ndef iter_slices(string, slice_length):\n    \"\"\"Iterate over slices of a string.\"\"\"\n    pos = 0\n    if slice_length is None or slice_length <= 0:\n        slice_length = len(string)\n    while pos < len(string):\n        yield string[pos : pos + slice_length]\n        pos += slice_length\n\n\ndef get_unicode_from_response(r):\n    \"\"\"Returns the requested content back in unicode.\n\n    :param r: Response object to get unicode content from.\n\n    Tried:\n\n    1. charset from content-type\n    2. fall back and replace all unicode characters\n\n    :rtype: str\n    \"\"\"\n    warnings.warn(\n        (\n            \"In requests 3.0, get_unicode_from_response will be removed. For \"\n            \"more information, please see the discussion on issue #2266. (This\"\n            \" warning should only appear once.)\"\n        ),\n        DeprecationWarning,\n    )\n\n    tried_encodings = []\n\n    # Try charset from content-type\n    encoding = get_encoding_from_headers(r.headers)\n\n    if encoding:\n        try:\n            return str(r.content, encoding)\n        except UnicodeError:\n            tried_encodings.append(encoding)\n\n    # Fall back:\n    try:\n        return str(r.content, encoding, errors=\"replace\")\n    except TypeError:\n        return r.content\n\n\n# The unreserved URI characters (RFC 3986)\nUNRESERVED_SET = frozenset(\n    \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\" + \"0123456789-._~\"\n)\n\n\ndef unquote_unreserved(uri):\n    \"\"\"Un-escape any percent-escape sequences in a URI that are unreserved\n    characters. This leaves all reserved, illegal and non-ASCII bytes encoded.\n\n    :rtype: str\n    \"\"\"\n    parts = uri.split(\"%\")\n    for i in range(1, len(parts)):\n        h = parts[i][0:2]\n        if len(h) == 2 and h.isalnum():\n            try:\n                c = chr(int(h, 16))\n            except ValueError:\n                raise InvalidURL(f\"Invalid percent-escape sequence: '{h}'\")\n\n            if c in UNRESERVED_SET:\n                parts[i] = c + parts[i][2:]\n            else:\n                parts[i] = f\"%{parts[i]}\"\n        else:\n            parts[i] = f\"%{parts[i]}\"\n    return \"\".join(parts)\n\n\ndef requote_uri(uri):\n    \"\"\"Re-quote the given URI.\n\n    This function passes the given URI through an unquote/quote cycle to\n    ensure that it is fully and consistently quoted.\n\n    :rtype: str\n    \"\"\"\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        # Unquote only the unreserved characters\n        # Then quote only illegal characters (do not quote reserved,\n        # unreserved, or '%')\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        # We couldn't unquote the given URI, so let's try quoting it, but\n        # there may be unquoted '%'s in the URI. We need to make sure they're\n        # properly quoted so they do not cause issues elsewhere.\n        return quote(uri, safe=safe_without_percent)\n\n\ndef address_in_network(ip, net):\n    \"\"\"This function allows you to check if an IP belongs to a network subnet\n\n    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24\n             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24\n\n    :rtype: bool\n    \"\"\"\n    ipaddr = struct.unpack(\"=L\", socket.inet_aton(ip))[0]\n    netaddr, bits = net.split(\"/\")\n    netmask = struct.unpack(\"=L\", socket.inet_aton(dotted_netmask(int(bits))))[0]\n    network = struct.unpack(\"=L\", socket.inet_aton(netaddr))[0] & netmask\n    return (ipaddr & netmask) == (network & netmask)\n\n\ndef dotted_netmask(mask):\n    \"\"\"Converts mask from /xx format to xxx.xxx.xxx.xxx\n\n    Example: if mask is 24 function returns 255.255.255.0\n\n    :rtype: str\n    \"\"\"\n    bits = 0xFFFFFFFF ^ (1 << 32 - mask) - 1\n    return socket.inet_ntoa(struct.pack(\">I\", bits))\n\n\ndef is_ipv4_address(string_ip):\n    \"\"\"\n    :rtype: bool\n    \"\"\"\n    try:\n        socket.inet_aton(string_ip)\n    except OSError:\n        return False\n    return True\n\n\ndef is_valid_cidr(string_network):\n    \"\"\"\n    Very simple check of the cidr format in no_proxy variable.\n\n    :rtype: bool\n    \"\"\"\n    if string_network.count(\"/\") == 1:\n        try:\n            mask = int(string_network.split(\"/\")[1])\n        except ValueError:\n            return False\n\n        if mask < 1 or mask > 32:\n            return False\n\n        try:\n            socket.inet_aton(string_network.split(\"/\")[0])\n        except OSError:\n            return False\n    else:\n        return False\n    return True\n\n\n@contextlib.contextmanager\ndef set_environ(env_name, value):\n    \"\"\"Set the environment variable 'env_name' to 'value'\n\n    Save previous value, yield, and then restore the previous value stored in\n    the environment variable 'env_name'.\n\n    If 'value' is None, do nothing\"\"\"\n    value_changed = value is not None\n    if value_changed:\n        old_value = os.environ.get(env_name)\n        os.environ[env_name] = value\n    try:\n        yield\n    finally:\n        if value_changed:\n            if old_value is None:\n                del os.environ[env_name]\n            else:\n                os.environ[env_name] = old_value\n\n\ndef should_bypass_proxies(url, no_proxy):\n    \"\"\"\n    Returns whether we should bypass proxies or not.\n\n    :rtype: bool\n    \"\"\"\n\n    # Prioritize lowercase environment variables over uppercase\n    # to keep a consistent behaviour with other http projects (curl, wget).\n    def get_proxy(key):\n        return os.environ.get(key) or os.environ.get(key.upper())\n\n    # First check whether no_proxy is defined. If it is, check that the URL\n    # we're getting isn't in the no_proxy list.\n    no_proxy_arg = no_proxy\n    if no_proxy is None:\n        no_proxy = get_proxy(\"no_proxy\")\n    parsed = urlparse(url)\n\n    if parsed.hostname is None:\n        # URLs don't always have hostnames, e.g. file:/// urls.\n        return True\n\n    if no_proxy:\n        # We need to check whether we match here. We need to see if we match\n        # the end of the hostname, both with and without the port.\n        no_proxy = (host for host in no_proxy.replace(\" \", \"\").split(\",\") if host)\n\n        if is_ipv4_address(parsed.hostname):\n            for proxy_ip in no_proxy:\n                if is_valid_cidr(proxy_ip):\n                    if address_in_network(parsed.hostname, proxy_ip):\n                        return True\n                elif parsed.hostname == proxy_ip:\n                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &\n                    # matches the IP of the index\n                    return True\n        else:\n            host_with_port = parsed.hostname\n            if parsed.port:\n                host_with_port += f\":{parsed.port}\"\n\n            for host in no_proxy:\n                if parsed.hostname.endswith(host) or host_with_port.endswith(host):\n                    # The URL does match something in no_proxy, so we don't want\n                    # to apply the proxies on this URL.\n                    return True\n\n    with set_environ(\"no_proxy\", no_proxy_arg):\n        # parsed.hostname can be `None` in cases such as a file URI.\n        try:\n            bypass = proxy_bypass(parsed.hostname)\n        except (TypeError, socket.gaierror):\n            bypass = False\n\n    if bypass:\n        return True\n\n    return False\n\n\ndef get_environ_proxies(url, no_proxy=None):\n    \"\"\"\n    Return a dict of environment proxies.\n\n    :rtype: dict\n    \"\"\"\n    if should_bypass_proxies(url, no_proxy=no_proxy):\n        return {}\n    else:\n        return getproxies()\n\n\ndef select_proxy(url, proxies):\n    \"\"\"Select a proxy for the url, if applicable.\n\n    :param url: The url being for the request\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    \"\"\"\n    proxies = proxies or {}\n    urlparts = urlparse(url)\n    if urlparts.hostname is None:\n        return proxies.get(urlparts.scheme, proxies.get(\"all\"))\n\n    proxy_keys = [\n        urlparts.scheme + \"://\" + urlparts.hostname,\n        urlparts.scheme,\n        \"all://\" + urlparts.hostname,\n        \"all\",\n    ]\n    proxy = None\n    for proxy_key in proxy_keys:\n        if proxy_key in proxies:\n            proxy = proxies[proxy_key]\n            break\n\n    return proxy\n\n\ndef resolve_proxies(request, proxies, trust_env=True):\n    \"\"\"This method takes proxy information from a request and configuration\n    input to resolve a mapping of target proxies. This will consider settings\n    such as NO_PROXY to strip proxy configurations.\n\n    :param request: Request or PreparedRequest\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    :param trust_env: Boolean declaring whether to trust environment configs\n\n    :rtype: dict\n    \"\"\"\n    proxies = proxies if proxies is not None else {}\n    url = request.url\n    scheme = urlparse(url).scheme\n    no_proxy = proxies.get(\"no_proxy\")\n    new_proxies = proxies.copy()\n\n    if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):\n        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n\n        proxy = environ_proxies.get(scheme, environ_proxies.get(\"all\"))\n\n        if proxy:\n            new_proxies.setdefault(scheme, proxy)\n    return new_proxies\n\n\ndef default_user_agent(name=\"python-requests\"):\n    \"\"\"\n    Return a string representing the default user agent.\n\n    :rtype: str\n    \"\"\"\n    return f\"{name}/{__version__}\"\n\n\ndef default_headers():\n    \"\"\"\n    :rtype: requests.structures.CaseInsensitiveDict\n    \"\"\"\n    return CaseInsensitiveDict(\n        {\n            \"User-Agent\": default_user_agent(),\n            \"Accept-Encoding\": DEFAULT_ACCEPT_ENCODING,\n            \"Accept\": \"*/*\",\n            \"Connection\": \"keep-alive\",\n        }\n    )\n\n\ndef parse_header_links(value):\n    \"\"\"Return a list of parsed link headers proxies.\n\n    i.e. Link: <http:/.../front.jpeg>; rel=front; type=\"image/jpeg\",<http://.../back.jpeg>; rel=back;type=\"image/jpeg\"\n\n    :rtype: list\n    \"\"\"\n\n    links = []\n\n    replace_chars = \" '\\\"\"\n\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n\n    for val in re.split(\", *<\", value):\n        try:\n            url, params = val.split(\";\", 1)\n        except ValueError:\n            url, params = val, \"\"\n\n        link = {\"url\": url.strip(\"<> '\\\"\")}\n\n        for param in params.split(\";\"):\n            try:\n                key, value = param.split(\"=\")\n            except ValueError:\n                break\n\n            link[key.strip(replace_chars)] = value.strip(replace_chars)\n\n        links.append(link)\n\n    return links\n\n\n# Null bytes; no need to recreate these on each call to guess_json_utf\n_null = \"\\x00\".encode(\"ascii\")  # encoding to ASCII for Python 3\n_null2 = _null * 2\n_null3 = _null * 3\n\n\ndef guess_json_utf(data):\n    \"\"\"\n    :rtype: str\n    \"\"\"\n    # JSON always starts with two ASCII characters, so detection is as\n    # easy as counting the nulls and from their location and count\n    # determine the encoding. Also detect a BOM, if present.\n    sample = data[:4]\n    if sample in (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):\n        return \"utf-32\"  # BOM included\n    if sample[:3] == codecs.BOM_UTF8:\n        return \"utf-8-sig\"  # BOM included, MS style (discouraged)\n    if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):\n        return \"utf-16\"  # BOM included\n    nullcount = sample.count(_null)\n    if nullcount == 0:\n        return \"utf-8\"\n    if nullcount == 2:\n        if sample[::2] == _null2:  # 1st and 3rd are null\n            return \"utf-16-be\"\n        if sample[1::2] == _null2:  # 2nd and 4th are null\n            return \"utf-16-le\"\n        # Did not detect 2 valid UTF-16 ascii-range characters\n    if nullcount == 3:\n        if sample[:3] == _null3:\n            return \"utf-32-be\"\n        if sample[1:] == _null3:\n            return \"utf-32-le\"\n        # Did not detect a valid UTF-32 ascii-range character\n    return None\n\n\ndef prepend_scheme_if_needed(url, new_scheme):\n    \"\"\"Given a URL that may or may not have a scheme, prepend the given scheme.\n    Does not replace a present scheme with the one provided as an argument.\n\n    :rtype: str\n    \"\"\"\n    parsed = parse_url(url)\n    scheme, auth, host, port, path, query, fragment = parsed\n\n    # A defect in urlparse determines that there isn't a netloc present in some\n    # urls. We previously assumed parsing was overly cautious, and swapped the\n    # netloc and path. Due to a lack of tests on the original defect, this is\n    # maintained with parse_url for backwards compatibility.\n    netloc = parsed.netloc\n    if not netloc:\n        netloc, path = path, netloc\n\n    if auth:\n        # parse_url doesn't provide the netloc with auth\n        # so we'll add it ourselves.\n        netloc = \"@\".join([auth, netloc])\n    if scheme is None:\n        scheme = new_scheme\n    if path is None:\n        path = \"\"\n\n    return urlunparse((scheme, netloc, path, \"\", query, fragment))\n\n\ndef get_auth_from_url(url):\n    \"\"\"Given a url with authentication components, extract them into a tuple of\n    username,password.\n\n    :rtype: (str,str)\n    \"\"\"\n    parsed = urlparse(url)\n\n    try:\n        auth = (unquote(parsed.username), unquote(parsed.password))\n    except (AttributeError, TypeError):\n        auth = (\"\", \"\")\n\n    return auth\n\n\ndef check_header_validity(header):\n    \"\"\"Verifies that header parts don't contain leading whitespace\n    reserved characters, or return characters.\n\n    :param header: tuple, in the format (name, value).\n    \"\"\"\n    name, value = header\n    _validate_header_part(header, name, 0)\n    _validate_header_part(header, value, 1)\n\n\ndef _validate_header_part(header, header_part, header_validator_index):\n    if isinstance(header_part, str):\n        validator = _HEADER_VALIDATORS_STR[header_validator_index]\n    elif isinstance(header_part, bytes):\n        validator = _HEADER_VALIDATORS_BYTE[header_validator_index]\n    else:\n        raise InvalidHeader(\n            f\"Header part ({header_part!r}) from {header} \"\n            f\"must be of type str or bytes, not {type(header_part)}\"\n        )\n\n    if not validator.match(header_part):\n        header_kind = \"name\" if header_validator_index == 0 else \"value\"\n        raise InvalidHeader(\n            f\"Invalid leading whitespace, reserved character(s), or return \"\n            f\"character(s) in header {header_kind}: {header_part!r}\"\n        )\n\n\ndef urldefragauth(url):\n    \"\"\"\n    Given a url remove the fragment and the authentication part.\n\n    :rtype: str\n    \"\"\"\n    scheme, netloc, path, params, query, fragment = urlparse(url)\n\n    # see func:`prepend_scheme_if_needed`\n    if not netloc:\n        netloc, path = path, netloc\n\n    netloc = netloc.rsplit(\"@\", 1)[-1]\n\n    return urlunparse((scheme, netloc, path, params, query, \"\"))\n\n\ndef rewind_body(prepared_request):\n    \"\"\"Move file pointer back to its recorded starting position\n    so it can be read again on redirect.\n    \"\"\"\n    body_seek = getattr(prepared_request.body, \"seek\", None)\n    if body_seek is not None and isinstance(\n        prepared_request._body_position, integer_types\n    ):\n        try:\n            body_seek(prepared_request._body_position)\n        except OSError:\n            raise UnrewindableBodyError(\n                \"An error occurred when rewinding request body for redirect.\"\n            )\n    else:\n        raise UnrewindableBodyError(\"Unable to rewind request body for redirect.\")\n\n### Source File Dependency Files Content\n### Dependency File: certs.py\n#!/usr/bin/env python\n\n\"\"\"\nrequests.certs\n~~~~~~~~~~~~~~\n\nThis module returns the preferred default CA certificate bundle. There is\nonly one \u2014 the one from the certifi package.\n\nIf you are packaging Requests, e.g., for a Linux distribution or a managed\nenvironment, you can change the definition of where() to return a separately\npackaged CA bundle.\n\"\"\"\nfrom certifi import where\n\nif __name__ == \"__main__\":\n    print(where())\n\n\n### Dependency File: compat.py\n\"\"\"\nrequests.compat\n~~~~~~~~~~~~~~~\n\nThis module previously handled import compatibility issues\nbetween Python 2 and Python 3. It remains for backwards\ncompatibility until the next major version.\n\"\"\"\n\nimport importlib\nimport sys\n\n# -------\n# urllib3\n# -------\nfrom urllib3 import __version__ as urllib3_version\n\n# Detect which major version of urllib3 is being used.\ntry:\n    is_urllib3_1 = int(urllib3_version.split(\".\")[0]) == 1\nexcept (TypeError, AttributeError):\n    # If we can't discern a version, prefer old functionality.\n    is_urllib3_1 = True\n\n# -------------------\n# Character Detection\n# -------------------\n\n\ndef _resolve_char_detection():\n    \"\"\"Find supported character detection libraries.\"\"\"\n    chardet = None\n    for lib in (\"chardet\", \"charset_normalizer\"):\n        if chardet is None:\n            try:\n                chardet = importlib.import_module(lib)\n            except ImportError:\n                pass\n    return chardet\n\n\nchardet = _resolve_char_detection()\n\n# -------\n# Pythons\n# -------\n\n# Syntax sugar.\n_ver = sys.version_info\n\n#: Python 2.x?\nis_py2 = _ver[0] == 2\n\n#: Python 3.x?\nis_py3 = _ver[0] == 3\n\n# json/simplejson module import resolution\nhas_simplejson = False\ntry:\n    import simplejson as json\n\n    has_simplejson = True\nexcept ImportError:\n    import json\n\nif has_simplejson:\n    from simplejson import JSONDecodeError\nelse:\n    from json import JSONDecodeError\n\n# Keep OrderedDict for backwards compatibility.\nfrom collections import OrderedDict\nfrom collections.abc import Callable, Mapping, MutableMapping\nfrom http import cookiejar as cookielib\nfrom http.cookies import Morsel\nfrom io import StringIO\n\n# --------------\n# Legacy Imports\n# --------------\nfrom urllib.parse import (\n    quote,\n    quote_plus,\n    unquote,\n    unquote_plus,\n    urldefrag,\n    urlencode,\n    urljoin,\n    urlparse,\n    urlsplit,\n    urlunparse,\n)\nfrom urllib.request import (\n    getproxies,\n    getproxies_environment,\n    parse_http_list,\n    proxy_bypass,\n    proxy_bypass_environment,\n)\n\nbuiltin_str = str\nstr = str\nbytes = bytes\nbasestring = (str, bytes)\nnumeric_types = (int, float)\ninteger_types = (int,)\n\n\n### Dependency File: cookies.py\n\"\"\"\nrequests.cookies\n~~~~~~~~~~~~~~~~\n\nCompatibility code to be able to use `http.cookiejar.CookieJar` with requests.\n\nrequests.utils imports from here, so be careful with imports.\n\"\"\"\n\nimport calendar\nimport copy\nimport time\n\nfrom ._internal_utils import to_native_string\nfrom .compat import Morsel, MutableMapping, cookielib, urlparse, urlunparse\n\ntry:\n    import threading\nexcept ImportError:\n    import dummy_threading as threading\n\n\nclass MockRequest:\n    \"\"\"Wraps a `requests.Request` to mimic a `urllib2.Request`.\n\n    The code in `http.cookiejar.CookieJar` expects this interface in order to correctly\n    manage cookie policies, i.e., determine whether a cookie can be set, given the\n    domains of the request and the cookie.\n\n    The original request object is read-only. The client is responsible for collecting\n    the new headers via `get_new_headers()` and interpreting them appropriately. You\n    probably want `get_cookie_header`, defined below.\n    \"\"\"\n\n    def __init__(self, request):\n        self._r = request\n        self._new_headers = {}\n        self.type = urlparse(self._r.url).scheme\n\n    def get_type(self):\n        return self.type\n\n    def get_host(self):\n        return urlparse(self._r.url).netloc\n\n    def get_origin_req_host(self):\n        return self.get_host()\n\n    def get_full_url(self):\n        # Only return the response's URL if the user hadn't set the Host\n        # header\n        if not self._r.headers.get(\"Host\"):\n            return self._r.url\n        # If they did set it, retrieve it and reconstruct the expected domain\n        host = to_native_string(self._r.headers[\"Host\"], encoding=\"utf-8\")\n        parsed = urlparse(self._r.url)\n        # Reconstruct the URL as we expect it\n        return urlunparse(\n            [\n                parsed.scheme,\n                host,\n                parsed.path,\n                parsed.params,\n                parsed.query,\n                parsed.fragment,\n            ]\n        )\n\n    def is_unverifiable(self):\n        return True\n\n    def has_header(self, name):\n        return name in self._r.headers or name in self._new_headers\n\n    def get_header(self, name, default=None):\n        return self._r.headers.get(name, self._new_headers.get(name, default))\n\n    def add_header(self, key, val):\n        \"\"\"cookiejar has no legitimate use for this method; add it back if you find one.\"\"\"\n        raise NotImplementedError(\n            \"Cookie headers should be added with add_unredirected_header()\"\n        )\n\n    def add_unredirected_header(self, name, value):\n        self._new_headers[name] = value\n\n    def get_new_headers(self):\n        return self._new_headers\n\n    @property\n    def unverifiable(self):\n        return self.is_unverifiable()\n\n    @property\n    def origin_req_host(self):\n        return self.get_origin_req_host()\n\n    @property\n    def host(self):\n        return self.get_host()\n\n\nclass MockResponse:\n    \"\"\"Wraps a `httplib.HTTPMessage` to mimic a `urllib.addinfourl`.\n\n    ...what? Basically, expose the parsed HTTP headers from the server response\n    the way `http.cookiejar` expects to see them.\n    \"\"\"\n\n    def __init__(self, headers):\n        \"\"\"Make a MockResponse for `cookiejar` to read.\n\n        :param headers: a httplib.HTTPMessage or analogous carrying the headers\n        \"\"\"\n        self._headers = headers\n\n    def info(self):\n        return self._headers\n\n    def getheaders(self, name):\n        self._headers.getheaders(name)\n\n\ndef extract_cookies_to_jar(jar, request, response):\n    \"\"\"Extract the cookies from the response into a CookieJar.\n\n    :param jar: http.cookiejar.CookieJar (not necessarily a RequestsCookieJar)\n    :param request: our own requests.Request object\n    :param response: urllib3.HTTPResponse object\n    \"\"\"\n    if not (hasattr(response, \"_original_response\") and response._original_response):\n        return\n    # the _original_response field is the wrapped httplib.HTTPResponse object,\n    req = MockRequest(request)\n    # pull out the HTTPMessage with the headers and put it in the mock:\n    res = MockResponse(response._original_response.msg)\n    jar.extract_cookies(res, req)\n\n\ndef get_cookie_header(jar, request):\n    \"\"\"\n    Produce an appropriate Cookie header string to be sent with `request`, or None.\n\n    :rtype: str\n    \"\"\"\n    r = MockRequest(request)\n    jar.add_cookie_header(r)\n    return r.get_new_headers().get(\"Cookie\")\n\n\ndef remove_cookie_by_name(cookiejar, name, domain=None, path=None):\n    \"\"\"Unsets a cookie by name, by default over all domains and paths.\n\n    Wraps CookieJar.clear(), is O(n).\n    \"\"\"\n    clearables = []\n    for cookie in cookiejar:\n        if cookie.name != name:\n            continue\n        if domain is not None and domain != cookie.domain:\n            continue\n        if path is not None and path != cookie.path:\n            continue\n        clearables.append((cookie.domain, cookie.path, cookie.name))\n\n    for domain, path, name in clearables:\n        cookiejar.clear(domain, path, name)\n\n\nclass CookieConflictError(RuntimeError):\n    \"\"\"There are two cookies that meet the criteria specified in the cookie jar.\n    Use .get and .set and include domain and path args in order to be more specific.\n    \"\"\"\n\n\nclass RequestsCookieJar(cookielib.CookieJar, MutableMapping):\n    \"\"\"Compatibility class; is a http.cookiejar.CookieJar, but exposes a dict\n    interface.\n\n    This is the CookieJar we create by default for requests and sessions that\n    don't specify one, since some clients may expect response.cookies and\n    session.cookies to support dict operations.\n\n    Requests does not use the dict interface internally; it's just for\n    compatibility with external client code. All requests code should work\n    out of the box with externally provided instances of ``CookieJar``, e.g.\n    ``LWPCookieJar`` and ``FileCookieJar``.\n\n    Unlike a regular CookieJar, this class is pickleable.\n\n    .. warning:: dictionary operations that are normally O(1) may be O(n).\n    \"\"\"\n\n    def get(self, name, default=None, domain=None, path=None):\n        \"\"\"Dict-like get() that also supports optional domain and path args in\n        order to resolve naming collisions from using one cookie jar over\n        multiple domains.\n\n        .. warning:: operation is O(n), not O(1).\n        \"\"\"\n        try:\n            return self._find_no_duplicates(name, domain, path)\n        except KeyError:\n            return default\n\n    def set(self, name, value, **kwargs):\n        \"\"\"Dict-like set() that also supports optional domain and path args in\n        order to resolve naming collisions from using one cookie jar over\n        multiple domains.\n        \"\"\"\n        # support client code that unsets cookies by assignment of a None value:\n        if value is None:\n            remove_cookie_by_name(\n                self, name, domain=kwargs.get(\"domain\"), path=kwargs.get(\"path\")\n            )\n            return\n\n        if isinstance(value, Morsel):\n            c = morsel_to_cookie(value)\n        else:\n            c = create_cookie(name, value, **kwargs)\n        self.set_cookie(c)\n        return c\n\n    def iterkeys(self):\n        \"\"\"Dict-like iterkeys() that returns an iterator of names of cookies\n        from the jar.\n\n        .. seealso:: itervalues() and iteritems().\n        \"\"\"\n        for cookie in iter(self):\n            yield cookie.name\n\n    def keys(self):\n        \"\"\"Dict-like keys() that returns a list of names of cookies from the\n        jar.\n\n        .. seealso:: values() and items().\n        \"\"\"\n        return list(self.iterkeys())\n\n    def itervalues(self):\n        \"\"\"Dict-like itervalues() that returns an iterator of values of cookies\n        from the jar.\n\n        .. seealso:: iterkeys() and iteritems().\n        \"\"\"\n        for cookie in iter(self):\n            yield cookie.value\n\n    def values(self):\n        \"\"\"Dict-like values() that returns a list of values of cookies from the\n        jar.\n\n        .. seealso:: keys() and items().\n        \"\"\"\n        return list(self.itervalues())\n\n    def iteritems(self):\n        \"\"\"Dict-like iteritems() that returns an iterator of name-value tuples\n        from the jar.\n\n        .. seealso:: iterkeys() and itervalues().\n        \"\"\"\n        for cookie in iter(self):\n            yield cookie.name, cookie.value\n\n    def items(self):\n        \"\"\"Dict-like items() that returns a list of name-value tuples from the\n        jar. Allows client-code to call ``dict(RequestsCookieJar)`` and get a\n        vanilla python dict of key value pairs.\n\n        .. seealso:: keys() and values().\n        \"\"\"\n        return list(self.iteritems())\n\n    def list_domains(self):\n        \"\"\"Utility method to list all the domains in the jar.\"\"\"\n        domains = []\n        for cookie in iter(self):\n            if cookie.domain not in domains:\n                domains.append(cookie.domain)\n        return domains\n\n    def list_paths(self):\n        \"\"\"Utility method to list all the paths in the jar.\"\"\"\n        paths = []\n        for cookie in iter(self):\n            if cookie.path not in paths:\n                paths.append(cookie.path)\n        return paths\n\n    def multiple_domains(self):\n        \"\"\"Returns True if there are multiple domains in the jar.\n        Returns False otherwise.\n\n        :rtype: bool\n        \"\"\"\n        domains = []\n        for cookie in iter(self):\n            if cookie.domain is not None and cookie.domain in domains:\n                return True\n            domains.append(cookie.domain)\n        return False  # there is only one domain in jar\n\n    def get_dict(self, domain=None, path=None):\n        \"\"\"Takes as an argument an optional domain and path and returns a plain\n        old Python dict of name-value pairs of cookies that meet the\n        requirements.\n\n        :rtype: dict\n        \"\"\"\n        dictionary = {}\n        for cookie in iter(self):\n            if (domain is None or cookie.domain == domain) and (\n                path is None or cookie.path == path\n            ):\n                dictionary[cookie.name] = cookie.value\n        return dictionary\n\n    def __contains__(self, name):\n        try:\n            return super().__contains__(name)\n        except CookieConflictError:\n            return True\n\n    def __getitem__(self, name):\n        \"\"\"Dict-like __getitem__() for compatibility with client code. Throws\n        exception if there are more than one cookie with name. In that case,\n        use the more explicit get() method instead.\n\n        .. warning:: operation is O(n), not O(1).\n        \"\"\"\n        return self._find_no_duplicates(name)\n\n    def __setitem__(self, name, value):\n        \"\"\"Dict-like __setitem__ for compatibility with client code. Throws\n        exception if there is already a cookie of that name in the jar. In that\n        case, use the more explicit set() method instead.\n        \"\"\"\n        self.set(name, value)\n\n    def __delitem__(self, name):\n        \"\"\"Deletes a cookie given a name. Wraps ``http.cookiejar.CookieJar``'s\n        ``remove_cookie_by_name()``.\n        \"\"\"\n        remove_cookie_by_name(self, name)\n\n    def set_cookie(self, cookie, *args, **kwargs):\n        if (\n            hasattr(cookie.value, \"startswith\")\n            and cookie.value.startswith('\"')\n            and cookie.value.endswith('\"')\n        ):\n            cookie.value = cookie.value.replace('\\\\\"', \"\")\n        return super().set_cookie(cookie, *args, **kwargs)\n\n    def update(self, other):\n        \"\"\"Updates this jar with cookies from another CookieJar or dict-like\"\"\"\n        if isinstance(other, cookielib.CookieJar):\n            for cookie in other:\n                self.set_cookie(copy.copy(cookie))\n        else:\n            super().update(other)\n\n    def _find(self, name, domain=None, path=None):\n        \"\"\"Requests uses this method internally to get cookie values.\n\n        If there are conflicting cookies, _find arbitrarily chooses one.\n        See _find_no_duplicates if you want an exception thrown if there are\n        conflicting cookies.\n\n        :param name: a string containing name of cookie\n        :param domain: (optional) string containing domain of cookie\n        :param path: (optional) string containing path of cookie\n        :return: cookie.value\n        \"\"\"\n        for cookie in iter(self):\n            if cookie.name == name:\n                if domain is None or cookie.domain == domain:\n                    if path is None or cookie.path == path:\n                        return cookie.value\n\n        raise KeyError(f\"name={name!r}, domain={domain!r}, path={path!r}\")\n\n    def _find_no_duplicates(self, name, domain=None, path=None):\n        \"\"\"Both ``__get_item__`` and ``get`` call this function: it's never\n        used elsewhere in Requests.\n\n        :param name: a string containing name of cookie\n        :param domain: (optional) string containing domain of cookie\n        :param path: (optional) string containing path of cookie\n        :raises KeyError: if cookie is not found\n        :raises CookieConflictError: if there are multiple cookies\n            that match name and optionally domain and path\n        :return: cookie.value\n        \"\"\"\n        toReturn = None\n        for cookie in iter(self):\n            if cookie.name == name:\n                if domain is None or cookie.domain == domain:\n                    if path is None or cookie.path == path:\n                        if toReturn is not None:\n                            # if there are multiple cookies that meet passed in criteria\n                            raise CookieConflictError(\n                                f\"There are multiple cookies with name, {name!r}\"\n                            )\n                        # we will eventually return this as long as no cookie conflict\n                        toReturn = cookie.value\n\n        if toReturn:\n            return toReturn\n        raise KeyError(f\"name={name!r}, domain={domain!r}, path={path!r}\")\n\n    def __getstate__(self):\n        \"\"\"Unlike a normal CookieJar, this class is pickleable.\"\"\"\n        state = self.__dict__.copy()\n        # remove the unpickleable RLock object\n        state.pop(\"_cookies_lock\")\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Unlike a normal CookieJar, this class is pickleable.\"\"\"\n        self.__dict__.update(state)\n        if \"_cookies_lock\" not in self.__dict__:\n            self._cookies_lock = threading.RLock()\n\n    def copy(self):\n        \"\"\"Return a copy of this RequestsCookieJar.\"\"\"\n        new_cj = RequestsCookieJar()\n        new_cj.set_policy(self.get_policy())\n        new_cj.update(self)\n        return new_cj\n\n    def get_policy(self):\n        \"\"\"Return the CookiePolicy instance used.\"\"\"\n        return self._policy\n\n\ndef _copy_cookie_jar(jar):\n    if jar is None:\n        return None\n\n    if hasattr(jar, \"copy\"):\n        # We're dealing with an instance of RequestsCookieJar\n        return jar.copy()\n    # We're dealing with a generic CookieJar instance\n    new_jar = copy.copy(jar)\n    new_jar.clear()\n    for cookie in jar:\n        new_jar.set_cookie(copy.copy(cookie))\n    return new_jar\n\n\ndef create_cookie(name, value, **kwargs):\n    \"\"\"Make a cookie from underspecified parameters.\n\n    By default, the pair of `name` and `value` will be set for the domain ''\n    and sent on every request (this is sometimes called a \"supercookie\").\n    \"\"\"\n    result = {\n        \"version\": 0,\n        \"name\": name,\n        \"value\": value,\n        \"port\": None,\n        \"domain\": \"\",\n        \"path\": \"/\",\n        \"secure\": False,\n        \"expires\": None,\n        \"discard\": True,\n        \"comment\": None,\n        \"comment_url\": None,\n        \"rest\": {\"HttpOnly\": None},\n        \"rfc2109\": False,\n    }\n\n    badargs = set(kwargs) - set(result)\n    if badargs:\n        raise TypeError(\n            f\"create_cookie() got unexpected keyword arguments: {list(badargs)}\"\n        )\n\n    result.update(kwargs)\n    result[\"port_specified\"] = bool(result[\"port\"])\n    result[\"domain_specified\"] = bool(result[\"domain\"])\n    result[\"domain_initial_dot\"] = result[\"domain\"].startswith(\".\")\n    result[\"path_specified\"] = bool(result[\"path\"])\n\n    return cookielib.Cookie(**result)\n\n\ndef morsel_to_cookie(morsel):\n    \"\"\"Convert a Morsel object into a Cookie containing the one k/v pair.\"\"\"\n\n    expires = None\n    if morsel[\"max-age\"]:\n        try:\n            expires = int(time.time() + int(morsel[\"max-age\"]))\n        except ValueError:\n            raise TypeError(f\"max-age: {morsel['max-age']} must be integer\")\n    elif morsel[\"expires\"]:\n        time_template = \"%a, %d-%b-%Y %H:%M:%S GMT\"\n        expires = calendar.timegm(time.strptime(morsel[\"expires\"], time_template))\n    return create_cookie(\n        comment=morsel[\"comment\"],\n        comment_url=bool(morsel[\"comment\"]),\n        discard=False,\n        domain=morsel[\"domain\"],\n        expires=expires,\n        name=morsel.key,\n        path=morsel[\"path\"],\n        port=None,\n        rest={\"HttpOnly\": morsel[\"httponly\"]},\n        rfc2109=False,\n        secure=bool(morsel[\"secure\"]),\n        value=morsel.value,\n        version=morsel[\"version\"] or 0,\n    )\n\n\ndef cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):\n    \"\"\"Returns a CookieJar from a key/value dictionary.\n\n    :param cookie_dict: Dict of key/values to insert into CookieJar.\n    :param cookiejar: (optional) A cookiejar to add the cookies to.\n    :param overwrite: (optional) If False, will not replace cookies\n        already in the jar with new ones.\n    :rtype: CookieJar\n    \"\"\"\n    if cookiejar is None:\n        cookiejar = RequestsCookieJar()\n\n    if cookie_dict is not None:\n        names_from_jar = [cookie.name for cookie in cookiejar]\n        for name in cookie_dict:\n            if overwrite or (name not in names_from_jar):\n                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))\n\n    return cookiejar\n\n\ndef merge_cookies(cookiejar, cookies):\n    \"\"\"Add cookies to cookiejar and returns a merged CookieJar.\n\n    :param cookiejar: CookieJar object to add the cookies to.\n    :param cookies: Dictionary or CookieJar object to be added.\n    :rtype: CookieJar\n    \"\"\"\n    if not isinstance(cookiejar, cookielib.CookieJar):\n        raise ValueError(\"You can only merge into CookieJar\")\n\n    if isinstance(cookies, dict):\n        cookiejar = cookiejar_from_dict(cookies, cookiejar=cookiejar, overwrite=False)\n    elif isinstance(cookies, cookielib.CookieJar):\n        try:\n            cookiejar.update(cookies)\n        except AttributeError:\n            for cookie_in_jar in cookies:\n                cookiejar.set_cookie(cookie_in_jar)\n\n    return cookiejar\n\n\n### Dependency File: exceptions.py\n\"\"\"\nrequests.exceptions\n~~~~~~~~~~~~~~~~~~~\n\nThis module contains the set of Requests' exceptions.\n\"\"\"\nfrom urllib3.exceptions import HTTPError as BaseHTTPError\n\nfrom .compat import JSONDecodeError as CompatJSONDecodeError\n\n\nclass RequestException(IOError):\n    \"\"\"There was an ambiguous exception that occurred while handling your\n    request.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize RequestException with `request` and `response` objects.\"\"\"\n        response = kwargs.pop(\"response\", None)\n        self.response = response\n        self.request = kwargs.pop(\"request\", None)\n        if response is not None and not self.request and hasattr(response, \"request\"):\n            self.request = self.response.request\n        super().__init__(*args, **kwargs)\n\n\nclass InvalidJSONError(RequestException):\n    \"\"\"A JSON error occurred.\"\"\"\n\n\nclass JSONDecodeError(InvalidJSONError, CompatJSONDecodeError):\n    \"\"\"Couldn't decode the text into json\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Construct the JSONDecodeError instance first with all\n        args. Then use it's args to construct the IOError so that\n        the json specific args aren't used as IOError specific args\n        and the error message from JSONDecodeError is preserved.\n        \"\"\"\n        CompatJSONDecodeError.__init__(self, *args)\n        InvalidJSONError.__init__(self, *self.args, **kwargs)\n\n    def __reduce__(self):\n        \"\"\"\n        The __reduce__ method called when pickling the object must\n        be the one from the JSONDecodeError (be it json/simplejson)\n        as it expects all the arguments for instantiation, not just\n        one like the IOError, and the MRO would by default call the\n        __reduce__ method from the IOError due to the inheritance order.\n        \"\"\"\n        return CompatJSONDecodeError.__reduce__(self)\n\n\nclass HTTPError(RequestException):\n    \"\"\"An HTTP error occurred.\"\"\"\n\n\nclass ConnectionError(RequestException):\n    \"\"\"A Connection error occurred.\"\"\"\n\n\nclass ProxyError(ConnectionError):\n    \"\"\"A proxy error occurred.\"\"\"\n\n\nclass SSLError(ConnectionError):\n    \"\"\"An SSL error occurred.\"\"\"\n\n\nclass Timeout(RequestException):\n    \"\"\"The request timed out.\n\n    Catching this error will catch both\n    :exc:`~requests.exceptions.ConnectTimeout` and\n    :exc:`~requests.exceptions.ReadTimeout` errors.\n    \"\"\"\n\n\nclass ConnectTimeout(ConnectionError, Timeout):\n    \"\"\"The request timed out while trying to connect to the remote server.\n\n    Requests that produced this error are safe to retry.\n    \"\"\"\n\n\nclass ReadTimeout(Timeout):\n    \"\"\"The server did not send any data in the allotted amount of time.\"\"\"\n\n\nclass URLRequired(RequestException):\n    \"\"\"A valid URL is required to make a request.\"\"\"\n\n\nclass TooManyRedirects(RequestException):\n    \"\"\"Too many redirects.\"\"\"\n\n\nclass MissingSchema(RequestException, ValueError):\n    \"\"\"The URL scheme (e.g. http or https) is missing.\"\"\"\n\n\nclass InvalidSchema(RequestException, ValueError):\n    \"\"\"The URL scheme provided is either invalid or unsupported.\"\"\"\n\n\nclass InvalidURL(RequestException, ValueError):\n    \"\"\"The URL provided was somehow invalid.\"\"\"\n\n\nclass InvalidHeader(RequestException, ValueError):\n    \"\"\"The header value provided was somehow invalid.\"\"\"\n\n\nclass InvalidProxyURL(InvalidURL):\n    \"\"\"The proxy URL provided is invalid.\"\"\"\n\n\nclass ChunkedEncodingError(RequestException):\n    \"\"\"The server declared chunked encoding but sent an invalid chunk.\"\"\"\n\n\nclass ContentDecodingError(RequestException, BaseHTTPError):\n    \"\"\"Failed to decode response content.\"\"\"\n\n\nclass StreamConsumedError(RequestException, TypeError):\n    \"\"\"The content for this response was already consumed.\"\"\"\n\n\nclass RetryError(RequestException):\n    \"\"\"Custom retries logic failed\"\"\"\n\n\nclass UnrewindableBodyError(RequestException):\n    \"\"\"Requests encountered an error when trying to rewind a body.\"\"\"\n\n\n# Warnings\n\n\nclass RequestsWarning(Warning):\n    \"\"\"Base warning for Requests.\"\"\"\n\n\nclass FileModeWarning(RequestsWarning, DeprecationWarning):\n    \"\"\"A file was opened in text mode, but Requests determined its binary length.\"\"\"\n\n\nclass RequestsDependencyWarning(RequestsWarning):\n    \"\"\"An imported dependency doesn't match the expected version range.\"\"\"\n\n\n### Dependency File: structures.py\n\"\"\"\nrequests.structures\n~~~~~~~~~~~~~~~~~~~\n\nData structures that power Requests.\n\"\"\"\n\nfrom collections import OrderedDict\n\nfrom .compat import Mapping, MutableMapping\n\n\nclass CaseInsensitiveDict(MutableMapping):\n    \"\"\"A case-insensitive ``dict``-like object.\n\n    Implements all methods and operations of\n    ``MutableMapping`` as well as dict's ``copy``. Also\n    provides ``lower_items``.\n\n    All keys are expected to be strings. The structure remembers the\n    case of the last key to be set, and ``iter(instance)``,\n    ``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``\n    will contain case-sensitive keys. However, querying and contains\n    testing is case insensitive::\n\n        cid = CaseInsensitiveDict()\n        cid['Accept'] = 'application/json'\n        cid['aCCEPT'] == 'application/json'  # True\n        list(cid) == ['Accept']  # True\n\n    For example, ``headers['content-encoding']`` will return the\n    value of a ``'Content-Encoding'`` response header, regardless\n    of how the header name was originally stored.\n\n    If the constructor, ``.update``, or equality comparison\n    operations are given keys that have equal ``.lower()``s, the\n    behavior is undefined.\n    \"\"\"\n\n    def __init__(self, data=None, **kwargs):\n        self._store = OrderedDict()\n        if data is None:\n            data = {}\n        self.update(data, **kwargs)\n\n    def __setitem__(self, key, value):\n        # Use the lowercased key for lookups, but store the actual\n        # key alongside the value.\n        self._store[key.lower()] = (key, value)\n\n    def __getitem__(self, key):\n        return self._store[key.lower()][1]\n\n    def __delitem__(self, key):\n        del self._store[key.lower()]\n\n    def __iter__(self):\n        return (casedkey for casedkey, mappedvalue in self._store.values())\n\n    def __len__(self):\n        return len(self._store)\n\n    def lower_items(self):\n        \"\"\"Like iteritems(), but with all lowercase keys.\"\"\"\n        return ((lowerkey, keyval[1]) for (lowerkey, keyval) in self._store.items())\n\n    def __eq__(self, other):\n        if isinstance(other, Mapping):\n            other = CaseInsensitiveDict(other)\n        else:\n            return NotImplemented\n        # Compare insensitively\n        return dict(self.lower_items()) == dict(other.lower_items())\n\n    # Copy is required\n    def copy(self):\n        return CaseInsensitiveDict(self._store.values())\n\n    def __repr__(self):\n        return str(dict(self.items()))\n\n\nclass LookupDict(dict):\n    \"\"\"Dictionary lookup object.\"\"\"\n\n    def __init__(self, name=None):\n        self.name = name\n        super().__init__()\n\n    def __repr__(self):\n        return f\"<lookup '{self.name}'>\"\n\n    def __getitem__(self, key):\n        # We allow fall-through here, so values default to None\n\n        return self.__dict__.get(key, None)\n\n    def get(self, key, default=None):\n        return self.__dict__.get(key, default)\n\n\n### Dependency File: _internal_utils.py\n\"\"\"\nrequests._internal_utils\n~~~~~~~~~~~~~~\n\nProvides utility functions that are consumed internally by Requests\nwhich depend on extremely few external helpers (such as compat)\n\"\"\"\nimport re\n\nfrom .compat import builtin_str\n\n_VALID_HEADER_NAME_RE_BYTE = re.compile(rb\"^[^:\\s][^:\\r\\n]*$\")\n_VALID_HEADER_NAME_RE_STR = re.compile(r\"^[^:\\s][^:\\r\\n]*$\")\n_VALID_HEADER_VALUE_RE_BYTE = re.compile(rb\"^\\S[^\\r\\n]*$|^$\")\n_VALID_HEADER_VALUE_RE_STR = re.compile(r\"^\\S[^\\r\\n]*$|^$\")\n\n_HEADER_VALIDATORS_STR = (_VALID_HEADER_NAME_RE_STR, _VALID_HEADER_VALUE_RE_STR)\n_HEADER_VALIDATORS_BYTE = (_VALID_HEADER_NAME_RE_BYTE, _VALID_HEADER_VALUE_RE_BYTE)\nHEADER_VALIDATORS = {\n    bytes: _HEADER_VALIDATORS_BYTE,\n    str: _HEADER_VALIDATORS_STR,\n}\n\n\ndef to_native_string(string, encoding=\"ascii\"):\n    \"\"\"Given a string object, regardless of type, returns a representation of\n    that string in the native string type, encoding and decoding where\n    necessary. This assumes ASCII unless told otherwise.\n    \"\"\"\n    if isinstance(string, builtin_str):\n        out = string\n    else:\n        out = string.decode(encoding)\n\n    return out\n\n\ndef unicode_is_ascii(u_string):\n    \"\"\"Determine if unicode string only contains ASCII characters.\n\n    :param str u_string: unicode string to check. Must be unicode\n        and not Python 2 `str`.\n    :rtype: bool\n    \"\"\"\n    assert isinstance(u_string, str)\n    try:\n        u_string.encode(\"ascii\")\n        return True\n    except UnicodeEncodeError:\n        return False\n\n\n### Dependency File: __version__.py\n# .-. .-. .-. . . .-. .-. .-. .-.\n# |(  |-  |.| | | |-  `-.  |  `-.\n# ' ' `-' `-`.`-' `-' `-'  '  `-'\n\n__title__ = \"requests\"\n__description__ = \"Python HTTP for Humans.\"\n__url__ = \"https://requests.readthedocs.io\"\n__version__ = \"2.32.3\"\n__build__ = 0x023203\n__author__ = \"Kenneth Reitz\"\n__author_email__ = \"me@kennethreitz.org\"\n__license__ = \"Apache-2.0\"\n__copyright__ = \"Copyright Kenneth Reitz\"\n__cake__ = \"\\u2728 \\U0001f370 \\u2728\"\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:13:32"}, {"role": "assistant", "content": "```python\nimport copy\nimport filecmp\nimport os\nimport tarfile\nimport zipfile\nfrom collections import deque\nfrom io import BytesIO\nfrom unittest import mock\n\nimport pytest\n\nfrom requests import compat\nfrom requests._internal_utils import unicode_is_ascii\nfrom requests.cookies import RequestsCookieJar\nfrom requests.structures import CaseInsensitiveDict\nfrom requests.utils import (\n    _parse_content_type_header,\n    add_dict_to_cookiejar,\n    address_in_network,\n    dotted_netmask,\n    extract_zipped_paths,\n    get_auth_from_url,\n    get_encoding_from_headers,\n    get_encodings_from_content,\n    get_environ_proxies,\n    guess_filename,\n    guess_json_utf,\n    is_ipv4_address,\n    is_valid_cidr,\n    iter_slices,\n    parse_dict_header,\n    parse_header_links,\n    prepend_scheme_if_needed,\n    requote_uri,\n    select_proxy,\n    set_environ,\n    should_bypass_proxies,\n    super_len,\n    to_key_val_list,\n    to_native_string,\n    unquote_header_value,\n    unquote_unreserved,\n    urldefragauth,\n)\n\nfrom .compat import StringIO, cStringIO\n\n\nclass TestSuperLen:\n    @pytest.mark.parametrize(\n        \"stream, value\",\n        (\n            (StringIO.StringIO, \"Test\"),\n            (BytesIO, b\"Test\"),\n            pytest.param(\n                cStringIO, \"Test\", marks=pytest.mark.skipif(\"cStringIO is None\")\n            ),\n        ),\n    )\n    def test_io_streams(self, stream, value):\n        \"\"\"Ensures that we properly deal with different kinds of IO streams.\"\"\"\n        assert super_len(stream()) == 0\n        assert super_len(stream(value)) == 4\n\n    def test_super_len_correctly_calculates_len_of_partially_read_file(self):\n        \"\"\"Ensure that we handle partially consumed file like objects.\"\"\"\n        s = StringIO.StringIO()\n        s.write(\"foobarbogus\")\n        assert super_len(s) == 0\n\n    @pytest.mark.parametrize(\"error\", [IOError, OSError])\n    def test_super_len_handles_files_raising_weird_errors_in_tell(self, error):\n        \"\"\"If tell() raises errors, assume the cursor is at position zero.\"\"\"\n\n        class BoomFile:\n            def __len__(self):\n                return 5\n\n            def tell(self):\n                raise error()\n\n        assert super_len(BoomFile()) == 0\n\n    @pytest.mark.parametrize(\"error\", [IOError, OSError])\n    def test_super_len_tell_ioerror(self, error):\n        \"\"\"Ensure that if tell gives an IOError super_len doesn't fail\"\"\"\n\n        class NoLenBoomFile:\n            def tell(self):\n                raise error()\n\n            def seek(self, offset, whence):\n                pass\n\n        assert super_len(NoLenBoomFile()) == 0\n\n    def test_string(self):\n        assert super_len(\"Test\") == 4\n\n    @pytest.mark.parametrize(\n        \"mode, warnings_num\",\n        (\n            (\"r\", 1),\n            (\"rb\", 0),\n        ),\n    )\n    def test_file(self, tmpdir, mode, warnings_num, recwarn):\n        file_obj = tmpdir.join(\"test.txt\")\n        file_obj.write(\"Test\")\n        with file_obj.open(mode) as fd:\n            assert super_len(fd) == 4\n        assert len(recwarn) == warnings_num\n\n    def test_tarfile_member(self, tmpdir):\n        file_obj = tmpdir.join(\"test.txt\")\n        file_obj.write(\"Test\")\n\n        tar_obj = str(tmpdir.join(\"test.tar\"))\n        with tarfile.open(tar_obj, \"w\") as tar:\n            tar.add(str(file_obj), arcname=\"test.txt\")\n\n        with tarfile.open(tar_obj) as tar:\n            member = tar.extractfile(\"test.txt\")\n            assert super_len(member) == 4\n\n    def test_super_len_with__len__(self):\n        foo = [1, 2, 3, 4]\n        len_foo = super_len(foo)\n        assert len_foo == 4\n\n    def test_super_len_with_no__len__(self):\n        class LenFile:\n            def __init__(self):\n                self.len = 5\n\n        assert super_len(LenFile()) == 5\n\n    def test_super_len_with_tell(self):\n        foo = StringIO.StringIO(\"12345\")\n        assert super_len(foo) == 5\n        foo.read(2)\n        assert super_len(foo) == 3\n\n    def test_super_len_with_fileno(self):\n        with open(__file__, \"rb\") as f:\n            length = super_len(f)\n            file_data = f.read()\n        assert length == len(file_data)\n\n    def test_super_len_with_no_matches(self):\n        \"\"\"Ensure that objects without any length methods default to 0\"\"\"\n        assert super_len(object()) == 0\n\n\nclass TestToKeyValList:\n    @pytest.mark.parametrize(\n        \"value, expected\",\n        (\n            ([(\"key\", \"val\")], [(\"key\", \"val\")]),\n            (((\"key\", \"val\"),), [(\"key\", \"val\")]),\n            ({\"key\": \"val\"}, [(\"key\", \"val\")]),\n            (None, None),\n        ),\n    )\n    def test_valid(self, value, expected):\n        assert to_key_val_list(value) == expected\n\n    def test_invalid(self):\n        with pytest.raises(ValueError):\n            to_key_val_list(\"string\")\n\n\nclass TestUnquoteHeaderValue:\n    @pytest.mark.parametrize(\n        \"value, expected\",\n        (\n            (None, None),\n            (\"Test\", \"Test\"),\n            ('\"Test\"', \"Test\"),\n            ('\"Test\\\\\\\\\"', \"Test\\\\\"),\n            ('\"\\\\\\\\Comp\\\\Res\"', \"\\\\Comp\\\\Res\"),\n        ),\n    )\n    def test_valid(self, value, expected):\n        assert unquote_header_value(value) == expected\n\n    def test_is_filename(self):\n        assert unquote_header_value('\"\\\\\\\\Comp\\\\Res\"', True) == \"\\\\\\\\Comp\\\\Res\"\n\n\nclass TestGetEnvironProxies:\n    \"\"\"Ensures that IP addresses are correctly matches with ranges\n    in no_proxy variable.\n    \"\"\"\n\n    @pytest.fixture(autouse=True, params=[\"no_proxy\", \"NO_PROXY\"])\n    def no_proxy(self, request, monkeypatch):\n        monkeypatch.setenv(\n            request.param, \"192.168.0.0/24,127.0.0.1,localhost.localdomain,172.16.1.1\"\n        )\n\n    @pytest.mark.parametrize(\n        \"url\",\n        (\n            \"http://192.168.0.1:5000/\",\n            \"http://192.168.0.1/\",\n            \"http://172.16.1.1/\",\n            \"http://172.16.1.1:5000/\",\n            \"http://localhost.localdomain:5000/v1.0/\",\n        ),\n    )\n    def test_bypass(self, url):\n        assert get_environ_proxies(url, no_proxy=None) == {}\n\n    @pytest.mark.parametrize(\n        \"url\",\n        (\n            \"http://192.168.1.1:5000/\",\n            \"http://192.168.1.1/\",\n            \"http://www.requests.com/\",\n        ),\n    )\n    def test_not_bypass(self, url):\n        assert get_environ_proxies(url, no_proxy=None) != {}\n\n    @pytest.mark.parametrize(\n        \"url\",\n        (\n            \"http://192.168.1.1:5000/\",\n            \"http://192.168.1.1/\",\n            \"http://www.requests.com/\",\n        ),\n    )\n    def test_bypass_no_proxy_keyword(self, url):\n        no_proxy = \"192.168.1.1,requests.com\"\n        assert get_environ_proxies(url, no_proxy=no_proxy) == {}\n\n    @pytest.mark.parametrize(\n        \"url\",\n        (\n            \"http://192.168.0.1:5000/\",\n            \"http://192.168.0.1/\",\n            \"http://172.16.1.1/\",\n            \"http://172.16.1.1:5000/\",\n            \"http://localhost.localdomain:5000/v1.0/\",\n        ),\n    )\n    def test_not_bypass_no_proxy_keyword(self, url, monkeypatch):\n        # This is testing that the 'no_proxy' argument overrides the\n        # environment variable 'no_proxy'\n        monkeypatch.setenv(\"http_proxy\", \"http://proxy.example.com:3128/\")\n        no_proxy = \"192.168.1.1,requests.com\"\n        assert get_environ_proxies(url, no_proxy=no_proxy) != {}\n\n\nclass TestIsIPv4Address:\n    def test_valid(self):\n        assert is_ipv4_address(\"8.8.8.8\")\n\n    @pytest.mark.parametrize(\"value\", (\"8.8.8.8.8\", \"localhost.localdomain\"))\n    def test_invalid(self, value):\n        assert not is_ipv4_address(value)\n\n\nclass TestIsValidCIDR:\n    def test_valid(self):\n        assert is_valid_cidr(\"192.168.1.0/24\")\n\n    @pytest.mark.parametrize(\n        \"value\",\n        (\n            \"8.8.8.8\",\n            \"192.168.1.0/a\",\n            \"192.168.1.0/128\",\n            \"192.168.1.0/-1\",\n            \"192.168.1.999/24\",\n        ),\n    )\n    def test_invalid(self, value):\n        assert not is_valid_cidr(value)\n\n\nclass TestAddressInNetwork:\n    def test_valid(self):\n        assert address_in_network(\"192.168.1.1\", \"192.168.1.0/24\")\n\n    def test_invalid(self):\n        assert not address_in_network(\"172.16.0.1\", \"192.168.1.0/24\")\n\n\nclass TestGuessFilename:\n    @pytest.mark.parametrize(\n        \"value\",\n        (1, type(\"Fake\", (object,), {\"name\": 1})()),\n    )\n    def test_guess_filename_invalid(self, value):\n        assert guess_filename(value) is None\n\n    @pytest.mark.parametrize(\n        \"value, expected_type\",\n        (\n            (b\"value\", compat.bytes),\n            (b\"value\".decode(\"utf-8\"), compat.str),\n        ),\n    )\n    def test_guess_filename_valid(self, value, expected_type):\n        obj = type(\"Fake\", (object,), {\"name\": value})()\n        result = guess_filename(obj)\n        assert result == value\n        assert isinstance(result, expected_type)\n\n\nclass TestExtractZippedPaths:\n    @pytest.mark.parametrize(\n        \"path\",\n        (\n            \"/\",\n            __file__,\n            pytest.__file__,\n            \"/etc/invalid/location\",\n        ),\n    )\n    def test_unzipped_paths_unchanged(self, path):\n        assert path == extract_zipped_paths(path)\n\n    def test_zipped_paths_extracted(self, tmpdir):\n        zipped_py = tmpdir.join(\"test.zip\")\n        with zipfile.ZipFile(zipped_py.strpath, \"w\") as f:\n            f.write(__file__)\n\n        _, name = os.path.splitdrive(__file__)\n        zipped_path = os.path.join(zipped_py.strpath, name.lstrip(r\"\\/\"))\n        extracted_path = extract_zipped_paths(zipped_path)\n\n        assert extracted_path != zipped_path\n        assert os.path.exists(extracted_path)\n        assert filecmp.cmp(extracted_path, __file__)\n\n    def test_invalid_unc_path(self):\n        path = r\"\\\\localhost\\invalid\\location\"\n        assert extract_zipped_paths(path) == path\n\n\nclass TestContentEncodingDetection:\n    def test_none(self):\n        encodings = get_encodings_from_content(\"\")\n        assert not len(encodings)\n\n    @pytest.mark.parametrize(\n        \"content\",\n        (\n            # HTML5 meta charset attribute\n            '<meta charset=\"UTF-8\">',\n            # HTML4 pragma directive\n            '<meta http-equiv=\"Content-type\" content=\"text/html;charset=UTF-8\">',\n            # XHTML 1.x served with text/html MIME type\n            '<meta http-equiv=\"Content-type\" content=\"text/html;charset=UTF-8\" />',\n            # XHTML 1.x served as XML\n            '<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n        ),\n    )\n    def test_pragmas(self, content):\n        encodings = get_encodings_from_content(content)\n        assert len(encodings) == 1\n        assert encodings[0] == \"UTF-8\"\n\n    def test_precedence(self):\n        content = \"\"\"\n        <?xml version=\"1.0\" encoding=\"XML\"?>\n        <meta charset=\"HTML5\">\n        <meta http-equiv=\"Content-type\" content=\"text/html;charset=HTML4\" />\n        \"\"\".strip()\n        assert get_encodings_from_content(content) == [\"HTML5\", \"HTML4\", \"XML\"]\n\n\nclass TestGuessJSONUTF:\n    @pytest.mark.parametrize(\n        \"encoding\",\n        (\n            \"utf-32\",\n            \"utf-8-sig\",\n            \"utf-16\",\n            \"utf-8\",\n            \"utf-16-be\",\n            \"utf-16-le\",\n            \"utf-32-be\",\n            \"utf-32-le\",\n        ),\n    )\n    def test_encoded(self, encoding):\n        data = \"{}\".encode(encoding)\n        assert guess_json_utf(data) == encoding\n\n    def test_bad_utf_like_encoding(self):\n        assert guess_json_utf(b\"\\x00\\x00\\x00\\x00\") is None\n\n    @pytest.mark.parametrize(\n        (\"encoding\", \"expected\"),\n        (\n            (\"utf-16-be\", \"utf-16\"),\n            (\"utf-16-le\", \"utf-16\"),\n            (\"utf-32-be\", \"utf-32\"),\n            (\"utf-32-le\", \"utf-32\"),\n        ),\n    )\n    def test_guess_by_bom(self, encoding, expected):\n        data = \"\\ufeff{}\".encode(encoding)\n        assert guess_json_utf(data) == expected\n\n\nUSER = PASSWORD = \"%!*'();:@&=+$,/?#[] \"\nENCODED_USER = compat.quote(USER, \"\")\nENCODED_PASSWORD = compat.quote(PASSWORD, \"\")\n\n\n@pytest.mark.parametrize(\n    \"url, auth\",\n    (\n        (\n            f\"http://{ENCODED_USER}:{ENCODED_PASSWORD}@request.com/url.html#test\",\n            (USER, PASSWORD),\n        ),\n        (\"http://user:pass@complex.url.com/path?query=yes\", (\"user\", \"pass\")),\n        (\n            \"http://user:pass%20pass@complex.url.com/path?query=yes\",\n            (\"user\", \"pass pass\"),\n        ),\n        (\"http://user:pass pass@complex.url.com/path?query=yes\", (\"user\", \"pass pass\")),\n        (\n            \"http://user%25user:pass@complex.url.com/path?query=yes\",\n            (\"user%user\", \"pass\"),\n        ),\n        (\n            \"http://user:pass%23pass@complex.url.com/path?query=yes\",\n            (\"user\", \"pass#pass\"),\n        ),\n        (\"http://complex.url.com/path?query=yes\", (\"\", \"\")),\n    ),\n)\ndef test_get_auth_from_url(url, auth):\n    assert get_auth_from_url(url) == auth\n\n\n@pytest.mark.parametrize(\n    \"uri, expected\",\n    (\n        (\n            # Ensure requoting doesn't break expectations\n            \"http://example.com/fiz?buz=%25ppicture\",\n            \"http://example.com/fiz?buz=%25ppicture\",\n        ),\n        (\n            # Ensure we handle unquoted percent signs in redirects\n            \"http://example.com/fiz?buz=%ppicture\",\n            \"http://example.com/fiz?buz=%25ppicture\",\n        ),\n    ),\n)\ndef test_requote_uri_with_unquoted_percents(uri, expected):\n    \"\"\"See: https://github.com/psf/requests/issues/2356\"\"\"\n    assert requote_uri(uri) == expected\n\n\n@pytest.mark.parametrize(\n    \"uri, expected\",\n    (\n        (\n            # Illegal bytes\n            \"http://example.com/?a=%--\",\n            \"http://example.com/?a=%--\",\n        ),\n        (\n            # Reserved characters\n            \"http://example.com/?a=%300\",\n            \"http://example.com/?a=00\",\n        ),\n    ),\n)\ndef test_unquote_unreserved(uri, expected):\n    assert unquote_unreserved(uri) == expected\n\n\n@pytest.mark.parametrize(\n    \"mask, expected\",\n    (\n        (8, \"255.0.0.0\"),\n        (24, \"255.255.255.0\"),\n        (25, \"255.255.255.128\"),\n    ),\n)\ndef test_dotted_netmask(mask, expected):\n    assert dotted_netmask(mask) == expected\n\n\nhttp_proxies = {\n    \"http\": \"http://http.proxy\",\n    \"http://some.host\": \"http://some.host.proxy\",\n}\nall_proxies = {\n    \"all\": \"socks5://http.proxy\",\n    \"all://some.host\": \"socks5://some.host.proxy\",\n}\nmixed_proxies = {\n    \"http\": \"http://http.proxy\",\n    \"http://some.host\": \"http://some.host.proxy\",\n    \"all\": \"socks5://http.proxy\",\n}\n\n\n@pytest.mark.parametrize(\n    \"url, expected, proxies\",\n    (\n        (\"hTTp://u:p@Some.Host/path\", \"http://some.host.proxy\", http_proxies),\n        (\"hTTp://u:p@Other.Host/path\", \"http://http.proxy\", http_proxies),\n        (\"hTTp:///path\", \"http://http.proxy\", http_proxies),\n        (\"hTTps://Other.Host\", None, http_proxies),\n        (\"file:///etc/motd\", None, http_proxies),\n        (\"hTTp://u:p@Some.Host/path\", \"socks5://some.host.proxy\", all_proxies),\n        (\"hTTp://u:p@Other.Host/path\", \"socks5://http.proxy\", all_proxies),\n        (\"hTTp:///path\", \"socks5://http.proxy\", all_proxies),\n        (\"hTTps://Other.Host\", \"socks5://http.proxy\", all_proxies),\n        (\"http://u:p@other.host/path\", \"http://http.proxy\", mixed_proxies),\n        (\"http://u:p@some.host/path\", \"http://some.host.proxy\", mixed_proxies),\n        (\"https://u:p@other.host/path\", \"socks5://http.proxy\", mixed_proxies),\n        (\"https://u:p@some.host/path\", \"socks5://http.proxy\", mixed_proxies),\n        (\"https://\", \"socks5://http.proxy\", mixed_proxies),\n        # XXX: unsure whether this is reasonable behavior\n        (\"file:///etc/motd\", \"socks5://http.proxy\", all_proxies),\n    ),\n)\ndef test_select_proxies(url, expected, proxies):\n    \"\"\"Make sure we can select per-host proxies correctly.\"\"\"\n    assert select_proxy(url, proxies) == expected\n\n\n@pytest.mark.parametrize(\n    \"value, expected\",\n    (\n        ('foo=\"is a fish\", bar=\"as well\"', {\"foo\": \"is a fish\", \"bar\": \"as well\"}),\n        (\"key_without_value\", {\"key_without_value\": None}),\n    ),\n)\ndef test_parse_dict_header(value, expected):\n    assert parse_dict_header(value) == expected\n\n\n@pytest.mark.parametrize(\n    \"value, expected\",\n    (\n        (\"application/xml\", (\"application/xml\", {})),\n        (\n            \"application/json ; charset=utf-8\",\n            (\"application/json\", {\"charset\": \"utf-8\"}),\n        ),\n        (\n            \"application/json ; Charset=utf-8\",\n            (\"application/json\", {\"charset\": \"utf-8\"}),\n        ),\n        (\"text/plain\", (\"text/plain\", {})),\n        (\n            \"multipart/form-data; boundary = something ; boundary2='something_else' ; no_equals \",\n            (\n                \"multipart/form-data\",\n                {\n                    \"boundary\": \"something\",\n                    \"boundary2\": \"something_else\",\n                    \"no_equals\": True,\n                },\n            ),\n        ),\n        (\n            'multipart/form-data; boundary = something ; boundary2=\"something_else\" ; no_equals ',\n            (\n                \"multipart/form-data\",\n                {\n                    \"boundary\": \"something\",\n                    \"boundary2\": \"something_else\",\n                    \"no_equals\": True,\n                },\n            ),\n        ),\n        (\n            \"multipart/form-data; boundary = something ; 'boundary2=something_else' ; no_equals \",\n            (\n                \"multipart/form-data\",\n                {\n                    \"boundary\": \"something\",\n                    \"boundary2\": \"something_else\",\n                    \"no_equals\": True,\n                },\n            ),\n        ),\n        (\n            'multipart/form-data; boundary = something ; \"boundary2=something_else\" ; no_equals ',\n            (\n                \"multipart/form-data\",\n                {\n                    \"boundary\": \"something\",\n                    \"boundary2\": \"something_else\",\n                    \"no_equals\": True,\n                },\n            ),\n        ),\n        (\"application/json ; ; \", (\"application/json\", {})),\n    ),\n)\ndef test__parse_content_type_header(value, expected):\n    assert _parse_content_type_header(value) == expected\n\n\n@pytest.mark.parametrize(\n    \"value, expected\",\n    (\n        (CaseInsensitiveDict(), None),\n        (\n            CaseInsensitiveDict({\"content-type\": \"application/json; charset=utf-8\"}),\n            \"utf-8\",\n        ),\n        (CaseInsensitiveDict({\"content-type\": \"text/plain\"}), \"ISO-8859-1\"),\n    ),\n)\ndef test_get_encoding_from_headers(value, expected):\n    assert get_encoding_from_headers(value) == expected\n\n\n@pytest.mark.parametrize(\n    \"value, length\",\n    (\n        (\"\", 0),\n        (\"T\", 1),\n        (\"Test\", 4),\n        (\"Cont\", 0),\n        (\"Other\", -5),\n        (\"Content\", None),\n    ),\n)\ndef test_iter_slices(value, length):\n    if length is None or (length <= 0 and len(value) > 0):\n        # Reads all content at once\n        assert len(list(iter_slices(value, length))) == 1\n    else:\n        assert len(list(iter_slices(value, 1))) == length\n\n\n@pytest.mark.parametrize(\n    \"value, expected\",\n    (\n        (\n            '<http:/.../front.jpeg>; rel=front; type=\"image/jpeg\"',\n            [{\"url\": \"http:/.../front.jpeg\", \"rel\": \"front\", \"type\": \"image/jpeg\"}],\n        ),\n        (\"<http:/.../front.jpeg>\", [{\"url\": \"http:/.../front.jpeg\"}]),\n        (\"<http:/.../front.jpeg>;\", [{\"url\": \"http:/.../front.jpeg\"}]),\n        (\n            '<http:/.../front.jpeg>; type=\"image/jpeg\",<http://.../back.jpeg>;',\n            [\n                {\"url\": \"http:/.../front.jpeg\", \"type\": \"image/jpeg\"},\n                {\"url\": \"http://.../back.jpeg\"},\n            ],\n        ),\n        (\"\", []),\n    ),\n)\ndef test_parse_header_links(value, expected):\n    assert parse_header_links(value) == expected\n\n\n@pytest.mark.parametrize(\n    \"value, expected\",\n    (\n        (\"example.com/path\", \"http://example.com/path\"),\n        (\"//example.com/path\", \"http://example.com/path\"),\n        (\"example.com:80\", \"http://example.com:80\"),\n        (\n            \"http://user:pass@example.com/path?query\",\n            \"http://user:pass@example.com/path?query\",\n        ),\n        (\"http://user@example.com/path?query\", \"http://user@example.com/path?query\"),\n    ),\n)\ndef test_prepend_scheme_if_needed(value, expected):\n    assert prepend_scheme_if_needed(value, \"http\") == expected\n\n\n@pytest.mark.parametrize(\n    \"value, expected\",\n    (\n        (\"T\", \"T\"),\n        (b\"T\", \"T\"),\n        (\"T\", \"T\"),\n    ),\n)\ndef test_to_native_string(value, expected):\n    assert to_native_string(value) == expected\n\n\n@pytest.mark.parametrize(\n    \"url, expected\",\n    (\n        (\"http://u:p@example.com/path?a=1#test\", \"http://example.com/path?a=1\"),\n        (\"http://example.com/path\", \"http://example.com/path\"),\n        (\"//u:p@example.com/path\", \"//example.com/path\"),\n        (\"//example.com/path\", \"//example.com/path\"),\n        (\"example.com/path\", \"//example.com/path\"),\n        (\"scheme:u:p@example.com/path\", \"scheme://example.com/path\"),\n    ),\n)\ndef test_urldefragauth(url, expected):\n    assert urldefragauth(url) == expected\n\n\n@pytest.mark.parametrize(\n    \"url, expected\",\n    (\n        (\"http://192.168.0.1:5000/\", True),\n        (\"http://192.168.0.1/\", True),\n        (\"http://172.16.1.1/\", True),\n        (\"http://172.16.1.1:5000/\", True),\n        (\"http://localhost.localdomain:5000/v1.0/\", True),\n        (\"http://google.com:6000/\", True),\n        (\"http://172.16.1.12/\", False),\n        (\"http://172.16.1.12:5000/\", False),\n        (\"http://google.com:5000/v1.0/\", False),\n        (\"file:///some/path/on/disk\", True),\n    ),\n)\ndef test_should_bypass_proxies(url, expected, monkeypatch):\n    \"\"\"Tests for function should_bypass_proxies to check if proxy\n    can be bypassed or not\n    \"\"\"\n    monkeypatch.setenv(\n        \"no_proxy\",\n        \"192.168.0.0/24,127.0.0.1,localhost.localdomain,172.16.1.1, google.com:6000\",\n    )\n    monkeypatch.setenv(\n        \"NO_PROXY\",\n        \"192.168.0.0/24,127.0.0.1,localhost.localdomain,172.16.1.1, google.com:6000\",\n    )\n    assert should_bypass_proxies(url, no_proxy=None) == expected\n\n\n@pytest.mark.parametrize(\n    \"url, expected\",\n    (\n        (\"http://172.16.1.1/\", \"172.16.1.1\"),\n        (\"http://172.16.1.1:5000/\", \"172.16.1.1\"),\n        (\"http://user:pass@172.16.1.1\", \"172.16.1.1\"),\n        (\"http://user:pass@172.16.1.1:5000\", \"172.16.1.1\"),\n        (\"http://hostname/\", \"hostname\"),\n        (\"http://hostname:5000/\", \"hostname\"),\n        (\"http://user:pass@hostname\", \"hostname\"),\n        (\"http://user:pass@hostname:5000\", \"hostname\"),\n    ),\n)\ndef test_should_bypass_proxies_pass_only_hostname(url, expected):\n    \"\"\"The proxy_bypass function should be called with a hostname or IP without\n    a port number or auth credentials.\n    \"\"\"\n    with mock.patch(\"requests.utils.proxy_bypass\") as proxy_bypass:\n        should_bypass_proxies(url, no_proxy=None)\n        proxy_bypass.assert_called_once_with(expected)\n\n\n@pytest.mark.parametrize(\n    \"cookiejar\",\n    (\n        compat.cookielib.CookieJar(),\n        RequestsCookieJar(),\n    ),\n)\ndef test_add_dict_to_cookiejar(cookiejar):\n    \"\"\"Ensure add_dict_to_cookiejar works for\n    non-RequestsCookieJar CookieJars\n    \"\"\"\n    cookiedict = {\"test\": \"cookies\", \"good\": \"cookies\"}\n    cj = add_dict_to_cookiejar(cookiejar, cookiedict)\n    cookies = {cookie.name: cookie.value for cookie in cj}\n    assert cookiedict == cookies\n\n\n@pytest.mark.parametrize(\n    \"value, expected\",\n    (\n        (\"test\", True),\n        (\"\u00e6\u00ed\u00f6\u00fb\", False),\n        (\"\u30b8\u30a7\u30fc\u30d4\u30fc\u30cb\u30c3\u30af\", False),\n    ),\n)\ndef test_unicode_is_ascii(value, expected):\n    assert unicode_is_ascii(value) is expected\n\n\n@pytest.mark.parametrize(\n    \"url, expected\",\n    (\n        (\"http://192.168.0.1:5000/\", True),\n        (\"http://192.168.0.1/\", True),\n        (\"http://172.16.1.1/\", True),\n        (\"http://172.16.1.1:5000/\", True),\n        (\"http://localhost.localdomain:5000/v1.0/\", True),\n        (\"http://172.16.1.12/\", False),\n        (\"http://172.16.1.12:5000/\", False),\n        (\"http://google.com:5000/v1.0/\", False),\n    ),\n)\ndef test_should_bypass_proxies_no_proxy(url, expected, monkeypatch):\n    \"\"\"Tests for function should_bypass_proxies to check if proxy\n    can be bypassed or not using the 'no_proxy' argument\n    \"\"\"\n    no_proxy = \"192.168.0.0/24,127.0.0.1,localhost.localdomain,172.16.1.1\"\n    # Test 'no_proxy' argument\n    assert should_bypass_proxies(url, no_proxy=no_proxy) == expected\n\n\n@pytest.mark.skipif(os.name != \"nt\", reason=\"Test only on Windows\")\n@pytest.mark.parametrize(\n    \"url, expected, override\",\n    (\n        (\"http://192.168.0.1:5000/\", True, None),\n        (\"http://192.168.0.1/\", True, None),\n        (\"http://172.16.1.1/\", True, None),\n        (\"http://172.16.1.1:5000/\", True, None),\n        (\"http://localhost.localdomain:5000/v1.0/\", True, None),\n        (\"http://172.16.1.22/\", False, None),\n        (\"http://172.16.1.22:5000/\", False, None),\n        (\"http://google.com:5000/v1.0/\", False, None),\n        (\"http://mylocalhostname:5000/v1.0/\", True, \"<local>\"),\n        (\"http://192.168.0.1/\", False, \"\"),\n    ),\n)\ndef test_should_bypass_proxies_win_registry(url, expected, override, monkeypatch):\n    \"\"\"Tests for function should_bypass_proxies to check if proxy\n    can be bypassed or not with Windows registry settings\n    \"\"\"\n    if override is None:\n        override = \"192.168.*;127.0.0.1;localhost.localdomain;172.16.1.1\"\n    import winreg\n\n    class RegHandle:\n        def Close(self):\n            pass\n\n    ie_settings = RegHandle()\n    proxyEnableValues = deque([1, \"1\"])\n\n    def OpenKey(key, subkey):\n        return ie_settings\n\n    def QueryValueEx(key, value_name):\n        if key is ie_settings:\n            if value_name == \"ProxyEnable\":\n                # this could be a string (REG_SZ) or a 32-bit number (REG_DWORD)\n                proxyEnableValues.rotate()\n                return [proxyEnableValues[0]]\n            elif value_name == \"ProxyOverride\":\n                return [override]\n\n    monkeypatch.setenv(\"http_proxy\", \"\")\n    monkeypatch.setenv(\"https_proxy\", \"\")\n    monkeypatch.setenv(\"ftp_proxy\", \"\")\n    monkeypatch.setenv(\"no_proxy\", \"\")\n    monkeypatch.setenv(\"NO_PROXY\", \"\")\n    monkeypatch.setattr(winreg, \"OpenKey\", OpenKey)\n    monkeypatch.setattr(winreg, \"QueryValueEx\", QueryValueEx)\n    assert should_bypass_proxies(url, None) == expected\n\n\n@pytest.mark.skipif(os.name != \"nt\", reason=\"Test only on Windows\")\ndef test_should_bypass_proxies_win_registry_bad_values(monkeypatch):\n    \"\"\"Tests for function should_bypass_proxies to check if proxy\n    can be bypassed or not with Windows invalid registry settings.\n    \"\"\"\n    import winreg\n\n    class RegHandle:\n        def Close(self):\n            pass\n\n    ie_settings = RegHandle()\n\n    def OpenKey(key, subkey):\n        return ie_settings\n\n    def QueryValueEx(key, value_name):\n        if key is ie_settings:\n            if value_name == \"ProxyEnable\":\n                # Invalid response; Should be an int or int-y value\n                return [\"\"]\n            elif value_name == \"ProxyOverride\":\n                return [\"192.168.*;127.0.0.1;localhost.localdomain;172.16.1.1\"]\n\n    monkeypatch.setenv(\"http_proxy\", \"\")\n    monkeypatch.setenv(\"https_proxy\", \"\")\n    monkeypatch.setenv(\"no_proxy\", \"\")\n    monkeypatch.setenv(\"NO_PROXY\", \"\")\n    monkeypatch.setattr(winreg, \"OpenKey\", OpenKey)\n    monkeypatch.setattr(winreg, \"QueryValueEx\", QueryValueEx)\n    assert should_bypass_proxies(\"http://172.16.1.1/\", None) is False\n\n\n@pytest.mark.parametrize(\n    \"env_name, value\",\n    (\n        (\"no_proxy\", \"192.168.0.0/24,127.0.0.1,localhost.localdomain\"),\n        (\"no_proxy\", None),\n        (\"a_new_key\", \"192.168.0.0/24,127.0.0.1,localhost.localdomain\"),\n        (\"a_new_key\", None),\n    ),\n)\ndef test_set_environ(env_name, value):\n    \"\"\"Tests set_environ will set environ values and will restore the environ.\"\"\"\n    environ_copy = copy.deepcopy(os.environ)\n    with set_environ(env_name, value):\n        assert os.environ.get(env_name) == value\n\n    assert os.environ == environ_copy\n\n\ndef test_set_environ_raises_exception():\n    \"\"\"Tests set_environ will raise exceptions in context when the\n    value parameter is None.\"\"\"\n    with pytest.raises(Exception) as exception:\n        with set_environ(\"test1\", None):\n            raise Exception(\"Expected exception\")\n\n    assert \"Expected exception\" in str(exception.value)\n\n\n@pytest.mark.skipif(os.name != \"nt\", reason=\"Test only on Windows\")\ndef test_should_bypass_proxies_win_registry_ProxyOverride_value(monkeypatch):\n    \"\"\"Tests for function should_bypass_proxies to check if proxy\n    can be bypassed or not with Windows ProxyOverride registry value ending with a semicolon.\n    \"\"\"\n    import winreg\n\n    class RegHandle:\n        def Close(self):\n            pass\n\n    ie_settings = RegHandle()\n\n    def OpenKey(key, subkey):\n        return ie_settings\n\n    def QueryValueEx(key, value_name):\n        if key is ie_settings:\n            if value_name == \"ProxyEnable\":\n                return [1]\n            elif value_name == \"ProxyOverride\":\n                return [\n                    \"192.168.*;127.0.0.1;localhost.localdomain;172.16.1.1;<-loopback>;\"\n                ]\n\n    monkeypatch.setenv(\"NO_PROXY\", \"\")\n    monkeypatch.setenv(\"no_proxy\", \"\")\n    monkeypatch.setattr(winreg, \"OpenKey\", OpenKey)\n    monkeypatch.setattr(winreg, \"QueryValueEx\", QueryValueEx)\n    assert should_bypass_proxies(\"http://example.com/\", None) is False\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pytest\nTest File Path: pytest\\test_argcomplete\\test_argcomplete.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"Allow bash-completion for argparse with argcomplete if installed.\n\nNeeds argcomplete>=0.5.6 for python 3.2/3.3 (older versions fail\nto find the magic string, so _ARGCOMPLETE env. var is never set, and\nthis does not need special code).\n\nFunction try_argcomplete(parser) should be called directly before\nthe call to ArgumentParser.parse_args().\n\nThe filescompleter is what you normally would use on the positional\narguments specification, in order to get \"dirname/\" after \"dirn<TAB>\"\ninstead of the default \"dirname \":\n\n   optparser.add_argument(Config._file_or_dir, nargs='*').completer=filescompleter\n\nOther, application specific, completers should go in the file\ndoing the add_argument calls as they need to be specified as .completer\nattributes as well. (If argcomplete is not installed, the function the\nattribute points to will not be used).\n\nSPEEDUP\n=======\n\nThe generic argcomplete script for bash-completion\n(/etc/bash_completion.d/python-argcomplete.sh)\nuses a python program to determine startup script generated by pip.\nYou can speed up completion somewhat by changing this script to include\n  # PYTHON_ARGCOMPLETE_OK\nso the python-argcomplete-check-easy-install-script does not\nneed to be called to find the entry point of the code and see if that is\nmarked  with PYTHON_ARGCOMPLETE_OK.\n\nINSTALL/DEBUGGING\n=================\n\nTo include this support in another application that has setup.py generated\nscripts:\n\n- Add the line:\n    # PYTHON_ARGCOMPLETE_OK\n  near the top of the main python entry point.\n\n- Include in the file calling parse_args():\n    from _argcomplete import try_argcomplete, filescompleter\n  Call try_argcomplete just before parse_args(), and optionally add\n  filescompleter to the positional arguments' add_argument().\n\nIf things do not work right away:\n\n- Switch on argcomplete debugging with (also helpful when doing custom\n  completers):\n    export _ARC_DEBUG=1\n\n- Run:\n    python-argcomplete-check-easy-install-script $(which appname)\n    echo $?\n  will echo 0 if the magic line has been found, 1 if not.\n\n- Sometimes it helps to find early on errors using:\n    _ARGCOMPLETE=1 _ARC_DEBUG=1 appname\n  which should throw a KeyError: 'COMPLINE' (which is properly set by the\n  global argcomplete script).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nfrom glob import glob\nimport os\nimport sys\nfrom typing import Any\n\n\nclass FastFilesCompleter:\n    \"\"\"Fast file completer class.\"\"\"\n\n    def __init__(self, directories: bool = True) -> None:\n        self.directories = directories\n\n    def __call__(self, prefix: str, **kwargs: Any) -> list[str]:\n        # Only called on non option completions.\n        if os.sep in prefix[1:]:\n            prefix_dir = len(os.path.dirname(prefix) + os.sep)\n        else:\n            prefix_dir = 0\n        completion = []\n        globbed = []\n        if \"*\" not in prefix and \"?\" not in prefix:\n            # We are on unix, otherwise no bash.\n            if not prefix or prefix[-1] == os.sep:\n                globbed.extend(glob(prefix + \".*\"))\n            prefix += \"*\"\n        globbed.extend(glob(prefix))\n        for x in sorted(globbed):\n            if os.path.isdir(x):\n                x += \"/\"\n            # Append stripping the prefix (like bash, not like compgen).\n            completion.append(x[prefix_dir:])\n        return completion\n\n\nif os.environ.get(\"_ARGCOMPLETE\"):\n    try:\n        import argcomplete.completers\n    except ImportError:\n        sys.exit(-1)\n    filescompleter: FastFilesCompleter | None = FastFilesCompleter()\n\n    def try_argcomplete(parser: argparse.ArgumentParser) -> None:\n        argcomplete.autocomplete(parser, always_complete_options=False)\n\nelse:\n\n    def try_argcomplete(parser: argparse.ArgumentParser) -> None:\n        pass\n\n    filescompleter = None\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:01"}, {"role": "assistant", "content": "```python\n# mypy: allow-untyped-defs\nfrom __future__ import annotations\n\nfrom pathlib import Path\nimport subprocess\nimport sys\n\nfrom _pytest.monkeypatch import MonkeyPatch\nimport pytest\n\n\n# Test for _argcomplete but not specific for any application.\n\n\ndef equal_with_bash(prefix, ffc, fc, out=None):\n    res = ffc(prefix)\n    res_bash = set(fc(prefix))\n    retval = set(res) == res_bash\n    if out:\n        out.write(f\"equal_with_bash({prefix}) {retval} {res}\\n\")\n        if not retval:\n            out.write(\" python - bash: %s\\n\" % (set(res) - res_bash))\n            out.write(\" bash - python: %s\\n\" % (res_bash - set(res)))\n    return retval\n\n\n# Copied from argcomplete.completers as import from there.\n# Also pulls in argcomplete.__init__ which opens filedescriptor 9.\n# This gives an OSError at the end of testrun.\n\n\ndef _wrapcall(*args, **kargs):\n    try:\n        return subprocess.check_output(*args, **kargs).decode().splitlines()\n    except subprocess.CalledProcessError:\n        return []\n\n\nclass FilesCompleter:\n    \"\"\"File completer class, optionally takes a list of allowed extensions.\"\"\"\n\n    def __init__(self, allowednames=(), directories=True):\n        # Fix if someone passes in a string instead of a list\n        if type(allowednames) is str:\n            allowednames = [allowednames]\n\n        self.allowednames = [x.lstrip(\"*\").lstrip(\".\") for x in allowednames]\n        self.directories = directories\n\n    def __call__(self, prefix, **kwargs):\n        completion = []\n        if self.allowednames:\n            if self.directories:\n                files = _wrapcall([\"bash\", \"-c\", f\"compgen -A directory -- '{prefix}'\"])\n                completion += [f + \"/\" for f in files]\n            for x in self.allowednames:\n                completion += _wrapcall(\n                    [\"bash\", \"-c\", f\"compgen -A file -X '!*.{x}' -- '{prefix}'\"]\n                )\n        else:\n            completion += _wrapcall([\"bash\", \"-c\", f\"compgen -A file -- '{prefix}'\"])\n\n            anticomp = _wrapcall([\"bash\", \"-c\", f\"compgen -A directory -- '{prefix}'\"])\n\n            completion = list(set(completion) - set(anticomp))\n\n            if self.directories:\n                completion += [f + \"/\" for f in anticomp]\n        return completion\n\n\nclass TestArgComplete:\n    @pytest.mark.skipif(\"sys.platform in ('win32', 'darwin')\")\n    def test_compare_with_compgen(\n        self, tmp_path: Path, monkeypatch: MonkeyPatch\n    ) -> None:\n        from _pytest._argcomplete import FastFilesCompleter\n\n        ffc = FastFilesCompleter()\n        fc = FilesCompleter()\n\n        monkeypatch.chdir(tmp_path)\n\n        assert equal_with_bash(\"\", ffc, fc, out=sys.stdout)\n\n        tmp_path.cwd().joinpath(\"data\").touch()\n\n        for x in [\"d\", \"data\", \"doesnotexist\", \"\"]:\n            assert equal_with_bash(x, ffc, fc, out=sys.stdout)\n\n    @pytest.mark.skipif(\"sys.platform in ('win32', 'darwin')\")\n    def test_remove_dir_prefix(self):\n        \"\"\"This is not compatible with compgen but it is with bash itself: ls /usr/<TAB>.\"\"\"\n        from _pytest._argcomplete import FastFilesCompleter\n\n        ffc = FastFilesCompleter()\n        fc = FilesCompleter()\n        for x in \"/usr/\".split():\n            assert not equal_with_bash(x, ffc, fc, out=sys.stdout)\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pytest\nTest File Path: pytest\\test_compat\\test_compat.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n# mypy: allow-untyped-defs\n\"\"\"Python version compatibility code.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Callable\nimport enum\nimport functools\nimport inspect\nfrom inspect import Parameter\nfrom inspect import signature\nimport os\nfrom pathlib import Path\nimport sys\nfrom typing import Any\nfrom typing import Final\nfrom typing import NoReturn\n\nimport py\n\n\n#: constant to prepare valuing pylib path replacements/lazy proxies later on\n#  intended for removal in pytest 8.0 or 9.0\n\n# fmt: off\n# intentional space to create a fake difference for the verification\nLEGACY_PATH = py.path. local\n# fmt: on\n\n\ndef legacy_path(path: str | os.PathLike[str]) -> LEGACY_PATH:\n    \"\"\"Internal wrapper to prepare lazy proxies for legacy_path instances\"\"\"\n    return LEGACY_PATH(path)\n\n\n# fmt: off\n# Singleton type for NOTSET, as described in:\n# https://www.python.org/dev/peps/pep-0484/#support-for-singleton-types-in-unions\nclass NotSetType(enum.Enum):\n    token = 0\nNOTSET: Final = NotSetType.token\n# fmt: on\n\n\ndef iscoroutinefunction(func: object) -> bool:\n    \"\"\"Return True if func is a coroutine function (a function defined with async\n    def syntax, and doesn't contain yield), or a function decorated with\n    @asyncio.coroutine.\n\n    Note: copied and modified from Python 3.5's builtin coroutines.py to avoid\n    importing asyncio directly, which in turns also initializes the \"logging\"\n    module as a side-effect (see issue #8).\n    \"\"\"\n    return inspect.iscoroutinefunction(func) or getattr(func, \"_is_coroutine\", False)\n\n\ndef is_async_function(func: object) -> bool:\n    \"\"\"Return True if the given function seems to be an async function or\n    an async generator.\"\"\"\n    return iscoroutinefunction(func) or inspect.isasyncgenfunction(func)\n\n\ndef getlocation(function, curdir: str | os.PathLike[str] | None = None) -> str:\n    function = get_real_func(function)\n    fn = Path(inspect.getfile(function))\n    lineno = function.__code__.co_firstlineno\n    if curdir is not None:\n        try:\n            relfn = fn.relative_to(curdir)\n        except ValueError:\n            pass\n        else:\n            return f\"{relfn}:{lineno + 1}\"\n    return f\"{fn}:{lineno + 1}\"\n\n\ndef num_mock_patch_args(function) -> int:\n    \"\"\"Return number of arguments used up by mock arguments (if any).\"\"\"\n    patchings = getattr(function, \"patchings\", None)\n    if not patchings:\n        return 0\n\n    mock_sentinel = getattr(sys.modules.get(\"mock\"), \"DEFAULT\", object())\n    ut_mock_sentinel = getattr(sys.modules.get(\"unittest.mock\"), \"DEFAULT\", object())\n\n    return len(\n        [\n            p\n            for p in patchings\n            if not p.attribute_name\n            and (p.new is mock_sentinel or p.new is ut_mock_sentinel)\n        ]\n    )\n\n\ndef getfuncargnames(\n    function: Callable[..., object],\n    *,\n    name: str = \"\",\n    cls: type | None = None,\n) -> tuple[str, ...]:\n    \"\"\"Return the names of a function's mandatory arguments.\n\n    Should return the names of all function arguments that:\n    * Aren't bound to an instance or type as in instance or class methods.\n    * Don't have default values.\n    * Aren't bound with functools.partial.\n    * Aren't replaced with mocks.\n\n    The cls arguments indicate that the function should be treated as a bound\n    method even though it's not unless the function is a static method.\n\n    The name parameter should be the original name in which the function was collected.\n    \"\"\"\n    # TODO(RonnyPfannschmidt): This function should be refactored when we\n    # revisit fixtures. The fixture mechanism should ask the node for\n    # the fixture names, and not try to obtain directly from the\n    # function object well after collection has occurred.\n\n    # The parameters attribute of a Signature object contains an\n    # ordered mapping of parameter names to Parameter instances.  This\n    # creates a tuple of the names of the parameters that don't have\n    # defaults.\n    try:\n        parameters = signature(function).parameters\n    except (ValueError, TypeError) as e:\n        from _pytest.outcomes import fail\n\n        fail(\n            f\"Could not determine arguments of {function!r}: {e}\",\n            pytrace=False,\n        )\n\n    arg_names = tuple(\n        p.name\n        for p in parameters.values()\n        if (\n            p.kind is Parameter.POSITIONAL_OR_KEYWORD\n            or p.kind is Parameter.KEYWORD_ONLY\n        )\n        and p.default is Parameter.empty\n    )\n    if not name:\n        name = function.__name__\n\n    # If this function should be treated as a bound method even though\n    # it's passed as an unbound method or function, remove the first\n    # parameter name.\n    if (\n        # Not using `getattr` because we don't want to resolve the staticmethod.\n        # Not using `cls.__dict__` because we want to check the entire MRO.\n        cls\n        and not isinstance(\n            inspect.getattr_static(cls, name, default=None), staticmethod\n        )\n    ):\n        arg_names = arg_names[1:]\n    # Remove any names that will be replaced with mocks.\n    if hasattr(function, \"__wrapped__\"):\n        arg_names = arg_names[num_mock_patch_args(function) :]\n    return arg_names\n\n\ndef get_default_arg_names(function: Callable[..., Any]) -> tuple[str, ...]:\n    # Note: this code intentionally mirrors the code at the beginning of\n    # getfuncargnames, to get the arguments which were excluded from its result\n    # because they had default values.\n    return tuple(\n        p.name\n        for p in signature(function).parameters.values()\n        if p.kind in (Parameter.POSITIONAL_OR_KEYWORD, Parameter.KEYWORD_ONLY)\n        and p.default is not Parameter.empty\n    )\n\n\n_non_printable_ascii_translate_table = {\n    i: f\"\\\\x{i:02x}\" for i in range(128) if i not in range(32, 127)\n}\n_non_printable_ascii_translate_table.update(\n    {ord(\"\\t\"): \"\\\\t\", ord(\"\\r\"): \"\\\\r\", ord(\"\\n\"): \"\\\\n\"}\n)\n\n\ndef ascii_escaped(val: bytes | str) -> str:\n    r\"\"\"If val is pure ASCII, return it as an str, otherwise, escape\n    bytes objects into a sequence of escaped bytes:\n\n    b'\\xc3\\xb4\\xc5\\xd6' -> r'\\xc3\\xb4\\xc5\\xd6'\n\n    and escapes strings into a sequence of escaped unicode ids, e.g.:\n\n    r'4\\nV\\U00043efa\\x0eMXWB\\x1e\\u3028\\u15fd\\xcd\\U0007d944'\n\n    Note:\n       The obvious \"v.decode('unicode-escape')\" will return\n       valid UTF-8 unicode if it finds them in bytes, but we\n       want to return escaped bytes for any byte, even if they match\n       a UTF-8 string.\n    \"\"\"\n    if isinstance(val, bytes):\n        ret = val.decode(\"ascii\", \"backslashreplace\")\n    else:\n        ret = val.encode(\"unicode_escape\").decode(\"ascii\")\n    return ret.translate(_non_printable_ascii_translate_table)\n\n\ndef get_real_func(obj):\n    \"\"\"Get the real function object of the (possibly) wrapped object by\n    :func:`functools.wraps`, or :func:`functools.partial`.\"\"\"\n    obj = inspect.unwrap(obj)\n\n    if isinstance(obj, functools.partial):\n        obj = obj.func\n    return obj\n\n\ndef getimfunc(func):\n    try:\n        return func.__func__\n    except AttributeError:\n        return func\n\n\ndef safe_getattr(object: Any, name: str, default: Any) -> Any:\n    \"\"\"Like getattr but return default upon any Exception or any OutcomeException.\n\n    Attribute access can potentially fail for 'evil' Python objects.\n    See issue #214.\n    It catches OutcomeException because of #2490 (issue #580), new outcomes\n    are derived from BaseException instead of Exception (for more details\n    check #2707).\n    \"\"\"\n    from _pytest.outcomes import TEST_OUTCOME\n\n    try:\n        return getattr(object, name, default)\n    except TEST_OUTCOME:\n        return default\n\n\ndef safe_isclass(obj: object) -> bool:\n    \"\"\"Ignore any exception via isinstance on Python 3.\"\"\"\n    try:\n        return inspect.isclass(obj)\n    except Exception:\n        return False\n\n\ndef get_user_id() -> int | None:\n    \"\"\"Return the current process's real user id or None if it could not be\n    determined.\n\n    :return: The user id or None if it could not be determined.\n    \"\"\"\n    # mypy follows the version and platform checking expectation of PEP 484:\n    # https://mypy.readthedocs.io/en/stable/common_issues.html?highlight=platform#python-version-and-system-platform-checks\n    # Containment checks are too complex for mypy v1.5.0 and cause failure.\n    if sys.platform == \"win32\" or sys.platform == \"emscripten\":\n        # win32 does not have a getuid() function.\n        # Emscripten has a return 0 stub.\n        return None\n    else:\n        # On other platforms, a return value of -1 is assumed to indicate that\n        # the current process's real user id could not be determined.\n        ERROR = -1\n        uid = os.getuid()\n        return uid if uid != ERROR else None\n\n\n# Perform exhaustiveness checking.\n#\n# Consider this example:\n#\n#     MyUnion = Union[int, str]\n#\n#     def handle(x: MyUnion) -> int {\n#         if isinstance(x, int):\n#             return 1\n#         elif isinstance(x, str):\n#             return 2\n#         else:\n#             raise Exception('unreachable')\n#\n# Now suppose we add a new variant:\n#\n#     MyUnion = Union[int, str, bytes]\n#\n# After doing this, we must remember ourselves to go and update the handle\n# function to handle the new variant.\n#\n# With `assert_never` we can do better:\n#\n#     // raise Exception('unreachable')\n#     return assert_never(x)\n#\n# Now, if we forget to handle the new variant, the type-checker will emit a\n# compile-time error, instead of the runtime error we would have gotten\n# previously.\n#\n# This also work for Enums (if you use `is` to compare) and Literals.\ndef assert_never(value: NoReturn) -> NoReturn:\n    assert False, f\"Unhandled value: {value} ({type(value).__name__})\"\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:01"}, {"role": "assistant", "content": "```python\n# mypy: allow-untyped-defs\nfrom __future__ import annotations\n\nimport enum\nfrom functools import cached_property\nfrom functools import partial\nfrom functools import wraps\nfrom typing import TYPE_CHECKING\n\nfrom _pytest.compat import assert_never\nfrom _pytest.compat import get_real_func\nfrom _pytest.compat import safe_getattr\nfrom _pytest.compat import safe_isclass\nfrom _pytest.outcomes import OutcomeException\nimport pytest\n\n\nif TYPE_CHECKING:\n    from typing_extensions import Literal\n\n\ndef test_real_func_loop_limit() -> None:\n    class Evil:\n        def __init__(self):\n            self.left = 1000\n\n        def __repr__(self):\n            return f\"<Evil left={self.left}>\"\n\n        def __getattr__(self, attr):\n            if not self.left:\n                raise RuntimeError(\"it's over\")  # pragma: no cover\n            self.left -= 1\n            return self\n\n    evil = Evil()\n\n    with pytest.raises(\n        ValueError,\n        match=(\"wrapper loop when unwrapping <Evil left=998>\"),\n    ):\n        get_real_func(evil)\n\n\ndef test_get_real_func() -> None:\n    \"\"\"Check that get_real_func correctly unwraps decorators until reaching the real function\"\"\"\n\n    def decorator(f):\n        @wraps(f)\n        def inner():\n            pass  # pragma: no cover\n\n        return inner\n\n    def func():\n        pass  # pragma: no cover\n\n    wrapped_func = decorator(decorator(func))\n    assert get_real_func(wrapped_func) is func\n\n    wrapped_func2 = decorator(decorator(wrapped_func))\n    assert get_real_func(wrapped_func2) is func\n\n    # obtain the function up until the point a function was wrapped by pytest itself\n    @pytest.fixture\n    def wrapped_func3():\n        pass  # pragma: no cover\n\n    wrapped_func4 = decorator(wrapped_func3)\n    assert get_real_func(wrapped_func4) is wrapped_func3._get_wrapped_function()\n\n\ndef test_get_real_func_partial() -> None:\n    \"\"\"Test get_real_func handles partial instances correctly\"\"\"\n\n    def foo(x):\n        return x\n\n    assert get_real_func(foo) is foo\n    assert get_real_func(partial(foo)) is foo\n\n\nclass ErrorsHelper:\n    @property\n    def raise_baseexception(self):\n        raise BaseException(\"base exception should be raised\")\n\n    @property\n    def raise_exception(self):\n        raise Exception(\"exception should be caught\")\n\n    @property\n    def raise_fail_outcome(self):\n        pytest.fail(\"fail should be caught\")\n\n\ndef test_helper_failures() -> None:\n    helper = ErrorsHelper()\n    with pytest.raises(Exception):  # noqa: B017\n        _ = helper.raise_exception\n    with pytest.raises(OutcomeException):\n        _ = helper.raise_fail_outcome\n\n\ndef test_safe_getattr() -> None:\n    helper = ErrorsHelper()\n    assert safe_getattr(helper, \"raise_exception\", \"default\") == \"default\"\n    assert safe_getattr(helper, \"raise_fail_outcome\", \"default\") == \"default\"\n    with pytest.raises(BaseException):  # noqa: B017\n        assert safe_getattr(helper, \"raise_baseexception\", \"default\")\n\n\ndef test_safe_isclass() -> None:\n    assert safe_isclass(type) is True\n\n    class CrappyClass(Exception):\n        # Type ignored because it's bypassed intentionally.\n        @property  # type: ignore\n        def __class__(self):\n            assert False, \"Should be ignored\"\n\n    assert safe_isclass(CrappyClass()) is False\n\n\ndef test_cached_property() -> None:\n    ncalls = 0\n\n    class Class:\n        @cached_property\n        def prop(self) -> int:\n            nonlocal ncalls\n            ncalls += 1\n            return ncalls\n\n    c1 = Class()\n    assert ncalls == 0\n    assert c1.prop == 1\n    assert c1.prop == 1\n    c2 = Class()\n    assert ncalls == 1\n    assert c2.prop == 2\n    assert c1.prop == 1\n\n\ndef test_assert_never_union() -> None:\n    x: int | str = 10\n\n    if isinstance(x, int):\n        pass\n    else:\n        with pytest.raises(AssertionError):\n            assert_never(x)  # type: ignore[arg-type]\n\n    if isinstance(x, int):\n        pass\n    elif isinstance(x, str):\n        pass\n    else:\n        assert_never(x)\n\n\ndef test_assert_never_enum() -> None:\n    E = enum.Enum(\"E\", \"a b\")\n    x: E = E.a\n\n    if x is E.a:\n        pass\n    else:\n        with pytest.raises(AssertionError):\n            assert_never(x)  # type: ignore[arg-type]\n\n    if x is E.a:\n        pass\n    elif x is E.b:\n        pass\n    else:\n        assert_never(x)\n\n\ndef test_assert_never_literal() -> None:\n    x: Literal[\"a\", \"b\"] = \"a\"\n\n    if x == \"a\":\n        pass\n    else:\n        with pytest.raises(AssertionError):\n            assert_never(x)  # type: ignore[arg-type]\n\n    if x == \"a\":\n        pass\n    elif x == \"b\":\n        pass\n    else:\n        assert_never(x)\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pytest\nTest File Path: pytest\\test_scope\\test_scope.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"\nScope definition and related utilities.\n\nThose are defined here, instead of in the 'fixtures' module because\ntheir use is spread across many other pytest modules, and centralizing it in 'fixtures'\nwould cause circular references.\n\nAlso this makes the module light to import, as it should.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom enum import Enum\nfrom functools import total_ordering\nfrom typing import Literal\n\n\n_ScopeName = Literal[\"session\", \"package\", \"module\", \"class\", \"function\"]\n\n\n@total_ordering\nclass Scope(Enum):\n    \"\"\"\n    Represents one of the possible fixture scopes in pytest.\n\n    Scopes are ordered from lower to higher, that is:\n\n              ->>> higher ->>>\n\n    Function < Class < Module < Package < Session\n\n              <<<- lower  <<<-\n    \"\"\"\n\n    # Scopes need to be listed from lower to higher.\n    Function = \"function\"\n    Class = \"class\"\n    Module = \"module\"\n    Package = \"package\"\n    Session = \"session\"\n\n    def next_lower(self) -> Scope:\n        \"\"\"Return the next lower scope.\"\"\"\n        index = _SCOPE_INDICES[self]\n        if index == 0:\n            raise ValueError(f\"{self} is the lower-most scope\")\n        return _ALL_SCOPES[index - 1]\n\n    def next_higher(self) -> Scope:\n        \"\"\"Return the next higher scope.\"\"\"\n        index = _SCOPE_INDICES[self]\n        if index == len(_SCOPE_INDICES) - 1:\n            raise ValueError(f\"{self} is the upper-most scope\")\n        return _ALL_SCOPES[index + 1]\n\n    def __lt__(self, other: Scope) -> bool:\n        self_index = _SCOPE_INDICES[self]\n        other_index = _SCOPE_INDICES[other]\n        return self_index < other_index\n\n    @classmethod\n    def from_user(\n        cls, scope_name: _ScopeName, descr: str, where: str | None = None\n    ) -> Scope:\n        \"\"\"\n        Given a scope name from the user, return the equivalent Scope enum. Should be used\n        whenever we want to convert a user provided scope name to its enum object.\n\n        If the scope name is invalid, construct a user friendly message and call pytest.fail.\n        \"\"\"\n        from _pytest.outcomes import fail\n\n        try:\n            # Holding this reference is necessary for mypy at the moment.\n            scope = Scope(scope_name)\n        except ValueError:\n            fail(\n                \"{} {}got an unexpected scope value '{}'\".format(\n                    descr, f\"from {where} \" if where else \"\", scope_name\n                ),\n                pytrace=False,\n            )\n        return scope\n\n\n_ALL_SCOPES = list(Scope)\n_SCOPE_INDICES = {scope: index for index, scope in enumerate(_ALL_SCOPES)}\n\n\n# Ordered list of scopes which can contain many tests (in practice all except Function).\nHIGH_SCOPES = [x for x in Scope if x is not Scope.Function]\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:01"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport re\n\nfrom _pytest.scope import Scope\nimport pytest\n\n\ndef test_ordering() -> None:\n    assert Scope.Session > Scope.Package\n    assert Scope.Package > Scope.Module\n    assert Scope.Module > Scope.Class\n    assert Scope.Class > Scope.Function\n\n\ndef test_next_lower() -> None:\n    assert Scope.Session.next_lower() is Scope.Package\n    assert Scope.Package.next_lower() is Scope.Module\n    assert Scope.Module.next_lower() is Scope.Class\n    assert Scope.Class.next_lower() is Scope.Function\n\n    with pytest.raises(ValueError, match=\"Function is the lower-most scope\"):\n        Scope.Function.next_lower()\n\n\ndef test_next_higher() -> None:\n    assert Scope.Function.next_higher() is Scope.Class\n    assert Scope.Class.next_higher() is Scope.Module\n    assert Scope.Module.next_higher() is Scope.Package\n    assert Scope.Package.next_higher() is Scope.Session\n\n    with pytest.raises(ValueError, match=\"Session is the upper-most scope\"):\n        Scope.Session.next_higher()\n\n\ndef test_from_user() -> None:\n    assert Scope.from_user(\"module\", \"for parametrize\", \"some::id\") is Scope.Module\n\n    expected_msg = \"for parametrize from some::id got an unexpected scope value 'foo'\"\n    with pytest.raises(pytest.fail.Exception, match=re.escape(expected_msg)):\n        Scope.from_user(\"foo\", \"for parametrize\", \"some::id\")  # type:ignore[arg-type]\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pytest\nTest File Path: pytest\\test_stash\\test_stash.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nfrom typing import Any\nfrom typing import cast\nfrom typing import Generic\nfrom typing import TypeVar\n\n\n__all__ = [\"Stash\", \"StashKey\"]\n\n\nT = TypeVar(\"T\")\nD = TypeVar(\"D\")\n\n\nclass StashKey(Generic[T]):\n    \"\"\"``StashKey`` is an object used as a key to a :class:`Stash`.\n\n    A ``StashKey`` is associated with the type ``T`` of the value of the key.\n\n    A ``StashKey`` is unique and cannot conflict with another key.\n\n    .. versionadded:: 7.0\n    \"\"\"\n\n    __slots__ = ()\n\n\nclass Stash:\n    r\"\"\"``Stash`` is a type-safe heterogeneous mutable mapping that\n    allows keys and value types to be defined separately from\n    where it (the ``Stash``) is created.\n\n    Usually you will be given an object which has a ``Stash``, for example\n    :class:`~pytest.Config` or a :class:`~_pytest.nodes.Node`:\n\n    .. code-block:: python\n\n        stash: Stash = some_object.stash\n\n    If a module or plugin wants to store data in this ``Stash``, it creates\n    :class:`StashKey`\\s for its keys (at the module level):\n\n    .. code-block:: python\n\n        # At the top-level of the module\n        some_str_key = StashKey[str]()\n        some_bool_key = StashKey[bool]()\n\n    To store information:\n\n    .. code-block:: python\n\n        # Value type must match the key.\n        stash[some_str_key] = \"value\"\n        stash[some_bool_key] = True\n\n    To retrieve the information:\n\n    .. code-block:: python\n\n        # The static type of some_str is str.\n        some_str = stash[some_str_key]\n        # The static type of some_bool is bool.\n        some_bool = stash[some_bool_key]\n\n    .. versionadded:: 7.0\n    \"\"\"\n\n    __slots__ = (\"_storage\",)\n\n    def __init__(self) -> None:\n        self._storage: dict[StashKey[Any], object] = {}\n\n    def __setitem__(self, key: StashKey[T], value: T) -> None:\n        \"\"\"Set a value for key.\"\"\"\n        self._storage[key] = value\n\n    def __getitem__(self, key: StashKey[T]) -> T:\n        \"\"\"Get the value for key.\n\n        Raises ``KeyError`` if the key wasn't set before.\n        \"\"\"\n        return cast(T, self._storage[key])\n\n    def get(self, key: StashKey[T], default: D) -> T | D:\n        \"\"\"Get the value for key, or return default if the key wasn't set\n        before.\"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def setdefault(self, key: StashKey[T], default: T) -> T:\n        \"\"\"Return the value of key if already set, otherwise set the value\n        of key to default and return default.\"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n\n    def __delitem__(self, key: StashKey[T]) -> None:\n        \"\"\"Delete the value for key.\n\n        Raises ``KeyError`` if the key wasn't set before.\n        \"\"\"\n        del self._storage[key]\n\n    def __contains__(self, key: StashKey[T]) -> bool:\n        \"\"\"Return whether key was set.\"\"\"\n        return key in self._storage\n\n    def __len__(self) -> int:\n        \"\"\"Return how many items exist in the stash.\"\"\"\n        return len(self._storage)\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:01"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nfrom _pytest.stash import Stash\nfrom _pytest.stash import StashKey\nimport pytest\n\n\ndef test_stash() -> None:\n    stash = Stash()\n\n    assert len(stash) == 0\n    assert not stash\n\n    key1 = StashKey[str]()\n    key2 = StashKey[int]()\n\n    # Basic functionality - single key.\n    assert key1 not in stash\n    stash[key1] = \"hello\"\n    assert key1 in stash\n    assert stash[key1] == \"hello\"\n    assert stash.get(key1, None) == \"hello\"\n    stash[key1] = \"world\"\n    assert stash[key1] == \"world\"\n    # Has correct type (no mypy error).\n    stash[key1] + \"string\"\n    assert len(stash) == 1\n    assert stash\n\n    # No interaction with another key.\n    assert key2 not in stash\n    assert stash.get(key2, None) is None\n    with pytest.raises(KeyError):\n        stash[key2]\n    with pytest.raises(KeyError):\n        del stash[key2]\n    stash[key2] = 1\n    assert stash[key2] == 1\n    # Has correct type (no mypy error).\n    stash[key2] + 20\n    del stash[key1]\n    with pytest.raises(KeyError):\n        del stash[key1]\n    with pytest.raises(KeyError):\n        stash[key1]\n\n    # setdefault\n    stash[key1] = \"existing\"\n    assert stash.setdefault(key1, \"default\") == \"existing\"\n    assert stash[key1] == \"existing\"\n    key_setdefault = StashKey[bytes]()\n    assert stash.setdefault(key_setdefault, b\"default\") == b\"default\"\n    assert stash[key_setdefault] == b\"default\"\n    assert len(stash) == 3\n    assert stash\n\n    # Can't accidentally add attributes to stash object itself.\n    with pytest.raises(AttributeError):\n        stash.foo = \"nope\"  # type: ignore[attr-defined]\n\n    # No interaction with another stash.\n    stash2 = Stash()\n    key3 = StashKey[int]()\n    assert key2 not in stash2\n    stash2[key2] = 100\n    stash2[key3] = 200\n    assert stash2[key2] + stash2[key3] == 300\n    assert stash[key2] == 1\n    assert key3 not in stash\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pytest\nTest File Path: pytest\\test_warning_types\\test_warning_types.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport dataclasses\nimport inspect\nfrom types import FunctionType\nfrom typing import Any\nfrom typing import final\nfrom typing import Generic\nfrom typing import TypeVar\nimport warnings\n\n\nclass PytestWarning(UserWarning):\n    \"\"\"Base class for all warnings emitted by pytest.\"\"\"\n\n    __module__ = \"pytest\"\n\n\n@final\nclass PytestAssertRewriteWarning(PytestWarning):\n    \"\"\"Warning emitted by the pytest assert rewrite module.\"\"\"\n\n    __module__ = \"pytest\"\n\n\n@final\nclass PytestCacheWarning(PytestWarning):\n    \"\"\"Warning emitted by the cache plugin in various situations.\"\"\"\n\n    __module__ = \"pytest\"\n\n\n@final\nclass PytestConfigWarning(PytestWarning):\n    \"\"\"Warning emitted for configuration issues.\"\"\"\n\n    __module__ = \"pytest\"\n\n\n@final\nclass PytestCollectionWarning(PytestWarning):\n    \"\"\"Warning emitted when pytest is not able to collect a file or symbol in a module.\"\"\"\n\n    __module__ = \"pytest\"\n\n\nclass PytestDeprecationWarning(PytestWarning, DeprecationWarning):\n    \"\"\"Warning class for features that will be removed in a future version.\"\"\"\n\n    __module__ = \"pytest\"\n\n\nclass PytestRemovedIn9Warning(PytestDeprecationWarning):\n    \"\"\"Warning class for features that will be removed in pytest 9.\"\"\"\n\n    __module__ = \"pytest\"\n\n\n@final\nclass PytestExperimentalApiWarning(PytestWarning, FutureWarning):\n    \"\"\"Warning category used to denote experiments in pytest.\n\n    Use sparingly as the API might change or even be removed completely in a\n    future version.\n    \"\"\"\n\n    __module__ = \"pytest\"\n\n    @classmethod\n    def simple(cls, apiname: str) -> PytestExperimentalApiWarning:\n        return cls(f\"{apiname} is an experimental api that may change over time\")\n\n\n@final\nclass PytestUnknownMarkWarning(PytestWarning):\n    \"\"\"Warning emitted on use of unknown markers.\n\n    See :ref:`mark` for details.\n    \"\"\"\n\n    __module__ = \"pytest\"\n\n\n@final\nclass PytestUnraisableExceptionWarning(PytestWarning):\n    \"\"\"An unraisable exception was reported.\n\n    Unraisable exceptions are exceptions raised in :meth:`__del__ <object.__del__>`\n    implementations and similar situations when the exception cannot be raised\n    as normal.\n    \"\"\"\n\n    __module__ = \"pytest\"\n\n\n@final\nclass PytestUnhandledThreadExceptionWarning(PytestWarning):\n    \"\"\"An unhandled exception occurred in a :class:`~threading.Thread`.\n\n    Such exceptions don't propagate normally.\n    \"\"\"\n\n    __module__ = \"pytest\"\n\n\n_W = TypeVar(\"_W\", bound=PytestWarning)\n\n\n@final\n@dataclasses.dataclass\nclass UnformattedWarning(Generic[_W]):\n    \"\"\"A warning meant to be formatted during runtime.\n\n    This is used to hold warnings that need to format their message at runtime,\n    as opposed to a direct message.\n    \"\"\"\n\n    category: type[_W]\n    template: str\n\n    def format(self, **kwargs: Any) -> _W:\n        \"\"\"Return an instance of the warning category, formatted with given kwargs.\"\"\"\n        return self.category(self.template.format(**kwargs))\n\n\n@final\nclass PytestFDWarning(PytestWarning):\n    \"\"\"When the lsof plugin finds leaked fds.\"\"\"\n\n    __module__ = \"pytest\"\n\n\ndef warn_explicit_for(method: FunctionType, message: PytestWarning) -> None:\n    \"\"\"\n    Issue the warning :param:`message` for the definition of the given :param:`method`\n\n    this helps to log warnings for functions defined prior to finding an issue with them\n    (like hook wrappers being marked in a legacy mechanism)\n    \"\"\"\n    lineno = method.__code__.co_firstlineno\n    filename = inspect.getfile(method)\n    module = method.__module__\n    mod_globals = method.__globals__\n    try:\n        warnings.warn_explicit(\n            message,\n            type(message),\n            filename=filename,\n            module=module,\n            registry=mod_globals.setdefault(\"__warningregistry__\", {}),\n            lineno=lineno,\n        )\n    except Warning as w:\n        # If warnings are errors (e.g. -Werror), location information gets lost, so we add it to the message.\n        raise type(w)(f\"{w}\\n at {filename}:{lineno}\") from None\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:01"}, {"role": "assistant", "content": "```python\n# mypy: allow-untyped-defs\nfrom __future__ import annotations\n\nimport inspect\n\nfrom _pytest import warning_types\nfrom _pytest.pytester import Pytester\nimport pytest\n\n\n@pytest.mark.parametrize(\n    \"warning_class\",\n    [\n        w\n        for n, w in vars(warning_types).items()\n        if inspect.isclass(w) and issubclass(w, Warning)\n    ],\n)\ndef test_warning_types(warning_class: UserWarning) -> None:\n    \"\"\"Make sure all warnings declared in _pytest.warning_types are displayed as coming\n    from 'pytest' instead of the internal module (#5452).\n    \"\"\"\n    assert warning_class.__module__ == \"pytest\"\n\n\n@pytest.mark.filterwarnings(\"error::pytest.PytestWarning\")\ndef test_pytest_warnings_repr_integration_test(pytester: Pytester) -> None:\n    \"\"\"Small integration test to ensure our small hack of setting the __module__ attribute\n    of our warnings actually works (#5452).\n    \"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import warnings\n\n        def test():\n            warnings.warn(pytest.PytestWarning(\"some warning\"))\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"E       pytest.PytestWarning: some warning\"])\n\n\n@pytest.mark.filterwarnings(\"error\")\ndef test_warn_explicit_for_annotates_errors_with_location():\n    with pytest.raises(Warning, match=\"(?m)test\\n at .*python_api.py:\\\\d+\"):\n        warning_types.warn_explicit_for(\n            pytest.raises,  # type: ignore[arg-type]\n            warning_types.PytestWarning(\"test\"),\n        )\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pynguin\nTest File Path: pynguin\\test_arithmetic\\test_arithmetic.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides arithmetic operators for mutation analysis.\n\nBased on https://github.com/se2p/mutpy-pynguin/blob/main/mutpy/operators/arithmetic.py\nand integrated in Pynguin.\n\"\"\"\n\nimport abc\nimport ast\n\nfrom pynguin.assertion.mutation_analysis.operators.base import (\n    AbstractUnaryOperatorDeletion,\n)\nfrom pynguin.assertion.mutation_analysis.operators.base import MutationOperator\n\n\nclass ArithmeticOperatorDeletion(AbstractUnaryOperatorDeletion):\n    \"\"\"A class that mutate arithmetic operators by deleting them.\"\"\"\n\n    def get_operator_type(self) -> type:  # noqa: D102\n        return ast.UAdd | ast.USub  # type: ignore[return-value]\n\n\nclass AbstractArithmeticOperatorReplacement(abc.ABC, MutationOperator):\n    \"\"\"An abstract class that mutates arithmetic operators by replacing them.\"\"\"\n\n    @abc.abstractmethod\n    def should_mutate(self, node: ast.AST) -> bool:\n        \"\"\"Check if the operator should be mutated.\n\n        Args:\n            node: The node to check.\n\n        Returns:\n            True if the operator should be mutated, False otherwise.\n        \"\"\"\n\n    def mutate_Add(self, node: ast.Add) -> ast.Sub | None:  # noqa: N802\n        \"\"\"Mutate an Add operator to a Sub operator.\n\n        Args:\n            node: The Add operator to mutate.\n\n        Returns:\n            The mutated operator, or None if the operator should not be mutated.\n        \"\"\"\n        if not self.should_mutate(node):\n            return None\n\n        return ast.Sub()\n\n    def mutate_Sub(self, node: ast.Sub) -> ast.Add | None:  # noqa: N802\n        \"\"\"Mutate a Sub operator to an Add operator.\n\n        Args:\n            node: The Sub operator to mutate.\n\n        Returns:\n            The mutated operator, or None if the operator should not be mutated.\n        \"\"\"\n        if not self.should_mutate(node):\n            return None\n\n        return ast.Add()\n\n    def mutate_Mult_to_Div(self, node: ast.Mult) -> ast.Div | None:  # noqa: N802\n        \"\"\"Mutate a Mult operator to a Div operator.\n\n        Args:\n            node: The Mult operator to mutate.\n\n        Returns:\n            The mutated operator, or None if the operator should not be mutated.\n        \"\"\"\n        if not self.should_mutate(node):\n            return None\n\n        return ast.Div()\n\n    def mutate_Mult_to_FloorDiv(  # noqa: N802\n        self, node: ast.Mult\n    ) -> ast.FloorDiv | None:\n        \"\"\"Mutate a Mult operator to a FloorDiv operator.\n\n        Args:\n            node: The Mult operator to mutate.\n\n        Returns:\n            The mutated operator, or None if the operator should not be mutated.\n        \"\"\"\n        if not self.should_mutate(node):\n            return None\n\n        return ast.FloorDiv()\n\n    def mutate_Mult_to_Pow(self, node: ast.Mult) -> ast.Pow | None:  # noqa: N802\n        \"\"\"Mutate a Mult operator to a Pow operator.\n\n        Args:\n            node: The Mult operator to mutate.\n\n        Returns:\n            The mutated operator, or None if the operator should not be mutated.\n        \"\"\"\n        if not self.should_mutate(node):\n            return None\n\n        return ast.Pow()\n\n    def mutate_Div_to_Mult(self, node: ast.Div) -> ast.Mult | None:  # noqa: N802\n        \"\"\"Mutate a Div operator to a Mult operator.\n\n        Args:\n            node: The Div operator to mutate.\n\n        Returns:\n            The mutated operator, or None if the operator should not be mutated.\n        \"\"\"\n        if not self.should_mutate(node):\n            return None\n\n        return ast.Mult()\n\n    def mutate_Div_to_FloorDiv(  # noqa: N802\n        self, node: ast.Div\n    ) -> ast.FloorDiv | None:\n        \"\"\"Mutate a Div operator to a FloorDiv operator.\n\n        Args:\n            node: The Div operator to mutate.\n\n        Returns:\n            The mutated operator, or None if the operator should not be mutated.\n        \"\"\"\n        if not self.should_mutate(node):\n            return None\n\n        return ast.FloorDiv()\n\n    def mutate_FloorDiv_to_Div(  # noqa: N802\n        self, node: ast.FloorDiv\n    ) -> ast.Div | None:\n        \"\"\"Mutate a FloorDiv operator to a Div operator.\n\n        Args:\n            node: The FloorDiv operator to mutate.\n\n        Returns:\n            The mutated operator, or None if the operator should not be mutated.\n        \"\"\"\n        if not self.should_mutate(node):\n            return None\n\n        return ast.Div()\n\n    def mutate_FloorDiv_to_Mult(  # noqa: N802\n        self, node: ast.FloorDiv\n    ) -> ast.Mult | None:\n        \"\"\"Mutate a FloorDiv operator to a Mult operator.\n\n        Args:\n            node: The FloorDiv operator to mutate.\n\n        Returns:\n            The mutated operator, or None if the operator should not be mutated.\n        \"\"\"\n        if not self.should_mutate(node):\n            return None\n\n        return ast.Mult()\n\n    def mutate_Mod(self, node: ast.Mod) -> ast.Mult | None:  # noqa: N802\n        \"\"\"Mutate a Mod operator to a Mult operator.\n\n        Args:\n            node: The Mod operator to mutate.\n\n        Returns:\n            The mutated operator, or None if the operator should not be mutated.\n        \"\"\"\n        if not self.should_mutate(node):\n            return None\n\n        return ast.Mult()\n\n    def mutate_Pow(self, node: ast.Pow) -> ast.Mult | None:  # noqa: N802\n        \"\"\"Mutate a Pow operator to a Mult operator.\n\n        Args:\n            node: The Pow operator to mutate.\n\n        Returns:\n            The mutated operator, or None if the operator should not be mutated.\n        \"\"\"\n        if not self.should_mutate(node):\n            return None\n\n        return ast.Mult()\n\n\nclass ArithmeticOperatorReplacement(AbstractArithmeticOperatorReplacement):\n    \"\"\"A class that mutates arithmetic operators by replacing them.\"\"\"\n\n    def should_mutate(self, node: ast.AST) -> bool:  # noqa: D102\n        parent = node.parent  # type: ignore[attr-defined]\n        return not isinstance(parent, ast.AugAssign)\n\n    def mutate_USub(self, node: ast.USub) -> ast.UAdd:  # noqa: N802\n        \"\"\"Mutate a USub operator to a UAdd operator.\n\n        Args:\n            node: The USub operator to mutate.\n\n        Returns:\n            The mutated operator.\n        \"\"\"\n        return ast.UAdd()\n\n    def mutate_UAdd(self, node: ast.UAdd) -> ast.USub:  # noqa: N802\n        \"\"\"Mutate a UAdd operator to a USub operator.\n\n        Args:\n            node: The UAdd operator to mutate.\n\n        Returns:\n            The mutated operator.\n        \"\"\"\n        return ast.USub()\n\n### Source File Dependency Files Content\n### Dependency File: base.py\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides base classes for mutation operators.\n\nBased on https://github.com/se2p/mutpy-pynguin/blob/main/mutpy/operators/base.py\nand integrated in Pynguin.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport abc\nimport ast\nimport copy\nimport re\n\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING\nfrom typing import TypeVar\n\n\nif TYPE_CHECKING:\n    import types\n\n    from collections.abc import Callable\n    from collections.abc import Generator\n    from collections.abc import Iterable\n\n\ndef fix_lineno(node: ast.AST) -> None:\n    \"\"\"Fix the line number of a node if it is not set.\n\n    Args:\n        node: The node to fix.\n    \"\"\"\n    parent = node.parent  # type: ignore[attr-defined]\n    if not hasattr(node, \"lineno\") and parent is not None and hasattr(parent, \"lineno\"):\n        parent_lineno = parent.lineno\n        node.lineno = parent_lineno  # type: ignore[attr-defined]\n\n\ndef fix_node_internals(old_node: ast.AST, new_node: ast.AST) -> None:\n    \"\"\"Fix the internals of a node.\n\n    Args:\n        old_node: The old node.\n        new_node: The new node.\n    \"\"\"\n    if not hasattr(new_node, \"parent\"):\n        old_node_children = old_node.children  # type: ignore[attr-defined]\n        old_node_parent = old_node.parent  # type: ignore[attr-defined]\n        new_node.children = old_node_children  # type: ignore[attr-defined]\n        new_node.parent = old_node_parent  # type: ignore[attr-defined]\n\n    if not hasattr(new_node, \"lineno\") and hasattr(old_node, \"lineno\"):\n        old_node_lineno = old_node.lineno\n        new_node.lineno = old_node_lineno  # type: ignore[attr-defined]\n\n    if hasattr(old_node, \"marker\"):\n        old_node_marker = old_node.marker\n        new_node.marker = old_node_marker  # type: ignore[attr-defined]\n\n\ndef set_lineno(node: ast.AST, lineno: int) -> None:\n    \"\"\"Set the line number of a node.\n\n    Args:\n        node: The node to set the line number for.\n        lineno: The line number to set.\n    \"\"\"\n    for child_node in ast.walk(node):\n        if hasattr(child_node, \"lineno\"):\n            child_node.lineno = lineno\n\n\nT = TypeVar(\"T\", bound=ast.AST)\n\n\ndef shift_lines(nodes: list[T], shift_by: int = 1) -> None:\n    \"\"\"Shift the line numbers of a list of nodes.\n\n    Args:\n        nodes: The nodes to shift.\n        shift_by: The amount to shift by.\n    \"\"\"\n    for node in nodes:\n        ast.increment_lineno(node, shift_by)\n\n\n@dataclass(frozen=True)\nclass Mutation:\n    \"\"\"Represents a mutation.\"\"\"\n\n    node: ast.AST\n    replacement_node: ast.AST\n    operator: type[MutationOperator]\n    visitor_name: str\n\n    def __post_init__(self):\n        \"\"\"Initialize the mutation.\n\n        Raises:\n            ValueError: If the visitor is not found in the operator.\n        \"\"\"\n        if self.visitor_name not in dir(self.operator):\n            raise ValueError(f\"Visitor {self.visitor_name} not found in operator {self.operator}\")\n\n\ndef copy_node(node: T) -> T:\n    \"\"\"Copy a node.\n\n    Args:\n        node: The node to copy.\n\n    Returns:\n        The copied node.\n    \"\"\"\n    parent = node.parent  # type: ignore[attr-defined]\n    return copy.deepcopy(\n        node,\n        memo={\n            id(parent): parent,\n        },\n    )\n\n\nclass MutationOperator:\n    \"\"\"A class that represents a mutation operator.\"\"\"\n\n    @classmethod\n    def mutate(\n        cls,\n        node: T,\n        module: types.ModuleType,\n        only_mutation: Mutation | None = None,\n    ) -> Generator[tuple[Mutation, ast.AST]]:\n        \"\"\"Mutate a node.\n\n        This method will temporarily modify the node provided and yield itself modified\n        with the mutations. If you want to keep the original node while using the\n        generator, you should copy it before passing it to this method.\n\n        Args:\n            node: The node to mutate.\n            module: The module to use.\n            only_mutation: The mutation to apply.\n\n        Yields:\n            A tuple containing the mutation and the mutated node.\n        \"\"\"\n        operator = cls(module, only_mutation)\n\n        for (\n            current_node,\n            replacement_node,\n            mutated_node,\n            visitor_name,\n        ) in operator.visit(node):\n            yield (\n                Mutation(current_node, replacement_node, cls, visitor_name),\n                mutated_node,\n            )\n\n    def __init__(\n        self,\n        module: types.ModuleType,\n        only_mutation: Mutation | None,\n    ) -> None:\n        \"\"\"Initializes the operator.\n\n        Args:\n            module: The module to use.\n            only_mutation: The mutation to apply.\n        \"\"\"\n        self.module = module\n        self.only_mutation = only_mutation\n\n    def visit(self, node: T) -> Generator[tuple[ast.AST, ast.AST, ast.AST, str]]:\n        \"\"\"Visit a node.\n\n        This method will temporarily modify the node provided and yield itself modified\n        with other information. If you want to keep the original node while using the\n        generator, you should copy it before passing it to this method.\n\n        Args:\n            node: The node to visit.\n\n        Yields:\n            A tuple (current node, replacement node, mutated node, visitor name).\n        \"\"\"\n        node_children = node.children  # type: ignore[attr-defined]\n\n        if (\n            self.only_mutation\n            and self.only_mutation.node != node\n            and self.only_mutation.node not in node_children\n        ):\n            return\n\n        fix_lineno(node)\n\n        for visitor in self._find_visitors(node):\n            if (\n                self.only_mutation is None\n                or (\n                    self.only_mutation.node == node\n                    and self.only_mutation.visitor_name == visitor.__name__\n                )\n            ) and (mutated_node := visitor(node)) is not None:\n                fix_node_internals(node, mutated_node)\n                ast.fix_missing_locations(mutated_node)\n\n                yield node, mutated_node, mutated_node, visitor.__name__\n\n        yield from self._generic_visit(node)\n\n    def _generic_visit(self, node: ast.AST) -> Generator[tuple[ast.AST, ast.AST, ast.AST, str]]:\n        for field, old_value in ast.iter_fields(node):\n            generator: Iterable[tuple[ast.AST, ast.AST, str]]\n            if isinstance(old_value, list):\n                generator = self._generic_visit_list(old_value)\n            elif isinstance(old_value, ast.AST):\n                generator = self._generic_visit_real_node(node, field, old_value)\n            else:\n                generator = ()\n\n            for current_node, replacement_node, visitor_name in generator:\n                yield current_node, replacement_node, node, visitor_name\n\n    def _generic_visit_list(self, old_value: list) -> Generator[tuple[ast.AST, ast.AST, str]]:\n        for position, value in enumerate(old_value.copy()):\n            if isinstance(value, ast.AST):\n                for (\n                    current_node,\n                    replacement_node,\n                    mutated_node,\n                    visitor_name,\n                ) in self.visit(value):\n                    old_value[position] = mutated_node\n                    yield current_node, replacement_node, visitor_name\n\n                old_value[position] = value\n\n    def _generic_visit_real_node(\n        self, node: ast.AST, field: str, old_value: ast.AST\n    ) -> Generator[tuple[ast.AST, ast.AST, str]]:\n        for current_node, replacement_node, mutated_node, visitor_name in self.visit(old_value):\n            setattr(node, field, mutated_node)\n            yield current_node, replacement_node, visitor_name\n\n        setattr(node, field, old_value)\n\n    def _find_visitors(self, node: T) -> list[Callable[[T], ast.AST | None]]:\n        node_name = node.__class__.__name__\n        method_prefix_pattern = re.compile(f\"^mutate_{node_name}(_\\\\w+)?$\")\n        return [\n            visitor\n            for attr in dir(self)\n            if method_prefix_pattern.match(attr) is not None\n            and callable(visitor := getattr(self, attr))\n        ]\n\n\nclass AbstractUnaryOperatorDeletion(abc.ABC, MutationOperator):\n    \"\"\"An abstract class that mutates unary operators by deleting them.\"\"\"\n\n    @abc.abstractmethod\n    def get_operator_type(self) -> type:\n        \"\"\"Get the operator type.\n\n        Returns:\n            The operator type.\n        \"\"\"\n\n    def mutate_UnaryOp(self, node: ast.UnaryOp) -> ast.expr | None:  # noqa: N802\n        \"\"\"Mutate a unary operator.\n\n        Args:\n            node: The node to mutate.\n\n        Returns:\n            The mutated node, or None if the node should not be mutated.\n        \"\"\"\n        if not isinstance(node.op, self.get_operator_type()):\n            return None\n\n        return node.operand\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:32"}, {"role": "assistant", "content": "```python\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132023 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\nimport ast\nimport inspect\n\nfrom pynguin.assertion.mutation_analysis.operators.arithmetic import (\n    ArithmeticOperatorDeletion,\n)\nfrom pynguin.assertion.mutation_analysis.operators.arithmetic import (\n    ArithmeticOperatorReplacement,\n)\nfrom tests.testutils import assert_mutation\n\n\ndef test_usub_deletion():\n    assert_mutation(\n        ArithmeticOperatorDeletion,\n        inspect.cleandoc(\n            \"\"\"\n            x = 0\n            y = -x\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x\n                \"\"\"\n            ): (\"mutate_UnaryOp\", ast.UnaryOp, ast.Name),\n        },\n    )\n\n\ndef test_uadd_deletion():\n    assert_mutation(\n        ArithmeticOperatorDeletion,\n        inspect.cleandoc(\n            \"\"\"\n            x = 0\n            y = +x\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x\n                \"\"\"\n            ): (\"mutate_UnaryOp\", ast.UnaryOp, ast.Name),\n        },\n    )\n\n\ndef test_add_to_sub_replacement():\n    assert_mutation(\n        ArithmeticOperatorReplacement,\n        inspect.cleandoc(\n            \"\"\"\n            x = 0\n            y = x + 1\n            z = x + 2\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x - 1\n                z = x + 2\n                \"\"\"\n            ): (\"mutate_Add\", ast.Add, ast.Sub),\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x + 1\n                z = x - 2\n                \"\"\"\n            ): (\"mutate_Add\", ast.Add, ast.Sub),\n        },\n    )\n\n\ndef test_sub_to_add_replacement():\n    assert_mutation(\n        ArithmeticOperatorReplacement,\n        inspect.cleandoc(\n            \"\"\"\n            x = 0\n            y = x - 1\n            z = x - 2\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x + 1\n                z = x - 2\n                \"\"\"\n            ): (\"mutate_Sub\", ast.Sub, ast.Add),\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x - 1\n                z = x + 2\n                \"\"\"\n            ): (\"mutate_Sub\", ast.Sub, ast.Add),\n        },\n    )\n\n\ndef test_mult_to_div_and_pow_replacement():\n    assert_mutation(\n        ArithmeticOperatorReplacement,\n        inspect.cleandoc(\n            \"\"\"\n            x = 0\n            y = x * 1\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x / 1\n                \"\"\"\n            ): (\"mutate_Mult_to_Div\", ast.Mult, ast.Div),\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x // 1\n                \"\"\"\n            ): (\"mutate_Mult_to_FloorDiv\", ast.Mult, ast.FloorDiv),\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x ** 1\n                \"\"\"\n            ): (\"mutate_Mult_to_Pow\", ast.Mult, ast.Pow),\n        },\n    )\n\n\ndef test_div_to_mult_and_floordiv_replacement():\n    assert_mutation(\n        ArithmeticOperatorReplacement,\n        inspect.cleandoc(\n            \"\"\"\n            x = 0\n            y = x / 1\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x * 1\n                \"\"\"\n            ): (\"mutate_Div_to_Mult\", ast.Div, ast.Mult),\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x // 1\n                \"\"\"\n            ): (\"mutate_Div_to_FloorDiv\", ast.Div, ast.FloorDiv),\n        },\n    )\n\n\ndef test_floor_div_to_mult_and_div_replacement():\n    assert_mutation(\n        ArithmeticOperatorReplacement,\n        inspect.cleandoc(\n            \"\"\"\n            x = 0\n            y = x // 1\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x * 1\n                \"\"\"\n            ): (\"mutate_FloorDiv_to_Mult\", ast.FloorDiv, ast.Mult),\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x / 1\n                \"\"\"\n            ): (\"mutate_FloorDiv_to_Div\", ast.FloorDiv, ast.Div),\n        },\n    )\n\n\ndef test_mod_to_mult_replacement():\n    assert_mutation(\n        ArithmeticOperatorReplacement,\n        inspect.cleandoc(\n            \"\"\"\n            x = 0\n            y = x % 1\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x * 1\n                \"\"\"\n            ): (\"mutate_Mod\", ast.Mod, ast.Mult),\n        },\n    )\n\n\ndef test_pow_to_mult_replacement():\n    assert_mutation(\n        ArithmeticOperatorReplacement,\n        inspect.cleandoc(\n            \"\"\"\n            x = 0\n            y = x ** 1\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = x * 1\n                \"\"\"\n            ): (\"mutate_Pow\", ast.Pow, ast.Mult),\n        },\n    )\n\n\ndef test_augmented_assign_ignore():\n    assert_mutation(\n        ArithmeticOperatorReplacement,\n        inspect.cleandoc(\n            \"\"\"\n            x = 0\n            x += 1\n            \"\"\"\n        ),\n        {},\n    )\n\n\ndef test_usub_replacement():\n    assert_mutation(\n        ArithmeticOperatorReplacement,\n        inspect.cleandoc(\n            \"\"\"\n            x = 0\n            y = -x\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = +x\n                \"\"\"\n            ): (\"mutate_USub\", ast.USub, ast.UAdd),\n        },\n    )\n\n\ndef test_uadd_replacement():\n    assert_mutation(\n        ArithmeticOperatorReplacement,\n        inspect.cleandoc(\n            \"\"\"\n            x = 0\n            y = +x\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                x = 0\n                y = -x\n                \"\"\"\n            ): (\"mutate_UAdd\", ast.UAdd, ast.USub),\n        },\n    )\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pynguin\nTest File Path: pynguin\\test_assertion_trace\\test_assertion_trace.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides an output trace.\"\"\"\n\nfrom __future__ import annotations\n\nimport dataclasses\n\nfrom collections import defaultdict\nfrom typing import TYPE_CHECKING\n\nfrom pynguin.utils.orderedset import OrderedSet\n\n\nif TYPE_CHECKING:\n    import pynguin.assertion.assertion as ass\n    import pynguin.testcase.statement as stmt\n\n\nclass AssertionTrace:\n    \"\"\"Stores the states that have been observed during execution.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Create new assertion trace.\"\"\"\n        # One entry per statement, i.e., the assertions after executing that statement.\n        self.trace: dict[int, OrderedSet[ass.Assertion]] = defaultdict(OrderedSet)\n\n    def add_entry(self, position: int, assertion: ass.Assertion) -> None:\n        \"\"\"Add an entry to this trace.\n\n        Args:\n            position: the position of the statement after whose execution the\n                state is observed.\n            assertion: the made assertion.\n        \"\"\"\n        self.trace[position].add(assertion)\n\n    def get_assertions(self, statement: stmt.Statement) -> OrderedSet[ass.Assertion]:\n        \"\"\"Get all assertions contained within this trace for the given statement.\n\n        Args:\n            statement: the statement for which all recorded assertions\n                should be generated.\n\n        Returns:\n            All assertions in this trace for the given statement.\n        \"\"\"\n        position = statement.get_position()\n        if position in self.trace:\n            return OrderedSet(self.trace[position])\n        return OrderedSet()\n\n    def get_all_assertions(self) -> dict[int, OrderedSet[ass.Assertion]]:\n        \"\"\"Get all generated assertions.\n\n        Returns:\n            A dict that maps every statement position to all recorded assertions.\n\n        \"\"\"\n        return dict(self.trace)\n\n    def clear(self) -> None:\n        \"\"\"Clear this trace.\"\"\"\n        self.trace.clear()\n\n    def clone(self) -> AssertionTrace:\n        \"\"\"Clone this trace.\n\n        Returns:\n            a clone of this trace.\n        \"\"\"\n        copy: AssertionTrace = AssertionTrace()\n        for stmt_key, stmt_value in self.trace.items():\n            copy.trace[stmt_key] = OrderedSet()\n            for entry in stmt_value:\n                copy.trace[stmt_key].add(entry)\n        return copy\n\n\n@dataclasses.dataclass\nclass AssertionVerificationTrace:\n    \"\"\"Trace for assertion verification.\"\"\"\n\n    # Assertion that did not hold\n    failed: dict[int, OrderedSet[int]] = dataclasses.field(\n        default_factory=lambda: defaultdict(OrderedSet)\n    )\n    # Assertion whose execution raised an error\n    error: dict[int, OrderedSet[int]] = dataclasses.field(\n        default_factory=lambda: defaultdict(OrderedSet)\n    )\n\n    def merge(self, other: AssertionVerificationTrace) -> None:\n        \"\"\"Merge another trace into this trace.\n\n        Args:\n            other: The other trace\n\n        \"\"\"\n        for pos, assertions in other.failed.items():\n            self.failed[pos].update(assertions)\n        for pos, assertions in other.error.items():\n            self.error[pos].update(assertions)\n\n    def was_violated(self, stmt_idx: int, assertion_idx: int) -> bool:\n        \"\"\"Was the assertion at the given position violated?\n\n        This may happen because the assertion failed or another error occurred.\n\n        Args:\n            stmt_idx: The statement index.\n            assertion_idx: The assertion index.\n\n        Returns:\n            True, if the assertion was violated.\n        \"\"\"\n        if stmt_idx in self.failed and assertion_idx in self.failed[stmt_idx]:\n            return True\n        return bool(stmt_idx in self.error and assertion_idx in self.error[stmt_idx])\n\n### Source File Dependency Files Content\n### Dependency File: assertion.py\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides a base class for assertions.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom typing import TYPE_CHECKING\nfrom typing import Any\n\n\nif TYPE_CHECKING:\n    import pynguin.testcase.variablereference as vr\n\n    from pynguin.slicer.executionflowbuilder import UniqueInstruction\n\n\nclass Assertion:\n    \"\"\"Base class for assertions.\"\"\"\n\n    def __init__(self):  # noqa: D107\n        # this makes an assertion stateful, but is required for caching in slicing\n        self._checked_instructions: list[UniqueInstruction] = []\n\n    @property\n    def checked_instructions(self) -> list[UniqueInstruction]:\n        \"\"\"The instructions that were checked by the execution of the assertion.\n\n        Returns:\n            The instructions that were checked by the execution of the assertion.\n        \"\"\"\n        return self._checked_instructions\n\n    @abstractmethod\n    def accept(self, visitor: AssertionVisitor) -> None:\n        \"\"\"Accept an assertion visitor.\n\n        Args:\n            visitor: the visitor that is accepted.\n        \"\"\"\n\n    @abstractmethod\n    def clone(self, memo: dict[vr.VariableReference, vr.VariableReference]) -> Assertion:\n        \"\"\"Clone this assertion.\n\n        Args:\n            memo: Mapping from old to new variables.\n\n        Returns: the cloned assertion\n        \"\"\"\n\n    @abstractmethod\n    def __eq__(self, other: object) -> bool:\n        pass  # pragma: no cover\n\n    @abstractmethod\n    def __hash__(self) -> int:\n        pass  # pragma: no cover\n\n\nclass ReferenceAssertion(Assertion, ABC):\n    \"\"\"An assertion on a single reference.\"\"\"\n\n    def __init__(self, source: vr.Reference):  # noqa: D107\n        super().__init__()\n        self._source = source\n\n    @property\n    def source(self) -> vr.Reference:\n        \"\"\"Provides the reference on which we assert something.\n\n        Returns:\n            The reference on which we assert something.\n        \"\"\"\n        return self._source\n\n\nclass TypeNameAssertion(ReferenceAssertion):\n    \"\"\"An assertion that a reference has a type with a certain name.\n\n    We compare the string representation of the fully qualified name.\n    Using an isinstance check would also be possible, but classes might not always\n    be accessible, for example when nested inside a function.\n\n    For example:\n        int_0 = 42\n        assert f\"{type(int_0).__module__}.{type(int_0).__qualname__}\" == \"builtins.int\"\n    \"\"\"\n\n    def __init__(self, source: vr.Reference, module: str, qualname: str):  # noqa: D107\n        super().__init__(source)\n        self._module = module\n        self._qualname = qualname\n\n    @property\n    def module(self) -> str:\n        \"\"\"Provides the module name.\n\n        Returns:\n            The module name\n        \"\"\"\n        return self._module\n\n    @property\n    def qualname(self) -> str:\n        \"\"\"Provides the qualname name.\n\n        Returns:\n            The qualname\n        \"\"\"\n        return self._qualname\n\n    def accept(self, visitor: AssertionVisitor) -> None:  # noqa: D102\n        visitor.visit_type_name_assertion(self)\n\n    def clone(  # noqa: D102\n        self, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> TypeNameAssertion:\n        return TypeNameAssertion(self._source.clone(memo), self._module, self._qualname)\n\n    def __eq__(self, other: object) -> bool:\n        return (\n            isinstance(other, TypeNameAssertion)\n            and self._source == other._source\n            and self._module == other._module\n            and self._qualname == other._qualname\n        )\n\n    def __hash__(self) -> int:\n        return hash((self._source, self._module, self._qualname))\n\n    def __repr__(self):\n        return f\"TypeNameAssertion({self._source!r}, {self._module}, {self._qualname})\"\n\n\nclass FloatAssertion(ReferenceAssertion):\n    \"\"\"An assertion on the float value of a reference.\n\n    For example:\n        assert float_0 == pytest.approx(42, rel=0.01, abs=0.01)\n    \"\"\"\n\n    def __init__(self, source: vr.Reference, value: float):  # noqa: D107\n        super().__init__(source)\n        self._value = value\n\n    @property\n    def value(self) -> float:\n        \"\"\"Provides the value.\n\n        Returns:\n            The float value\n        \"\"\"\n        return self._value\n\n    def accept(self, visitor: AssertionVisitor) -> None:  # noqa: D102\n        visitor.visit_float_assertion(self)\n\n    def clone(  # noqa: D102\n        self, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> FloatAssertion:\n        return FloatAssertion(self.source.clone(memo), self._value)\n\n    def __eq__(self, other: object) -> bool:\n        return (\n            isinstance(other, FloatAssertion)\n            and self._source == other._source\n            and self._value == other._value\n        )\n\n    def __hash__(self) -> int:\n        return hash((self._source, self._value))\n\n    def __repr__(self):\n        return f\"FloatAssertion({self._source!r}, {self._value!r})\"\n\n\nclass ObjectAssertion(ReferenceAssertion):\n    \"\"\"An assertion on the object behind a reference.\n\n    This can be anything where the object can be reliably copied, besides float, which\n    is handled by FloatAssertion.\n\n    For example:\n        assert var_0 == [1,2,3]\n        assert var_1 == {Foo.BAR}\n        assert var_2 == \"Foobar\"\n    \"\"\"\n\n    def __init__(self, source: vr.Reference, value: Any):  # noqa: D107\n        super().__init__(source)\n        self._object = value\n\n    @property\n    def object(self) -> Any:\n        \"\"\"Provides the object used for comparison.\n\n        Returns:\n            The object used for comparison.\n        \"\"\"\n        return self._object\n\n    def accept(self, visitor: AssertionVisitor) -> None:  # noqa: D102\n        visitor.visit_object_assertion(self)\n\n    def clone(  # noqa: D102\n        self, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> ObjectAssertion:\n        return ObjectAssertion(self.source.clone(memo), self._object)\n\n    def __eq__(self, other: Any) -> bool:  # noqa: PYI032\n        return (\n            isinstance(other, ObjectAssertion)\n            and self._source == other._source\n            and self._object == other._object\n        )\n\n    def __hash__(self) -> int:\n        # We cannot include the value in the hash, as it might be unhashable.\n        # For example, dicts, lists and sets are not hashable.\n        return 17 * hash(self._source) + 31\n\n    def __repr__(self):\n        return f\"ObjectAssertion({self._source!r}, {self._object!r})\"\n\n\nclass CollectionLengthAssertion(ReferenceAssertion):\n    \"\"\"An assertion on the length of a reference.\n\n    This can be necessary for a collection that contains elements that we can not\n    directly assert on. Though this kind of assertion is less preferable because it is\n    less precise.\n\n    For example:\n        assert len(var_0) == 42\n    \"\"\"\n\n    def __init__(self, source: vr.Reference, length: int):  # noqa: D107\n        super().__init__(source)\n        self._length = length\n\n    @property\n    def length(self) -> int:\n        \"\"\"The expected length.\n\n        Returns:\n            The expected length.\n        \"\"\"\n        return self._length\n\n    def accept(self, visitor: AssertionVisitor) -> None:  # noqa: D102\n        visitor.visit_collection_length_assertion(self)\n\n    def clone(  # noqa: D102\n        self, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> CollectionLengthAssertion:\n        return CollectionLengthAssertion(self.source.clone(memo), self._length)\n\n    def __eq__(self, other: object) -> bool:\n        return (\n            isinstance(other, CollectionLengthAssertion)\n            and self._source == other._source\n            and self._length == other._length\n        )\n\n    def __hash__(self) -> int:\n        return hash((self._source, self._length))\n\n    def __repr__(self):\n        return f\"CollectionLengthAssertion({self._source!r}, {self._length})\"\n\n\nclass ExceptionAssertion(Assertion):\n    \"\"\"An assertion that indicates that a statement raised an exception.\"\"\"\n\n    def __init__(self, module: str, exception_type_name: str):\n        \"\"\"Create a new exception assertion.\n\n        Args:\n            module: The module of the raised exception.\n            exception_type_name: The name of the raised exception.\n        \"\"\"\n        super().__init__()\n        self._module: str = module\n        # We use the name here because the type may be defined multiple times,\n        # for example during mutation analysis, however, equality on such types does not\n        # hold\n        self._exception_type_name: str = exception_type_name\n\n    def accept(self, visitor: AssertionVisitor) -> None:  # noqa: D102\n        visitor.visit_exception_assertion(self)\n\n    def clone(  # noqa: D102\n        self, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> Assertion:\n        return ExceptionAssertion(self._module, self._exception_type_name)\n\n    @property\n    def exception_type_name(self) -> str:\n        \"\"\"Provides the name of the raised exception.\n\n        Returns:\n            the name of the raised exception.\n        \"\"\"\n        return self._exception_type_name\n\n    @property\n    def module(self) -> str:\n        \"\"\"Provides the module of the raised exception.\n\n        Returns:\n            the module of the raised exception\n        \"\"\"\n        return self._module\n\n    def __eq__(self, other: object) -> bool:\n        return (\n            isinstance(other, ExceptionAssertion)\n            and self._exception_type_name == other._exception_type_name\n            and self._module == other._module\n        )\n\n    def __hash__(self) -> int:\n        return hash((self._module, self._exception_type_name))\n\n    def __repr__(self):\n        return f\"ExceptionAssertion({self._module}, {self._exception_type_name})\"\n\n\nclass AssertionVisitor:\n    \"\"\"Abstract visitor for assertions.\"\"\"\n\n    @abstractmethod\n    def visit_type_name_assertion(self, assertion: TypeNameAssertion) -> None:\n        \"\"\"Visit a type name assertion.\n\n        Args:\n            assertion: the visited assertion\n\n        \"\"\"\n\n    @abstractmethod\n    def visit_float_assertion(self, assertion: FloatAssertion) -> None:\n        \"\"\"Visit a float assertion.\n\n        Args:\n            assertion: the visited assertion\n\n        \"\"\"\n\n    @abstractmethod\n    def visit_object_assertion(self, assertion: ObjectAssertion) -> None:\n        \"\"\"Visit an object assertion.\n\n        Args:\n            assertion: the visited assertion\n\n        \"\"\"\n\n    @abstractmethod\n    def visit_collection_length_assertion(self, assertion: CollectionLengthAssertion) -> None:\n        \"\"\"Visit a collection length assertion.\n\n        Args:\n            assertion: the visited assertion\n\n        \"\"\"\n\n    @abstractmethod\n    def visit_exception_assertion(self, assertion: ExceptionAssertion) -> None:\n        \"\"\"Visit an exception assertion.\n\n        Args:\n            assertion: the visited assertion\n\n        \"\"\"\n\n\n### Dependency File: orderedset.py\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides the implementation of an ordered set.\n\nThe following code is taken from https://github.com/pantsbuild/pants\nWe made minor changes so that our linting pipeline accepts the code.\n\nCopyright 2020 Pants project contributors.\nLicensed under the Apache License, Version 2.0.\nSPDX-FileCopyrightText: 2020 Pants project contributors\nSPDX-License-Identifier: Apache-2.0\n\nAn OrderedSet is a set that remembers its insertion order, and a FrozenOrderedSet is\none that is also immutable. Based on the library `ordered-set` developed by\nElia Robyn Lake (Robyn Speer) and released under the MIT license:\nhttps://github.com/LuminosoInsight/ordered-set.\nCopyright (c) 2012-2022 Elia Robyn Lake\nSPDX-FileCopyrightText: 2012-2022 Elia Robyn Lake\nSPDX-License-Identifier: MIT\n\n\nThe library `ordered-set` is itself originally based on a recipe originally posted to\nActivateState Recipes by Raymond Hettiger and released under the MIT license:\nhttp://code.activestate.com/recipes/576694/.\nCopyright (c) 2009 Raymond Hettiger\nSPDX-FileCopyrightText: 2009 Raymond Hettiger\nSPDX-License-Identifier: MIT\n\"\"\"\n\nfrom __future__ import annotations\n\nimport itertools\n\nfrom collections.abc import Hashable\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import MutableSet\nfrom collections.abc import Sequence\nfrom collections.abc import Set as AbstractSet\nfrom typing import Any\nfrom typing import TypeVar\nfrom typing import cast\nfrom typing import overload\n\nfrom typing_extensions import Self\n\n\nT = TypeVar(\"T\")\nT_co = TypeVar(\"T_co\", covariant=True)\n\n\nclass _AbstractOrderedSet(AbstractSet[T], Sequence[T]):  # noqa: PLW1641\n    \"\"\"Common functionality shared between OrderedSet and FrozenOrderedSet.\"\"\"\n\n    @overload\n    def __getitem__(self, index: int) -> T:\n        pass\n\n    @overload\n    def __getitem__(self, index: slice) -> _AbstractOrderedSet[T]:\n        pass\n\n    def __getitem__(self, index: int | slice) -> T:  # type: ignore[misc]\n        \"\"\"Lookup item at given position. Caution, as this runs in O(n).\n\n        Args:\n            index: The index whose value we want to retrieve.\n\n        Returns:\n            The value of the given index.\n\n        Raises:\n            NotImplementedError: When given a slice.\n            IndexError: When the index is out of range.\n        \"\"\"\n        if isinstance(index, slice):\n            raise NotImplementedError(\"Slicing currently not supported.\")\n        for i, key in enumerate(self._items.keys()):\n            if i == index:\n                return key\n        raise IndexError(\"Index out of range.\")\n\n    def __init__(self, iterable: Iterable[T] | None = None) -> None:\n        # Using a dictionary, rather than using the recipe's original\n        # `self |= iterable`, results in a ~20% performance increase for the\n        # constructor.\n        #\n        # NB: Dictionaries are ordered in Python 3.6+. While this was not formalized\n        # until Python 3.7, Python 3.6 uses this behavior; Pants requires CPython 3.6+\n        # to run, so this assumption is safe for us to rely on.\n        self._items: dict[T, None] = dict.fromkeys(iterable or ())\n\n    def __len__(self) -> int:\n        return len(self._items)\n\n    def __copy__(self) -> Self:\n        return self.__class__(self)\n\n    def __contains__(self, key: Any) -> bool:\n        return key in self._items\n\n    def __iter__(self) -> Iterator[T]:\n        return iter(self._items)\n\n    def __reversed__(self) -> Iterator[T]:\n        return reversed(tuple(self._items.keys()))\n\n    def __repr__(self) -> str:\n        name = self.__class__.__name__\n        if not self:\n            return f\"{name}()\"\n        return f\"{name}({list(self)!r})\"\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return len(self._items) == len(other._items) and all(\n            x == y for x, y in zip(self._items, other._items, strict=True)\n        )\n\n    def __or__(self, other: Iterable[T]) -> Self:  # type: ignore[override]\n        return self.union(other)\n\n    def union(self, *others: Iterable[T]) -> Self:\n        \"\"\"Combines all unique items.\n\n        Each item's order is defined by its first appearance # noqa: DAR201,DAR101.\n\n        Args:\n            *others: the iterables to union with\n\n        Returns:\n            A new ordered set containing the union.\n        \"\"\"\n        # Differences with AbstractSet: our set union forces \"other\" to have the same\n        # type. That is, while AbstractSet allows {1, 2, 3} | {(True, False)} resulting\n        # in set[int | tuple[bool, bool]], the analogous for descendants  of\n        # _TAbstractOrderedSet is not allowed.\n        #\n        # GOTCHA: given _TAbstractOrderedSet[S]:\n        #   if T is a subclass of S => _TAbstractOrderedSet[S] => *appears* to perform\n        #     unification but it doesn't\n        #   if S is a subclass of T => type error (while AbstractSet would resolve to\n        #     AbstractSet[T])\n        merged_iterables = itertools.chain([cast(\"Iterable[T]\", self)], others)\n        return self.__class__(itertools.chain.from_iterable(merged_iterables))\n\n    def __and__(self, other: Iterable[T]) -> Self:\n        # The parent class's implementation of this is backwards.\n        return self.intersection(other)\n\n    def intersection(self, *others: Iterable[T]) -> Self:\n        \"\"\"Returns elements in common between all sets.\n\n        Order is defined only by the first set.\n\n        Args:\n            *others: The iterable to intersect with\n\n        Returns:\n            A new set containing the intersection.\n        \"\"\"\n        cls = self.__class__\n        if not others:\n            return cls(self)\n        common = set.intersection(*(set(other) for other in others))\n        return cls(item for item in self if item in common)\n\n    def difference(self, *others: Iterable[T]) -> Self:\n        \"\"\"Returns all elements that are in this set but not the others.\n\n        Args:\n            *others: The iterable to intersect with\n\n        Returns:\n            A new set containing the difference.\n        \"\"\"\n        cls = self.__class__\n        if not others:\n            return cls(self)\n        other = set.union(*(set(other) for other in others))\n        return cls(item for item in self if item not in other)\n\n    def issubset(self, other: Iterable[T]) -> bool:\n        \"\"\"Report whether another set contains this set.\n\n        Args:\n            other: The set to check\n\n        Returns:\n            True, if this is a subset of other.\n        \"\"\"\n        try:\n            # Fast check for obvious cases\n            if len(self) > len(other):  # type: ignore[arg-type]\n                return False\n        except TypeError:\n            pass\n        return all(item in other for item in self)\n\n    def issuperset(self, other: Iterable[T]) -> bool:\n        \"\"\"Report whether this set contains another set.\n\n        Args:\n            other: The set to check\n\n        Returns:\n            True, if this is a superset of other.\n        \"\"\"\n        try:\n            # Fast check for obvious cases\n            if len(self) < len(other):  # type: ignore[arg-type]\n                return False\n        except TypeError:\n            pass\n        return all(item in self for item in other)\n\n    def __xor__(self, other: Iterable[T]) -> Self:  # type: ignore[override]\n        return self.symmetric_difference(other)\n\n    def symmetric_difference(self, other: Iterable[T]) -> Self:\n        \"\"\"Computes the symmetric difference.\n\n        Return the symmetric difference of this OrderedSet and another set as a new\n        OrderedSet. That is, the new set will contain all elements that are in exactly\n        one of the sets. Their order will be preserved, with elements from `self`\n        preceding elements from `other`.\n\n        Args:\n            other: The other set\n\n        Returns:\n            The symmetric difference.\n        \"\"\"\n        cls = self.__class__\n        diff1 = cls(self).difference(other)\n        diff2 = cls(other).difference(self)\n        return diff1.union(diff2)\n\n\nclass OrderedSet(_AbstractOrderedSet[T], MutableSet[T]):\n    \"\"\"A mutable set that retains its order.\n\n    This is not safe to use with the V2 engine.\n    \"\"\"\n\n    def add(self, value: T) -> None:\n        \"\"\"Add `value` as an item to this OrderedSet.\n\n        Args:\n            value: the object to add\n        \"\"\"\n        self._items[value] = None\n\n    def update(self, iterable: Iterable[T]) -> None:\n        \"\"\"Update the set with the given iterable sequence.\n\n        Args:\n            iterable: The iterable to insert into this set\n        \"\"\"\n        for item in iterable:\n            self._items[item] = None\n\n    def discard(self, value: T) -> None:  # noqa: D102\n        self._items.pop(value, None)\n\n    def clear(self) -> None:  # noqa: D102\n        self._items.clear()\n\n    def difference_update(self, *others: Iterable[T]) -> None:\n        \"\"\"Update this OrderedSet to remove items from one or more other sets.\n\n        Args:\n            *others: The sets to difference update with.\n        \"\"\"\n        items_to_remove: set[T] = set()\n        for other in others:\n            items_as_set = set(other)\n            items_to_remove |= items_as_set\n        self._items = {item: None for item in self._items if item not in items_to_remove}\n\n    def intersection_update(self, other: Iterable[T]) -> None:\n        \"\"\"Computes the intersection inplace.\n\n        Update this OrderedSet to keep only items in another set, preserving their\n        order in this set.\n\n        Args:\n            other: The set to intersection update with.\n        \"\"\"\n        other = set(other)\n        self._items = {item: None for item in self._items if item in other}\n\n    def symmetric_difference_update(self, other: Iterable[T]) -> None:\n        \"\"\"Computes the symmetric difference inplace.\n\n        Update this OrderedSet to remove items from another set, then add items from\n        the other set that were not present in this set.\n\n        Args:\n            other: The other set.\n        \"\"\"\n        items_to_add = [item for item in other if item not in self]\n        items_to_remove = cast(\"set[T]\", set(other))\n        self._items = {item: None for item in self._items if item not in items_to_remove}\n        for item in items_to_add:\n            self._items[item] = None\n\n\nclass FrozenOrderedSet(_AbstractOrderedSet[T_co], Hashable):  # type: ignore[type-var]\n    \"\"\"A frozen (i.e. immutable) set that retains its order.\n\n    This is safe to use with the V2 engine.\n    \"\"\"\n\n    def __init__(self, iterable: Iterable[T_co] | None = None) -> None:  # noqa: D107\n        super().__init__(iterable)\n        self.__hash: int | None = None\n\n    def __hash__(self) -> int:\n        if self.__hash is None:\n            self.__hash = 0\n            for item in self._items:\n                self.__hash ^= hash(item)\n        return self.__hash\n\n\n### Dependency File: statement.py\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides a base implementation of a statement representation.\"\"\"\n\nfrom __future__ import annotations\n\nimport abc\nimport logging\nimport math\n\nfrom abc import abstractmethod\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import Generic\nfrom typing import TypeVar\nfrom typing import cast\n\nimport pynguin.assertion.assertion as ass\nimport pynguin.configuration as config\nimport pynguin.testcase.variablereference as vr\nimport pynguin.utils.generic.genericaccessibleobject as gao\n\nfrom pynguin.analyses.typesystem import ANY\nfrom pynguin.analyses.typesystem import InferredSignature\nfrom pynguin.analyses.typesystem import Instance\nfrom pynguin.analyses.typesystem import NoneType\nfrom pynguin.analyses.typesystem import ProperType\nfrom pynguin.analyses.typesystem import TypeInfo\nfrom pynguin.utils import randomness\nfrom pynguin.utils.mutation_utils import alpha_exponent_insertion\nfrom pynguin.utils.orderedset import OrderedSet\nfrom pynguin.utils.type_utils import is_optional_parameter\n\n\nif TYPE_CHECKING:\n    import pynguin.testcase.testcase as tc\n\n    from pynguin.analyses import constants\n\nT = TypeVar(\"T\")\n\n\nclass Statement(abc.ABC):\n    \"\"\"An abstract base class of a statement representation.\"\"\"\n\n    _logger = logging.getLogger(__name__)\n\n    def __init__(self, test_case: tc.TestCase) -> None:\n        \"\"\"Constructs a new statement.\n\n        Args:\n            test_case: The test case the statement belongs to\n        \"\"\"\n        self._test_case = test_case\n        self._assertions: OrderedSet[ass.Assertion] = OrderedSet()\n\n        # The variable defined by this statement, if any.\n        # This is intentionally not named 'return_value' because that name is reserved\n        # by the mocking framework which is used in our tests.\n        self.ret_val: vr.VariableReference | None = None\n\n    @property\n    def test_case(self) -> tc.TestCase:\n        \"\"\"Provides the test case in which this statement is used.\n\n        Returns:\n            The containing test case\n        \"\"\"\n        return self._test_case\n\n    @abstractmethod\n    def clone(\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> Statement:\n        \"\"\"Provides a deep clone of this statement.\n\n        Args:\n            test_case: the new test case in which the clone will be used.\n            memo: A memo that maps old variable reference to new ones.\n\n        Returns:\n            A deep clone of this statement  # noqa: DAR202\n        \"\"\"\n\n    @abstractmethod\n    def accept(self, visitor: StatementVisitor) -> None:\n        \"\"\"Accepts a visitor to visit this statement.\n\n        Args:\n            visitor: the statement visitor\n        \"\"\"\n\n    @abstractmethod\n    def accessible_object(self) -> gao.GenericAccessibleObject | None:\n        \"\"\"Provides the accessible which is used in this statement.\n\n        Returns:\n            The accessible used in the statement  # noqa: DAR202\n        \"\"\"\n\n    @abstractmethod\n    def mutate(self) -> bool:\n        \"\"\"Mutate this statement.\n\n        Returns:\n            True, if a mutation happened.  # noqa: DAR202\n        \"\"\"\n\n    @abstractmethod\n    def get_variable_references(self) -> set[vr.VariableReference]:\n        \"\"\"Get all references that are used in this statement.\n\n        Including return values.\n\n        Returns:\n            A set of references that are used in this statements  # noqa: DAR202\n        \"\"\"\n\n    def references(self, var: vr.VariableReference) -> bool:\n        \"\"\"Check if this statement makes use of the given variable.\n\n        Args:\n            var: the given variable\n\n        Returns:\n            Whether this statement makes use of the given variable\n        \"\"\"\n        return var in self.get_variable_references()\n\n    @abstractmethod\n    def replace(self, old: vr.VariableReference, new: vr.VariableReference) -> None:\n        \"\"\"Replace the old variable with the new variable.\n\n        Args:\n            old: the old variable\n            new: the new variable\n        \"\"\"\n\n    def get_position(self) -> int:\n        \"\"\"Provides the position of this statement in the test case.\n\n        Raises:\n            RuntimeError: if the statement is not found in the test case\n\n        Returns:\n            The position of this statement\n\n        \"\"\"\n        for idx, stmt in enumerate(self._test_case.statements):\n            if stmt == self:\n                return idx\n        raise RuntimeError(\"Statement is not part of it's test case\")\n\n    def add_assertion(self, assertion: ass.Assertion) -> None:\n        \"\"\"Add the given assertion to this statement.\n\n        Args:\n            assertion: The assertion to add\n        \"\"\"\n        self._assertions.add(assertion)\n\n    def copy_assertions(\n        self, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> OrderedSet[ass.Assertion]:\n        \"\"\"Returns a copy of the assertions of this statement.\n\n        Args:\n            memo: The dictionary of mappings\n\n        Returns:\n            A set of assertions\n        \"\"\"\n        copy: OrderedSet[ass.Assertion] = OrderedSet()\n        for assertion in self._assertions:\n            copy.add(assertion.clone(memo))\n        return copy\n\n    def has_only_exception_assertion(self) -> bool:\n        \"\"\"Does this statement only have an exception assertion?\n\n        Returns:\n            True, if there is only an exception assertion.\n        \"\"\"\n        return len(self._assertions) == 1 and isinstance(\n            next(iter(self._assertions)), ass.ExceptionAssertion\n        )\n\n    @property\n    def assertions(self) -> OrderedSet[ass.Assertion]:\n        \"\"\"Provides the assertions of this statement.\n\n        The assertions are expected to hold after the execution of this statement.\n\n        Returns:\n            The set of assertions of this statements\n        \"\"\"\n        return self._assertions\n\n    @assertions.setter\n    def assertions(self, assertions: OrderedSet[ass.Assertion]) -> None:\n        self._assertions = assertions\n\n    @property\n    def affects_assertions(self) -> bool:\n        \"\"\"Does the execution of this statement possibly affects assertions.\n\n        Returns:\n            Whether the execution of this statement possibly affect assertions.\n        \"\"\"\n        return False\n\n    @abstractmethod\n    def structural_eq(\n        self, other: Statement, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> bool:\n        \"\"\"Implements a structural equivalence criterion.\n\n        Comparing a statement with another statement only makes sense in the context\n        of a test case. This context is added by the memo, which maps variable used in\n        this test case to their counterparts in the other test case.\n\n        Args:\n            other: Check if this statement is equal to the other.\n            memo: A dictionary that maps variable to their corresponding values in the\n                other test case.\n\n        Returns:\n            True, if this statement is equal to the other statement and references the\n            same variables.\n        \"\"\"\n\n    @abstractmethod\n    def structural_hash(self, memo: dict[vr.VariableReference, int]) -> int:\n        \"\"\"Required for structural_eq to work.\n\n        Args:\n            memo: A dictionary that maps variables to their position in this test case.\n\n        Returns:\n            A hash.\n        \"\"\"\n\n\nclass VariableCreatingStatement(Statement, abc.ABC):\n    \"\"\"Abstract superclass for statements that create new variables.\"\"\"\n\n    def __init__(self, test_case: tc.TestCase, ret_val: vr.VariableReference):\n        \"\"\"Constructs a variable-creating statement.\n\n        Args:\n            test_case: The test case the statement belongs to\n            ret_val: The reference to the statement's return value\n        \"\"\"\n        super().__init__(test_case)\n        self.ret_val: vr.VariableReference = ret_val\n\n\nclass StatementVisitor(abc.ABC):\n    \"\"\"An abstract statement visitor.\"\"\"\n\n    @abstractmethod\n    def visit_int_primitive_statement(self, stmt) -> None:\n        \"\"\"Visit int primitive.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_float_primitive_statement(self, stmt) -> None:\n        \"\"\"Visit float primitive.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_complex_primitive_statement(self, stmt) -> None:\n        \"\"\"Visit complex primitive.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_string_primitive_statement(self, stmt) -> None:\n        \"\"\"Visit string primitive.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_bytes_primitive_statement(self, stmt) -> None:\n        \"\"\"Visit bytes primitive.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_boolean_primitive_statement(self, stmt) -> None:\n        \"\"\"Visit boolean primitive.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_enum_statement(self, stmt) -> None:\n        \"\"\"Visit enum.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_class_primitive_statement(self, stmt) -> None:\n        \"\"\"Visit class primitive statement.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_none_statement(self, stmt) -> None:\n        \"\"\"Visit none.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_constructor_statement(self, stmt) -> None:\n        \"\"\"Visit constructor.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_method_statement(self, stmt) -> None:\n        \"\"\"Visit method.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_function_statement(self, stmt) -> None:\n        \"\"\"Visit function.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_field_statement(self, stmt) -> None:\n        \"\"\"Visit field.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_assignment_statement(self, stmt) -> None:\n        \"\"\"Visit assignment.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_list_statement(self, stmt) -> None:\n        \"\"\"Visit list.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_set_statement(self, stmt) -> None:\n        \"\"\"Visit set.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_tuple_statement(self, stmt) -> None:\n        \"\"\"Visit tuple.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n    @abstractmethod\n    def visit_dict_statement(self, stmt) -> None:\n        \"\"\"Visit dict.\n\n        Args:\n            stmt: the statement to visit\n        \"\"\"\n\n\nclass AssignmentStatement(Statement):\n    \"\"\"A statement that assigns the value of a variable to a reference.\n\n    This statement does not create new variables, it only assigns values to\n    possibly nested fields.\n\n    For example:\n    foo_0.baz = int_0\n    \"\"\"\n\n    def __init__(\n        self,\n        test_case: tc.TestCase,\n        lhs: vr.Reference,\n        rhs: vr.VariableReference,\n    ):\n        \"\"\"Initializes a new assignment statement.\n\n        Args:\n            test_case: The test case the statement belongs to\n            lhs: The left-hand side of the assignment\n            rhs: The right-hand side of the assignment\n        \"\"\"\n        super().__init__(test_case)\n        self._lhs = lhs\n        self._rhs = rhs\n\n    @property\n    def lhs(self) -> vr.Reference:\n        \"\"\"The reference that is used on the left-hand side.\n\n        Returns:\n            The reference that is used on the left-hand side\n        \"\"\"\n        return self._lhs\n\n    @property\n    def rhs(self) -> vr.VariableReference:\n        \"\"\"The variable that is used as the right-hand side.\n\n        Returns:\n            The variable used as the right-hand side\n        \"\"\"\n        return self._rhs\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> Statement:\n        return AssignmentStatement(\n            test_case,\n            self._lhs.clone(memo),\n            self._rhs.clone(memo),\n        )\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_assignment_statement(self)\n\n    def accessible_object(self) -> gao.GenericAccessibleObject | None:  # noqa: D102\n        return None\n\n    def mutate(self) -> bool:  # noqa: D102\n        raise NotImplementedError(\"Implement me\")\n\n    def get_variable_references(self) -> set[vr.VariableReference]:  # noqa: D102\n        refs = {self._rhs}\n        if (l_var := self._lhs.get_variable_reference()) is not None:\n            refs.add(l_var)\n        return refs\n\n    def replace(  # noqa: D102\n        self, old: vr.VariableReference, new: vr.VariableReference\n    ) -> None:\n        if self._lhs == old:\n            self._lhs = new\n        else:\n            self._lhs.replace_variable_reference(old, new)\n        if self._rhs == old:\n            self._rhs = new\n\n    def structural_hash(  # noqa: D102\n        self, memo: dict[vr.VariableReference, int]\n    ) -> int:\n        return hash((self._lhs.structural_hash(memo), self._rhs.structural_hash(memo)))\n\n    def structural_eq(  # noqa: D102\n        self, other: Any, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> bool:\n        if not isinstance(other, AssignmentStatement):\n            return False\n        return self._lhs.structural_eq(\n            other._lhs,  # noqa: SLF001\n            memo,\n        ) and self._rhs.structural_eq(\n            other._rhs,  # noqa: SLF001\n            memo,\n        )\n\n\nclass CollectionStatement(VariableCreatingStatement, Generic[T]):\n    \"\"\"Abstract base class for collection statements.\"\"\"\n\n    def __init__(\n        self,\n        test_case: tc.TestCase,\n        type_: ProperType,\n        elements: list[T],\n    ):\n        \"\"\"Initializes the collection statement.\n\n        Args:\n            test_case: The test case the statement belongs to\n            type_: The type of the elements in the collection\n            elements: A list of elements\n        \"\"\"\n        super().__init__(\n            test_case,\n            vr.VariableReference(test_case, type_),\n        )\n        self._elements = elements\n\n    @property\n    def elements(self) -> list[T]:\n        \"\"\"The elements of the collection.\n\n        Returns:\n            A list of elements\n        \"\"\"\n        return self._elements\n\n    def accessible_object(self) -> gao.GenericAccessibleObject | None:  # noqa: D102\n        return None\n\n    def mutate(self) -> bool:  # noqa: D102\n        changed = False\n        if (\n            randomness.next_float() < config.configuration.search_algorithm.test_delete_probability\n            and len(self._elements) > 0\n        ):\n            changed |= self._random_deletion()\n\n        if (\n            randomness.next_float() < config.configuration.search_algorithm.test_change_probability\n            and len(self._elements) > 0\n        ):\n            changed |= self._random_replacement()\n\n        if randomness.next_float() < config.configuration.search_algorithm.test_insert_probability:\n            changed |= self._random_insertion()\n        return changed\n\n    def _random_deletion(self) -> bool:\n        p_per_element = 1.0 / len(self._elements)\n        previous_length = len(self._elements)\n        self._elements = [\n            element for element in self._elements if randomness.next_float() >= p_per_element\n        ]\n        return previous_length != len(self._elements)\n\n    @abstractmethod\n    def _replacement_supplier(self, element: T) -> T:\n        \"\"\"Provide an element that can replace the current element.\n\n        May also be the original element.\n\n        Args:\n            element: the element to be replaced\n\n        Returns:\n            A fitting replacement.\n        \"\"\"\n\n    def _random_replacement(self) -> bool:\n        \"\"\"Randomly replace elements in the collection.\n\n        Returns:\n            True, iff an element was replaced.\n        \"\"\"\n        p_per_element = 1.0 / len(self._elements)\n        changed = False\n        for i, elem in enumerate(self._elements):\n            if randomness.next_float() < p_per_element:\n                replace = self._replacement_supplier(elem)\n                self._elements[i] = replace\n                changed |= replace != elem\n        return changed\n\n    @abstractmethod\n    def _insertion_supplier(self) -> T | None:\n        \"\"\"Supply appropriate values for insertion during mutation.\n\n        Returns:\n            None, if no value can be generated.\n        \"\"\"\n\n    def _random_insertion(self) -> bool:\n        \"\"\"Randomly insert elements into the collection.\n\n        Returns:\n            True, iff an element was inserted.\n        \"\"\"\n        return alpha_exponent_insertion(self._elements, self._insertion_supplier)\n\n\nclass NonDictCollection(CollectionStatement[vr.VariableReference], abc.ABC):\n    \"\"\"Abstract base class for collections that are not dicts.\n\n    We have to handle dicts in a special way, because mutation can affect either\n    the key or the value of an item.\n    \"\"\"\n\n    def _insertion_supplier(self) -> vr.VariableReference | None:\n        arg_type = cast(\"Instance\", self.ret_val.type).args[0]\n        # TODO(fk) what if the current type is not correct?\n        possible_insertions = self.test_case.get_objects(arg_type, self.get_position())\n        if len(possible_insertions) == 0:\n            return None\n        return randomness.choice(possible_insertions)\n\n    def _replacement_supplier(self, element: vr.VariableReference) -> vr.VariableReference:\n        # TODO(fk) what if the current type is not correct?\n        return randomness.choice([\n            *self.test_case.get_objects(element.type, self.get_position()),\n            element,\n        ])\n\n    def structural_hash(  # noqa: D102\n        self, memo: dict[vr.VariableReference, int]\n    ) -> int:\n        return hash((\n            self.ret_val.structural_hash(memo),\n            frozenset((v.structural_hash(memo)) for v in self._elements),\n        ))\n\n    def structural_eq(  # noqa: D102\n        self, other: Any, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> bool:\n        if not isinstance(other, self.__class__):\n            return False\n        return (\n            self.ret_val.structural_eq(other.ret_val, memo)\n            and len(self._elements) == len(other._elements)  # noqa: SLF001\n            and all(\n                left.structural_eq(right, memo)\n                for left, right in zip(\n                    self._elements,\n                    other._elements,  # noqa: SLF001\n                    strict=True,\n                )\n            )\n        )\n\n\nclass ListStatement(NonDictCollection):\n    \"\"\"Represents a list.\"\"\"\n\n    def get_variable_references(self) -> set[vr.VariableReference]:  # noqa: D102\n        references = set()\n        references.add(self.ret_val)\n        references.update(self._elements)\n        return references\n\n    def replace(  # noqa: D102\n        self, old: vr.VariableReference, new: vr.VariableReference\n    ) -> None:\n        if self.ret_val == old:\n            self.ret_val = new\n        self._elements = [new if arg == old else arg for arg in self._elements]\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> ListStatement:\n        return ListStatement(\n            test_case,\n            self.ret_val.type,\n            [var.clone(memo) for var in self._elements],\n        )\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_list_statement(self)\n\n\nclass SetStatement(NonDictCollection):\n    \"\"\"Represents a set.\"\"\"\n\n    def get_variable_references(self) -> set[vr.VariableReference]:  # noqa: D102\n        references = set()\n        references.add(self.ret_val)\n        references.update(self._elements)\n        return references\n\n    def replace(  # noqa: D102\n        self, old: vr.VariableReference, new: vr.VariableReference\n    ) -> None:\n        if self.ret_val == old:\n            self.ret_val = new\n        self._elements = [new if arg == old else arg for arg in self._elements]\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> SetStatement:\n        return SetStatement(\n            test_case,\n            self.ret_val.type,\n            [var.clone(memo) for var in self._elements],\n        )\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_set_statement(self)\n\n\nclass TupleStatement(NonDictCollection):\n    \"\"\"Represents a tuple.\"\"\"\n\n    def get_variable_references(self) -> set[vr.VariableReference]:  # noqa: D102\n        references = set()\n        references.add(self.ret_val)\n        references.update(self._elements)\n        return references\n\n    def replace(  # noqa: D102\n        self, old: vr.VariableReference, new: vr.VariableReference\n    ) -> None:\n        if self.ret_val == old:\n            self.ret_val = new\n        self._elements = [new if arg == old else arg for arg in self._elements]\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> TupleStatement:\n        return TupleStatement(\n            test_case,\n            self.ret_val.type,\n            [var.clone(memo) for var in self._elements],\n        )\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_tuple_statement(self)\n\n    # No deletion or insertion on tuple\n    # Maybe consider if structure of tuple is unknown?\n    def _random_insertion(self) -> bool:\n        return False\n\n    def _random_deletion(self) -> bool:\n        return False\n\n\nclass DictStatement(CollectionStatement[tuple[vr.VariableReference, vr.VariableReference]]):\n    \"\"\"Represents a dict. The tuples represent key-value pairs.\"\"\"\n\n    def get_variable_references(self) -> set[vr.VariableReference]:  # noqa: D102\n        references = set()\n        references.add(self.ret_val)\n        for entry in self._elements:\n            references.add(entry[0])\n            references.add(entry[1])\n        return references\n\n    def replace(  # noqa: D102\n        self, old: vr.VariableReference, new: vr.VariableReference\n    ) -> None:\n        if self.ret_val == old:\n            self.ret_val = new\n        self._elements = [\n            (new if elem[0] == old else elem[0], new if elem[1] == old else elem[1])\n            for elem in self._elements\n        ]\n\n    def _replacement_supplier(\n        self, element: tuple[vr.VariableReference, vr.VariableReference]\n    ) -> tuple[vr.VariableReference, vr.VariableReference]:\n        change_idx = randomness.next_int(0, 2)\n        new = list(element)\n        # TODO(fk) what if the current type is not correct?\n        new[change_idx] = randomness.choice([\n            *self.test_case.get_objects(element[change_idx].type, self.get_position()),\n            element[change_idx],\n        ])\n        assert len(new) == 2, \"Tuple must consist of key and value\"\n        return new[0], new[1]\n\n    def _insertion_supplier(\n        self,\n    ) -> tuple[vr.VariableReference, vr.VariableReference] | None:\n        # TODO(fk) what if the current type is not correct?\n        key_type = cast(\"Instance\", self.ret_val.type).args[0]\n        val_type = cast(\"Instance\", self.ret_val.type).args[1]\n        possibles_keys = self.test_case.get_objects(key_type, self.get_position())\n        possibles_values = self.test_case.get_objects(val_type, self.get_position())\n        if len(possibles_keys) == 0 or len(possibles_values) == 0:\n            return None\n        return (\n            randomness.choice(possibles_keys),\n            randomness.choice(possibles_values),\n        )\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> DictStatement:\n        return DictStatement(\n            test_case,\n            self.ret_val.type,\n            [(var[0].clone(memo), var[1].clone(memo)) for var in self._elements],\n        )\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_dict_statement(self)\n\n    def structural_hash(  # noqa: D102\n        self, memo: dict[vr.VariableReference, int]\n    ) -> int:\n        return hash((\n            self.ret_val.structural_hash(memo),\n            frozenset(\n                (k.structural_hash(memo), v.structural_hash(memo)) for k, v in self._elements\n            ),\n        ))\n\n    def structural_eq(  # noqa: D102\n        self, other: Any, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> bool:\n        if not isinstance(other, self.__class__):\n            return False\n        return (\n            self.ret_val.structural_eq(other.ret_val, memo)\n            and len(self._elements) == len(other._elements)  # noqa: SLF001\n            and all(\n                lk.structural_eq(rk, memo) and lv.structural_eq(rv, memo)\n                for (lk, lv), (rk, rv) in zip(\n                    self._elements,\n                    other._elements,  # noqa: SLF001\n                    strict=True,\n                )\n            )\n        )\n\n\nclass FieldStatement(VariableCreatingStatement):\n    \"\"\"A statement which reads a public field or a property of an object.\n\n    For example:\n        int_0 = foo_0.baz\n    \"\"\"\n\n    # TODO(fk) add subclasses for staticfield and modulestaticfield?\n\n    def __init__(\n        self,\n        test_case: tc.TestCase,\n        field: gao.GenericField,\n        source: vr.Reference,\n    ):\n        \"\"\"Initializes a new field statement.\n\n        Args:\n            test_case: The test case the statement belongs to\n            field: The reference to the field\n            source: The reference to the object the field belongs to\n        \"\"\"\n        super().__init__(test_case, vr.VariableReference(test_case, field.generated_type()))\n        self._field = field\n        self._source = source\n\n    @property\n    def source(self) -> vr.Reference:\n        \"\"\"Provides the reference whose field is accessed.\n\n        Returns:\n            The reference whose field is accessed\n        \"\"\"\n        return self._source\n\n    @source.setter\n    def source(self, new_source: vr.Reference) -> None:\n        \"\"\"Set new source.\n\n        Args:\n            new_source: The new variable to access\n        \"\"\"\n        self._source = new_source\n\n    def accessible_object(self) -> gao.GenericAccessibleObject | None:  # noqa: D102\n        return self._field\n\n    def mutate(self) -> bool:  # noqa: D102\n        if (\n            randomness.next_float()\n            >= config.configuration.search_algorithm.change_parameter_probability\n        ):\n            return False\n\n        objects = self.test_case.get_objects(self.source.type, self.get_position())\n        if (old_var := self._source.get_variable_reference()) is not None:\n            objects.remove(old_var)\n        if len(objects) > 0:\n            self.source = randomness.choice(objects)\n            return True\n        return False\n\n    @property\n    def field(self) -> gao.GenericField:\n        \"\"\"The used field.\n\n        Returns:\n            The used field\n        \"\"\"\n        return self._field\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> Statement:\n        return FieldStatement(test_case, self._field, self._source.clone(memo))\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_field_statement(self)\n\n    def get_variable_references(self) -> set[vr.VariableReference]:  # noqa: D102\n        refs = {self.ret_val}\n        if (var := self._source.get_variable_reference()) is not None:\n            refs.add(var)\n        return refs\n\n    def replace(  # noqa: D102\n        self, old: vr.VariableReference, new: vr.VariableReference\n    ) -> None:\n        if self._source == old:\n            self._source = new\n        else:\n            self._source.replace_variable_reference(old, new)\n        if self.ret_val == old:\n            self.ret_val = new\n\n    def structural_eq(  # noqa: D102\n        self, other: Any, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> bool:\n        if not isinstance(other, FieldStatement):\n            return False\n        return (\n            self._field == other._field  # noqa: SLF001\n            and self.ret_val.structural_eq(other.ret_val, memo)\n            and self._source.structural_eq(other._source, memo)  # noqa: SLF001\n        )\n\n    def structural_hash(  # noqa: D102\n        self, memo: dict[vr.VariableReference, int]\n    ) -> int:\n        return hash((self._field, self.ret_val.structural_hash(memo)))\n\n\nclass ParametrizedStatement(VariableCreatingStatement, abc.ABC):\n    \"\"\"An abstract statement that has parameters.\n\n    Superclass for e.g., method or constructor statement.\n    \"\"\"\n\n    def __init__(\n        self,\n        test_case: tc.TestCase,\n        generic_callable: gao.GenericCallableAccessibleObject,\n        args: dict[str, vr.VariableReference] | None = None,\n    ):\n        \"\"\"Create a new statement with parameters.\n\n        Args:\n            test_case: the containing test case.\n            generic_callable: the callable\n            args: A map of parameter names to their values.\n        \"\"\"\n        super().__init__(\n            test_case,\n            vr.CallBasedVariableReference(test_case, generic_callable),\n        )\n        self._generic_callable = generic_callable\n        self._args = args or {}\n\n    @property\n    def args(self) -> dict[str, vr.VariableReference]:\n        \"\"\"The dictionary mapping parameter names to the used values.\n\n        Returns:\n            A dict mapping parameter names to their values.\n        \"\"\"\n        return self._args\n\n    @args.setter\n    def args(self, args: dict[str, vr.VariableReference]):\n        self._args = args\n\n    @property\n    def raised_exceptions(self) -> set[str]:\n        \"\"\"Provides the set of exceptions raised by this call.\n\n        Returns:\n            The set of exceptions that can be raised by this call\n        \"\"\"\n        return self._generic_callable.raised_exceptions\n\n    def get_variable_references(self) -> set[vr.VariableReference]:  # noqa: D102\n        references = set()\n        references.add(self.ret_val)\n        references.update(self.args.values())\n        return references\n\n    def replace(  # noqa: D102\n        self, old: vr.VariableReference, new: vr.VariableReference\n    ) -> None:\n        if self.ret_val == old:\n            self.ret_val = new\n        for key, value in self._args.items():\n            if value == old:\n                self._args[key] = new\n\n    def _clone_args(\n        self, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> dict[str, vr.VariableReference]:\n        \"\"\"Small helper method, to clone the args into a new test case.\n\n        Args:\n            memo: foo\n\n        Returns:\n            A dictionary of key-value argument references\n        \"\"\"\n        return {name: var.clone(memo) for name, var in self._args.items()}\n\n    def mutate(self) -> bool:  # noqa: D102\n        if (\n            randomness.next_float()\n            >= config.configuration.search_algorithm.change_parameter_probability\n        ):\n            return False\n\n        changed = False\n        mutable_param_count = self._mutable_argument_count()\n        if mutable_param_count > 0:\n            p_per_param = 1.0 / mutable_param_count\n            changed |= self._mutate_special_parameters(p_per_param)\n            changed |= self._mutate_parameters(p_per_param)\n        return changed\n\n    def _mutable_argument_count(self) -> int:\n        \"\"\"Returns the amount of mutable parameters.\n\n        Returns:\n            The amount of mutable parameters\n        \"\"\"\n        return len(self.args)\n\n    def _mutate_special_parameters(self, p_per_param: float) -> bool:\n        \"\"\"Overwrite this method to mutate any parameter.\n\n        The parameter must not be in arg or kwargs, e.g., the callee in an instance\n        method call.\n\n        Args:\n            p_per_param: the probability per parameter\n\n        Returns:\n            Whether mutation should be applied\n        \"\"\"\n        return False\n\n    def _mutate_parameters(self, p_per_param: float) -> bool:\n        \"\"\"Mutates args and kwargs with the given probability.\n\n        Args:\n            p_per_param: The probability for one parameter to be mutated.\n\n        Returns:\n            Whether mutation changed anything\n        \"\"\"\n        changed = False\n        for param_name in self._generic_callable.inferred_signature.original_parameters:\n            if randomness.next_float() < p_per_param:\n                changed |= self._mutate_parameter(\n                    param_name, self._generic_callable.inferred_signature\n                )\n\n        return changed\n\n    def _mutate_parameter(self, param_name: str, inf_sig: InferredSignature) -> bool:\n        \"\"\"Replace the given parameter with another one that also fits the type.\n\n        Args:\n            param_name: the name of the parameter that should be mutated.\n            inf_sig: the inferred signature for the parameters\n\n        Returns:\n            True, if the parameter was mutated.\n        \"\"\"\n        current = self._args.get(param_name, None)\n        param_type = inf_sig.get_parameter_types({})[param_name]\n        possible_replacements = self.test_case.get_objects(param_type, self.get_position())\n\n        # Param has to be optional, otherwise it would be set.\n        if current is None:\n            # Create value for currently unset parameter.\n            if (\n                randomness.next_float()\n                > config.configuration.test_creation.skip_optional_parameter_probability\n                and len(possible_replacements) > 0\n            ):\n                self._args[param_name] = randomness.choice(possible_replacements)\n                return True\n            return False\n\n        if (\n            is_optional_parameter(inf_sig, param_name)\n            and randomness.next_float()\n            < config.configuration.test_creation.skip_optional_parameter_probability\n        ):\n            # unset parameters that are not necessary with a certain probability,\n            # e.g., if they have default value or are *args, **kwargs.\n            self._args.pop(param_name)\n\n        if current in possible_replacements:\n            possible_replacements.remove(current)\n\n        # Consider duplicating an existing statement/variable.\n        copy: Statement | None = None\n        if self._param_count_of_type(param_type) > len(possible_replacements) + 1:\n            original_param_source = self.test_case.get_statement(current.get_statement_position())\n            copy = cast(\n                \"VariableCreatingStatement\",\n                original_param_source.clone(\n                    self.test_case,\n                    {\n                        s.ret_val: s.ret_val\n                        for s in self.test_case.statements\n                        if s.ret_val is not None\n                    },\n                ),\n            )\n            possible_replacements.append(copy.ret_val)\n\n        # to make the selection broader, but this requires access to\n        # the test cluster, to select a concrete type.\n        # Using None as parameter value is also a possibility.\n        none_statement = NoneStatement(self.test_case)\n        possible_replacements.append(none_statement.ret_val)\n\n        replacement = randomness.choice(possible_replacements)\n\n        if copy and replacement == copy.ret_val:\n            # The chosen replacement is a copy, so we have to add it to the test case.\n            self.test_case.add_statement(copy, self.get_position())\n            copy.mutate()\n        elif replacement is none_statement.ret_val:\n            # The chosen replacement is a none statement, so we have to add it to the\n            # test case.\n            self.test_case.add_statement(none_statement, self.get_position())\n\n        self._args[param_name] = replacement\n        return True\n\n    def _param_count_of_type(self, type_: ProperType) -> int:\n        \"\"\"Return the number of parameters that have the specified type.\n\n        Args:\n            type_: The type, whose occurrences should be counted.\n\n        Returns:\n            The number of occurrences.\n        \"\"\"\n        count = 0\n        if type_ is None:\n            return 0\n        for var_ref in self.args.values():\n            if self.test_case.test_cluster.type_system.is_maybe_subtype(var_ref.type, type_):\n                count += 1\n        return count\n\n    @property\n    def affects_assertions(self) -> bool:  # noqa: D102\n        return True\n\n    def structural_hash(  # noqa: D102\n        self, memo: dict[vr.VariableReference, int]\n    ) -> int:\n        return hash((\n            self.ret_val.structural_hash(memo),\n            self._generic_callable,\n            frozenset((k, v.structural_hash(memo)) for k, v in self._args.items()),\n        ))\n\n    def structural_eq(  # noqa: D102\n        self, other: Any, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> bool:\n        if not isinstance(other, self.__class__):\n            return False\n        return (\n            self.ret_val.structural_eq(other.ret_val, memo)\n            and self._generic_callable == other._generic_callable  # noqa: SLF001\n            and self._args.keys() == other._args.keys()  # noqa: SLF001\n            and all(\n                v.structural_eq(other._args[k], memo)  # noqa: SLF001\n                for k, v in self._args.items()\n            )\n        )\n\n\nclass ConstructorStatement(ParametrizedStatement):\n    \"\"\"A statement that constructs an object.\"\"\"\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> Statement:\n        return ConstructorStatement(test_case, self.accessible_object(), self._clone_args(memo))\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_constructor_statement(self)\n\n    def accessible_object(self) -> gao.GenericConstructor:\n        \"\"\"The used constructor.\n\n        Returns:\n            The used constructor\n        \"\"\"\n        return cast(\"gao.GenericConstructor\", self._generic_callable)\n\n    def __repr__(self) -> str:\n        return (\n            f\"ConstructorStatement({self._test_case}, {self._generic_callable}(args={self._args})\"\n        )\n\n    def __str__(self) -> str:\n        return f\"{self._generic_callable}(args={self._args})\" + \"-> None\"\n\n\nclass MethodStatement(ParametrizedStatement):\n    \"\"\"A statement that calls a method on an object.\"\"\"\n\n    def __init__(\n        self,\n        test_case: tc.TestCase,\n        generic_callable: gao.GenericMethod,\n        callee: vr.VariableReference,\n        args: dict[str, vr.VariableReference] | None = None,\n    ):\n        \"\"\"Create new method statement.\n\n        Args:\n            test_case: The containing test case\n            generic_callable: The generic callable method\n            callee: the object on which the method is called\n            args: the arguments\n        \"\"\"\n        super().__init__(test_case, generic_callable, args)\n        self._callee = callee\n\n    def accessible_object(self) -> gao.GenericMethod:\n        \"\"\"The used method.\n\n        Returns:\n            The used method\n        \"\"\"\n        return cast(\"gao.GenericMethod\", self._generic_callable)\n\n    def _mutable_argument_count(self) -> int:\n        # We add +1 to the count, because the callee itself can also be mutated.\n        return super()._mutable_argument_count() + 1\n\n    def _mutate_special_parameters(self, p_per_param: float) -> bool:\n        # We mutate the callee here, as the special parameter.\n        if randomness.next_float() < p_per_param:\n            callee = self.callee\n            typ = (\n                ANY\n                if randomness.next_float()\n                < config.configuration.test_creation.use_random_object_for_call\n                else callee.type\n            )\n            objects = self.test_case.get_objects(typ, self.get_position())\n            if callee in objects:\n                objects.remove(callee)\n\n            if len(objects) > 0:\n                self.callee = randomness.choice(objects)\n                return True\n        return False\n\n    def get_variable_references(self) -> set[vr.VariableReference]:  # noqa: D102\n        references = super().get_variable_references()\n        references.add(self._callee)\n        return references\n\n    def replace(  # noqa: D102\n        self, old: vr.VariableReference, new: vr.VariableReference\n    ) -> None:\n        super().replace(old, new)\n        if self._callee == old:\n            self._callee = new\n\n    @property\n    def callee(self) -> vr.VariableReference:\n        \"\"\"Provides the variable on which the method is invoked.\n\n        Returns:\n            The variable on which the method is invoked\n        \"\"\"\n        return self._callee\n\n    @callee.setter\n    def callee(self, new_callee: vr.VariableReference) -> None:\n        \"\"\"Set new callee on which the method is invoked.\n\n        Args:\n            new_callee: Sets a new callee\n        \"\"\"\n        self._callee = new_callee\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> Statement:\n        return MethodStatement(\n            test_case,\n            self.accessible_object(),\n            self._callee.clone(memo),\n            self._clone_args(memo),\n        )\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_method_statement(self)\n\n    def structural_hash(  # noqa: D102\n        self, memo: dict[vr.VariableReference, int]\n    ) -> int:\n        return hash((super().structural_hash(memo), self._callee.structural_hash(memo)))\n\n    def structural_eq(  # noqa: D102\n        self, other: Any, memo: dict[vr.VariableReference, vr.VariableReference]\n    ) -> bool:\n        return super().structural_eq(other, memo) and self._callee.structural_eq(\n            other._callee,  # noqa: SLF001\n            memo,\n        )\n\n    def __repr__(self) -> str:\n        return (\n            f\"MethodStatement({self._test_case}, \"\n            f\"{self._generic_callable}, {self._callee.type}, \"\n            f\"args={self._args})\"\n        )\n\n    def __str__(self) -> str:\n        return (\n            f\"{self._generic_callable}(args={self._args}) -> \"\n            f\"{self._generic_callable.generated_type()}\"\n        )\n\n\nclass FunctionStatement(ParametrizedStatement):\n    \"\"\"A statement that calls a function.\"\"\"\n\n    def accessible_object(self) -> gao.GenericFunction:\n        \"\"\"The used function.\n\n        Returns:\n            The used function\n        \"\"\"\n        return cast(\"gao.GenericFunction\", self._generic_callable)\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> Statement:\n        return FunctionStatement(test_case, self.accessible_object(), self._clone_args(memo))\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_function_statement(self)\n\n    def __repr__(self) -> str:\n        return (\n            f\"FunctionStatement({self._test_case}, \"\n            f\"{self._generic_callable}, {self.ret_val.type}, \"\n            f\"args={self._args})\"\n        )\n\n    def __str__(self) -> str:\n        return f\"{self._generic_callable}(args={self._args}) -> \" + f\"{self.ret_val.type}\"\n\n\nclass PrimitiveStatement(VariableCreatingStatement, Generic[T]):\n    \"\"\"Abstract primitive statement which holds a value.\"\"\"\n\n    def __init__(\n        self,\n        test_case: tc.TestCase,\n        variable_type: ProperType,\n        value: T | None = None,\n        constant_provider: constants.ConstantProvider | None = None,\n    ) -> None:\n        \"\"\"Initializes a primitive statement.\n\n        Args:\n            test_case: The test case this statement belongs to\n            variable_type: The type of the value\n            value: The value\n            constant_provider: The provider for seeded constants\n        \"\"\"\n        super().__init__(test_case, vr.VariableReference(test_case, variable_type))\n        self._value = value\n        self._constant_provider: constants.ConstantProvider | None = constant_provider\n        if value is None:\n            self.randomize_value()\n\n    @property\n    def value(self) -> T | None:\n        \"\"\"Provides the primitive value of this statement.\n\n        Returns:\n            The primitive value\n        \"\"\"\n        return self._value\n\n    @value.setter\n    def value(self, value: T) -> None:\n        self._value = value\n\n    def accessible_object(self) -> gao.GenericAccessibleObject | None:  # noqa: D102\n        return None\n\n    def mutate(self) -> bool:  # noqa: D102\n        old_value = self._value\n        while self._value == old_value and self._value is not None:\n            if randomness.next_float() < config.configuration.search_algorithm.random_perturbation:\n                self.randomize_value()\n            else:\n                self.delta()\n        return True\n\n    def get_variable_references(self) -> set[vr.VariableReference]:  # noqa: D102\n        return {self.ret_val}\n\n    def replace(  # noqa: D102\n        self, old: vr.VariableReference, new: vr.VariableReference\n    ) -> None:\n        if self.ret_val == old:\n            self.ret_val = new\n\n    @abstractmethod\n    def randomize_value(self) -> None:\n        \"\"\"Randomize the primitive value of this statement.\"\"\"\n\n    @abstractmethod\n    def delta(self) -> None:\n        \"\"\"Add a random delta to the value.\"\"\"\n\n    def __repr__(self) -> str:\n        return f\"PrimitiveStatement({self._test_case}, {self.ret_val}, {self._value})\"\n\n    def __str__(self) -> str:\n        return f\"{self._value}: {self.ret_val}\"\n\n    def structural_eq(  # noqa: D102\n        self,\n        other: Statement,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> bool:\n        if not isinstance(other, self.__class__):\n            return False\n        return (\n            self.ret_val.structural_eq(other.ret_val, memo) and self._value == other._value  # noqa: SLF001\n        )\n\n    def structural_hash(  # noqa: D102\n        self, memo: dict[vr.VariableReference, int]\n    ) -> int:\n        return hash((self.ret_val.structural_hash(memo), hash(self._value)))\n\n\nclass IntPrimitiveStatement(PrimitiveStatement[int]):\n    \"\"\"Primitive Statement that creates an int.\"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        test_case: tc.TestCase,\n        value: int | None = None,\n        constant_provider: constants.ConstantProvider | None = None,\n    ) -> None:\n        super().__init__(\n            test_case,\n            Instance(test_case.test_cluster.type_system.to_type_info(int)),\n            value,\n            constant_provider=constant_provider,\n        )\n\n    def randomize_value(self) -> None:  # noqa: D102\n        if (\n            self._constant_provider\n            and randomness.next_float()\n            <= config.configuration.seeding.seeded_primitives_reuse_probability\n            and (seeded_value := self._constant_provider.get_constant_for(int)) is not None\n        ):\n            self._value = seeded_value\n        else:\n            self._value = int(\n                randomness.next_gaussian() * config.configuration.test_creation.max_int\n            )\n\n    def delta(self) -> None:  # noqa: D102\n        assert self._value is not None\n        delta = math.floor(\n            randomness.next_gaussian() * config.configuration.test_creation.max_delta\n        )\n        self._value += delta\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> IntPrimitiveStatement:\n        return IntPrimitiveStatement(\n            test_case, self._value, constant_provider=self._constant_provider\n        )\n\n    def __repr__(self) -> str:\n        return f\"IntPrimitiveStatement({self._test_case}, {self._value})\"\n\n    def __str__(self) -> str:\n        return f\"{self._value}: int\"\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_int_primitive_statement(self)\n\n\nclass FloatPrimitiveStatement(PrimitiveStatement[float]):\n    \"\"\"Primitive Statement that creates a float.\"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        test_case: tc.TestCase,\n        value: float | None = None,\n        constant_provider: constants.ConstantProvider | None = None,\n    ) -> None:\n        super().__init__(\n            test_case,\n            Instance(test_case.test_cluster.type_system.to_type_info(float)),\n            value,\n            constant_provider=constant_provider,\n        )\n\n    def randomize_value(self) -> None:  # noqa: D102\n        if (\n            self._constant_provider\n            and randomness.next_float()\n            <= config.configuration.seeding.seeded_primitives_reuse_probability\n            and (seeded_value := self._constant_provider.get_constant_for(float)) is not None\n        ):\n            self._value = seeded_value\n        else:\n            val = randomness.next_gaussian() * config.configuration.test_creation.max_int\n            precision = randomness.next_int(0, 7)\n            self._value = round(val, precision)\n\n    def delta(self) -> None:  # noqa: D102\n        assert self._value is not None\n        probability = randomness.next_float()\n        if probability < 1.0 / 3.0:\n            self._value += randomness.next_gaussian() * config.configuration.test_creation.max_delta\n        elif probability < 2.0 / 3.0:\n            self._value += randomness.next_gaussian()\n        else:\n            self._value = round(self._value, randomness.next_int(0, 7))\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> FloatPrimitiveStatement:\n        return FloatPrimitiveStatement(\n            test_case, self._value, constant_provider=self._constant_provider\n        )\n\n    def __repr__(self) -> str:\n        return f\"FloatPrimitiveStatement({self._test_case}, {self._value})\"\n\n    def __str__(self) -> str:\n        return f\"{self._value}: float\"\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_float_primitive_statement(self)\n\n\nclass ComplexPrimitiveStatement(PrimitiveStatement[complex]):\n    \"\"\"Primitive Statement that creates a complex.\"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        test_case: tc.TestCase,\n        value: complex | None = None,\n        constant_provider: constants.ConstantProvider | None = None,\n    ) -> None:\n        super().__init__(\n            test_case,\n            Instance(test_case.test_cluster.type_system.to_type_info(complex)),\n            value,\n            constant_provider=constant_provider,\n        )\n\n    def randomize_value(self) -> None:  # noqa: D102\n        if (\n            self._constant_provider\n            and randomness.next_float()\n            <= config.configuration.seeding.seeded_primitives_reuse_probability\n            and (seeded_value := self._constant_provider.get_constant_for(complex)) is not None\n        ):\n            self._value = seeded_value\n        else:\n            real = randomness.next_gaussian() * config.configuration.test_creation.max_int\n            precision_real = randomness.next_int(0, 7)\n            imag = randomness.next_gaussian() * config.configuration.test_creation.max_int\n            precision_imag = randomness.next_int(0, 7)\n            self._value = complex(round(real, precision_real), round(imag, precision_imag))\n\n    def delta(self) -> None:  # noqa: D102\n        assert self._value is not None\n        probability = randomness.next_float()\n        real_or_imag = randomness.next_bool()\n        if probability < 1 / 3:\n            if real_or_imag:\n                self._value = complex(\n                    self._value.real\n                    + randomness.next_gaussian() * config.configuration.test_creation.max_delta,\n                    self._value.imag,\n                )\n            else:\n                self._value = complex(\n                    self._value.real,\n                    self._value.imag\n                    + randomness.next_gaussian() * config.configuration.test_creation.max_delta,\n                )\n        elif probability < 2 / 3:\n            if real_or_imag:\n                self._value = complex(\n                    self._value.real + randomness.next_gaussian(), self._value.imag\n                )\n            else:\n                self._value = complex(\n                    self._value.real, self._value.imag + +randomness.next_gaussian()\n                )\n        elif real_or_imag:\n            self._value = complex(\n                round(self._value.real, randomness.next_int(0, 7)), self._value.imag\n            )\n        else:\n            self._value = complex(\n                self._value.real, round(self._value.imag, randomness.next_int(0, 7))\n            )\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> ComplexPrimitiveStatement:\n        return ComplexPrimitiveStatement(\n            test_case, self._value, constant_provider=self._constant_provider\n        )\n\n    def __repr__(self) -> str:\n        return f\"ComplexPrimitiveStatement({self._test_case}, {self._value})\"\n\n    def __str__(self) -> str:\n        return f\"{self._value}: complex\"\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_complex_primitive_statement(self)\n\n\nclass StringPrimitiveStatement(PrimitiveStatement[str]):\n    \"\"\"Primitive Statement that creates a String.\"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        test_case: tc.TestCase,\n        value: str | None = None,\n        constant_provider: constants.ConstantProvider | None = None,\n    ) -> None:\n        super().__init__(\n            test_case,\n            Instance(test_case.test_cluster.type_system.to_type_info(str)),\n            value,\n            constant_provider=constant_provider,\n        )\n\n    def randomize_value(self) -> None:  # noqa: D102\n        if (\n            self._constant_provider\n            and randomness.next_float()\n            <= config.configuration.seeding.seeded_primitives_reuse_probability\n            and (seeded_value := self._constant_provider.get_constant_for(str)) is not None\n        ):\n            self._value = seeded_value\n        else:\n            length = randomness.next_int(0, config.configuration.test_creation.string_length + 1)\n            self._value = randomness.next_string(length)\n\n    def delta(self) -> None:  # noqa: D102\n        assert self._value is not None\n        working_on = list(self._value)\n        p_perform_action = 1.0 / 3.0\n        if randomness.next_float() < p_perform_action and len(working_on) > 0:\n            working_on = self._random_deletion(working_on)\n\n        if randomness.next_float() < p_perform_action and len(working_on) > 0:\n            working_on = self._random_replacement(working_on)\n\n        if randomness.next_float() < p_perform_action:\n            working_on = self._random_insertion(working_on)\n\n        self._value = \"\".join(working_on)\n\n    @staticmethod\n    def _random_deletion(working_on: list[str]) -> list[str]:\n        p_per_char = 1.0 / len(working_on)\n        return [char for char in working_on if randomness.next_float() >= p_per_char]\n\n    @staticmethod\n    def _random_replacement(working_on: list[str]) -> list[str]:\n        p_per_char = 1.0 / len(working_on)\n        return [\n            randomness.next_char() if randomness.next_float() < p_per_char else char\n            for char in working_on\n        ]\n\n    @staticmethod\n    def _random_insertion(working_on: list[str]) -> list[str]:\n        pos = 0\n        if len(working_on) > 0:\n            pos = randomness.next_int(0, len(working_on) + 1)\n        alpha = 0.5\n        exponent = 1\n        while (\n            randomness.next_float() <= pow(alpha, exponent)\n            and len(working_on) < config.configuration.test_creation.string_length\n        ):\n            exponent += 1\n            working_on = working_on[:pos] + [randomness.next_char()] + working_on[pos:]\n        return working_on\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> StringPrimitiveStatement:\n        return StringPrimitiveStatement(\n            test_case, self._value, constant_provider=self._constant_provider\n        )\n\n    def __repr__(self) -> str:\n        return f\"StringPrimitiveStatement({self._test_case}, {self._value})\"\n\n    def __str__(self) -> str:\n        return f\"{self._value}: str\"\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_string_primitive_statement(self)\n\n\nclass BytesPrimitiveStatement(PrimitiveStatement[bytes]):\n    \"\"\"Primitive Statement that creates bytes.\"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        test_case: tc.TestCase,\n        value: bytes | None = None,\n        constant_provider: constants.ConstantProvider | None = None,\n    ) -> None:\n        super().__init__(\n            test_case,\n            Instance(test_case.test_cluster.type_system.to_type_info(bytes)),\n            value,\n            constant_provider=constant_provider,\n        )\n\n    def randomize_value(self) -> None:  # noqa: D102\n        if (\n            self._constant_provider\n            and randomness.next_float()\n            <= config.configuration.seeding.seeded_primitives_reuse_probability\n            and (seeded_value := self._constant_provider.get_constant_for(bytes)) is not None\n        ):\n            self._value = seeded_value\n        else:\n            length = randomness.next_int(0, config.configuration.test_creation.bytes_length + 1)\n            self._value = randomness.next_bytes(length)\n\n    def delta(self) -> None:  # noqa: D102\n        assert self._value is not None\n        working_on = list(self._value)\n        p_perform_action = 1.0 / 3.0\n        if randomness.next_float() < p_perform_action and len(working_on) > 0:\n            working_on = self._random_deletion(working_on)\n\n        if randomness.next_float() < p_perform_action and len(working_on) > 0:\n            working_on = self._random_replacement(working_on)\n\n        if randomness.next_float() < p_perform_action:\n            working_on = self._random_insertion(working_on)\n\n        self._value = bytes(working_on)\n\n    @staticmethod\n    def _random_deletion(working_on: list[int]) -> list[int]:\n        p_per_char = 1.0 / len(working_on)\n        return [char for char in working_on if randomness.next_float() >= p_per_char]\n\n    @staticmethod\n    def _random_replacement(working_on: list[int]) -> list[int]:\n        p_per_char = 1.0 / len(working_on)\n        return [\n            randomness.next_byte() if randomness.next_float() < p_per_char else byte\n            for byte in working_on\n        ]\n\n    @staticmethod\n    def _random_insertion(working_on: list[int]) -> list[int]:\n        pos = 0\n        if len(working_on) > 0:\n            pos = randomness.next_int(0, len(working_on) + 1)\n        alpha = 0.5\n        exponent = 1\n        while (\n            randomness.next_float() <= pow(alpha, exponent)\n            and len(working_on) < config.configuration.test_creation.bytes_length\n        ):\n            exponent += 1\n            working_on = working_on[:pos] + [randomness.next_byte()] + working_on[pos:]\n        return working_on\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> BytesPrimitiveStatement:\n        return BytesPrimitiveStatement(\n            test_case, self._value, constant_provider=self._constant_provider\n        )\n\n    def __repr__(self) -> str:\n        return f\"BytesPrimitiveStatement({self._test_case}, {self._value!r})\"\n\n    def __str__(self) -> str:\n        return f\"{self._value!r}: bytes\"\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_bytes_primitive_statement(self)\n\n\nclass BooleanPrimitiveStatement(PrimitiveStatement[bool]):\n    \"\"\"Primitive Statement that creates a boolean.\"\"\"\n\n    def __init__(\n        self,\n        test_case: tc.TestCase,\n        value: bool | None = None,  # noqa: FBT001\n    ) -> None:\n        \"\"\"Initializes a primitive statement for a boolean.\n\n        Args:\n            test_case: The test case this statement belongs to\n            value: The boolean value\n        \"\"\"\n        super().__init__(\n            test_case,\n            Instance(test_case.test_cluster.type_system.to_type_info(bool)),\n            value,\n        )\n\n    def randomize_value(self) -> None:  # noqa: D102\n        self._value = bool(randomness.RNG.getrandbits(1))\n\n    def delta(self) -> None:  # noqa: D102\n        assert self._value is not None\n        self._value = not self._value\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> BooleanPrimitiveStatement:\n        return BooleanPrimitiveStatement(test_case, self._value)\n\n    def __repr__(self) -> str:\n        return f\"BooleanPrimitiveStatement({self._test_case}, {self._value})\"\n\n    def __str__(self) -> str:\n        return f\"{self._value}: bool\"\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_boolean_primitive_statement(self)\n\n\nclass EnumPrimitiveStatement(PrimitiveStatement[int]):\n    \"\"\"Primitive Statement that references the value of an enum.\n\n    We simply store the index of the element in the Enum.\n    \"\"\"\n\n    def __init__(\n        self,\n        test_case: tc.TestCase,\n        generic_enum: gao.GenericEnum,\n        value: int | None = None,\n    ):\n        \"\"\"Initializes an enum statement.\n\n        Args:\n            test_case: The test case the statement belongs to\n            generic_enum: The enum\n            value: The value\n        \"\"\"\n        self._generic_enum = generic_enum\n        super().__init__(test_case, generic_enum.generated_type(), value)\n\n    def accessible_object(self) -> gao.GenericEnum:  # noqa: D102\n        return self._generic_enum\n\n    @property\n    def value_name(self) -> str:\n        \"\"\"Convenience method to access the enum name that is associated with the index.\n\n        Returns:\n            The associated enum value.\n        \"\"\"\n        assert self._value is not None\n        return self._generic_enum.names[self._value]\n\n    def randomize_value(self) -> None:  # noqa: D102\n        self._value = randomness.next_int(0, len(self._generic_enum.names))\n\n    def delta(self) -> None:  # noqa: D102\n        assert self._value is not None\n        self._value += randomness.choice([-1, 1])\n        self._value = (self._value + len(self._generic_enum.names)) % len(self._generic_enum.names)\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> EnumPrimitiveStatement:\n        return EnumPrimitiveStatement(test_case, self._generic_enum, value=self.value)\n\n    def __repr__(self) -> str:\n        return f\"EnumPrimitiveStatement({self._test_case}, {self._value})\"\n\n    def __str__(self) -> str:\n        return f\"{self.value_name}: Enum\"\n\n    def structural_eq(  # noqa: D102\n        self,\n        other: Statement,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> bool:\n        return (\n            super().structural_eq(other, memo)\n            and isinstance(other, EnumPrimitiveStatement)\n            and other._generic_enum == self._generic_enum  # noqa: SLF001\n        )\n\n    def structural_hash(  # noqa: D102\n        self, memo: dict[vr.VariableReference, int]\n    ) -> int:\n        return hash((super().structural_hash(memo), self._generic_enum))\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_enum_statement(self)\n\n\nclass ClassPrimitiveStatement(PrimitiveStatement[int]):\n    \"\"\"Primitive Statement that references a class.\"\"\"\n\n    def __init__(self, test_case: tc.TestCase, value: int | None = None):  # noqa: D107\n        # TODO(fk) think about type being generic/bound, e.g., type[Foo]\n        # We store the index in the global class list here.\n        super().__init__(\n            test_case,\n            Instance(test_case.test_cluster.type_system.to_type_info(type)),\n            value,\n        )\n\n    @property\n    def type_info(self) -> TypeInfo:\n        \"\"\"Convenience method to access the type that is associated with the index.\n\n        Returns:\n            The associated type info.\n        \"\"\"\n        assert self._value is not None\n        return self._test_case.test_cluster.type_system.get_all_types()[self._value]\n\n    def randomize_value(self) -> None:  # noqa: D102\n        self._value = randomness.next_int(\n            0, len(self._test_case.test_cluster.type_system.get_all_types())\n        )\n\n    def delta(self) -> None:  # noqa: D102\n        assert self._value is not None\n        num_classes = len(self._test_case.test_cluster.type_system.get_all_types())\n        self._value += randomness.choice([-1, 1])\n        self._value = (self._value + num_classes) % num_classes\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> ClassPrimitiveStatement:\n        return ClassPrimitiveStatement(test_case, value=self.value)\n\n    def __repr__(self) -> str:\n        return f\"EnumPrimitiveStatement({self._test_case}, {self._value})\"\n\n    def __str__(self) -> str:\n        assert self._value is not None\n        return f\"{self.type_info.full_name}: type\"\n\n    def structural_eq(  # noqa: D102\n        self,\n        other: Statement,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> bool:\n        return super().structural_eq(other, memo) and isinstance(other, ClassPrimitiveStatement)\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_class_primitive_statement(self)\n\n\nclass NoneStatement(PrimitiveStatement[None]):\n    \"\"\"A statement serving as a None reference.\"\"\"\n\n    def __init__(self, test_case: tc.TestCase):  # noqa: D107\n        super().__init__(test_case, NoneType())\n\n    def clone(  # noqa: D102\n        self,\n        test_case: tc.TestCase,\n        memo: dict[vr.VariableReference, vr.VariableReference],\n    ) -> NoneStatement:\n        return NoneStatement(test_case)\n\n    def accept(self, visitor: StatementVisitor) -> None:  # noqa: D102\n        visitor.visit_none_statement(self)\n\n    def randomize_value(self) -> None:\n        \"\"\"Cannot randomize a value for None.\"\"\"\n\n    def delta(self) -> None:\n        \"\"\"Cannot compute a delta for None.\"\"\"\n\n    def __repr__(self) -> str:\n        return f\"NoneStatement({self._test_case})\"\n\n    def __str__(self) -> str:\n        return \"None\"\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:32"}, {"role": "assistant", "content": "```python\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\nfrom unittest.mock import MagicMock\n\nimport pytest\n\nimport pynguin.assertion.assertion_trace as at\n\nfrom pynguin.utils.orderedset import OrderedSet\n\n\n@pytest.fixture\ndef assertion_trace():\n    return at.AssertionTrace()\n\n\ndef test_empty(assertion_trace):\n    assert assertion_trace.trace == {}\n\n\ndef test_add_entry(assertion_trace):\n    variable = MagicMock()\n    variable.get_statement_position.return_value = 42\n    entry = MagicMock()\n    assertion_trace.add_entry(1337, entry)\n    assert assertion_trace.trace == {1337: OrderedSet([entry])}\n\n\ndef test_add_entry_same_position(assertion_trace):\n    entry = MagicMock()\n    assertion_trace.add_entry(1337, entry)\n    assertion_trace.add_entry(1337, entry)\n    assert assertion_trace.trace == {1337: OrderedSet([entry])}\n\n\ndef test_clear(assertion_trace):\n    entry = MagicMock()\n    assertion_trace.add_entry(1337, entry)\n    assertion_trace.clear()\n    assert assertion_trace.trace == {}\n\n\ndef test_clone(assertion_trace):\n    entry = MagicMock()\n    assertion_trace.add_entry(1337, entry)\n    clone = assertion_trace.clone()\n    assert dict(clone.trace) == {1337: OrderedSet([entry])}\n    assert assertion_trace.trace is not clone.trace\n\n\ndef test_get_assertions_empty(assertion_trace):\n    statement = MagicMock()\n    statement.get_position.return_value = 3\n    assert assertion_trace.get_assertions(statement) == OrderedSet()\n\n\ndef test_get_assertions(assertion_trace):\n    entry = MagicMock()\n    assertion_trace.add_entry(3, entry)\n    statement = MagicMock()\n    statement.get_position.return_value = 3\n    assert assertion_trace.get_assertions(statement) == OrderedSet([entry])\n\n\ndef test_merge():\n    ver_trace = at.AssertionVerificationTrace()\n    assert not ver_trace.was_violated(0, 0)\n\n\ndef test_merge_2():\n    ver_trace = at.AssertionVerificationTrace()\n    ver_trace.error[0].add(0)\n    assert ver_trace.was_violated(0, 0)\n\n\ndef test_merge_3():\n    ver_trace = at.AssertionVerificationTrace()\n    ver_trace.failed[0].add(0)\n    assert ver_trace.was_violated(0, 0)\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pynguin\nTest File Path: pynguin\\test_cfg\\test_cfg.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides analyses regarding the control-flow of the program.\"\"\"\n\nfrom __future__ import annotations\n\nimport queue\nimport sys\n\nfrom dataclasses import dataclass\nfrom typing import Any\nfrom typing import Generic\nfrom typing import TypeVar\n\nimport networkx as nx\n\nfrom bytecode import UNSET\nfrom bytecode import BasicBlock\nfrom bytecode import Bytecode\nfrom bytecode import Compare\nfrom bytecode import ControlFlowGraph\nfrom bytecode import Instr\n\nimport pynguin.utils.opcodes as op\n\nfrom pynguin.utils.orderedset import OrderedSet\n\n\n# Key for storing branch value in networkx edge.\nEDGE_DATA_BRANCH_VALUE = \"branch_value\"\n\n\nclass ProgramGraphNode:\n    \"\"\"A base class for a node of the program graph.\"\"\"\n\n    def __init__(\n        self,\n        index: int,\n        *,\n        offset: int = 0,\n        basic_block: BasicBlock | None = None,\n        is_artificial: bool = False,\n    ) -> None:\n        \"\"\"Instantiates a node for a program graph.\n\n        Args:\n            index: The index of the node\n            offset: The offset of the first instruction of the node\n            basic_block: The basic block in the code\n            is_artificial: Whether the node is an artificial node\n        \"\"\"\n        self._index = index\n        self._offset = offset\n        self._basic_block = basic_block\n        self._is_artificial = is_artificial\n        self._predicate_id: int | None = None\n\n    @property\n    def index(self) -> int:\n        \"\"\"Provides the index of the node.\n\n        Returns:\n            The index of the node\n        \"\"\"\n        return self._index\n\n    @property\n    def offset(self) -> int:\n        \"\"\"Provides the offset of the node the first instruction of the node.\n\n        Returns:\n            The offset of the node\n        \"\"\"\n        return self._offset\n\n    @offset.setter\n    def offset(self, offset: int) -> None:\n        \"\"\"Set a new offset.\n\n        Args:\n            offset: The offset\n        \"\"\"\n        self._offset = offset\n\n    @property\n    def basic_block(self) -> BasicBlock | None:\n        \"\"\"Provides the basic block attached to this node.\n\n        Returns:\n            The optional basic block attached to this node\n        \"\"\"\n        return self._basic_block\n\n    @property\n    def is_artificial(self) -> bool:\n        \"\"\"Whether a node is artificially inserted into the graph.\n\n        Returns:\n            Whether a node is artificially inserted into the graph\n        \"\"\"\n        return self._is_artificial\n\n    @property\n    def predicate_id(self) -> int | None:\n        \"\"\"Provides the predicate ID of the node, if any.\n\n        Returns:\n            The predicate id assigned to this node, if any.\n        \"\"\"\n        return self._predicate_id\n\n    @predicate_id.setter\n    def predicate_id(self, predicate_id: int) -> None:\n        \"\"\"Set a new predicate id.\n\n        Args:\n            predicate_id: The predicate id\n        \"\"\"\n        self._predicate_id = predicate_id\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, ProgramGraphNode):\n            return False\n        if self is other:\n            return True\n        return self._index == other.index\n\n    def __hash__(self) -> int:\n        return 31 + 17 * self._index\n\n    def __str__(self) -> str:\n        result = f\"ProgramGraphNode({self._index})\"\n        if self._predicate_id is not None:\n            result += f\"\\npredicate_id {self._predicate_id}\"\n        if self._basic_block is not None:\n            instructions = []\n            for instr in self._basic_block:\n                arg = instr.arg  # type: ignore[union-attr]\n                if isinstance(arg, BasicBlock):\n                    # We cannot determine which ProgramGraphNode this is.\n                    arg = \"ProgramGraphNode\"\n                elif isinstance(arg, Compare):\n                    arg = arg.name\n                elif arg is UNSET:\n                    arg = \"\"\n                else:\n                    arg = repr(arg)\n                formatted = instr.name  # type: ignore[union-attr]\n                if arg:\n                    formatted += f\" {arg}\"\n                instructions.append(formatted)\n            result += \"\\n\" + \"\\n\".join(instructions)\n        return result\n\n    def __repr__(self) -> str:\n        return f\"ProgramGraphNode(index={self._index}, basic_block={self._basic_block})\"\n\n\nN = TypeVar(\"N\", bound=ProgramGraphNode)\n\n\nclass ProgramGraph(Generic[N]):\n    \"\"\"Provides a base implementation for a program graph.\n\n    Internally, this program graph uses the `NetworkX` library to hold the graph and\n    do all the operations on it.\n    \"\"\"\n\n    def __init__(self) -> None:  # noqa: D107\n        self._graph = nx.DiGraph()\n\n    def add_node(self, node: N, **attr: Any) -> None:\n        \"\"\"Add a node to the graph.\n\n        Args:\n            node: The node\n            attr: A dict of attributes that will be attached to the node\n        \"\"\"\n        self._graph.add_node(node, **attr)\n\n    def add_edge(self, start: N, end: N, **attr: Any) -> None:\n        \"\"\"Add an edge between two nodes to the graph.\n\n        Args:\n            start: The start node of the edge\n            end: The end node of the edge\n            attr: A dict of attributes that will be attached to the edge.\n        \"\"\"\n        self._graph.add_edge(start, end, **attr)\n\n    def get_predecessors(self, node: N) -> set[N]:\n        \"\"\"Provides a set of all direct predecessors of a node.\n\n        Args:\n            node: The node to start\n\n        Returns:\n            A set of direct predecessors of the node\n        \"\"\"\n        return set(self._graph.predecessors(node))\n\n    def get_successors(self, node: N) -> set[N]:\n        \"\"\"Provides a set of all direct successors of a node.\n\n        Args:\n            node: The node to start\n\n        Returns:\n            A set of direct successors of the node\n        \"\"\"\n        return set(self._graph.successors(node))\n\n    @property\n    def nodes(self) -> set[N]:\n        \"\"\"Provides all nodes in the graph.\n\n        Returns:\n            The set of all nodes in the graph\n        \"\"\"\n        return set(self._graph.nodes)\n\n    @property\n    def graph(self) -> nx.DiGraph:\n        \"\"\"The internal graph.\n\n        Returns:\n            The internal graph\n        \"\"\"\n        return self._graph\n\n    @property\n    def entry_node(self) -> N | None:\n        \"\"\"Provides the entry node of the graph.\n\n        Returns:\n            The entry node of the graph\n        \"\"\"\n        for node in self._graph.nodes:\n            if len(self.get_predecessors(node)) == 0:\n                return node\n        return None\n\n    @property\n    def exit_nodes(self) -> set[N]:\n        \"\"\"Provides the exit nodes of the graph.\n\n        Returns:\n            The set of exit nodes of the graph\n        \"\"\"\n        exit_nodes: set[N] = set()\n        for node in self._graph.nodes:\n            if len(self.get_successors(node)) == 0:\n                exit_nodes.add(node)\n        return exit_nodes\n\n    def get_transitive_successors(self, node: N) -> set[N]:\n        \"\"\"Calculates the transitive closure (the transitive successors) of a node.\n\n        Args:\n            node: The node to start with\n\n        Returns:\n            The transitive closure of the node\n        \"\"\"\n        return self._get_transitive_successors(node, set())\n\n    def _get_transitive_successors(self, node: N, done: set[N]) -> set[N]:\n        successors: set[N] = set()\n        for successor_node in self.get_successors(node):\n            if successor_node not in done:\n                successors.add(successor_node)\n                done.add(successor_node)\n                successors.update(self._get_transitive_successors(successor_node, done))\n        return successors\n\n    def get_least_common_ancestor(self, first: N, second: N) -> N:\n        \"\"\"Calculates the least or lowest common ancestor node of two nodes.\n\n        Both nodes have to be part of the graph!\n\n        Args:\n            first: The first node\n            second: The second node\n\n        Returns:\n            The least common ancestor node of the two nodes\n        \"\"\"\n        return nx.lowest_common_ancestor(self._graph, first, second)\n\n    @property\n    def dot(self) -> str:\n        \"\"\"Provides the DOT representation of this graph.\n\n        Returns:\n            The DOT representation of this graph\n        \"\"\"\n        graph = [\"strict digraph  {\"]\n        graph.extend(f'\"{node}\";' for node in self._graph.nodes)\n        for source, target, edge_data in self._graph.edges(data=True):\n            if edge_data == {}:\n                graph.append(f'\"{source}\" -> \"{target}\";')\n            else:\n                str_edge_data = \", \".join([f\"{k}={v}\" for k, v in edge_data.items()])\n                graph.append(f'\"{source}\" -> \"{target}\"  [{str_edge_data}];')\n        graph.append(\"}\")\n        return \"\\n\".join(graph)\n\n\nG = TypeVar(\"G\", bound=ProgramGraph)\n\n\ndef filter_dead_code_nodes(graph: G, entry_node_index: int = 0) -> G:\n    \"\"\"Prunes dead nodes from the given graph.\n\n    A dead node is a node that has no entry node.  To specify a legal entry node,\n    one can use the `entry_node_index` parameter.\n\n    Args:\n        graph: The graph to prune nodes from\n        entry_node_index: The index of the valid entry node\n\n    Returns:\n        The graph without the pruned dead nodes\n    \"\"\"\n    has_changed = True\n    while has_changed:\n        # Do this until we have reached a fixed point, i.e., removed all dead\n        # nodes from the graph.\n        has_changed = False\n        for node in graph.nodes:\n            if graph.get_predecessors(node) == set() and node.index != entry_node_index:\n                # The only node in the graph that is allowed to have no predecessor\n                # is the entry node, i.e., the node with index 0.  All other nodes\n                # without predecessors are considered dead code and thus removed.\n                graph.graph.remove_node(node)\n                has_changed = True\n    return graph\n\n\nclass CFG(ProgramGraph[ProgramGraphNode]):\n    \"\"\"The control-flow graph implementation based on the program graph.\"\"\"\n\n    # Attribute where the predicate id of the instrumentation is stored\n    PREDICATE_ID: str = \"predicate_id\"\n\n    def __init__(self, bytecode_cfg: ControlFlowGraph):\n        \"\"\"Create new CFG. Do not call directly, use static factory methods.\n\n        Args:\n            bytecode_cfg: the control flow graph of the underlying bytecode.\n        \"\"\"\n        super().__init__()\n        self._bytecode_cfg = bytecode_cfg\n        self._diameter: int | None = None\n\n    @staticmethod\n    def from_bytecode(bytecode: Bytecode) -> CFG:\n        \"\"\"Generates a new control-flow graph from a bytecode segment.\n\n        Besides generating a node for each block in the bytecode segment, as returned by\n        `bytecode`'s `ControlFlowGraph` implementation, we add two artificial nodes to\n        the generated CFG:\n         - an artificial entry node, having index -1, that is guaranteed to fulfill the\n           property of an entry node, i.e., there is no incoming edge, and\n         - an artificial exit node, having index `sys.maxsize`, that is guaranteed to\n           fulfill the property of an exit node, i.e., there is no outgoing edge, and\n           that is the only such node in the graph, which is important, e.g., for graph\n           reversal.\n        The index values are chosen that they do not appear in regular graphs, thus one\n        can easily distinguish them from the normal nodes in the graph by checking for\n        their index-property's value.\n\n        Args:\n            bytecode: The bytecode segment\n\n        Returns:\n            The control-flow graph for the segment\n        \"\"\"\n        blocks = ControlFlowGraph.from_bytecode(bytecode)\n        cfg = CFG(blocks)\n\n        # Create the nodes and a mapping of all edges to generate\n        edges, nodes = CFG._create_nodes_and_edges(blocks)\n\n        # Insert all edges between the previously generated nodes\n        CFG._create_graph(cfg, edges, nodes)\n\n        # Filter all dead-code nodes\n        cfg = filter_dead_code_nodes(cfg)\n\n        # Insert dummy exit and entry nodes\n        cfg = CFG._insert_dummy_exit_node(cfg)\n        cfg = CFG._insert_dummy_entry_node(cfg)\n        return cfg  # noqa: RET504\n\n    def bytecode_cfg(self) -> ControlFlowGraph:\n        \"\"\"Provide the raw control flow graph from the code object.\n\n        Can be used to instrument the control flow.\n\n        Returns:\n            The raw control-flow graph from the code object\n        \"\"\"\n        return self._bytecode_cfg\n\n    @staticmethod\n    def reverse(cfg: CFG) -> CFG:\n        \"\"\"Reverses a control-flow graph.\n\n        Generates a copy of the original control-flow graph.\n\n        Args:\n            cfg: The control-flow graph to reverse\n\n        Returns:\n            The reversed control-flow graph\n        \"\"\"\n        reversed_cfg = CFG(cfg.bytecode_cfg())\n\n        reversed_cfg._graph = cfg._graph.reverse(copy=True)  # noqa: SLF001\n        return reversed_cfg\n\n    def reversed(self) -> CFG:\n        \"\"\"Provides the reversed graph of this graph.\n\n        Returns:\n            The reversed graph\n        \"\"\"\n        return CFG.reverse(self)\n\n    @staticmethod\n    def copy_graph(cfg: CFG) -> CFG:\n        \"\"\"Provides a copy of the control-flow graph.\n\n        Args:\n            cfg: The original graph\n\n        Returns:\n            The copied graph\n        \"\"\"\n        copy = CFG(ControlFlowGraph())  # TODO(fk) Cloning the bytecode cfg is complicated.\n\n        copy._graph = cfg._graph.copy()  # noqa: SLF001\n        return copy\n\n    def copy(self) -> CFG:\n        \"\"\"Provides a copy of the control-flow graph.\n\n        Returns:\n            The copied graph\n        \"\"\"\n        return CFG.copy_graph(self)\n\n    @staticmethod\n    def _create_nodes_and_edges(\n        blocks: ControlFlowGraph,\n    ) -> tuple[dict[int, list[tuple[int, dict]]], dict[int, ProgramGraphNode]]:\n        nodes: dict[int, ProgramGraphNode] = {}\n        edges: dict[int, list[tuple[int, dict]]] = {}\n        offset = 0\n        for node_index, block in enumerate(blocks):\n            node = ProgramGraphNode(index=node_index, basic_block=block, offset=offset)\n            # each instruction increases the offset by 2, therefore the offset at the\n            # beginning of the next block is the current offset plus twice the length\n            # of the current block\n            offset += len(block) * 2\n\n            nodes[node_index] = node\n            if node_index not in edges:\n                edges[node_index] = []\n\n            next_block = block.next_block\n            target_block = block.get_jump()\n\n            last_instr = block[-1]\n            if isinstance(last_instr, Instr) and (\n                last_instr.is_cond_jump() or last_instr.opcode == op.FOR_ITER\n            ):\n                if last_instr.opcode in {op.POP_JUMP_IF_TRUE, op.JUMP_IF_TRUE_OR_POP}:\n                    # These jump to arg if ToS is True\n                    true_branch = target_block\n                    false_branch = next_block\n                elif last_instr.opcode in {\n                    op.POP_JUMP_IF_FALSE,\n                    op.JUMP_IF_FALSE_OR_POP,\n                    op.JUMP_IF_NOT_EXC_MATCH,\n                    op.FOR_ITER,\n                }:\n                    # These jump to arg if ToS is False, is Empty or if Exc does\n                    # not match.\n                    true_branch = next_block\n                    false_branch = target_block\n                else:\n                    raise RuntimeError(\n                        \"Unknown conditional Jump instruction in bytecode \" + last_instr.name\n                    )\n                for next_branch, value in [(true_branch, True), (false_branch, False)]:\n                    next_index = blocks.get_block_index(\n                        next_branch  # type: ignore[arg-type]\n                    )\n                    # 'label' is also set to value, to get a nicer DOT representation,\n                    # because 'label' is a keyword for labelling edges.\n                    edges[node_index].append((\n                        next_index,\n                        {EDGE_DATA_BRANCH_VALUE: value, \"label\": value},\n                    ))\n            else:\n                if next_block:\n                    next_index = blocks.get_block_index(next_block)\n                    edges[node_index].append((next_index, {}))\n                if target_block := block.get_jump():\n                    next_index = blocks.get_block_index(target_block)\n                    edges[node_index].append((next_index, {}))\n        return edges, nodes\n\n    @staticmethod\n    def _create_graph(\n        cfg: CFG,\n        edges: dict[int, list[tuple[int, dict]]],\n        nodes: dict[int, ProgramGraphNode],\n    ):\n        # add nodes to graph\n        for node in nodes.values():\n            cfg.add_node(node)\n\n        # add edges to graph\n        for predecessor, successors in edges.items():\n            for successor, attrs in successors:\n                predecessor_node = nodes.get(predecessor)\n                successor_node = nodes.get(successor)\n                assert predecessor_node\n                assert successor_node\n                cfg.add_edge(predecessor_node, successor_node, **attrs)\n\n    @staticmethod\n    def _infinite_loop_nodes(cfg: CFG) -> set[ProgramGraphNode]:\n        nodes: set[ProgramGraphNode] = set()\n        exit_nodes = cfg.exit_nodes\n        for node in cfg.nodes:\n            successors = cfg.get_successors(node)\n            if node in successors and successors.isdisjoint(exit_nodes):\n                nodes.add(node)\n        return nodes\n\n    @staticmethod\n    def _insert_dummy_entry_node(cfg: CFG) -> CFG:\n        dummy_entry_node = ProgramGraphNode(index=-1, is_artificial=True)\n        # Search node with index 0. This block contains the instruction where\n        # the execution of a code object begins.\n        node_zero = [n for n in cfg.nodes if n.index == 0]\n        assert len(node_zero) == 1, \"Execution has to start at exactly one node that has index 0.\"\n        entry_node = node_zero[0]\n        cfg.add_node(dummy_entry_node)\n        cfg.add_edge(dummy_entry_node, entry_node)\n        return cfg\n\n    @staticmethod\n    def _insert_dummy_exit_node(cfg: CFG) -> CFG:\n        dummy_exit_node = ProgramGraphNode(index=sys.maxsize, is_artificial=True)\n        exit_nodes = cfg.exit_nodes\n        assert exit_nodes, (\n            \"Control flow must have at least one exit node. Offending CFG: \" + cfg.dot\n        )\n        cfg.add_node(dummy_exit_node)\n        for exit_node in exit_nodes:\n            cfg.add_edge(exit_node, dummy_exit_node)\n        for infinite_loop_node in CFG._infinite_loop_nodes(cfg):\n            cfg.add_edge(infinite_loop_node, dummy_exit_node)\n        return cfg\n\n    @property\n    def cyclomatic_complexity(self) -> int:\n        \"\"\"Calculates McCabe's cyclomatic complexity for this control-flow graph.\n\n        Returns:\n            McCabe's cyclocmatic complexity number\n        \"\"\"\n        return len(self._graph.edges) - len(self._graph.nodes) + 2\n\n    @property\n    def diameter(self) -> int:\n        \"\"\"Computes the diameter of the graph.\n\n        Returns:\n            The diameter of the graph\n        \"\"\"\n        if self._diameter is None:\n            # Do this computation lazily\n            try:\n                self._diameter = nx.diameter(self._graph, usebounds=True)\n            except nx.NetworkXError:\n                # It seems like NetworkX makes some assumptions on the graph which\n                # are not documented (or which I could not find at least) that caused\n                # these errors.\n                # If the diameter computation fails for some reason, use an upper bound\n                self._diameter = len(self._graph.edges)\n        return self._diameter\n\n\nclass DominatorTree(ProgramGraph[ProgramGraphNode]):\n    \"\"\"Implements a dominator tree.\"\"\"\n\n    @staticmethod\n    def compute(graph: CFG) -> DominatorTree:\n        \"\"\"Computes the dominator tree for a control-flow graph.\n\n        Args:\n            graph: The control-flow graph\n\n        Returns:\n            The dominator tree for the control-flow graph\n        \"\"\"\n        return DominatorTree.compute_dominance_tree(graph)\n\n    @staticmethod\n    def compute_post_dominator_tree(graph: CFG) -> DominatorTree:\n        \"\"\"Computes the post-dominator tree for a control-flow graph.\n\n        Args:\n            graph: The control-flow graph\n\n        Returns:\n            The post-dominator tree for the control-flow graph\n        \"\"\"\n        reversed_cfg = graph.reversed()\n        return DominatorTree.compute(reversed_cfg)\n\n    @staticmethod\n    def compute_dominance_tree(graph: CFG) -> DominatorTree:\n        \"\"\"Computes the dominance tree for a control-flow graph.\n\n        Args:\n            graph: The control-flow graph\n\n        Returns:\n            The dominance tree for the control-flow graph\n        \"\"\"\n        dominance: dict[ProgramGraphNode, set[ProgramGraphNode]] = (\n            DominatorTree._calculate_dominance(graph)\n        )\n        for dominance_node, nodes in dominance.items():\n            nodes.discard(dominance_node)\n        dominance_tree = DominatorTree()\n        entry_node = graph.entry_node\n        assert entry_node is not None\n        dominance_tree.add_node(entry_node)\n\n        node_queue: queue.SimpleQueue = queue.SimpleQueue()\n        node_queue.put(entry_node)\n        while not node_queue.empty():\n            node: ProgramGraphNode = node_queue.get()\n            for current, dominators in dominance.items():\n                if node in dominators:\n                    dominators.remove(node)\n                    if len(dominators) == 0:\n                        dominance_tree.add_node(current)\n                        dominance_tree.add_edge(node, current)\n                        node_queue.put(current)\n        return dominance_tree\n\n    @staticmethod\n    def _calculate_dominance(\n        graph: CFG,\n    ) -> dict[ProgramGraphNode, set[ProgramGraphNode]]:\n        dominance_map: dict[ProgramGraphNode, set[ProgramGraphNode]] = {}\n        entry = graph.entry_node\n        assert entry, \"Cannot work with a graph without entry nodes\"\n        entry_dominators: set[ProgramGraphNode] = {entry}\n        dominance_map[entry] = entry_dominators\n\n        for node in graph.nodes:\n            if node == entry:\n                continue\n            all_nodes: set[ProgramGraphNode] = set(graph.nodes)\n            dominance_map[node] = all_nodes\n\n        changed: bool = True\n        while changed:\n            changed = False\n            for node in graph.nodes:\n                if node == entry:\n                    continue\n                current_dominators = dominance_map.get(node)\n                new_dominators = DominatorTree._calculate_dominators(graph, dominance_map, node)\n\n                if current_dominators != new_dominators:\n                    changed = True\n                    dominance_map[node] = new_dominators\n                    break\n\n        return dominance_map\n\n    @staticmethod\n    def _calculate_dominators(\n        graph: CFG,\n        dominance_map: dict[ProgramGraphNode, set[ProgramGraphNode]],\n        node: ProgramGraphNode,\n    ) -> set[ProgramGraphNode]:\n        dominators: set[ProgramGraphNode] = {node}\n        intersection: set[ProgramGraphNode] = set()\n        predecessors = graph.get_predecessors(node)\n        if not predecessors:\n            return set()\n\n        first_time: bool = True\n        for predecessor in predecessors:\n            predecessor_dominators = dominance_map.get(predecessor)\n            assert predecessor_dominators is not None, \"Cannot be None\"\n            if first_time:\n                intersection = intersection.union(predecessor_dominators)\n                first_time = False\n            else:\n                intersection.intersection_update(predecessor_dominators)\n        intersection = intersection.union(dominators)\n        return intersection  # noqa: RET504\n\n\nclass ControlDependenceGraph(ProgramGraph[ProgramGraphNode]):\n    \"\"\"Implements a control-dependence graph.\"\"\"\n\n    @staticmethod\n    def compute(graph: CFG) -> ControlDependenceGraph:\n        \"\"\"Computes the control-dependence graph for a given control-flow graph.\n\n        Args:\n            graph: The control-flow graph\n\n        Returns:\n            The control-dependence graph\n        \"\"\"\n        augmented_cfg = ControlDependenceGraph._create_augmented_graph(graph)\n        post_dominator_tree = DominatorTree.compute_post_dominator_tree(augmented_cfg)\n        cdg = ControlDependenceGraph()\n        nodes = augmented_cfg.nodes\n\n        for node in nodes:\n            cdg.add_node(node)\n\n        # Find matching edges in the CFG.\n        edges: set[ControlDependenceGraph._Edge] = set()\n        for source in nodes:\n            for target in augmented_cfg.get_successors(source):\n                if source not in post_dominator_tree.get_transitive_successors(target):\n                    # Store branching data from edge, i.e., which outcome of the\n                    # branching node leads to this node.\n                    data = frozenset(augmented_cfg.graph.get_edge_data(source, target).items())\n                    edges.add(ControlDependenceGraph._Edge(source=source, target=target, data=data))\n\n        # Mark nodes in the PDT and construct edges for them.\n        for edge in edges:\n            least_common_ancestor = post_dominator_tree.get_least_common_ancestor(\n                edge.source, edge.target\n            )\n            current = edge.target\n            while current != least_common_ancestor:\n                # TODO(fk) can the branching info be actually used here?\n                # Seems ok?\n                cdg.add_edge(edge.source, current, **dict(edge.data))\n                predecessors = post_dominator_tree.get_predecessors(current)\n                assert len(predecessors) == 1, (\n                    \"Cannot have more than one predecessor in a tree, this violates a \"\n                    \"tree invariant\"\n                )\n                current = predecessors.pop()\n\n            if least_common_ancestor is edge.source:\n                cdg.add_edge(edge.source, least_common_ancestor, **dict(edge.data))\n\n        return filter_dead_code_nodes(cdg, entry_node_index=-sys.maxsize)\n\n    def get_control_dependencies(self, node: ProgramGraphNode) -> OrderedSet[ControlDependency]:\n        \"\"\"Get the immediate control dependencies of this node.\n\n        Args:\n            node: the node whose dependencies should be retrieved.\n\n        Returns:\n            The direct control dependencies of the given node, if any.\n        \"\"\"\n        assert node is not None\n        assert node in self.graph.nodes\n        return self._retrieve_control_dependencies(node, OrderedSet())\n\n    def _retrieve_control_dependencies(\n        self, node: ProgramGraphNode, handled: OrderedSet\n    ) -> OrderedSet[ControlDependency]:\n        result: OrderedSet[ControlDependency] = OrderedSet()\n        for pred in self._graph.predecessors(node):\n            if (pred, node) in handled:\n                continue\n            handled.add((pred, node))\n\n            if (\n                branch_value := self._graph.get_edge_data(pred, node).get(\n                    EDGE_DATA_BRANCH_VALUE, None\n                )\n            ) is not None:\n                assert pred.predicate_id is not None\n                result.add(ControlDependency(pred.predicate_id, branch_value))\n            else:\n                result.update(self._retrieve_control_dependencies(pred, handled))\n        return result\n\n    def is_control_dependent_on_root(self, node: ProgramGraphNode) -> bool:\n        \"\"\"Does this node directly depend on entering the code object?\n\n        Args:\n            node: The program-graph node for the check\n\n        Returns:\n            Whether the given node is directly dependent on the entry of the code object\n        \"\"\"\n        return self._is_control_dependent_on_root(node, set())\n\n    def _is_control_dependent_on_root(\n        self, node: ProgramGraphNode, visited: set[ProgramGraphNode]\n    ) -> bool:\n        if (self.entry_node, node) in self.graph.edges:\n            return True\n        for pred in self.graph.predecessors(node):\n            if pred in visited:\n                continue\n            visited.add(pred)\n            if pred.predicate_id is not None:\n                continue\n            if pred == node:\n                continue\n            if self._is_control_dependent_on_root(pred, visited):\n                return True\n        return False\n\n    @staticmethod\n    def _create_augmented_graph(graph: CFG) -> CFG:\n        entry_node = graph.entry_node\n        assert entry_node, \"Cannot work with CFG without entry node\"\n        exit_nodes = graph.exit_nodes\n        augmented_graph = graph.copy()\n        start_node = ProgramGraphNode(index=-sys.maxsize, is_artificial=True)\n        augmented_graph.add_node(start_node)\n        augmented_graph.add_edge(start_node, entry_node)\n        for exit_node in exit_nodes:\n            augmented_graph.add_edge(start_node, exit_node)\n        return augmented_graph\n\n    @dataclass(frozen=True)\n    class _Edge:\n        source: ProgramGraphNode\n        target: ProgramGraphNode\n        data: frozenset\n\n\n@dataclass(frozen=True)\nclass ControlDependency:\n    \"\"\"Models a control dependency.\"\"\"\n\n    predicate_id: int\n    branch_value: bool\n\n\n### Source File Content:\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\n\nclass Foo:\n    def receive(self):\n        \"\"\"A dummy docstring.\"\"\"\n        while True:\n            print(\"True\")\n\n### Source File Dependency Files Content\n### Dependency File: opcodes.py\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\n# Idea and structure are taken from the pyChecco project, see:\n# https://github.com/ipsw1/pychecco\n\n\"\"\"Provides enums for opcode numbers of instructions in bytecode.\"\"\"\n\nPOP_TOP = 1\nROT_TWO = 2\nROT_THREE = 3\nDUP_TOP = 4\nDUP_TOP_TWO = 5\nROT_FOUR = 6\n\nNOP = 9\nUNARY_POSITIVE = 10\nUNARY_NEGATIVE = 11\nUNARY_NOT = 12\n\nUNARY_INVERT = 15\n\nBINARY_MATRIX_MULTIPLY = 16\nINPLACE_MATRIX_MULTIPLY = 17\n\nBINARY_POWER = 19\nBINARY_MULTIPLY = 20\n\nBINARY_MODULO = 22\nBINARY_ADD = 23\nBINARY_SUBTRACT = 24\nBINARY_SUBSCR = 25\nBINARY_FLOOR_DIVIDE = 26\nBINARY_TRUE_DIVIDE = 27\nINPLACE_FLOOR_DIVIDE = 28\nINPLACE_TRUE_DIVIDE = 29\n\nGET_LEN = 30\nMATCH_MAPPING = 31\nMATCH_SEQUENCE = 32\nMATCH_KEYS = 33\nCOPY_DICT_WITHOUT_KEYS = 34\n\nWITH_EXCEPT_START = 49\nGET_AITER = 50\nGET_ANEXT = 51\nBEFORE_ASYNC_WITH = 52\n\nEND_ASYNC_FOR = 54\nINPLACE_ADD = 55\nINPLACE_SUBTRACT = 56\nINPLACE_MULTIPLY = 57\n\nINPLACE_MODULO = 59\nSTORE_SUBSCR = 60\nDELETE_SUBSCR = 61\nBINARY_LSHIFT = 62\nBINARY_RSHIFT = 63\nBINARY_AND = 64\nBINARY_XOR = 65\nBINARY_OR = 66\nINPLACE_POWER = 67\nGET_ITER = 68\nGET_YIELD_FROM_ITER = 69\n\nPRINT_EXPR = 70\nLOAD_BUILD_CLASS = 71\nYIELD_FROM = 72\nGET_AWAITABLE = 73\n\nLOAD_ASSERTION_ERROR = 74\n\nINPLACE_LSHIFT = 75\nINPLACE_RSHIFT = 76\nINPLACE_AND = 77\nINPLACE_XOR = 78\nINPLACE_OR = 79\n\nRETURN_VALUE = 83\nIMPORT_STAR = 84\nSETUP_ANNOTATIONS = 85\nYIELD_VALUE = 86\nPOP_BLOCK = 87\n\nPOP_EXCEPT = 89\n\n# opcodes below 90 ignore their argument\n\nSTORE_NAME = 90\nDELETE_NAME = 91\nUNPACK_SEQUENCE = 92\nFOR_ITER = 93\nUNPACK_EX = 94\nSTORE_ATTR = 95\nDELETE_ATTR = 96\nSTORE_GLOBAL = 97\nDELETE_GLOBAL = 98\nROT_N = 99\nLOAD_CONST = 100\n\nLOAD_NAME = 101\nBUILD_TUPLE = 102\nBUILD_LIST = 103\nBUILD_SET = 104\nBUILD_MAP = 105\nLOAD_ATTR = 106\nCOMPARE_OP = 107\nIMPORT_NAME = 108\nIMPORT_FROM = 109\n\nJUMP_FORWARD = 110\nJUMP_IF_FALSE_OR_POP = 111\nJUMP_IF_TRUE_OR_POP = 112\nJUMP_ABSOLUTE = 113\nPOP_JUMP_IF_FALSE = 114\nPOP_JUMP_IF_TRUE = 115\n\nLOAD_GLOBAL = 116\n\nIS_OP = 117\nCONTAINS_OP = 118\n\nRERAISE = 119\n\nJUMP_IF_NOT_EXC_MATCH = 121\n\nSETUP_FINALLY = 122\n\nLOAD_FAST = 124\nSTORE_FAST = 125\nDELETE_FAST = 126\n\nGEN_START = 129\n\nRAISE_VARARGS = 130\nCALL_FUNCTION = 131\nMAKE_FUNCTION = 132\nBUILD_SLICE = 133\nLOAD_CLOSURE = 135\nLOAD_DEREF = 136\nSTORE_DEREF = 137\nDELETE_DEREF = 138\n\nCALL_FUNCTION_KW = 141\nCALL_FUNCTION_EX = 142\n\nSETUP_WITH = 143\n\nLIST_APPEND = 145\nSET_ADD = 146\nMAP_ADD = 147\n\nLOAD_CLASSDEREF = 148\n\nEXTENDED_ARG = 144\n\nMATCH_CLASS = 152\n\nSETUP_ASYNC_WITH = 154\n\nFORMAT_VALUE = 155\nBUILD_CONST_KEY_MAP = 156\nBUILD_STRING = 157\n\nLOAD_METHOD = 160\nCALL_METHOD = 161\nLIST_EXTEND = 162\nSET_UPDATE = 163\nDICT_MERGE = 164\nDICT_UPDATE = 165\n\n\nOP_UNARY = (\n    UNARY_POSITIVE,\n    UNARY_NEGATIVE,\n    UNARY_NOT,\n    UNARY_INVERT,\n    GET_ITER,\n    GET_YIELD_FROM_ITER,\n)\nOP_BINARY = (\n    BINARY_POWER,\n    BINARY_MULTIPLY,\n    BINARY_MATRIX_MULTIPLY,\n    BINARY_FLOOR_DIVIDE,\n    BINARY_TRUE_DIVIDE,\n    BINARY_MODULO,\n    BINARY_ADD,\n    BINARY_SUBTRACT,\n    BINARY_LSHIFT,\n    BINARY_RSHIFT,\n    BINARY_AND,\n    BINARY_XOR,\n    BINARY_OR,\n)\nOP_INPLACE = (\n    INPLACE_POWER,\n    INPLACE_MULTIPLY,\n    INPLACE_MATRIX_MULTIPLY,\n    INPLACE_FLOOR_DIVIDE,\n    INPLACE_TRUE_DIVIDE,\n    INPLACE_MODULO,\n    INPLACE_ADD,\n    INPLACE_SUBTRACT,\n    INPLACE_LSHIFT,\n    INPLACE_RSHIFT,\n    INPLACE_AND,\n    INPLACE_XOR,\n    INPLACE_OR,\n)\nOP_COMPARE = (COMPARE_OP, IS_OP, CONTAINS_OP)\nOP_LOCAL_ACCESS = (STORE_FAST, LOAD_FAST, DELETE_FAST)\nOP_NAME_ACCESS = (STORE_NAME, LOAD_NAME, DELETE_NAME)\nOP_GLOBAL_ACCESS = (STORE_GLOBAL, LOAD_GLOBAL, DELETE_GLOBAL)\nOP_DEREF_ACCESS = (STORE_DEREF, LOAD_DEREF, DELETE_DEREF, LOAD_CLASSDEREF)\nOP_ATTR_ACCESS = (STORE_ATTR, LOAD_ATTR, DELETE_ATTR, IMPORT_FROM, LOAD_METHOD)\nOP_SUBSCR_ACCESS = (STORE_SUBSCR, DELETE_SUBSCR, BINARY_SUBSCR)\nOP_IMPORT_NAME = (IMPORT_NAME,)\nOP_ABSOLUTE_JUMP = (\n    JUMP_IF_FALSE_OR_POP,\n    JUMP_IF_TRUE_OR_POP,\n    JUMP_ABSOLUTE,\n    POP_JUMP_IF_FALSE,\n    POP_JUMP_IF_TRUE,\n    JUMP_IF_NOT_EXC_MATCH,\n)\nOP_RELATIVE_JUMP = (\n    FOR_ITER,\n    JUMP_FORWARD,\n    SETUP_FINALLY,\n    SETUP_WITH,\n    SETUP_ASYNC_WITH,\n)\nOP_CALL = (CALL_FUNCTION, CALL_FUNCTION_KW, CALL_FUNCTION_EX, CALL_METHOD, YIELD_FROM)\nOP_RETURN = (RETURN_VALUE, YIELD_VALUE)\n\nTRACED_INSTRUCTIONS = (\n    OP_UNARY\n    + OP_BINARY\n    + OP_INPLACE\n    + OP_COMPARE\n    + OP_LOCAL_ACCESS\n    + OP_NAME_ACCESS\n    + OP_GLOBAL_ACCESS\n    + OP_DEREF_ACCESS\n    + OP_ATTR_ACCESS\n    + OP_SUBSCR_ACCESS\n    + OP_IMPORT_NAME\n    + OP_ABSOLUTE_JUMP\n    + OP_RELATIVE_JUMP\n    + OP_CALL\n    + OP_RETURN\n)\n\nMEMORY_USE_INSTRUCTIONS = (\n    LOAD_FAST,\n    LOAD_NAME,\n    LOAD_GLOBAL,\n    LOAD_ATTR,\n    LOAD_DEREF,\n    BINARY_SUBSCR,\n    LOAD_METHOD,\n    IMPORT_FROM,\n    LOAD_CLOSURE,\n    LOAD_CLASSDEREF,\n)\nMEMORY_DEF_INSTRUCTIONS = (\n    STORE_FAST,\n    STORE_NAME,\n    STORE_GLOBAL,\n    STORE_DEREF,\n    STORE_ATTR,\n    STORE_SUBSCR,\n    BINARY_SUBSCR,\n    DELETE_FAST,\n    DELETE_NAME,\n    DELETE_GLOBAL,\n    DELETE_ATTR,\n    DELETE_SUBSCR,\n    DELETE_DEREF,\n    IMPORT_NAME,\n)  # compensate incorrect stack effect for IMPORT_NAME\nCOND_BRANCH_INSTRUCTIONS = (\n    POP_JUMP_IF_TRUE,\n    POP_JUMP_IF_FALSE,\n    JUMP_IF_TRUE_OR_POP,\n    JUMP_IF_FALSE_OR_POP,\n    JUMP_IF_NOT_EXC_MATCH,\n    FOR_ITER,\n)\n\n\n### Dependency File: orderedset.py\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides the implementation of an ordered set.\n\nThe following code is taken from https://github.com/pantsbuild/pants\nWe made minor changes so that our linting pipeline accepts the code.\n\nCopyright 2020 Pants project contributors.\nLicensed under the Apache License, Version 2.0.\nSPDX-FileCopyrightText: 2020 Pants project contributors\nSPDX-License-Identifier: Apache-2.0\n\nAn OrderedSet is a set that remembers its insertion order, and a FrozenOrderedSet is\none that is also immutable. Based on the library `ordered-set` developed by\nElia Robyn Lake (Robyn Speer) and released under the MIT license:\nhttps://github.com/LuminosoInsight/ordered-set.\nCopyright (c) 2012-2022 Elia Robyn Lake\nSPDX-FileCopyrightText: 2012-2022 Elia Robyn Lake\nSPDX-License-Identifier: MIT\n\n\nThe library `ordered-set` is itself originally based on a recipe originally posted to\nActivateState Recipes by Raymond Hettiger and released under the MIT license:\nhttp://code.activestate.com/recipes/576694/.\nCopyright (c) 2009 Raymond Hettiger\nSPDX-FileCopyrightText: 2009 Raymond Hettiger\nSPDX-License-Identifier: MIT\n\"\"\"\n\nfrom __future__ import annotations\n\nimport itertools\n\nfrom collections.abc import Hashable\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import MutableSet\nfrom collections.abc import Sequence\nfrom collections.abc import Set as AbstractSet\nfrom typing import Any\nfrom typing import TypeVar\nfrom typing import cast\nfrom typing import overload\n\nfrom typing_extensions import Self\n\n\nT = TypeVar(\"T\")\nT_co = TypeVar(\"T_co\", covariant=True)\n\n\nclass _AbstractOrderedSet(AbstractSet[T], Sequence[T]):  # noqa: PLW1641\n    \"\"\"Common functionality shared between OrderedSet and FrozenOrderedSet.\"\"\"\n\n    @overload\n    def __getitem__(self, index: int) -> T:\n        pass\n\n    @overload\n    def __getitem__(self, index: slice) -> _AbstractOrderedSet[T]:\n        pass\n\n    def __getitem__(self, index: int | slice) -> T:  # type: ignore[misc]\n        \"\"\"Lookup item at given position. Caution, as this runs in O(n).\n\n        Args:\n            index: The index whose value we want to retrieve.\n\n        Returns:\n            The value of the given index.\n\n        Raises:\n            NotImplementedError: When given a slice.\n            IndexError: When the index is out of range.\n        \"\"\"\n        if isinstance(index, slice):\n            raise NotImplementedError(\"Slicing currently not supported.\")\n        for i, key in enumerate(self._items.keys()):\n            if i == index:\n                return key\n        raise IndexError(\"Index out of range.\")\n\n    def __init__(self, iterable: Iterable[T] | None = None) -> None:\n        # Using a dictionary, rather than using the recipe's original\n        # `self |= iterable`, results in a ~20% performance increase for the\n        # constructor.\n        #\n        # NB: Dictionaries are ordered in Python 3.6+. While this was not formalized\n        # until Python 3.7, Python 3.6 uses this behavior; Pants requires CPython 3.6+\n        # to run, so this assumption is safe for us to rely on.\n        self._items: dict[T, None] = dict.fromkeys(iterable or ())\n\n    def __len__(self) -> int:\n        return len(self._items)\n\n    def __copy__(self) -> Self:\n        return self.__class__(self)\n\n    def __contains__(self, key: Any) -> bool:\n        return key in self._items\n\n    def __iter__(self) -> Iterator[T]:\n        return iter(self._items)\n\n    def __reversed__(self) -> Iterator[T]:\n        return reversed(tuple(self._items.keys()))\n\n    def __repr__(self) -> str:\n        name = self.__class__.__name__\n        if not self:\n            return f\"{name}()\"\n        return f\"{name}({list(self)!r})\"\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return len(self._items) == len(other._items) and all(\n            x == y for x, y in zip(self._items, other._items, strict=True)\n        )\n\n    def __or__(self, other: Iterable[T]) -> Self:  # type: ignore[override]\n        return self.union(other)\n\n    def union(self, *others: Iterable[T]) -> Self:\n        \"\"\"Combines all unique items.\n\n        Each item's order is defined by its first appearance # noqa: DAR201,DAR101.\n\n        Args:\n            *others: the iterables to union with\n\n        Returns:\n            A new ordered set containing the union.\n        \"\"\"\n        # Differences with AbstractSet: our set union forces \"other\" to have the same\n        # type. That is, while AbstractSet allows {1, 2, 3} | {(True, False)} resulting\n        # in set[int | tuple[bool, bool]], the analogous for descendants  of\n        # _TAbstractOrderedSet is not allowed.\n        #\n        # GOTCHA: given _TAbstractOrderedSet[S]:\n        #   if T is a subclass of S => _TAbstractOrderedSet[S] => *appears* to perform\n        #     unification but it doesn't\n        #   if S is a subclass of T => type error (while AbstractSet would resolve to\n        #     AbstractSet[T])\n        merged_iterables = itertools.chain([cast(\"Iterable[T]\", self)], others)\n        return self.__class__(itertools.chain.from_iterable(merged_iterables))\n\n    def __and__(self, other: Iterable[T]) -> Self:\n        # The parent class's implementation of this is backwards.\n        return self.intersection(other)\n\n    def intersection(self, *others: Iterable[T]) -> Self:\n        \"\"\"Returns elements in common between all sets.\n\n        Order is defined only by the first set.\n\n        Args:\n            *others: The iterable to intersect with\n\n        Returns:\n            A new set containing the intersection.\n        \"\"\"\n        cls = self.__class__\n        if not others:\n            return cls(self)\n        common = set.intersection(*(set(other) for other in others))\n        return cls(item for item in self if item in common)\n\n    def difference(self, *others: Iterable[T]) -> Self:\n        \"\"\"Returns all elements that are in this set but not the others.\n\n        Args:\n            *others: The iterable to intersect with\n\n        Returns:\n            A new set containing the difference.\n        \"\"\"\n        cls = self.__class__\n        if not others:\n            return cls(self)\n        other = set.union(*(set(other) for other in others))\n        return cls(item for item in self if item not in other)\n\n    def issubset(self, other: Iterable[T]) -> bool:\n        \"\"\"Report whether another set contains this set.\n\n        Args:\n            other: The set to check\n\n        Returns:\n            True, if this is a subset of other.\n        \"\"\"\n        try:\n            # Fast check for obvious cases\n            if len(self) > len(other):  # type: ignore[arg-type]\n                return False\n        except TypeError:\n            pass\n        return all(item in other for item in self)\n\n    def issuperset(self, other: Iterable[T]) -> bool:\n        \"\"\"Report whether this set contains another set.\n\n        Args:\n            other: The set to check\n\n        Returns:\n            True, if this is a superset of other.\n        \"\"\"\n        try:\n            # Fast check for obvious cases\n            if len(self) < len(other):  # type: ignore[arg-type]\n                return False\n        except TypeError:\n            pass\n        return all(item in self for item in other)\n\n    def __xor__(self, other: Iterable[T]) -> Self:  # type: ignore[override]\n        return self.symmetric_difference(other)\n\n    def symmetric_difference(self, other: Iterable[T]) -> Self:\n        \"\"\"Computes the symmetric difference.\n\n        Return the symmetric difference of this OrderedSet and another set as a new\n        OrderedSet. That is, the new set will contain all elements that are in exactly\n        one of the sets. Their order will be preserved, with elements from `self`\n        preceding elements from `other`.\n\n        Args:\n            other: The other set\n\n        Returns:\n            The symmetric difference.\n        \"\"\"\n        cls = self.__class__\n        diff1 = cls(self).difference(other)\n        diff2 = cls(other).difference(self)\n        return diff1.union(diff2)\n\n\nclass OrderedSet(_AbstractOrderedSet[T], MutableSet[T]):\n    \"\"\"A mutable set that retains its order.\n\n    This is not safe to use with the V2 engine.\n    \"\"\"\n\n    def add(self, value: T) -> None:\n        \"\"\"Add `value` as an item to this OrderedSet.\n\n        Args:\n            value: the object to add\n        \"\"\"\n        self._items[value] = None\n\n    def update(self, iterable: Iterable[T]) -> None:\n        \"\"\"Update the set with the given iterable sequence.\n\n        Args:\n            iterable: The iterable to insert into this set\n        \"\"\"\n        for item in iterable:\n            self._items[item] = None\n\n    def discard(self, value: T) -> None:  # noqa: D102\n        self._items.pop(value, None)\n\n    def clear(self) -> None:  # noqa: D102\n        self._items.clear()\n\n    def difference_update(self, *others: Iterable[T]) -> None:\n        \"\"\"Update this OrderedSet to remove items from one or more other sets.\n\n        Args:\n            *others: The sets to difference update with.\n        \"\"\"\n        items_to_remove: set[T] = set()\n        for other in others:\n            items_as_set = set(other)\n            items_to_remove |= items_as_set\n        self._items = {item: None for item in self._items if item not in items_to_remove}\n\n    def intersection_update(self, other: Iterable[T]) -> None:\n        \"\"\"Computes the intersection inplace.\n\n        Update this OrderedSet to keep only items in another set, preserving their\n        order in this set.\n\n        Args:\n            other: The set to intersection update with.\n        \"\"\"\n        other = set(other)\n        self._items = {item: None for item in self._items if item in other}\n\n    def symmetric_difference_update(self, other: Iterable[T]) -> None:\n        \"\"\"Computes the symmetric difference inplace.\n\n        Update this OrderedSet to remove items from another set, then add items from\n        the other set that were not present in this set.\n\n        Args:\n            other: The other set.\n        \"\"\"\n        items_to_add = [item for item in other if item not in self]\n        items_to_remove = cast(\"set[T]\", set(other))\n        self._items = {item: None for item in self._items if item not in items_to_remove}\n        for item in items_to_add:\n            self._items[item] = None\n\n\nclass FrozenOrderedSet(_AbstractOrderedSet[T_co], Hashable):  # type: ignore[type-var]\n    \"\"\"A frozen (i.e. immutable) set that retains its order.\n\n    This is safe to use with the V2 engine.\n    \"\"\"\n\n    def __init__(self, iterable: Iterable[T_co] | None = None) -> None:  # noqa: D107\n        super().__init__(iterable)\n        self.__hash: int | None = None\n\n    def __hash__(self) -> int:\n        if self.__hash is None:\n            self.__hash = 0\n            for item in self._items:\n                self.__hash ^= hash(item)\n        return self.__hash\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:32"}, {"role": "assistant", "content": "```python\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\nimport pytest\n\nfrom bytecode import Bytecode\n\nfrom pynguin.analyses.controlflow import CFG\nfrom tests.fixtures.programgraph.whileloop import Foo\n\n\ndef test_integration_create_cfg(conditional_jump_example_bytecode):\n    cfg = CFG.from_bytecode(conditional_jump_example_bytecode)\n    dot_representation = cfg.dot\n    graph = \"\"\"strict digraph  {\n\"ProgramGraphNode(0)\nLOAD_NAME 'print'\nLOAD_NAME 'test'\nPOP_JUMP_IF_FALSE ProgramGraphNode\";\n\"ProgramGraphNode(1)\nLOAD_CONST 'yes'\nJUMP_FORWARD ProgramGraphNode\";\n\"ProgramGraphNode(2)\nLOAD_CONST 'no'\";\n\"ProgramGraphNode(3)\nCALL_FUNCTION 1\nLOAD_CONST None\nRETURN_VALUE\";\n\"ProgramGraphNode(9223372036854775807)\";\n\"ProgramGraphNode(-1)\";\n\"ProgramGraphNode(0)\nLOAD_NAME 'print'\nLOAD_NAME 'test'\nPOP_JUMP_IF_FALSE ProgramGraphNode\" -> \"ProgramGraphNode(1)\nLOAD_CONST 'yes'\nJUMP_FORWARD ProgramGraphNode\"  [branch_value=True, label=True];\n\"ProgramGraphNode(0)\nLOAD_NAME 'print'\nLOAD_NAME 'test'\nPOP_JUMP_IF_FALSE ProgramGraphNode\" -> \"ProgramGraphNode(2)\nLOAD_CONST 'no'\"  [branch_value=False, label=False];\n\"ProgramGraphNode(1)\nLOAD_CONST 'yes'\nJUMP_FORWARD ProgramGraphNode\" -> \"ProgramGraphNode(3)\nCALL_FUNCTION 1\nLOAD_CONST None\nRETURN_VALUE\";\n\"ProgramGraphNode(2)\nLOAD_CONST 'no'\" -> \"ProgramGraphNode(3)\nCALL_FUNCTION 1\nLOAD_CONST None\nRETURN_VALUE\";\n\"ProgramGraphNode(3)\nCALL_FUNCTION 1\nLOAD_CONST None\nRETURN_VALUE\" -> \"ProgramGraphNode(9223372036854775807)\";\n\"ProgramGraphNode(-1)\" -> \"ProgramGraphNode(0)\nLOAD_NAME 'print'\nLOAD_NAME 'test'\nPOP_JUMP_IF_FALSE ProgramGraphNode\";\n}\"\"\"\n    assert cfg.cyclomatic_complexity == 2\n    assert cfg.diameter == 6\n    assert cfg.entry_node.is_artificial\n    assert len(cfg.exit_nodes) == 1\n    # Stupid string encoding >:[\n    assert bytes(dot_representation, \"utf-8\").decode(\"unicode_escape\") == bytes(\n        graph, \"utf-8\"\n    ).decode(\"unicode_escape\")\n\n\ndef test_integration_reverse_cfg(conditional_jump_example_bytecode):\n    cfg = CFG.from_bytecode(conditional_jump_example_bytecode)\n    reversed_cfg = cfg.reversed()\n    dot_representation = reversed_cfg.dot\n    graph = \"\"\"strict digraph  {\n\"ProgramGraphNode(0)\nLOAD_NAME 'print'\nLOAD_NAME 'test'\nPOP_JUMP_IF_FALSE ProgramGraphNode\";\n\"ProgramGraphNode(1)\nLOAD_CONST 'yes'\nJUMP_FORWARD ProgramGraphNode\";\n\"ProgramGraphNode(2)\nLOAD_CONST 'no'\";\n\"ProgramGraphNode(3)\nCALL_FUNCTION 1\nLOAD_CONST None\nRETURN_VALUE\";\n\"ProgramGraphNode(9223372036854775807)\";\n\"ProgramGraphNode(-1)\";\n\"ProgramGraphNode(0)\nLOAD_NAME 'print'\nLOAD_NAME 'test'\nPOP_JUMP_IF_FALSE ProgramGraphNode\" -> \"ProgramGraphNode(-1)\";\n\"ProgramGraphNode(1)\nLOAD_CONST 'yes'\nJUMP_FORWARD ProgramGraphNode\" -> \"ProgramGraphNode(0)\nLOAD_NAME 'print'\nLOAD_NAME 'test'\nPOP_JUMP_IF_FALSE ProgramGraphNode\"  [branch_value=True, label=True];\n\"ProgramGraphNode(2)\nLOAD_CONST 'no'\" -> \"ProgramGraphNode(0)\nLOAD_NAME 'print'\nLOAD_NAME 'test'\nPOP_JUMP_IF_FALSE ProgramGraphNode\"  [branch_value=False, label=False];\n\"ProgramGraphNode(3)\nCALL_FUNCTION 1\nLOAD_CONST None\nRETURN_VALUE\" -> \"ProgramGraphNode(1)\nLOAD_CONST 'yes'\nJUMP_FORWARD ProgramGraphNode\";\n\"ProgramGraphNode(3)\nCALL_FUNCTION 1\nLOAD_CONST None\nRETURN_VALUE\" -> \"ProgramGraphNode(2)\nLOAD_CONST 'no'\";\n\"ProgramGraphNode(9223372036854775807)\" -> \"ProgramGraphNode(3)\nCALL_FUNCTION 1\nLOAD_CONST None\nRETURN_VALUE\";\n}\"\"\"\n    assert reversed_cfg.cyclomatic_complexity == 2\n    assert cfg.diameter == 6\n    assert cfg.entry_node.is_artificial\n    assert len(cfg.exit_nodes) == 1\n    assert bytes(dot_representation, \"utf-8\").decode(\"unicode_escape\") == bytes(\n        graph, \"utf-8\"\n    ).decode(\"unicode_escape\")\n\n\ndef control_flow_labelling(foo):  # pragma: no cover\n    if foo:\n        print(\"a\")  # noqa: T201\n    elif foo == 42:\n        print(\"bar\")  # noqa: T201\n    for f in foo:\n        print(f)  # noqa: T201\n    if not foo:\n        print(\"foo\")  # noqa: T201\n\n\n@pytest.mark.parametrize(\n    \"expected\",\n    [\n        pytest.param(\n            \"\"\"strict digraph  {\n\"ProgramGraphNode(0)\nLOAD_FAST 'foo'\nPOP_JUMP_IF_FALSE ProgramGraphNode\";\n\"ProgramGraphNode(1)\nLOAD_GLOBAL 'print'\nLOAD_CONST 'a'\nCALL_FUNCTION 1\nPOP_TOP\nJUMP_FORWARD ProgramGraphNode\";\n\"ProgramGraphNode(2)\nLOAD_FAST 'foo'\nLOAD_CONST 42\nCOMPARE_OP EQ\nPOP_JUMP_IF_FALSE ProgramGraphNode\";\n\"ProgramGraphNode(3)\nLOAD_GLOBAL 'print'\nLOAD_CONST 'bar'\nCALL_FUNCTION 1\nPOP_TOP\";\n\"ProgramGraphNode(4)\nLOAD_FAST 'foo'\nGET_ITER\";\n\"ProgramGraphNode(5)\nFOR_ITER ProgramGraphNode\";\n\"ProgramGraphNode(6)\nSTORE_FAST 'f'\nLOAD_GLOBAL 'print'\nLOAD_FAST 'f'\nCALL_FUNCTION 1\nPOP_TOP\nJUMP_ABSOLUTE ProgramGraphNode\";\n\"ProgramGraphNode(7)\nLOAD_FAST 'foo'\nPOP_JUMP_IF_TRUE ProgramGraphNode\";\n\"ProgramGraphNode(8)\nLOAD_GLOBAL 'print'\nLOAD_CONST 'foo'\nCALL_FUNCTION 1\nPOP_TOP\nLOAD_CONST None\nRETURN_VALUE\";\n\"ProgramGraphNode(9)\nLOAD_CONST None\nRETURN_VALUE\";\n\"ProgramGraphNode(9223372036854775807)\";\n\"ProgramGraphNode(-1)\";\n\"ProgramGraphNode(0)\nLOAD_FAST 'foo'\nPOP_JUMP_IF_FALSE ProgramGraphNode\" -> \"ProgramGraphNode(1)\nLOAD_GLOBAL 'print'\nLOAD_CONST 'a'\nCALL_FUNCTION 1\nPOP_TOP\nJUMP_FORWARD ProgramGraphNode\"  [branch_value=True, label=True];\n\"ProgramGraphNode(0)\nLOAD_FAST 'foo'\nPOP_JUMP_IF_FALSE ProgramGraphNode\" -> \"ProgramGraphNode(2)\nLOAD_FAST 'foo'\nLOAD_CONST 42\nCOMPARE_OP EQ\nPOP_JUMP_IF_FALSE ProgramGraphNode\"  [branch_value=False, label=False];\n\"ProgramGraphNode(1)\nLOAD_GLOBAL 'print'\nLOAD_CONST 'a'\nCALL_FUNCTION 1\nPOP_TOP\nJUMP_FORWARD ProgramGraphNode\" -> \"ProgramGraphNode(4)\nLOAD_FAST 'foo'\nGET_ITER\";\n\"ProgramGraphNode(2)\nLOAD_FAST 'foo'\nLOAD_CONST 42\nCOMPARE_OP EQ\nPOP_JUMP_IF_FALSE ProgramGraphNode\" -> \"ProgramGraphNode(3)\nLOAD_GLOBAL 'print'\nLOAD_CONST 'bar'\nCALL_FUNCTION 1\nPOP_TOP\"  [branch_value=True, label=True];\n\"ProgramGraphNode(2)\nLOAD_FAST 'foo'\nLOAD_CONST 42\nCOMPARE_OP EQ\nPOP_JUMP_IF_FALSE ProgramGraphNode\" -> \"ProgramGraphNode(4)\nLOAD_FAST 'foo'\nGET_ITER\"  [branch_value=False, label=False];\n\"ProgramGraphNode(3)\nLOAD_GLOBAL 'print'\nLOAD_CONST 'bar'\nCALL_FUNCTION 1\nPOP_TOP\" -> \"ProgramGraphNode(4)\nLOAD_FAST 'foo'\nGET_ITER\";\n\"ProgramGraphNode(4)\nLOAD_FAST 'foo'\nGET_ITER\" -> \"ProgramGraphNode(5)\nFOR_ITER ProgramGraphNode\";\n\"ProgramGraphNode(5)\nFOR_ITER ProgramGraphNode\" -> \"ProgramGraphNode(6)\nSTORE_FAST 'f'\nLOAD_GLOBAL 'print'\nLOAD_FAST 'f'\nCALL_FUNCTION 1\nPOP_TOP\nJUMP_ABSOLUTE ProgramGraphNode\"  [branch_value=True, label=True];\n\"ProgramGraphNode(5)\nFOR_ITER ProgramGraphNode\" -> \"ProgramGraphNode(7)\nLOAD_FAST 'foo'\nPOP_JUMP_IF_TRUE ProgramGraphNode\"  [branch_value=False, label=False];\n\"ProgramGraphNode(6)\nSTORE_FAST 'f'\nLOAD_GLOBAL 'print'\nLOAD_FAST 'f'\nCALL_FUNCTION 1\nPOP_TOP\nJUMP_ABSOLUTE ProgramGraphNode\" -> \"ProgramGraphNode(5)\nFOR_ITER ProgramGraphNode\";\n\"ProgramGraphNode(7)\nLOAD_FAST 'foo'\nPOP_JUMP_IF_TRUE ProgramGraphNode\" -> \"ProgramGraphNode(9)\nLOAD_CONST None\nRETURN_VALUE\"  [branch_value=True, label=True];\n\"ProgramGraphNode(7)\nLOAD_FAST 'foo'\nPOP_JUMP_IF_TRUE ProgramGraphNode\" -> \"ProgramGraphNode(8)\nLOAD_GLOBAL 'print'\nLOAD_CONST 'foo'\nCALL_FUNCTION 1\nPOP_TOP\nLOAD_CONST None\nRETURN_VALUE\"  [branch_value=False, label=False];\n\"ProgramGraphNode(8)\nLOAD_GLOBAL 'print'\nLOAD_CONST 'foo'\nCALL_FUNCTION 1\nPOP_TOP\nLOAD_CONST None\nRETURN_VALUE\" -> \"ProgramGraphNode(9223372036854775807)\";\n\"ProgramGraphNode(9)\nLOAD_CONST None\nRETURN_VALUE\" -> \"ProgramGraphNode(9223372036854775807)\";\n\"ProgramGraphNode(-1)\" -> \"ProgramGraphNode(0)\nLOAD_FAST 'foo'\nPOP_JUMP_IF_FALSE ProgramGraphNode\";\n}\"\"\",\n            id=\"Python 3.10+, extract return None into separate node\",\n        ),\n    ],\n)\ndef test_all_control_flow(expected):\n    cfg = CFG.from_bytecode(Bytecode.from_code(control_flow_labelling.__code__))\n    assert bytes(cfg.dot, \"utf-8\").decode(\"unicode_escape\") == bytes(expected, \"utf-8\").decode(\n        \"unicode_escape\"\n    )\n\n\ndef test_integration_copy_cfg(conditional_jump_example_bytecode):\n    cfg = CFG.from_bytecode(conditional_jump_example_bytecode)\n    copied_cfg = cfg.copy()\n    assert copied_cfg.dot == cfg.dot\n\n\ndef test_integration_no_exit():\n    with pytest.raises(AssertionError):\n        CFG.from_bytecode(Bytecode.from_code(Foo.receive.__code__))\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pynguin\nTest File Path: pynguin\\test_chromosome\\test_chromosome.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides an abstract base class for chromosomes.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC\nfrom abc import abstractmethod\n\nimport pynguin.ga.chromosomevisitor as cv\nimport pynguin.ga.computations as ff\n\n\nclass Chromosome(ABC):\n    \"\"\"An abstract base class for chromosomes.\"\"\"\n\n    def __init__(self, orig: Chromosome | None = None):\n        \"\"\"Initializes the chromosome.\n\n        If a chromosome is given, we clone from that.\n\n        Args:\n            orig: Original, if we clone an existing chromosome.\n        \"\"\"\n        if orig is None:\n            self.computation_cache = ff.ComputationCache(self)\n            self.changed: bool = True\n            self.distance: float = -1\n            self.rank: int = -1\n        else:\n            self.computation_cache = orig.computation_cache.clone(self)\n            self.changed = orig.changed\n            self.distance = orig.distance\n            self.rank = orig.rank\n\n    @abstractmethod\n    def size(self) -> int:\n        \"\"\"Return the size of an individual.\n\n        This should be number of elements it contains.\n\n        Returns:\n            The size of an individual  # noqa: DAR202\n        \"\"\"\n\n    @abstractmethod\n    def length(self) -> int:\n        \"\"\"Provide the length of an individual.\n\n        This should be the total length of all contained elements and possible\n        sub-elements. Look at the implementation to see the difference to size().\n\n        Returns: The length of this individual.\n        \"\"\"\n\n    def get_fitness_functions(self) -> list[ff.FitnessFunction]:\n        \"\"\"Provide the currently configured fitness functions of this chromosome.\n\n        Returns:\n            The list of currently configured fitness functions\n        \"\"\"\n        return self.computation_cache.get_fitness_functions()\n\n    def add_fitness_function(\n        self,\n        fitness_function: ff.FitnessFunction,\n    ) -> None:\n        \"\"\"Adds a fitness function.\n\n        Args:\n            fitness_function: A fitness function\n        \"\"\"\n        self.computation_cache.add_fitness_function(fitness_function)\n\n    def get_coverage_functions(self) -> list[ff.CoverageFunction]:\n        \"\"\"Provide the currently configured coverage functions of this chromosome.\n\n        Returns:\n            The list of currently configured coverage functions.\n        \"\"\"\n        return self.computation_cache.get_coverage_functions()\n\n    def add_coverage_function(\n        self,\n        coverage_function: ff.CoverageFunction,\n    ) -> None:\n        \"\"\"Adds a coverage function.\n\n        Args:\n            coverage_function: A fitness function\n        \"\"\"\n        self.computation_cache.add_coverage_function(coverage_function)\n\n    def invalidate_cache(self) -> None:\n        \"\"\"Invalidate all cached computation values.\"\"\"\n        self.computation_cache.invalidate_cache()\n\n    def get_fitness(self) -> float:\n        \"\"\"Provide a sum of the current fitness values.\n\n        Returns:\n            The sum of the current fitness values\n        \"\"\"\n        return self.computation_cache.get_fitness()\n\n    def get_fitness_for(self, fitness_function: ff.FitnessFunction) -> float:\n        \"\"\"Returns the fitness values of a specific fitness function.\n\n        Args:\n            fitness_function: The fitness function\n\n        Returns:\n            Its fitness value\n        \"\"\"\n        return self.computation_cache.get_fitness_for(fitness_function)\n\n    def get_is_covered(self, fitness_function: ff.FitnessFunction) -> bool:\n        \"\"\"Check if the individual covers this fitness function.\n\n        Args:\n            fitness_function: The fitness function to check\n\n        Returns:\n            True, iff the individual covers the fitness function.\n        \"\"\"\n        return self.computation_cache.get_is_covered(fitness_function)\n\n    def get_coverage(self) -> float:\n        \"\"\"Provides the mean coverage value.\n\n        Returns:\n            The mean coverage value\n        \"\"\"\n        return self.computation_cache.get_coverage()\n\n    def get_coverage_for(self, coverage_function: ff.CoverageFunction) -> float:\n        \"\"\"Provides the coverage value for a certain coverage function.\n\n        Args:\n            coverage_function: The fitness function whose coverage value shall be\n                returned\n\n        Returns:\n            The coverage value for the fitness function\n        \"\"\"\n        return self.computation_cache.get_coverage_for(coverage_function)\n\n    @abstractmethod\n    def cross_over(self, other: Chromosome, position1: int, position2: int) -> None:\n        \"\"\"Single point cross over.\n\n        This chromosome will be split at `position1`, the other at `position2`,\n        and the crossover will be performed with these pre- and suffixes.\n\n        Args:\n            other: The other chromosome to perform the crossover with\n            position1: The point in the first chromosome\n            position2: The point in the second chromosome\n        \"\"\"\n\n    @abstractmethod\n    def mutate(self) -> None:\n        \"\"\"Mutate this chromosome.\"\"\"\n\n    @abstractmethod\n    def clone(self) -> Chromosome:\n        \"\"\"Create a clone of this chromosome.\n\n        Returns:\n            The cloned chromosome  # noqa: DAR202\n        \"\"\"\n\n    @abstractmethod\n    def accept(self, visitor: cv.ChromosomeVisitor) -> None:\n        \"\"\"Accept a chromosome visitor.\n\n        Args:\n            visitor: the visitor that is accepted.\n        \"\"\"\n\n    @abstractmethod\n    def __eq__(self, other):\n        pass\n\n    @abstractmethod\n    def __hash__(self):\n        pass\n\n### Source File Dependency Files Content\n### Dependency File: chromosomevisitor.py\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides a chromosome visitor.\"\"\"\n\nfrom abc import ABC\nfrom abc import abstractmethod\n\n\nclass ChromosomeVisitor(ABC):\n    \"\"\"An abstract chromosome visitor.\"\"\"\n\n    @abstractmethod\n    def visit_test_suite_chromosome(self, chromosome) -> None:\n        \"\"\"Visit a test suite chromosome.\n\n        Args:\n            chromosome: The test suite chromosome\n        \"\"\"\n\n    @abstractmethod\n    def visit_test_case_chromosome(self, chromosome) -> None:\n        \"\"\"Visit a test suite chromosome.\n\n        Args:\n            chromosome: The test case chromosome\n        \"\"\"\n\n\n### Dependency File: computations.py\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides classes for computations on chromosomes, e.g., fitness and coverage.\"\"\"\n\nfrom __future__ import annotations\n\nimport abc\nimport dataclasses\nimport math\nimport statistics\n\nfrom abc import abstractmethod\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import TypeVar\n\nimport pynguin.utils.opcodes as op\n\nfrom pynguin.slicer.dynamicslicer import AssertionSlicer\nfrom pynguin.slicer.dynamicslicer import DynamicSlicer\nfrom pynguin.testcase.execution import ExecutionTrace\n\n\nif TYPE_CHECKING:\n    from collections.abc import Callable\n\n    from pynguin.slicer.dynamicslicer import SlicingCriterion\n    from pynguin.testcase.execution import AbstractTestCaseExecutor\n    from pynguin.testcase.execution import ExecutionResult\n    from pynguin.testcase.execution import SubjectProperties\n    from pynguin.testcase.statement import Statement\n\n\n@dataclasses.dataclass(eq=False)\nclass ChromosomeComputation(abc.ABC):\n    \"\"\"An abstract computation on chromosomes.\"\"\"\n\n    _executor: AbstractTestCaseExecutor\n    \"\"\"Executor that will be used by the computation to execute chromosomes.\"\"\"\n\n\nclass TestCaseChromosomeComputation(ChromosomeComputation, abc.ABC):\n    \"\"\"A function that computes something on a test case chromosome.\"\"\"\n\n    def _run_test_case_chromosome(self, individual) -> ExecutionResult:\n        \"\"\"Runs a test suite and updates the execution results.\n\n        Updates all test cases that were changed.\n\n        Args:\n            individual: The individual to run\n\n        Returns:\n            A list of execution results\n        \"\"\"\n        if individual.changed or individual.get_last_execution_result() is None:\n            individual.set_last_execution_result(self._executor.execute(individual.test_case))\n            individual.changed = False\n        result = individual.get_last_execution_result()\n        assert result is not None\n        return result\n\n\nclass TestSuiteChromosomeComputation(ChromosomeComputation, abc.ABC):\n    \"\"\"A function that computes something on a test suite chromosome.\"\"\"\n\n    def _run_test_suite_chromosome(self, individual) -> list[ExecutionResult]:\n        \"\"\"Runs a test suite and updates the execution results.\n\n        Updates all test cases that were changed.\n\n        Args:\n            individual: The individual to run\n\n        Returns:\n            A list of execution results\n        \"\"\"\n        results: list[ExecutionResult] = []\n        for test_case_chromosome in individual.test_case_chromosomes:\n            if (\n                test_case_chromosome.changed\n                or test_case_chromosome.get_last_execution_result() is None\n            ):\n                test_case_chromosome.set_last_execution_result(\n                    self._executor.execute(test_case_chromosome.test_case)\n                )\n                test_case_chromosome.changed = False\n                # If we execute a suite which in turn executes it's test cases,\n                # then we have to invalidate the values of the test cases, because\n                # the test case is no longer aware that it was changed.\n                test_case_chromosome.invalidate_cache()\n            result = test_case_chromosome.get_last_execution_result()\n            assert result is not None\n            results.append(result)\n        return results\n\n\nclass FitnessFunction:\n    \"\"\"Interface for a fitness function.\"\"\"\n\n    @abstractmethod\n    def compute_fitness(self, individual) -> float:\n        \"\"\"Calculate the fitness value.\n\n        Args:\n            individual: the chromosome to compute the fitness for.\n\n        Returns:\n            the new fitness  # noqa: DAR202\n        \"\"\"\n\n    @abstractmethod\n    def compute_is_covered(self, individual) -> bool:\n        \"\"\"Compute if the goal of this fitness function is covered.\n\n        This computation is usually cheaper than computing the fitness, because\n        we are not interested in the distance, but only a boolean result.\n\n        Args:\n            individual: the chromosome to check coverage on.\n\n        Returns:\n            True, if the goal of this fitness function is covered.\n        \"\"\"\n\n    @abstractmethod\n    def is_maximisation_function(self) -> bool:\n        \"\"\"Do we need to maximise or minimise this function?\n\n        Returns:\n             Whether or not this is a maximisation function  # noqa: DAR202\n        \"\"\"\n\n\nclass TestCaseFitnessFunction(TestCaseChromosomeComputation, FitnessFunction, abc.ABC):\n    \"\"\"Base class for test case fitness functions.\"\"\"\n\n    def __init__(self, executor, code_object_id: int):  # noqa: D107\n        super().__init__(executor)\n        self._code_object_id = code_object_id\n\n    @property\n    def code_object_id(self) -> int:\n        \"\"\"The code object id, where the target of the fitness function is located.\n\n        Returns:\n            The code object id where the target of the fitness function is located.\n        \"\"\"\n        return self._code_object_id\n\n\nclass BranchDistanceTestCaseFitnessFunction(TestCaseFitnessFunction):\n    \"\"\"A fitness function based on branch distances and entered code objects.\"\"\"\n\n    def compute_fitness(self, individual) -> float:  # noqa: D102\n        result = self._run_test_case_chromosome(individual)\n        merged_trace = analyze_results([result])\n        tracer = self._executor.tracer\n\n        return compute_branch_distance_fitness(merged_trace, tracer.get_subject_properties())\n\n    def compute_is_covered(self, individual) -> bool:  # noqa: D102\n        result = self._run_test_case_chromosome(individual)\n        merged_trace = analyze_results([result])\n        tracer = self._executor.tracer\n\n        return compute_branch_distance_fitness_is_covered(\n            merged_trace, tracer.get_subject_properties()\n        )\n\n    def is_maximisation_function(self) -> bool:  # noqa: D102\n        return False\n\n\nclass TestSuiteFitnessFunction(TestSuiteChromosomeComputation, FitnessFunction, abc.ABC):\n    \"\"\"Base class for test suite fitness functions.\"\"\"\n\n\nclass BranchDistanceTestSuiteFitnessFunction(TestSuiteFitnessFunction):\n    \"\"\"A fitness function based on branch distances and entered code objects.\"\"\"\n\n    def __init__(self, executor):  # noqa: D107\n        super().__init__(executor)\n        self._excluded_code_objects: set[int] = set()\n        self._excluded_true_predicates: set[int] = set()\n        self._excluded_false_predicates: set[int] = set()\n\n    def restrict(\n        self, exclude_code: set[int], exclude_true: set[int], exclude_false: set[int]\n    ) -> None:\n        \"\"\"Restrict this fitness function.\n\n        Restricts the fitness function with respect to the branches/code objects it\n        considers.\n\n        Args:\n            exclude_code: Ids of the code objects that should not be considered.\n            exclude_true: Ids of predicates whose True branch should not be considered.\n            exclude_false: Ids of predicates whose False branch should not be\n                considered.\n        \"\"\"\n        self._excluded_code_objects.update(exclude_code)\n        self._excluded_true_predicates.update(exclude_true)\n        self._excluded_false_predicates.update(exclude_false)\n\n    def compute_fitness(self, individual) -> float:  # noqa: D102\n        results = self._run_test_suite_chromosome(individual)\n        merged_trace = analyze_results(results)\n        tracer = self._executor.tracer\n\n        return compute_branch_distance_fitness(\n            merged_trace,\n            tracer.get_subject_properties(),\n            self._excluded_code_objects,\n            self._excluded_true_predicates,\n            self._excluded_false_predicates,\n        )\n\n    def compute_is_covered(self, individual) -> bool:  # noqa: D102\n        results = self._run_test_suite_chromosome(individual)\n        merged_trace = analyze_results(results)\n        tracer = self._executor.tracer\n\n        return compute_branch_distance_fitness_is_covered(\n            merged_trace,\n            tracer.get_subject_properties(),\n            self._excluded_code_objects,\n            self._excluded_true_predicates,\n            self._excluded_false_predicates,\n        )\n\n    def is_maximisation_function(self) -> bool:  # noqa: D102\n        return False\n\n\nclass LineTestSuiteFitnessFunction(TestSuiteFitnessFunction):\n    \"\"\"A fitness function based on lines covered and entered code objects.\"\"\"\n\n    def compute_fitness(self, individual) -> float:  # noqa: D102\n        results = self._run_test_suite_chromosome(individual)\n        merged_trace = analyze_results(results)\n        tracer = self._executor.tracer\n        existing_lines = tracer.get_subject_properties().existing_lines\n        return len(existing_lines) - len(merged_trace.covered_line_ids)\n\n    def compute_is_covered(self, individual) -> bool:  # noqa: D102\n        results = self._run_test_suite_chromosome(individual)\n        merged_trace = analyze_results(results)\n        tracer = self._executor.tracer\n\n        return compute_line_coverage_fitness_is_covered(\n            merged_trace,\n            tracer.get_subject_properties(),\n        )\n\n    def is_maximisation_function(self) -> bool:  # noqa: D102\n        return False\n\n\nclass StatementCheckedTestSuiteFitnessFunction(TestSuiteFitnessFunction):\n    \"\"\"A fitness function for the checked statement coverage of test suites.\n\n    A fitness function based on lines included in the backward slice of each statement\n    of a test suite.\n    \"\"\"\n\n    def compute_fitness(self, individual) -> float:  # noqa: D102\n        results = self._run_test_suite_chromosome(individual)\n        merged_trace = analyze_results(results)\n        tracer = self._executor.tracer\n\n        return len(tracer.get_subject_properties().existing_lines) - len(merged_trace.checked_lines)\n\n    def compute_is_covered(self, individual) -> bool:  # noqa: D102\n        results = self._run_test_suite_chromosome(individual)\n        merged_trace = analyze_results(results)\n        tracer = self._executor.tracer\n\n        return compute_checked_coverage_statement_fitness_is_covered(\n            merged_trace,\n            tracer.get_subject_properties(),\n        )\n\n    def is_maximisation_function(self) -> bool:  # noqa: D102\n        return False\n\n\nclass CoverageFunction:\n    \"\"\"Interface for a coverage function.\"\"\"\n\n    @abstractmethod\n    def compute_coverage(self, individual) -> float:\n        \"\"\"Compute the coverage of the given individual.\n\n        Args:\n            individual: the chromosome to compute the coverage for.\n\n        Returns:\n            The computed coverage.\n        \"\"\"\n\n\nclass TestSuiteCoverageFunction(TestSuiteChromosomeComputation, CoverageFunction, abc.ABC):\n    \"\"\"Base class for all coverage functions that act on test suite level.\"\"\"\n\n\nclass TestCaseCoverageFunction(TestCaseChromosomeComputation, CoverageFunction, abc.ABC):\n    \"\"\"Base class for all coverage functions that act on test case level.\"\"\"\n\n\nclass TestSuiteBranchCoverageFunction(TestSuiteCoverageFunction):\n    \"\"\"Computes branch coverage on test suites.\"\"\"\n\n    def compute_coverage(self, individual) -> float:  # noqa: D102\n        results = self._run_test_suite_chromosome(individual)\n        merged_trace = analyze_results(results)\n        tracer = self._executor.tracer\n        return compute_branch_coverage(merged_trace, tracer.get_subject_properties())\n\n\nclass TestCaseBranchCoverageFunction(TestCaseCoverageFunction):\n    \"\"\"Computes branch coverage on test cases.\"\"\"\n\n    def compute_coverage(self, individual) -> float:  # noqa: D102\n        result = self._run_test_case_chromosome(individual)\n        merged_trace = analyze_results([result])\n        tracer = self._executor.tracer\n        return compute_branch_coverage(merged_trace, tracer.get_subject_properties())\n\n\nclass TestSuiteLineCoverageFunction(TestSuiteCoverageFunction):\n    \"\"\"Computes line coverage on test suites.\"\"\"\n\n    def compute_coverage(self, individual) -> float:  # noqa: D102\n        results = self._run_test_suite_chromosome(individual)\n        merged_trace = analyze_results(results)\n        tracer = self._executor.tracer\n        return compute_line_coverage(merged_trace, tracer.get_subject_properties())\n\n\nclass TestCaseLineCoverageFunction(TestCaseCoverageFunction):\n    \"\"\"Computes line coverage on test cases.\"\"\"\n\n    def compute_coverage(self, individual) -> float:  # noqa: D102\n        result = self._run_test_case_chromosome(individual)\n        merged_trace = analyze_results([result])\n        tracer = self._executor.tracer\n        return compute_line_coverage(merged_trace, tracer.get_subject_properties())\n\n\nclass TestSuiteStatementCheckedCoverageFunction(TestSuiteCoverageFunction):\n    \"\"\"Computes checked coverage on the statements of test suites.\"\"\"\n\n    def compute_coverage(self, individual) -> float:  # noqa: D102\n        results = self._run_test_suite_chromosome(individual)\n        merged_trace = analyze_results(results)\n        tracer = self._executor.tracer\n\n        existing = len(tracer.get_subject_properties().existing_lines)\n\n        if existing == 0:\n            # Nothing to cover => everything is covered.\n            coverage = 1.0\n        else:\n            covered = len(merged_trace.checked_lines)\n            coverage = covered / existing\n        assert 0.0 <= coverage <= 1.0, \"Coverage must be in [0,1]\"\n        return coverage\n\n\nclass TestCaseStatementCheckedCoverageFunction(TestCaseCoverageFunction):\n    \"\"\"Computes checked coverage on the statements of test cases.\"\"\"\n\n    def compute_coverage(self, individual) -> float:  # noqa: D102\n        result = self._run_test_case_chromosome(individual)\n        merged_trace = analyze_results([result])\n        tracer = self._executor.tracer\n        existing = len(tracer.get_subject_properties().existing_lines)\n\n        if existing == 0:\n            # Nothing to cover => everything is covered.\n            coverage = 1.0\n        else:\n            covered = len(merged_trace.checked_lines)\n            coverage = covered / existing\n        assert 0.0 <= coverage <= 1.0, \"Coverage must be in [0,1]\"\n        return coverage\n\n\nclass TestSuiteAssertionCheckedCoverageFunction(TestSuiteCoverageFunction):\n    \"\"\"Computes checked coverage on test suites with assertions.\"\"\"\n\n    def compute_coverage(self, individual) -> float:  # noqa: D102\n        results = self._run_test_suite_chromosome(individual)\n        merged_trace = analyze_results(results)\n        tracer = self._executor.tracer\n        return compute_assertion_checked_coverage(merged_trace, tracer.get_subject_properties())\n\n\nclass TestCaseAssertionCheckedCoverageFunction(TestCaseCoverageFunction):\n    \"\"\"Computes checked coverage on test cases with assertions.\"\"\"\n\n    def compute_coverage(self, individual) -> float:  # noqa: D102\n        result = self._run_test_case_chromosome(individual)\n        merged_trace = analyze_results([result])\n        tracer = self._executor.tracer\n        return compute_assertion_checked_coverage(merged_trace, tracer.get_subject_properties())\n\n\nclass ComputationCache:\n    \"\"\"Caches computation results and computes values on demand.\"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        chromosome,\n        *,\n        fitness_functions: list[FitnessFunction] | None = None,\n        coverage_functions: list[CoverageFunction] | None = None,\n        fitness_cache: dict[FitnessFunction, float] | None = None,\n        is_covered_cache: dict[FitnessFunction, bool] | None = None,\n        coverage_cache: dict[CoverageFunction, float] | None = None,\n    ):\n        self._chromosome = chromosome\n        self._fitness_functions = fitness_functions or []\n        self._coverage_functions = coverage_functions or []\n        self._fitness_cache: dict[FitnessFunction, float] = fitness_cache or {}\n        self._is_covered_cache: dict[FitnessFunction, bool] = is_covered_cache or {}\n        self._coverage_cache: dict[CoverageFunction, float] = coverage_cache or {}\n\n    def clone(self, new_chromosome) -> ComputationCache:\n        \"\"\"Create a deep copy of this cache.\n\n        Args:\n            new_chromosome: The chromosome with which this cache is associated.\n\n        Returns:\n            A deep copy.\n        \"\"\"\n        return ComputationCache(\n            new_chromosome,\n            fitness_functions=list(self._fitness_functions),\n            coverage_functions=list(self._coverage_functions),\n            fitness_cache=dict(self._fitness_cache),\n            is_covered_cache=dict(self._is_covered_cache),\n            coverage_cache=dict(self._coverage_cache),\n        )\n\n    def get_fitness_functions(self) -> list[FitnessFunction]:\n        \"\"\"Provide the currently configured fitness functions of this chromosome.\n\n        Returns:\n            The list of currently configured fitness functions\n        \"\"\"\n        return self._fitness_functions\n\n    def add_fitness_function(\n        self,\n        fitness_function: FitnessFunction,\n    ) -> None:\n        \"\"\"Adds the given fitness function.\n\n        Args:\n            fitness_function: A fitness function\n        \"\"\"\n        assert not fitness_function.is_maximisation_function(), (\n            \"Currently only minimization is supported\"\n        )\n        self._fitness_functions.append(fitness_function)\n\n    def get_coverage_functions(self) -> list[CoverageFunction]:\n        \"\"\"Provide the currently configured coverage functions of this chromosome.\n\n        Returns:\n            The list of currently configured coverage functions.\n        \"\"\"\n        return self._coverage_functions\n\n    def add_coverage_function(\n        self,\n        coverage_function: CoverageFunction,\n    ) -> None:\n        \"\"\"Adds a coverage function.\n\n        Args:\n            coverage_function: A fitness function\n        \"\"\"\n        self._coverage_functions.append(coverage_function)\n\n    T = TypeVar(\"T\", CoverageFunction, FitnessFunction)\n\n    def _check_cache(\n        self,\n        comp: Callable[[T | None], None],\n        cache: dict[T, Any],\n        funcs: list[T],\n        only: T | None = None,\n    ) -> None:\n        \"\"\"Check if values need to be computed.\n\n        Args:\n            comp: The function to execute, if values need to be computed.\n            cache: The cache that should be checked.\n            funcs: The functions that are used to fill the respective cache.\n            only: Only compute the values for this function, optional.\n        \"\"\"\n        if self._chromosome.changed:\n            # If the chromosome has changed, we invalidate all values computed so far\n            self.invalidate_cache()\n            # Compute those values in which we are interested.\n            comp(only)\n            # Mark individual as no longer changed.\n            self._chromosome.changed = False\n        elif len(cache) != len(funcs):\n            # The individual has not changed, but not all values are cached.\n            # So we might have to compute the missing ones.\n            comp(only)\n\n    def _compute_fitness(self, only: FitnessFunction | None = None):\n        for fitness_func in self._fitness_functions if only is None else (only,):\n            if fitness_func not in self._fitness_cache:\n                new_value = fitness_func.compute_fitness(self._chromosome)\n                assert (  # noqa: PT018\n                    not math.isnan(new_value) and not math.isinf(new_value) and new_value >= 0\n                ), f\"Invalid fitness value {new_value}\"\n                self._fitness_cache[fitness_func] = new_value\n                # When computing a minimising fitness value, we can also determine\n                # if the goal is covered, i.e. if it is zero.\n                self._is_covered_cache[fitness_func] = new_value == 0.0\n\n    def _compute_is_covered(self, only: FitnessFunction | None = None):\n        for fitness_func in self._fitness_functions if only is None else (only,):\n            if fitness_func not in self._is_covered_cache:\n                new_value = fitness_func.compute_is_covered(self._chromosome)\n                self._is_covered_cache[fitness_func] = new_value\n\n    def _compute_coverage(self, only: CoverageFunction | None = None):\n        for coverage_func in self._coverage_functions if only is None else (only,):\n            if coverage_func not in self._coverage_cache:\n                new_value = coverage_func.compute_coverage(self._chromosome)\n                assert (  # noqa: PT018\n                    not math.isnan(new_value)\n                    and not math.isinf(new_value)\n                    and (0 <= new_value <= 1)\n                ), f\"Invalid coverage value {new_value}\"\n                self._coverage_cache[coverage_func] = new_value\n\n    def invalidate_cache(self) -> None:\n        \"\"\"Invalidate all cached computation values.\"\"\"\n        self._fitness_cache.clear()\n        self._is_covered_cache.clear()\n        self._coverage_cache.clear()\n\n    def get_fitness(self) -> float:\n        \"\"\"Provide a sum of the current fitness values.\n\n        Returns:\n            The sum of the current fitness values\n        \"\"\"\n        self._check_cache(\n            self._compute_fitness,\n            self._fitness_cache,\n            self._fitness_functions,\n        )\n        return sum(self._fitness_cache.values())\n\n    def get_fitness_for(self, fitness_function: FitnessFunction) -> float:\n        \"\"\"Returns the fitness values of a specific fitness function.\n\n        Args:\n            fitness_function: The fitness function\n\n        Returns:\n            Its fitness value\n        \"\"\"\n        self._check_cache(\n            self._compute_fitness,\n            self._fitness_cache,\n            self._fitness_functions,\n            fitness_function,\n        )\n        return self._fitness_cache[fitness_function]\n\n    def get_is_covered(self, fitness_function: FitnessFunction) -> bool:\n        \"\"\"Check if the individual covers this fitness function.\n\n        Args:\n            fitness_function: The fitness function to check\n\n        Returns:\n            True, iff the individual covers the fitness function.\n        \"\"\"\n        self._check_cache(\n            self._compute_is_covered,\n            self._is_covered_cache,\n            self._fitness_functions,\n            fitness_function,\n        )\n        return self._is_covered_cache[fitness_function]\n\n    def get_coverage(self) -> float:\n        \"\"\"Provides the mean coverage value.\n\n        Returns:\n            The mean coverage value\n        \"\"\"\n        self._check_cache(\n            self._compute_coverage,\n            self._coverage_cache,\n            self._coverage_functions,\n        )\n        return statistics.mean(self._coverage_cache.values())\n\n    def get_coverage_for(self, coverage_function: CoverageFunction) -> float:\n        \"\"\"Provides the coverage value for a certain coverage function.\n\n        Args:\n            coverage_function: The fitness function whose coverage value shall be\n                returned\n\n        Returns:\n            The coverage value for the fitness function\n        \"\"\"\n        self._check_cache(\n            self._compute_coverage,\n            self._coverage_cache,\n            self._coverage_functions,\n            coverage_function,\n        )\n        return self._coverage_cache[coverage_function]\n\n\ndef normalise(value: float) -> float:\n    \"\"\"Normalise a value.\n\n    Args:\n        value: The value to normalise\n\n    Returns:\n        The normalised value\n\n    Raises:\n        RuntimeError: if the value is negative\n    \"\"\"\n    if value < 0:\n        raise RuntimeError(\"Values to normalise cannot be negative\")\n    if math.isinf(value):\n        return 1.0\n    return value / (1.0 + value)\n\n\ndef analyze_results(results: list[ExecutionResult]) -> ExecutionTrace:\n    \"\"\"Merge the trace of the given results.\n\n    Args:\n        results: The list of execution results to analyze\n\n    Returns:\n        the merged traces.\n    \"\"\"\n    merged = ExecutionTrace()\n    for result in results:\n        trace = result.execution_trace\n        assert trace is not None\n        merged.merge(trace)\n    return merged\n\n\ndef compute_branch_distance_fitness(\n    trace: ExecutionTrace,\n    subject_properties: SubjectProperties,\n    exclude_code: set[int] | None = None,\n    exclude_true: set[int] | None = None,\n    exclude_false: set[int] | None = None,\n) -> float:\n    \"\"\"Computes fitness based on covered branches and branch distances.\n\n    Args:\n        trace: The execution trace\n        subject_properties: All known data\n        exclude_code: Ids of the code objects that should not be considered.\n        exclude_true: Ids of predicates whose True branch should not be considered.\n        exclude_false: Ids of predicates whose False branch should not be considered.\n\n    Returns:\n        The computed fitness value\n    \"\"\"\n    # Handle None. Cannot use empty set as default, because of mutable default args.\n    exclude_code = set() if exclude_code is None else exclude_code\n\n    # Check if all branch-less code objects were executed.\n    code_objects_missing: float = len(\n        subject_properties.branch_less_code_objects.difference(\n            trace.executed_code_objects, exclude_code\n        )\n    )\n    assert code_objects_missing >= 0.0, \"Amount of non covered code objects cannot be negative\"\n\n    # Handle None for branches.\n    exclude_true = set() if exclude_true is None else exclude_true\n    exclude_false = set() if exclude_false is None else exclude_false\n\n    # Check if all predicates are covered\n    predicate_fitness: float = 0.0\n    for predicate in subject_properties.existing_predicates:\n        if predicate not in exclude_true:\n            predicate_fitness += _predicate_fitness(predicate, trace.true_distances, trace)\n        if predicate not in exclude_false:\n            predicate_fitness += _predicate_fitness(predicate, trace.false_distances, trace)\n\n    assert predicate_fitness >= 0.0, \"Predicate fitness cannot be negative.\"\n    return code_objects_missing + predicate_fitness\n\n\ndef _predicate_fitness(\n    predicate: int, branch_distances: dict[int, float], trace: ExecutionTrace\n) -> float:\n    if predicate in branch_distances and branch_distances[predicate] == 0.0:\n        return 0.0\n    if predicate in trace.executed_predicates and trace.executed_predicates[predicate] >= 2:\n        return normalise(branch_distances[predicate])\n    return 1.0\n\n\ndef compute_branch_distance_fitness_is_covered(\n    trace: ExecutionTrace,\n    subject_properties: SubjectProperties,\n    exclude_code: set[int] | None = None,\n    exclude_true: set[int] | None = None,\n    exclude_false: set[int] | None = None,\n) -> bool:\n    \"\"\"Computes if all branches and code objects have been executed.\n\n    Args:\n        trace: The execution trace\n        subject_properties: All known data\n        exclude_code: Ids of the code objects that should not be considered.\n        exclude_true: Ids of predicates whose True branch should not be considered.\n        exclude_false: Ids of predicates whose False branch should not be considered.\n\n    Returns:\n        True, if all branches were covered\n    \"\"\"\n    # Handle None. Cannot use empty set as default, because of mutable default args.\n    exclude_code = set() if exclude_code is None else exclude_code\n\n    # Check if all branch-less code objects were executed.\n    if (\n        len(\n            subject_properties.branch_less_code_objects.difference(\n                trace.executed_code_objects, exclude_code\n            )\n        )\n        > 0\n    ):\n        return False\n\n    # Handle None for branches.\n    exclude_true = set() if exclude_true is None else exclude_true\n    exclude_false = set() if exclude_false is None else exclude_false\n\n    # Check if all predicates are covered\n    for predicate in subject_properties.existing_predicates:\n        if predicate not in exclude_true and (predicate, 0.0) not in trace.true_distances:\n            return False\n        if predicate not in exclude_false and (predicate, 0.0) not in trace.false_distances:\n            return False\n    return True\n\n\ndef compute_line_coverage_fitness_is_covered(\n    trace: ExecutionTrace, subject_properties: SubjectProperties\n) -> bool:\n    \"\"\"Computes if all lines and code objects have been executed.\n\n    Args:\n        trace: The execution trace\n        subject_properties: All known data\n\n    Returns:\n        True, if all lines were covered, false otherwise\n    \"\"\"\n    return len(trace.covered_line_ids) == len(subject_properties.existing_lines)\n\n\ndef compute_checked_coverage_statement_fitness_is_covered(\n    trace: ExecutionTrace, subject_properties: SubjectProperties\n) -> bool:\n    \"\"\"Computes if all lines and code objects are checked by a return statement.\n\n    Args:\n        trace: The execution trace\n        subject_properties: All known data\n\n    Returns:\n        True, if all lines were checked by a return, false otherwise\n    \"\"\"\n    return len(trace.checked_lines) == len(subject_properties.existing_lines)\n\n\ndef compute_branch_coverage(trace: ExecutionTrace, subject_properties: SubjectProperties) -> float:\n    \"\"\"Computes branch coverage on bytecode instructions.\n\n    The resulting coverage should be equal to decision coverage on source code.\n\n    Args:\n        trace: The execution trace\n        subject_properties: All known data\n\n    Returns:\n        The computed coverage value\n    \"\"\"\n    covered = len(\n        trace.executed_code_objects.intersection(subject_properties.branch_less_code_objects)\n    )\n    existing = len(subject_properties.branch_less_code_objects)\n\n    # Every predicate creates two branches\n    existing += len(subject_properties.existing_predicates) * 2\n\n    # A branch is covered if it has a distance of 0.0\n    # Must consider both branches created by a predicate, i.e. true and false.\n    covered += len([v for v in trace.true_distances.values() if v == 0.0])\n    covered += len([v for v in trace.false_distances.values() if v == 0.0])\n\n    coverage = 1.0 if existing == 0 else covered / existing\n    assert 0.0 <= coverage <= 1.0, \"Coverage must be in [0,1]\"\n    return coverage\n\n\ndef compute_line_coverage(trace: ExecutionTrace, subject_properties: SubjectProperties) -> float:\n    \"\"\"Computes line coverage on bytecode instructions.\n\n    Args:\n        trace: The execution trace\n        subject_properties: All known data\n\n    Returns:\n        The computed coverage value\n    \"\"\"\n    existing = len(subject_properties.existing_lines)\n\n    if existing == 0:\n        # Nothing to cover => everything is covered.\n        coverage = 1.0\n    else:\n        covered = len(trace.covered_line_ids)\n        coverage = covered / existing\n    assert 0.0 <= coverage <= 1.0, \"Coverage must be in [0,1]\"\n    return coverage\n\n\ndef _cleanse_included_implicit_return_none(\n    subject_properties, statement_checked_lines, statement_slice\n):\n    if (  # noqa: SIM102\n        # check if the last included instructions before the store\n        # are a \"return None\"\n        len(statement_slice) >= 3\n        and statement_slice[-3].opcode == op.LOAD_CONST\n        and statement_slice[-3].arg is None\n        and statement_slice[-2].opcode == op.RETURN_VALUE\n    ):\n        if (\n            # check if the \"return None\" is implicit or explicit\n            len(statement_slice) != 3 and statement_slice[-4].lineno != statement_slice[-3].lineno\n        ):\n            statement_checked_lines.remove(\n                DynamicSlicer.get_line_id_by_instruction(statement_slice[-3], subject_properties)\n            )\n\n\ndef compute_statement_checked_lines(\n    statements: list[Statement],\n    trace: ExecutionTrace,\n    subject_properties: SubjectProperties,\n    statement_slicing_criteria: dict[int, SlicingCriterion],\n) -> set[int]:\n    \"\"\"Computes checked coverage on bytecode instructions.\n\n    Each statement can be sliced, returning a list of instructions\n    that are checked by the return value of the statement.\n    If we combine all lists of instructions returned by slicing all statements,\n    we get the combined dynamic slice of the test execution's statements.\n    We then can map all instructions inside the slice to lines\n    that are checked covered of the module under test.\n\n    Args:\n        statements: The sliced instructions\n        trace: The execution trace\n        subject_properties: All known data\n        statement_slicing_criteria: a dictionary of statement positions\n            and its slicing criteria\n\n    Returns:\n        The checked line ids of lines checked by the statements\n    \"\"\"\n    known_code_objects = subject_properties.existing_code_objects\n    dynamic_slicer = DynamicSlicer(known_code_objects)\n    checked_lines_ids = set()\n    for statement in statements:\n        if statement.get_position() not in statement_slicing_criteria:\n            # if there is no slicing criterion there was an exception during\n            # the test case execution and the latter statements after the one\n            # with an exception will never be executed,\n            # thus having no slicing criterion\n            break\n        statement_slice = dynamic_slicer.slice(\n            trace,\n            statement_slicing_criteria[statement.get_position()],\n        )\n        statement_checked_lines = DynamicSlicer.map_instructions_to_lines(\n            statement_slice, subject_properties\n        )\n\n        _cleanse_included_implicit_return_none(\n            subject_properties, statement_checked_lines, statement_slice\n        )\n\n        checked_lines_ids.update(statement_checked_lines)\n    return checked_lines_ids\n\n\ndef compute_assertion_checked_coverage(\n    trace: ExecutionTrace, subject_properties: SubjectProperties\n) -> float:\n    \"\"\"Computes checked coverage on bytecode instructions.\n\n    Each assertion can be sliced, returning a list of instructions\n    that are checked by an assertion.\n    If we combine all lists of instructions returned by slicing all assertions,\n    we get the combined dynamic slice of the test execution's assertions.\n    We then can map all instructions inside the slice to lines\n    that are checked covered of the module under test.\n    To calculate the coverage we can then divide the amount of lines checked\n    covered through the test execution by the lines overall available in the\n    module under test.\n\n    Args:\n        trace: The execution trace\n        subject_properties: All known data\n\n    Returns:\n        The computed coverage value\n    \"\"\"\n    existing = len(subject_properties.existing_lines)\n\n    if existing == 0:\n        # Nothing to cover => everything is covered.\n        coverage = 1.0\n    else:\n        assertion_slicer = AssertionSlicer(subject_properties.existing_code_objects)\n        checked_instructions = []\n        for executed_assertion in trace.executed_assertions:\n            assertion_checked_instructions = assertion_slicer.slice_assertion(\n                executed_assertion, trace\n            )\n            executed_assertion.assertion.checked_instructions.extend(assertion_checked_instructions)\n            # checked at any point by the assertion of a statement\n            checked_instructions.extend(assertion_checked_instructions)\n\n        # reduce coverage to lines instead of instructions\n        checked_lines = DynamicSlicer.map_instructions_to_lines(\n            checked_instructions, subject_properties\n        )\n\n        covered = len(checked_lines)\n        coverage = covered / existing\n    assert 0.0 <= coverage <= 1.0, \"Coverage must be in [0,1]\"\n    return coverage\n\n\ndef compare(fitness_1: float, fitness_2: float) -> int:\n    \"\"\"Compare the two specified values.\n\n    Args:\n        fitness_1: The first value to compare\n        fitness_2: The second value to compare\n\n    Returns:\n        the value 0 if fitness_1 is equal to fitness_2; a value less than 0 if\n        fitness_1 is less than fitness_2; and a value greater than 0 if fitness_1 is\n        greater than fitness_2\n    \"\"\"\n    if fitness_1 < fitness_2:\n        return -1\n    if fitness_1 > fitness_2:\n        return 1\n    return 0\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:32"}, {"role": "assistant", "content": "```python\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\nfrom unittest.mock import MagicMock\n\nimport pytest\n\nimport pynguin.ga.chromosome as chrom\nimport pynguin.ga.computations as ff\n\nfrom pynguin.ga.chromosome import Chromosome\n\n\n@pytest.fixture\ndef chromosome():\n    class DummyChromosome(chrom.Chromosome):\n        def mutate(self):\n            pass  # pragma: no cover\n\n        def size(self) -> int:\n            return 0  # pragma: no cover\n\n        def clone(self) -> Chromosome:\n            pass  # pragma: no cover\n\n        def cross_over(self, other: chrom.Chromosome, position1: int, position2: int) -> None:\n            pass  # pragma: no cover\n\n        def __hash__(self):\n            return 0  # pragma: no cover\n\n        def __eq__(self, other):\n            return True  # pragma: no cover\n\n        def length(self) -> int:\n            return 0  # pragma: no cover\n\n        def accept(self, visitor) -> None:\n            pass  # pragma: no cover\n\n    return DummyChromosome()\n\n\ndef test_has_changed_default(chromosome):\n    assert chromosome.changed\n\n\ndef test_has_changed(chromosome):\n    chromosome.changed = False\n    assert not chromosome.changed\n\n\ndef test_get_fitness_functions(chromosome):\n    func1 = MagicMock(ff.FitnessFunction)\n    func1.is_maximisation_function.return_value = False\n    func2 = MagicMock(ff.FitnessFunction)\n    func2.is_maximisation_function.return_value = False\n    chromosome.add_fitness_function(func1)\n    chromosome.add_fitness_function(func2)\n    assert chromosome.get_fitness_functions() == [func1, func2]\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pynguin\nTest File Path: pynguin\\test_loop\\test_loop.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides loop operators for mutation analysis.\n\nBased on https://github.com/se2p/mutpy-pynguin/blob/main/mutpy/operators/loop.py\nand integrated in Pynguin.\n\"\"\"\n\nimport ast\nimport typing\n\nfrom pynguin.assertion.mutation_analysis.operators.base import MutationOperator\nfrom pynguin.assertion.mutation_analysis.operators.base import copy_node\n\n\nT = typing.TypeVar(\"T\", ast.For, ast.While)\n\n\ndef one_iteration(node: T) -> T | None:\n    \"\"\"Mutate a loop to have only one iteration.\n\n    Args:\n        node: The loop to mutate.\n\n    Returns:\n        The mutated loop, or None if the loop should not be mutated.\n    \"\"\"\n    if not node.body:\n        return None\n\n    mutated_node = copy_node(node)\n    mutated_node.body.append(ast.Break(lineno=mutated_node.body[-1].lineno + 1))\n    return mutated_node\n\n\ndef zero_iteration(node: T) -> T | None:\n    \"\"\"Mutate a loop to have zero iterations.\n\n    Args:\n        node: The loop to mutate.\n\n    Returns:\n        The mutated loop, or None if the loop should not be mutated.\n    \"\"\"\n    if not node.body:\n        return None\n\n    mutated_node = copy_node(node)\n    mutated_node.body = [ast.Break(lineno=mutated_node.body[0].lineno)]\n    return mutated_node\n\n\nclass OneIterationLoop(MutationOperator):\n    \"\"\"A class that mutates loops to have only one iteration.\"\"\"\n\n    def mutate_For(self, node: ast.For) -> ast.For | None:  # noqa: N802\n        \"\"\"Mutate a For loop to have only one iteration.\n\n        Args:\n            node: The For loop to mutate.\n\n        Returns:\n            The mutated loop, or None if the loop should not be mutated.\n        \"\"\"\n        return one_iteration(node)\n\n    def mutate_While(self, node: ast.While) -> ast.While | None:  # noqa: N802\n        \"\"\"Mutate a While loop to have only one iteration.\n\n        Args:\n            node: The While loop to mutate.\n\n        Returns:\n            The mutated loop, or None if the loop should not be mutated.\n        \"\"\"\n        return one_iteration(node)\n\n\nclass ReverseIterationLoop(MutationOperator):\n    \"\"\"A class that mutates loops by reversing their iteration.\"\"\"\n\n    def mutate_For(self, node: ast.For) -> ast.For:  # noqa: N802\n        \"\"\"Mutate a For loop by reversing its iteration.\n\n        Args:\n            node: The For loop to mutate.\n\n        Returns:\n            The mutated loop.\n        \"\"\"\n        mutated_node = copy_node(node)\n        old_iter = mutated_node.iter\n        mutated_node.iter = ast.Call(  # type: ignore[call-arg]\n            func=ast.Name(id=reversed.__name__, ctx=ast.Load()),\n            args=[old_iter],\n            keywords=[],\n            starargs=None,\n            kwargs=None,\n        )\n        return mutated_node\n\n\nclass ZeroIterationLoop(MutationOperator):\n    \"\"\"A class that mutates loops to have zero iterations.\"\"\"\n\n    def mutate_For(self, node: ast.For) -> ast.For | None:  # noqa: N802\n        \"\"\"Mutate a For loop to have zero iterations.\n\n        Args:\n            node: The For loop to mutate.\n\n        Returns:\n            The mutated loop, or None if the loop should not be mutated.\n        \"\"\"\n        return zero_iteration(node)\n\n    def mutate_While(self, node: ast.While) -> ast.While | None:  # noqa: N802\n        \"\"\"Mutate a While loop to have zero iterations.\n\n        Args:\n            node: The While loop to mutate.\n\n        Returns:\n            The mutated loop, or None if the loop should not be mutated.\n        \"\"\"\n        return zero_iteration(node)\n\n### Source File Dependency Files Content\n### Dependency File: base.py\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides base classes for mutation operators.\n\nBased on https://github.com/se2p/mutpy-pynguin/blob/main/mutpy/operators/base.py\nand integrated in Pynguin.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport abc\nimport ast\nimport copy\nimport re\n\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING\nfrom typing import TypeVar\n\n\nif TYPE_CHECKING:\n    import types\n\n    from collections.abc import Callable\n    from collections.abc import Generator\n    from collections.abc import Iterable\n\n\ndef fix_lineno(node: ast.AST) -> None:\n    \"\"\"Fix the line number of a node if it is not set.\n\n    Args:\n        node: The node to fix.\n    \"\"\"\n    parent = node.parent  # type: ignore[attr-defined]\n    if not hasattr(node, \"lineno\") and parent is not None and hasattr(parent, \"lineno\"):\n        parent_lineno = parent.lineno\n        node.lineno = parent_lineno  # type: ignore[attr-defined]\n\n\ndef fix_node_internals(old_node: ast.AST, new_node: ast.AST) -> None:\n    \"\"\"Fix the internals of a node.\n\n    Args:\n        old_node: The old node.\n        new_node: The new node.\n    \"\"\"\n    if not hasattr(new_node, \"parent\"):\n        old_node_children = old_node.children  # type: ignore[attr-defined]\n        old_node_parent = old_node.parent  # type: ignore[attr-defined]\n        new_node.children = old_node_children  # type: ignore[attr-defined]\n        new_node.parent = old_node_parent  # type: ignore[attr-defined]\n\n    if not hasattr(new_node, \"lineno\") and hasattr(old_node, \"lineno\"):\n        old_node_lineno = old_node.lineno\n        new_node.lineno = old_node_lineno  # type: ignore[attr-defined]\n\n    if hasattr(old_node, \"marker\"):\n        old_node_marker = old_node.marker\n        new_node.marker = old_node_marker  # type: ignore[attr-defined]\n\n\ndef set_lineno(node: ast.AST, lineno: int) -> None:\n    \"\"\"Set the line number of a node.\n\n    Args:\n        node: The node to set the line number for.\n        lineno: The line number to set.\n    \"\"\"\n    for child_node in ast.walk(node):\n        if hasattr(child_node, \"lineno\"):\n            child_node.lineno = lineno\n\n\nT = TypeVar(\"T\", bound=ast.AST)\n\n\ndef shift_lines(nodes: list[T], shift_by: int = 1) -> None:\n    \"\"\"Shift the line numbers of a list of nodes.\n\n    Args:\n        nodes: The nodes to shift.\n        shift_by: The amount to shift by.\n    \"\"\"\n    for node in nodes:\n        ast.increment_lineno(node, shift_by)\n\n\n@dataclass(frozen=True)\nclass Mutation:\n    \"\"\"Represents a mutation.\"\"\"\n\n    node: ast.AST\n    replacement_node: ast.AST\n    operator: type[MutationOperator]\n    visitor_name: str\n\n    def __post_init__(self):\n        \"\"\"Initialize the mutation.\n\n        Raises:\n            ValueError: If the visitor is not found in the operator.\n        \"\"\"\n        if self.visitor_name not in dir(self.operator):\n            raise ValueError(f\"Visitor {self.visitor_name} not found in operator {self.operator}\")\n\n\ndef copy_node(node: T) -> T:\n    \"\"\"Copy a node.\n\n    Args:\n        node: The node to copy.\n\n    Returns:\n        The copied node.\n    \"\"\"\n    parent = node.parent  # type: ignore[attr-defined]\n    return copy.deepcopy(\n        node,\n        memo={\n            id(parent): parent,\n        },\n    )\n\n\nclass MutationOperator:\n    \"\"\"A class that represents a mutation operator.\"\"\"\n\n    @classmethod\n    def mutate(\n        cls,\n        node: T,\n        module: types.ModuleType,\n        only_mutation: Mutation | None = None,\n    ) -> Generator[tuple[Mutation, ast.AST]]:\n        \"\"\"Mutate a node.\n\n        This method will temporarily modify the node provided and yield itself modified\n        with the mutations. If you want to keep the original node while using the\n        generator, you should copy it before passing it to this method.\n\n        Args:\n            node: The node to mutate.\n            module: The module to use.\n            only_mutation: The mutation to apply.\n\n        Yields:\n            A tuple containing the mutation and the mutated node.\n        \"\"\"\n        operator = cls(module, only_mutation)\n\n        for (\n            current_node,\n            replacement_node,\n            mutated_node,\n            visitor_name,\n        ) in operator.visit(node):\n            yield (\n                Mutation(current_node, replacement_node, cls, visitor_name),\n                mutated_node,\n            )\n\n    def __init__(\n        self,\n        module: types.ModuleType,\n        only_mutation: Mutation | None,\n    ) -> None:\n        \"\"\"Initializes the operator.\n\n        Args:\n            module: The module to use.\n            only_mutation: The mutation to apply.\n        \"\"\"\n        self.module = module\n        self.only_mutation = only_mutation\n\n    def visit(self, node: T) -> Generator[tuple[ast.AST, ast.AST, ast.AST, str]]:\n        \"\"\"Visit a node.\n\n        This method will temporarily modify the node provided and yield itself modified\n        with other information. If you want to keep the original node while using the\n        generator, you should copy it before passing it to this method.\n\n        Args:\n            node: The node to visit.\n\n        Yields:\n            A tuple (current node, replacement node, mutated node, visitor name).\n        \"\"\"\n        node_children = node.children  # type: ignore[attr-defined]\n\n        if (\n            self.only_mutation\n            and self.only_mutation.node != node\n            and self.only_mutation.node not in node_children\n        ):\n            return\n\n        fix_lineno(node)\n\n        for visitor in self._find_visitors(node):\n            if (\n                self.only_mutation is None\n                or (\n                    self.only_mutation.node == node\n                    and self.only_mutation.visitor_name == visitor.__name__\n                )\n            ) and (mutated_node := visitor(node)) is not None:\n                fix_node_internals(node, mutated_node)\n                ast.fix_missing_locations(mutated_node)\n\n                yield node, mutated_node, mutated_node, visitor.__name__\n\n        yield from self._generic_visit(node)\n\n    def _generic_visit(self, node: ast.AST) -> Generator[tuple[ast.AST, ast.AST, ast.AST, str]]:\n        for field, old_value in ast.iter_fields(node):\n            generator: Iterable[tuple[ast.AST, ast.AST, str]]\n            if isinstance(old_value, list):\n                generator = self._generic_visit_list(old_value)\n            elif isinstance(old_value, ast.AST):\n                generator = self._generic_visit_real_node(node, field, old_value)\n            else:\n                generator = ()\n\n            for current_node, replacement_node, visitor_name in generator:\n                yield current_node, replacement_node, node, visitor_name\n\n    def _generic_visit_list(self, old_value: list) -> Generator[tuple[ast.AST, ast.AST, str]]:\n        for position, value in enumerate(old_value.copy()):\n            if isinstance(value, ast.AST):\n                for (\n                    current_node,\n                    replacement_node,\n                    mutated_node,\n                    visitor_name,\n                ) in self.visit(value):\n                    old_value[position] = mutated_node\n                    yield current_node, replacement_node, visitor_name\n\n                old_value[position] = value\n\n    def _generic_visit_real_node(\n        self, node: ast.AST, field: str, old_value: ast.AST\n    ) -> Generator[tuple[ast.AST, ast.AST, str]]:\n        for current_node, replacement_node, mutated_node, visitor_name in self.visit(old_value):\n            setattr(node, field, mutated_node)\n            yield current_node, replacement_node, visitor_name\n\n        setattr(node, field, old_value)\n\n    def _find_visitors(self, node: T) -> list[Callable[[T], ast.AST | None]]:\n        node_name = node.__class__.__name__\n        method_prefix_pattern = re.compile(f\"^mutate_{node_name}(_\\\\w+)?$\")\n        return [\n            visitor\n            for attr in dir(self)\n            if method_prefix_pattern.match(attr) is not None\n            and callable(visitor := getattr(self, attr))\n        ]\n\n\nclass AbstractUnaryOperatorDeletion(abc.ABC, MutationOperator):\n    \"\"\"An abstract class that mutates unary operators by deleting them.\"\"\"\n\n    @abc.abstractmethod\n    def get_operator_type(self) -> type:\n        \"\"\"Get the operator type.\n\n        Returns:\n            The operator type.\n        \"\"\"\n\n    def mutate_UnaryOp(self, node: ast.UnaryOp) -> ast.expr | None:  # noqa: N802\n        \"\"\"Mutate a unary operator.\n\n        Args:\n            node: The node to mutate.\n\n        Returns:\n            The mutated node, or None if the node should not be mutated.\n        \"\"\"\n        if not isinstance(node.op, self.get_operator_type()):\n            return None\n\n        return node.operand\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:32"}, {"role": "assistant", "content": "```python\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132023 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\nimport ast\nimport inspect\n\nfrom pynguin.assertion.mutation_analysis.operators.loop import OneIterationLoop\nfrom pynguin.assertion.mutation_analysis.operators.loop import ReverseIterationLoop\nfrom pynguin.assertion.mutation_analysis.operators.loop import ZeroIterationLoop\nfrom tests.testutils import assert_mutation\n\n\ndef test_one_iteration_for_loop_break():\n    assert_mutation(\n        OneIterationLoop,\n        inspect.cleandoc(\n            \"\"\"\n            for x in range(10):\n                pass\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                for x in range(10):\n                    pass\n                    break\n                \"\"\"\n            ): (\"mutate_For\", ast.For, ast.For),\n        },\n    )\n\n\ndef test_one_iteration_while_loop_break():\n    assert_mutation(\n        OneIterationLoop,\n        inspect.cleandoc(\n            \"\"\"\n            i = 0\n            while i < 10:\n                i += 1\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                i = 0\n                while i < 10:\n                    i += 1\n                    break\n                \"\"\"\n            ): (\"mutate_While\", ast.While, ast.While),\n        },\n    )\n\n\ndef test_one_iteration_multiple_loops_break():\n    assert_mutation(\n        OneIterationLoop,\n        inspect.cleandoc(\n            \"\"\"\n            for x in range(10):\n                pass\n            i = 0\n            while i < 10:\n                i += 1\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                for x in range(10):\n                    pass\n                    break\n                i = 0\n                while i < 10:\n                    i += 1\n                \"\"\"\n            ): (\"mutate_For\", ast.For, ast.For),\n            inspect.cleandoc(\n                \"\"\"\n                for x in range(10):\n                    pass\n                i = 0\n                while i < 10:\n                    i += 1\n                    break\n                \"\"\"\n            ): (\"mutate_While\", ast.While, ast.While),\n        },\n    )\n\n\ndef test_zero_iteration_for_loop_break():\n    assert_mutation(\n        ZeroIterationLoop,\n        inspect.cleandoc(\n            \"\"\"\n            for x in range(10):\n                pass\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                for x in range(10):\n                    break\n                \"\"\"\n            ): (\"mutate_For\", ast.For, ast.For),\n        },\n    )\n\n\ndef test_zero_iteration_while_loop_break():\n    assert_mutation(\n        ZeroIterationLoop,\n        inspect.cleandoc(\n            \"\"\"\n            i = 0\n            while i < 10:\n                i += 1\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                i = 0\n                while i < 10:\n                    break\n                \"\"\"\n            ): (\"mutate_While\", ast.While, ast.While),\n        },\n    )\n\n\ndef test_zero_iteration_for_loop_multiple_statements_break():\n    assert_mutation(\n        ZeroIterationLoop,\n        inspect.cleandoc(\n            \"\"\"\n            for x in range(10):\n                pass\n                pass\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                for x in range(10):\n                    break\n                \"\"\"\n            ): (\"mutate_For\", ast.For, ast.For),\n        },\n    )\n\n\ndef test_reverse_iteration_for_loop():\n    assert_mutation(\n        ReverseIterationLoop,\n        inspect.cleandoc(\n            \"\"\"\n            for x in range(10):\n                pass\n            \"\"\"\n        ),\n        {\n            inspect.cleandoc(\n                \"\"\"\n                for x in reversed(range(10)):\n                    pass\n                \"\"\"\n            ): (\"mutate_For\", ast.For, ast.For),\n        },\n    )\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pynguin\nTest File Path: pynguin\\test_programgraph\\test_programgraph.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides analyses regarding the control-flow of the program.\"\"\"\n\nfrom __future__ import annotations\n\nimport queue\nimport sys\n\nfrom dataclasses import dataclass\nfrom typing import Any\nfrom typing import Generic\nfrom typing import TypeVar\n\nimport networkx as nx\n\nfrom bytecode import UNSET\nfrom bytecode import BasicBlock\nfrom bytecode import Bytecode\nfrom bytecode import Compare\nfrom bytecode import ControlFlowGraph\nfrom bytecode import Instr\n\nimport pynguin.utils.opcodes as op\n\nfrom pynguin.utils.orderedset import OrderedSet\n\n\n# Key for storing branch value in networkx edge.\nEDGE_DATA_BRANCH_VALUE = \"branch_value\"\n\n\nclass ProgramGraphNode:\n    \"\"\"A base class for a node of the program graph.\"\"\"\n\n    def __init__(\n        self,\n        index: int,\n        *,\n        offset: int = 0,\n        basic_block: BasicBlock | None = None,\n        is_artificial: bool = False,\n    ) -> None:\n        \"\"\"Instantiates a node for a program graph.\n\n        Args:\n            index: The index of the node\n            offset: The offset of the first instruction of the node\n            basic_block: The basic block in the code\n            is_artificial: Whether the node is an artificial node\n        \"\"\"\n        self._index = index\n        self._offset = offset\n        self._basic_block = basic_block\n        self._is_artificial = is_artificial\n        self._predicate_id: int | None = None\n\n    @property\n    def index(self) -> int:\n        \"\"\"Provides the index of the node.\n\n        Returns:\n            The index of the node\n        \"\"\"\n        return self._index\n\n    @property\n    def offset(self) -> int:\n        \"\"\"Provides the offset of the node the first instruction of the node.\n\n        Returns:\n            The offset of the node\n        \"\"\"\n        return self._offset\n\n    @offset.setter\n    def offset(self, offset: int) -> None:\n        \"\"\"Set a new offset.\n\n        Args:\n            offset: The offset\n        \"\"\"\n        self._offset = offset\n\n    @property\n    def basic_block(self) -> BasicBlock | None:\n        \"\"\"Provides the basic block attached to this node.\n\n        Returns:\n            The optional basic block attached to this node\n        \"\"\"\n        return self._basic_block\n\n    @property\n    def is_artificial(self) -> bool:\n        \"\"\"Whether a node is artificially inserted into the graph.\n\n        Returns:\n            Whether a node is artificially inserted into the graph\n        \"\"\"\n        return self._is_artificial\n\n    @property\n    def predicate_id(self) -> int | None:\n        \"\"\"Provides the predicate ID of the node, if any.\n\n        Returns:\n            The predicate id assigned to this node, if any.\n        \"\"\"\n        return self._predicate_id\n\n    @predicate_id.setter\n    def predicate_id(self, predicate_id: int) -> None:\n        \"\"\"Set a new predicate id.\n\n        Args:\n            predicate_id: The predicate id\n        \"\"\"\n        self._predicate_id = predicate_id\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, ProgramGraphNode):\n            return False\n        if self is other:\n            return True\n        return self._index == other.index\n\n    def __hash__(self) -> int:\n        return 31 + 17 * self._index\n\n    def __str__(self) -> str:\n        result = f\"ProgramGraphNode({self._index})\"\n        if self._predicate_id is not None:\n            result += f\"\\npredicate_id {self._predicate_id}\"\n        if self._basic_block is not None:\n            instructions = []\n            for instr in self._basic_block:\n                arg = instr.arg  # type: ignore[union-attr]\n                if isinstance(arg, BasicBlock):\n                    # We cannot determine which ProgramGraphNode this is.\n                    arg = \"ProgramGraphNode\"\n                elif isinstance(arg, Compare):\n                    arg = arg.name\n                elif arg is UNSET:\n                    arg = \"\"\n                else:\n                    arg = repr(arg)\n                formatted = instr.name  # type: ignore[union-attr]\n                if arg:\n                    formatted += f\" {arg}\"\n                instructions.append(formatted)\n            result += \"\\n\" + \"\\n\".join(instructions)\n        return result\n\n    def __repr__(self) -> str:\n        return f\"ProgramGraphNode(index={self._index}, basic_block={self._basic_block})\"\n\n\nN = TypeVar(\"N\", bound=ProgramGraphNode)\n\n\nclass ProgramGraph(Generic[N]):\n    \"\"\"Provides a base implementation for a program graph.\n\n    Internally, this program graph uses the `NetworkX` library to hold the graph and\n    do all the operations on it.\n    \"\"\"\n\n    def __init__(self) -> None:  # noqa: D107\n        self._graph = nx.DiGraph()\n\n    def add_node(self, node: N, **attr: Any) -> None:\n        \"\"\"Add a node to the graph.\n\n        Args:\n            node: The node\n            attr: A dict of attributes that will be attached to the node\n        \"\"\"\n        self._graph.add_node(node, **attr)\n\n    def add_edge(self, start: N, end: N, **attr: Any) -> None:\n        \"\"\"Add an edge between two nodes to the graph.\n\n        Args:\n            start: The start node of the edge\n            end: The end node of the edge\n            attr: A dict of attributes that will be attached to the edge.\n        \"\"\"\n        self._graph.add_edge(start, end, **attr)\n\n    def get_predecessors(self, node: N) -> set[N]:\n        \"\"\"Provides a set of all direct predecessors of a node.\n\n        Args:\n            node: The node to start\n\n        Returns:\n            A set of direct predecessors of the node\n        \"\"\"\n        return set(self._graph.predecessors(node))\n\n    def get_successors(self, node: N) -> set[N]:\n        \"\"\"Provides a set of all direct successors of a node.\n\n        Args:\n            node: The node to start\n\n        Returns:\n            A set of direct successors of the node\n        \"\"\"\n        return set(self._graph.successors(node))\n\n    @property\n    def nodes(self) -> set[N]:\n        \"\"\"Provides all nodes in the graph.\n\n        Returns:\n            The set of all nodes in the graph\n        \"\"\"\n        return set(self._graph.nodes)\n\n    @property\n    def graph(self) -> nx.DiGraph:\n        \"\"\"The internal graph.\n\n        Returns:\n            The internal graph\n        \"\"\"\n        return self._graph\n\n    @property\n    def entry_node(self) -> N | None:\n        \"\"\"Provides the entry node of the graph.\n\n        Returns:\n            The entry node of the graph\n        \"\"\"\n        for node in self._graph.nodes:\n            if len(self.get_predecessors(node)) == 0:\n                return node\n        return None\n\n    @property\n    def exit_nodes(self) -> set[N]:\n        \"\"\"Provides the exit nodes of the graph.\n\n        Returns:\n            The set of exit nodes of the graph\n        \"\"\"\n        exit_nodes: set[N] = set()\n        for node in self._graph.nodes:\n            if len(self.get_successors(node)) == 0:\n                exit_nodes.add(node)\n        return exit_nodes\n\n    def get_transitive_successors(self, node: N) -> set[N]:\n        \"\"\"Calculates the transitive closure (the transitive successors) of a node.\n\n        Args:\n            node: The node to start with\n\n        Returns:\n            The transitive closure of the node\n        \"\"\"\n        return self._get_transitive_successors(node, set())\n\n    def _get_transitive_successors(self, node: N, done: set[N]) -> set[N]:\n        successors: set[N] = set()\n        for successor_node in self.get_successors(node):\n            if successor_node not in done:\n                successors.add(successor_node)\n                done.add(successor_node)\n                successors.update(self._get_transitive_successors(successor_node, done))\n        return successors\n\n    def get_least_common_ancestor(self, first: N, second: N) -> N:\n        \"\"\"Calculates the least or lowest common ancestor node of two nodes.\n\n        Both nodes have to be part of the graph!\n\n        Args:\n            first: The first node\n            second: The second node\n\n        Returns:\n            The least common ancestor node of the two nodes\n        \"\"\"\n        return nx.lowest_common_ancestor(self._graph, first, second)\n\n    @property\n    def dot(self) -> str:\n        \"\"\"Provides the DOT representation of this graph.\n\n        Returns:\n            The DOT representation of this graph\n        \"\"\"\n        graph = [\"strict digraph  {\"]\n        graph.extend(f'\"{node}\";' for node in self._graph.nodes)\n        for source, target, edge_data in self._graph.edges(data=True):\n            if edge_data == {}:\n                graph.append(f'\"{source}\" -> \"{target}\";')\n            else:\n                str_edge_data = \", \".join([f\"{k}={v}\" for k, v in edge_data.items()])\n                graph.append(f'\"{source}\" -> \"{target}\"  [{str_edge_data}];')\n        graph.append(\"}\")\n        return \"\\n\".join(graph)\n\n\nG = TypeVar(\"G\", bound=ProgramGraph)\n\n\ndef filter_dead_code_nodes(graph: G, entry_node_index: int = 0) -> G:\n    \"\"\"Prunes dead nodes from the given graph.\n\n    A dead node is a node that has no entry node.  To specify a legal entry node,\n    one can use the `entry_node_index` parameter.\n\n    Args:\n        graph: The graph to prune nodes from\n        entry_node_index: The index of the valid entry node\n\n    Returns:\n        The graph without the pruned dead nodes\n    \"\"\"\n    has_changed = True\n    while has_changed:\n        # Do this until we have reached a fixed point, i.e., removed all dead\n        # nodes from the graph.\n        has_changed = False\n        for node in graph.nodes:\n            if graph.get_predecessors(node) == set() and node.index != entry_node_index:\n                # The only node in the graph that is allowed to have no predecessor\n                # is the entry node, i.e., the node with index 0.  All other nodes\n                # without predecessors are considered dead code and thus removed.\n                graph.graph.remove_node(node)\n                has_changed = True\n    return graph\n\n\nclass CFG(ProgramGraph[ProgramGraphNode]):\n    \"\"\"The control-flow graph implementation based on the program graph.\"\"\"\n\n    # Attribute where the predicate id of the instrumentation is stored\n    PREDICATE_ID: str = \"predicate_id\"\n\n    def __init__(self, bytecode_cfg: ControlFlowGraph):\n        \"\"\"Create new CFG. Do not call directly, use static factory methods.\n\n        Args:\n            bytecode_cfg: the control flow graph of the underlying bytecode.\n        \"\"\"\n        super().__init__()\n        self._bytecode_cfg = bytecode_cfg\n        self._diameter: int | None = None\n\n    @staticmethod\n    def from_bytecode(bytecode: Bytecode) -> CFG:\n        \"\"\"Generates a new control-flow graph from a bytecode segment.\n\n        Besides generating a node for each block in the bytecode segment, as returned by\n        `bytecode`'s `ControlFlowGraph` implementation, we add two artificial nodes to\n        the generated CFG:\n         - an artificial entry node, having index -1, that is guaranteed to fulfill the\n           property of an entry node, i.e., there is no incoming edge, and\n         - an artificial exit node, having index `sys.maxsize`, that is guaranteed to\n           fulfill the property of an exit node, i.e., there is no outgoing edge, and\n           that is the only such node in the graph, which is important, e.g., for graph\n           reversal.\n        The index values are chosen that they do not appear in regular graphs, thus one\n        can easily distinguish them from the normal nodes in the graph by checking for\n        their index-property's value.\n\n        Args:\n            bytecode: The bytecode segment\n\n        Returns:\n            The control-flow graph for the segment\n        \"\"\"\n        blocks = ControlFlowGraph.from_bytecode(bytecode)\n        cfg = CFG(blocks)\n\n        # Create the nodes and a mapping of all edges to generate\n        edges, nodes = CFG._create_nodes_and_edges(blocks)\n\n        # Insert all edges between the previously generated nodes\n        CFG._create_graph(cfg, edges, nodes)\n\n        # Filter all dead-code nodes\n        cfg = filter_dead_code_nodes(cfg)\n\n        # Insert dummy exit and entry nodes\n        cfg = CFG._insert_dummy_exit_node(cfg)\n        cfg = CFG._insert_dummy_entry_node(cfg)\n        return cfg  # noqa: RET504\n\n    def bytecode_cfg(self) -> ControlFlowGraph:\n        \"\"\"Provide the raw control flow graph from the code object.\n\n        Can be used to instrument the control flow.\n\n        Returns:\n            The raw control-flow graph from the code object\n        \"\"\"\n        return self._bytecode_cfg\n\n    @staticmethod\n    def reverse(cfg: CFG) -> CFG:\n        \"\"\"Reverses a control-flow graph.\n\n        Generates a copy of the original control-flow graph.\n\n        Args:\n            cfg: The control-flow graph to reverse\n\n        Returns:\n            The reversed control-flow graph\n        \"\"\"\n        reversed_cfg = CFG(cfg.bytecode_cfg())\n\n        reversed_cfg._graph = cfg._graph.reverse(copy=True)  # noqa: SLF001\n        return reversed_cfg\n\n    def reversed(self) -> CFG:\n        \"\"\"Provides the reversed graph of this graph.\n\n        Returns:\n            The reversed graph\n        \"\"\"\n        return CFG.reverse(self)\n\n    @staticmethod\n    def copy_graph(cfg: CFG) -> CFG:\n        \"\"\"Provides a copy of the control-flow graph.\n\n        Args:\n            cfg: The original graph\n\n        Returns:\n            The copied graph\n        \"\"\"\n        copy = CFG(ControlFlowGraph())  # TODO(fk) Cloning the bytecode cfg is complicated.\n\n        copy._graph = cfg._graph.copy()  # noqa: SLF001\n        return copy\n\n    def copy(self) -> CFG:\n        \"\"\"Provides a copy of the control-flow graph.\n\n        Returns:\n            The copied graph\n        \"\"\"\n        return CFG.copy_graph(self)\n\n    @staticmethod\n    def _create_nodes_and_edges(\n        blocks: ControlFlowGraph,\n    ) -> tuple[dict[int, list[tuple[int, dict]]], dict[int, ProgramGraphNode]]:\n        nodes: dict[int, ProgramGraphNode] = {}\n        edges: dict[int, list[tuple[int, dict]]] = {}\n        offset = 0\n        for node_index, block in enumerate(blocks):\n            node = ProgramGraphNode(index=node_index, basic_block=block, offset=offset)\n            # each instruction increases the offset by 2, therefore the offset at the\n            # beginning of the next block is the current offset plus twice the length\n            # of the current block\n            offset += len(block) * 2\n\n            nodes[node_index] = node\n            if node_index not in edges:\n                edges[node_index] = []\n\n            next_block = block.next_block\n            target_block = block.get_jump()\n\n            last_instr = block[-1]\n            if isinstance(last_instr, Instr) and (\n                last_instr.is_cond_jump() or last_instr.opcode == op.FOR_ITER\n            ):\n                if last_instr.opcode in {op.POP_JUMP_IF_TRUE, op.JUMP_IF_TRUE_OR_POP}:\n                    # These jump to arg if ToS is True\n                    true_branch = target_block\n                    false_branch = next_block\n                elif last_instr.opcode in {\n                    op.POP_JUMP_IF_FALSE,\n                    op.JUMP_IF_FALSE_OR_POP,\n                    op.JUMP_IF_NOT_EXC_MATCH,\n                    op.FOR_ITER,\n                }:\n                    # These jump to arg if ToS is False, is Empty or if Exc does\n                    # not match.\n                    true_branch = next_block\n                    false_branch = target_block\n                else:\n                    raise RuntimeError(\n                        \"Unknown conditional Jump instruction in bytecode \" + last_instr.name\n                    )\n                for next_branch, value in [(true_branch, True), (false_branch, False)]:\n                    next_index = blocks.get_block_index(\n                        next_branch  # type: ignore[arg-type]\n                    )\n                    # 'label' is also set to value, to get a nicer DOT representation,\n                    # because 'label' is a keyword for labelling edges.\n                    edges[node_index].append((\n                        next_index,\n                        {EDGE_DATA_BRANCH_VALUE: value, \"label\": value},\n                    ))\n            else:\n                if next_block:\n                    next_index = blocks.get_block_index(next_block)\n                    edges[node_index].append((next_index, {}))\n                if target_block := block.get_jump():\n                    next_index = blocks.get_block_index(target_block)\n                    edges[node_index].append((next_index, {}))\n        return edges, nodes\n\n    @staticmethod\n    def _create_graph(\n        cfg: CFG,\n        edges: dict[int, list[tuple[int, dict]]],\n        nodes: dict[int, ProgramGraphNode],\n    ):\n        # add nodes to graph\n        for node in nodes.values():\n            cfg.add_node(node)\n\n        # add edges to graph\n        for predecessor, successors in edges.items():\n            for successor, attrs in successors:\n                predecessor_node = nodes.get(predecessor)\n                successor_node = nodes.get(successor)\n                assert predecessor_node\n                assert successor_node\n                cfg.add_edge(predecessor_node, successor_node, **attrs)\n\n    @staticmethod\n    def _infinite_loop_nodes(cfg: CFG) -> set[ProgramGraphNode]:\n        nodes: set[ProgramGraphNode] = set()\n        exit_nodes = cfg.exit_nodes\n        for node in cfg.nodes:\n            successors = cfg.get_successors(node)\n            if node in successors and successors.isdisjoint(exit_nodes):\n                nodes.add(node)\n        return nodes\n\n    @staticmethod\n    def _insert_dummy_entry_node(cfg: CFG) -> CFG:\n        dummy_entry_node = ProgramGraphNode(index=-1, is_artificial=True)\n        # Search node with index 0. This block contains the instruction where\n        # the execution of a code object begins.\n        node_zero = [n for n in cfg.nodes if n.index == 0]\n        assert len(node_zero) == 1, \"Execution has to start at exactly one node that has index 0.\"\n        entry_node = node_zero[0]\n        cfg.add_node(dummy_entry_node)\n        cfg.add_edge(dummy_entry_node, entry_node)\n        return cfg\n\n    @staticmethod\n    def _insert_dummy_exit_node(cfg: CFG) -> CFG:\n        dummy_exit_node = ProgramGraphNode(index=sys.maxsize, is_artificial=True)\n        exit_nodes = cfg.exit_nodes\n        assert exit_nodes, (\n            \"Control flow must have at least one exit node. Offending CFG: \" + cfg.dot\n        )\n        cfg.add_node(dummy_exit_node)\n        for exit_node in exit_nodes:\n            cfg.add_edge(exit_node, dummy_exit_node)\n        for infinite_loop_node in CFG._infinite_loop_nodes(cfg):\n            cfg.add_edge(infinite_loop_node, dummy_exit_node)\n        return cfg\n\n    @property\n    def cyclomatic_complexity(self) -> int:\n        \"\"\"Calculates McCabe's cyclomatic complexity for this control-flow graph.\n\n        Returns:\n            McCabe's cyclocmatic complexity number\n        \"\"\"\n        return len(self._graph.edges) - len(self._graph.nodes) + 2\n\n    @property\n    def diameter(self) -> int:\n        \"\"\"Computes the diameter of the graph.\n\n        Returns:\n            The diameter of the graph\n        \"\"\"\n        if self._diameter is None:\n            # Do this computation lazily\n            try:\n                self._diameter = nx.diameter(self._graph, usebounds=True)\n            except nx.NetworkXError:\n                # It seems like NetworkX makes some assumptions on the graph which\n                # are not documented (or which I could not find at least) that caused\n                # these errors.\n                # If the diameter computation fails for some reason, use an upper bound\n                self._diameter = len(self._graph.edges)\n        return self._diameter\n\n\nclass DominatorTree(ProgramGraph[ProgramGraphNode]):\n    \"\"\"Implements a dominator tree.\"\"\"\n\n    @staticmethod\n    def compute(graph: CFG) -> DominatorTree:\n        \"\"\"Computes the dominator tree for a control-flow graph.\n\n        Args:\n            graph: The control-flow graph\n\n        Returns:\n            The dominator tree for the control-flow graph\n        \"\"\"\n        return DominatorTree.compute_dominance_tree(graph)\n\n    @staticmethod\n    def compute_post_dominator_tree(graph: CFG) -> DominatorTree:\n        \"\"\"Computes the post-dominator tree for a control-flow graph.\n\n        Args:\n            graph: The control-flow graph\n\n        Returns:\n            The post-dominator tree for the control-flow graph\n        \"\"\"\n        reversed_cfg = graph.reversed()\n        return DominatorTree.compute(reversed_cfg)\n\n    @staticmethod\n    def compute_dominance_tree(graph: CFG) -> DominatorTree:\n        \"\"\"Computes the dominance tree for a control-flow graph.\n\n        Args:\n            graph: The control-flow graph\n\n        Returns:\n            The dominance tree for the control-flow graph\n        \"\"\"\n        dominance: dict[ProgramGraphNode, set[ProgramGraphNode]] = (\n            DominatorTree._calculate_dominance(graph)\n        )\n        for dominance_node, nodes in dominance.items():\n            nodes.discard(dominance_node)\n        dominance_tree = DominatorTree()\n        entry_node = graph.entry_node\n        assert entry_node is not None\n        dominance_tree.add_node(entry_node)\n\n        node_queue: queue.SimpleQueue = queue.SimpleQueue()\n        node_queue.put(entry_node)\n        while not node_queue.empty():\n            node: ProgramGraphNode = node_queue.get()\n            for current, dominators in dominance.items():\n                if node in dominators:\n                    dominators.remove(node)\n                    if len(dominators) == 0:\n                        dominance_tree.add_node(current)\n                        dominance_tree.add_edge(node, current)\n                        node_queue.put(current)\n        return dominance_tree\n\n    @staticmethod\n    def _calculate_dominance(\n        graph: CFG,\n    ) -> dict[ProgramGraphNode, set[ProgramGraphNode]]:\n        dominance_map: dict[ProgramGraphNode, set[ProgramGraphNode]] = {}\n        entry = graph.entry_node\n        assert entry, \"Cannot work with a graph without entry nodes\"\n        entry_dominators: set[ProgramGraphNode] = {entry}\n        dominance_map[entry] = entry_dominators\n\n        for node in graph.nodes:\n            if node == entry:\n                continue\n            all_nodes: set[ProgramGraphNode] = set(graph.nodes)\n            dominance_map[node] = all_nodes\n\n        changed: bool = True\n        while changed:\n            changed = False\n            for node in graph.nodes:\n                if node == entry:\n                    continue\n                current_dominators = dominance_map.get(node)\n                new_dominators = DominatorTree._calculate_dominators(graph, dominance_map, node)\n\n                if current_dominators != new_dominators:\n                    changed = True\n                    dominance_map[node] = new_dominators\n                    break\n\n        return dominance_map\n\n    @staticmethod\n    def _calculate_dominators(\n        graph: CFG,\n        dominance_map: dict[ProgramGraphNode, set[ProgramGraphNode]],\n        node: ProgramGraphNode,\n    ) -> set[ProgramGraphNode]:\n        dominators: set[ProgramGraphNode] = {node}\n        intersection: set[ProgramGraphNode] = set()\n        predecessors = graph.get_predecessors(node)\n        if not predecessors:\n            return set()\n\n        first_time: bool = True\n        for predecessor in predecessors:\n            predecessor_dominators = dominance_map.get(predecessor)\n            assert predecessor_dominators is not None, \"Cannot be None\"\n            if first_time:\n                intersection = intersection.union(predecessor_dominators)\n                first_time = False\n            else:\n                intersection.intersection_update(predecessor_dominators)\n        intersection = intersection.union(dominators)\n        return intersection  # noqa: RET504\n\n\nclass ControlDependenceGraph(ProgramGraph[ProgramGraphNode]):\n    \"\"\"Implements a control-dependence graph.\"\"\"\n\n    @staticmethod\n    def compute(graph: CFG) -> ControlDependenceGraph:\n        \"\"\"Computes the control-dependence graph for a given control-flow graph.\n\n        Args:\n            graph: The control-flow graph\n\n        Returns:\n            The control-dependence graph\n        \"\"\"\n        augmented_cfg = ControlDependenceGraph._create_augmented_graph(graph)\n        post_dominator_tree = DominatorTree.compute_post_dominator_tree(augmented_cfg)\n        cdg = ControlDependenceGraph()\n        nodes = augmented_cfg.nodes\n\n        for node in nodes:\n            cdg.add_node(node)\n\n        # Find matching edges in the CFG.\n        edges: set[ControlDependenceGraph._Edge] = set()\n        for source in nodes:\n            for target in augmented_cfg.get_successors(source):\n                if source not in post_dominator_tree.get_transitive_successors(target):\n                    # Store branching data from edge, i.e., which outcome of the\n                    # branching node leads to this node.\n                    data = frozenset(augmented_cfg.graph.get_edge_data(source, target).items())\n                    edges.add(ControlDependenceGraph._Edge(source=source, target=target, data=data))\n\n        # Mark nodes in the PDT and construct edges for them.\n        for edge in edges:\n            least_common_ancestor = post_dominator_tree.get_least_common_ancestor(\n                edge.source, edge.target\n            )\n            current = edge.target\n            while current != least_common_ancestor:\n                # TODO(fk) can the branching info be actually used here?\n                # Seems ok?\n                cdg.add_edge(edge.source, current, **dict(edge.data))\n                predecessors = post_dominator_tree.get_predecessors(current)\n                assert len(predecessors) == 1, (\n                    \"Cannot have more than one predecessor in a tree, this violates a \"\n                    \"tree invariant\"\n                )\n                current = predecessors.pop()\n\n            if least_common_ancestor is edge.source:\n                cdg.add_edge(edge.source, least_common_ancestor, **dict(edge.data))\n\n        return filter_dead_code_nodes(cdg, entry_node_index=-sys.maxsize)\n\n    def get_control_dependencies(self, node: ProgramGraphNode) -> OrderedSet[ControlDependency]:\n        \"\"\"Get the immediate control dependencies of this node.\n\n        Args:\n            node: the node whose dependencies should be retrieved.\n\n        Returns:\n            The direct control dependencies of the given node, if any.\n        \"\"\"\n        assert node is not None\n        assert node in self.graph.nodes\n        return self._retrieve_control_dependencies(node, OrderedSet())\n\n    def _retrieve_control_dependencies(\n        self, node: ProgramGraphNode, handled: OrderedSet\n    ) -> OrderedSet[ControlDependency]:\n        result: OrderedSet[ControlDependency] = OrderedSet()\n        for pred in self._graph.predecessors(node):\n            if (pred, node) in handled:\n                continue\n            handled.add((pred, node))\n\n            if (\n                branch_value := self._graph.get_edge_data(pred, node).get(\n                    EDGE_DATA_BRANCH_VALUE, None\n                )\n            ) is not None:\n                assert pred.predicate_id is not None\n                result.add(ControlDependency(pred.predicate_id, branch_value))\n            else:\n                result.update(self._retrieve_control_dependencies(pred, handled))\n        return result\n\n    def is_control_dependent_on_root(self, node: ProgramGraphNode) -> bool:\n        \"\"\"Does this node directly depend on entering the code object?\n\n        Args:\n            node: The program-graph node for the check\n\n        Returns:\n            Whether the given node is directly dependent on the entry of the code object\n        \"\"\"\n        return self._is_control_dependent_on_root(node, set())\n\n    def _is_control_dependent_on_root(\n        self, node: ProgramGraphNode, visited: set[ProgramGraphNode]\n    ) -> bool:\n        if (self.entry_node, node) in self.graph.edges:\n            return True\n        for pred in self.graph.predecessors(node):\n            if pred in visited:\n                continue\n            visited.add(pred)\n            if pred.predicate_id is not None:\n                continue\n            if pred == node:\n                continue\n            if self._is_control_dependent_on_root(pred, visited):\n                return True\n        return False\n\n    @staticmethod\n    def _create_augmented_graph(graph: CFG) -> CFG:\n        entry_node = graph.entry_node\n        assert entry_node, \"Cannot work with CFG without entry node\"\n        exit_nodes = graph.exit_nodes\n        augmented_graph = graph.copy()\n        start_node = ProgramGraphNode(index=-sys.maxsize, is_artificial=True)\n        augmented_graph.add_node(start_node)\n        augmented_graph.add_edge(start_node, entry_node)\n        for exit_node in exit_nodes:\n            augmented_graph.add_edge(start_node, exit_node)\n        return augmented_graph\n\n    @dataclass(frozen=True)\n    class _Edge:\n        source: ProgramGraphNode\n        target: ProgramGraphNode\n        data: frozenset\n\n\n@dataclass(frozen=True)\nclass ControlDependency:\n    \"\"\"Models a control dependency.\"\"\"\n\n    predicate_id: int\n    branch_value: bool\n\n### Source File Dependency Files Content\n### Dependency File: opcodes.py\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\n# Idea and structure are taken from the pyChecco project, see:\n# https://github.com/ipsw1/pychecco\n\n\"\"\"Provides enums for opcode numbers of instructions in bytecode.\"\"\"\n\nPOP_TOP = 1\nROT_TWO = 2\nROT_THREE = 3\nDUP_TOP = 4\nDUP_TOP_TWO = 5\nROT_FOUR = 6\n\nNOP = 9\nUNARY_POSITIVE = 10\nUNARY_NEGATIVE = 11\nUNARY_NOT = 12\n\nUNARY_INVERT = 15\n\nBINARY_MATRIX_MULTIPLY = 16\nINPLACE_MATRIX_MULTIPLY = 17\n\nBINARY_POWER = 19\nBINARY_MULTIPLY = 20\n\nBINARY_MODULO = 22\nBINARY_ADD = 23\nBINARY_SUBTRACT = 24\nBINARY_SUBSCR = 25\nBINARY_FLOOR_DIVIDE = 26\nBINARY_TRUE_DIVIDE = 27\nINPLACE_FLOOR_DIVIDE = 28\nINPLACE_TRUE_DIVIDE = 29\n\nGET_LEN = 30\nMATCH_MAPPING = 31\nMATCH_SEQUENCE = 32\nMATCH_KEYS = 33\nCOPY_DICT_WITHOUT_KEYS = 34\n\nWITH_EXCEPT_START = 49\nGET_AITER = 50\nGET_ANEXT = 51\nBEFORE_ASYNC_WITH = 52\n\nEND_ASYNC_FOR = 54\nINPLACE_ADD = 55\nINPLACE_SUBTRACT = 56\nINPLACE_MULTIPLY = 57\n\nINPLACE_MODULO = 59\nSTORE_SUBSCR = 60\nDELETE_SUBSCR = 61\nBINARY_LSHIFT = 62\nBINARY_RSHIFT = 63\nBINARY_AND = 64\nBINARY_XOR = 65\nBINARY_OR = 66\nINPLACE_POWER = 67\nGET_ITER = 68\nGET_YIELD_FROM_ITER = 69\n\nPRINT_EXPR = 70\nLOAD_BUILD_CLASS = 71\nYIELD_FROM = 72\nGET_AWAITABLE = 73\n\nLOAD_ASSERTION_ERROR = 74\n\nINPLACE_LSHIFT = 75\nINPLACE_RSHIFT = 76\nINPLACE_AND = 77\nINPLACE_XOR = 78\nINPLACE_OR = 79\n\nRETURN_VALUE = 83\nIMPORT_STAR = 84\nSETUP_ANNOTATIONS = 85\nYIELD_VALUE = 86\nPOP_BLOCK = 87\n\nPOP_EXCEPT = 89\n\n# opcodes below 90 ignore their argument\n\nSTORE_NAME = 90\nDELETE_NAME = 91\nUNPACK_SEQUENCE = 92\nFOR_ITER = 93\nUNPACK_EX = 94\nSTORE_ATTR = 95\nDELETE_ATTR = 96\nSTORE_GLOBAL = 97\nDELETE_GLOBAL = 98\nROT_N = 99\nLOAD_CONST = 100\n\nLOAD_NAME = 101\nBUILD_TUPLE = 102\nBUILD_LIST = 103\nBUILD_SET = 104\nBUILD_MAP = 105\nLOAD_ATTR = 106\nCOMPARE_OP = 107\nIMPORT_NAME = 108\nIMPORT_FROM = 109\n\nJUMP_FORWARD = 110\nJUMP_IF_FALSE_OR_POP = 111\nJUMP_IF_TRUE_OR_POP = 112\nJUMP_ABSOLUTE = 113\nPOP_JUMP_IF_FALSE = 114\nPOP_JUMP_IF_TRUE = 115\n\nLOAD_GLOBAL = 116\n\nIS_OP = 117\nCONTAINS_OP = 118\n\nRERAISE = 119\n\nJUMP_IF_NOT_EXC_MATCH = 121\n\nSETUP_FINALLY = 122\n\nLOAD_FAST = 124\nSTORE_FAST = 125\nDELETE_FAST = 126\n\nGEN_START = 129\n\nRAISE_VARARGS = 130\nCALL_FUNCTION = 131\nMAKE_FUNCTION = 132\nBUILD_SLICE = 133\nLOAD_CLOSURE = 135\nLOAD_DEREF = 136\nSTORE_DEREF = 137\nDELETE_DEREF = 138\n\nCALL_FUNCTION_KW = 141\nCALL_FUNCTION_EX = 142\n\nSETUP_WITH = 143\n\nLIST_APPEND = 145\nSET_ADD = 146\nMAP_ADD = 147\n\nLOAD_CLASSDEREF = 148\n\nEXTENDED_ARG = 144\n\nMATCH_CLASS = 152\n\nSETUP_ASYNC_WITH = 154\n\nFORMAT_VALUE = 155\nBUILD_CONST_KEY_MAP = 156\nBUILD_STRING = 157\n\nLOAD_METHOD = 160\nCALL_METHOD = 161\nLIST_EXTEND = 162\nSET_UPDATE = 163\nDICT_MERGE = 164\nDICT_UPDATE = 165\n\n\nOP_UNARY = (\n    UNARY_POSITIVE,\n    UNARY_NEGATIVE,\n    UNARY_NOT,\n    UNARY_INVERT,\n    GET_ITER,\n    GET_YIELD_FROM_ITER,\n)\nOP_BINARY = (\n    BINARY_POWER,\n    BINARY_MULTIPLY,\n    BINARY_MATRIX_MULTIPLY,\n    BINARY_FLOOR_DIVIDE,\n    BINARY_TRUE_DIVIDE,\n    BINARY_MODULO,\n    BINARY_ADD,\n    BINARY_SUBTRACT,\n    BINARY_LSHIFT,\n    BINARY_RSHIFT,\n    BINARY_AND,\n    BINARY_XOR,\n    BINARY_OR,\n)\nOP_INPLACE = (\n    INPLACE_POWER,\n    INPLACE_MULTIPLY,\n    INPLACE_MATRIX_MULTIPLY,\n    INPLACE_FLOOR_DIVIDE,\n    INPLACE_TRUE_DIVIDE,\n    INPLACE_MODULO,\n    INPLACE_ADD,\n    INPLACE_SUBTRACT,\n    INPLACE_LSHIFT,\n    INPLACE_RSHIFT,\n    INPLACE_AND,\n    INPLACE_XOR,\n    INPLACE_OR,\n)\nOP_COMPARE = (COMPARE_OP, IS_OP, CONTAINS_OP)\nOP_LOCAL_ACCESS = (STORE_FAST, LOAD_FAST, DELETE_FAST)\nOP_NAME_ACCESS = (STORE_NAME, LOAD_NAME, DELETE_NAME)\nOP_GLOBAL_ACCESS = (STORE_GLOBAL, LOAD_GLOBAL, DELETE_GLOBAL)\nOP_DEREF_ACCESS = (STORE_DEREF, LOAD_DEREF, DELETE_DEREF, LOAD_CLASSDEREF)\nOP_ATTR_ACCESS = (STORE_ATTR, LOAD_ATTR, DELETE_ATTR, IMPORT_FROM, LOAD_METHOD)\nOP_SUBSCR_ACCESS = (STORE_SUBSCR, DELETE_SUBSCR, BINARY_SUBSCR)\nOP_IMPORT_NAME = (IMPORT_NAME,)\nOP_ABSOLUTE_JUMP = (\n    JUMP_IF_FALSE_OR_POP,\n    JUMP_IF_TRUE_OR_POP,\n    JUMP_ABSOLUTE,\n    POP_JUMP_IF_FALSE,\n    POP_JUMP_IF_TRUE,\n    JUMP_IF_NOT_EXC_MATCH,\n)\nOP_RELATIVE_JUMP = (\n    FOR_ITER,\n    JUMP_FORWARD,\n    SETUP_FINALLY,\n    SETUP_WITH,\n    SETUP_ASYNC_WITH,\n)\nOP_CALL = (CALL_FUNCTION, CALL_FUNCTION_KW, CALL_FUNCTION_EX, CALL_METHOD, YIELD_FROM)\nOP_RETURN = (RETURN_VALUE, YIELD_VALUE)\n\nTRACED_INSTRUCTIONS = (\n    OP_UNARY\n    + OP_BINARY\n    + OP_INPLACE\n    + OP_COMPARE\n    + OP_LOCAL_ACCESS\n    + OP_NAME_ACCESS\n    + OP_GLOBAL_ACCESS\n    + OP_DEREF_ACCESS\n    + OP_ATTR_ACCESS\n    + OP_SUBSCR_ACCESS\n    + OP_IMPORT_NAME\n    + OP_ABSOLUTE_JUMP\n    + OP_RELATIVE_JUMP\n    + OP_CALL\n    + OP_RETURN\n)\n\nMEMORY_USE_INSTRUCTIONS = (\n    LOAD_FAST,\n    LOAD_NAME,\n    LOAD_GLOBAL,\n    LOAD_ATTR,\n    LOAD_DEREF,\n    BINARY_SUBSCR,\n    LOAD_METHOD,\n    IMPORT_FROM,\n    LOAD_CLOSURE,\n    LOAD_CLASSDEREF,\n)\nMEMORY_DEF_INSTRUCTIONS = (\n    STORE_FAST,\n    STORE_NAME,\n    STORE_GLOBAL,\n    STORE_DEREF,\n    STORE_ATTR,\n    STORE_SUBSCR,\n    BINARY_SUBSCR,\n    DELETE_FAST,\n    DELETE_NAME,\n    DELETE_GLOBAL,\n    DELETE_ATTR,\n    DELETE_SUBSCR,\n    DELETE_DEREF,\n    IMPORT_NAME,\n)  # compensate incorrect stack effect for IMPORT_NAME\nCOND_BRANCH_INSTRUCTIONS = (\n    POP_JUMP_IF_TRUE,\n    POP_JUMP_IF_FALSE,\n    JUMP_IF_TRUE_OR_POP,\n    JUMP_IF_FALSE_OR_POP,\n    JUMP_IF_NOT_EXC_MATCH,\n    FOR_ITER,\n)\n\n\n### Dependency File: orderedset.py\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides the implementation of an ordered set.\n\nThe following code is taken from https://github.com/pantsbuild/pants\nWe made minor changes so that our linting pipeline accepts the code.\n\nCopyright 2020 Pants project contributors.\nLicensed under the Apache License, Version 2.0.\nSPDX-FileCopyrightText: 2020 Pants project contributors\nSPDX-License-Identifier: Apache-2.0\n\nAn OrderedSet is a set that remembers its insertion order, and a FrozenOrderedSet is\none that is also immutable. Based on the library `ordered-set` developed by\nElia Robyn Lake (Robyn Speer) and released under the MIT license:\nhttps://github.com/LuminosoInsight/ordered-set.\nCopyright (c) 2012-2022 Elia Robyn Lake\nSPDX-FileCopyrightText: 2012-2022 Elia Robyn Lake\nSPDX-License-Identifier: MIT\n\n\nThe library `ordered-set` is itself originally based on a recipe originally posted to\nActivateState Recipes by Raymond Hettiger and released under the MIT license:\nhttp://code.activestate.com/recipes/576694/.\nCopyright (c) 2009 Raymond Hettiger\nSPDX-FileCopyrightText: 2009 Raymond Hettiger\nSPDX-License-Identifier: MIT\n\"\"\"\n\nfrom __future__ import annotations\n\nimport itertools\n\nfrom collections.abc import Hashable\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import MutableSet\nfrom collections.abc import Sequence\nfrom collections.abc import Set as AbstractSet\nfrom typing import Any\nfrom typing import TypeVar\nfrom typing import cast\nfrom typing import overload\n\nfrom typing_extensions import Self\n\n\nT = TypeVar(\"T\")\nT_co = TypeVar(\"T_co\", covariant=True)\n\n\nclass _AbstractOrderedSet(AbstractSet[T], Sequence[T]):  # noqa: PLW1641\n    \"\"\"Common functionality shared between OrderedSet and FrozenOrderedSet.\"\"\"\n\n    @overload\n    def __getitem__(self, index: int) -> T:\n        pass\n\n    @overload\n    def __getitem__(self, index: slice) -> _AbstractOrderedSet[T]:\n        pass\n\n    def __getitem__(self, index: int | slice) -> T:  # type: ignore[misc]\n        \"\"\"Lookup item at given position. Caution, as this runs in O(n).\n\n        Args:\n            index: The index whose value we want to retrieve.\n\n        Returns:\n            The value of the given index.\n\n        Raises:\n            NotImplementedError: When given a slice.\n            IndexError: When the index is out of range.\n        \"\"\"\n        if isinstance(index, slice):\n            raise NotImplementedError(\"Slicing currently not supported.\")\n        for i, key in enumerate(self._items.keys()):\n            if i == index:\n                return key\n        raise IndexError(\"Index out of range.\")\n\n    def __init__(self, iterable: Iterable[T] | None = None) -> None:\n        # Using a dictionary, rather than using the recipe's original\n        # `self |= iterable`, results in a ~20% performance increase for the\n        # constructor.\n        #\n        # NB: Dictionaries are ordered in Python 3.6+. While this was not formalized\n        # until Python 3.7, Python 3.6 uses this behavior; Pants requires CPython 3.6+\n        # to run, so this assumption is safe for us to rely on.\n        self._items: dict[T, None] = dict.fromkeys(iterable or ())\n\n    def __len__(self) -> int:\n        return len(self._items)\n\n    def __copy__(self) -> Self:\n        return self.__class__(self)\n\n    def __contains__(self, key: Any) -> bool:\n        return key in self._items\n\n    def __iter__(self) -> Iterator[T]:\n        return iter(self._items)\n\n    def __reversed__(self) -> Iterator[T]:\n        return reversed(tuple(self._items.keys()))\n\n    def __repr__(self) -> str:\n        name = self.__class__.__name__\n        if not self:\n            return f\"{name}()\"\n        return f\"{name}({list(self)!r})\"\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return len(self._items) == len(other._items) and all(\n            x == y for x, y in zip(self._items, other._items, strict=True)\n        )\n\n    def __or__(self, other: Iterable[T]) -> Self:  # type: ignore[override]\n        return self.union(other)\n\n    def union(self, *others: Iterable[T]) -> Self:\n        \"\"\"Combines all unique items.\n\n        Each item's order is defined by its first appearance # noqa: DAR201,DAR101.\n\n        Args:\n            *others: the iterables to union with\n\n        Returns:\n            A new ordered set containing the union.\n        \"\"\"\n        # Differences with AbstractSet: our set union forces \"other\" to have the same\n        # type. That is, while AbstractSet allows {1, 2, 3} | {(True, False)} resulting\n        # in set[int | tuple[bool, bool]], the analogous for descendants  of\n        # _TAbstractOrderedSet is not allowed.\n        #\n        # GOTCHA: given _TAbstractOrderedSet[S]:\n        #   if T is a subclass of S => _TAbstractOrderedSet[S] => *appears* to perform\n        #     unification but it doesn't\n        #   if S is a subclass of T => type error (while AbstractSet would resolve to\n        #     AbstractSet[T])\n        merged_iterables = itertools.chain([cast(\"Iterable[T]\", self)], others)\n        return self.__class__(itertools.chain.from_iterable(merged_iterables))\n\n    def __and__(self, other: Iterable[T]) -> Self:\n        # The parent class's implementation of this is backwards.\n        return self.intersection(other)\n\n    def intersection(self, *others: Iterable[T]) -> Self:\n        \"\"\"Returns elements in common between all sets.\n\n        Order is defined only by the first set.\n\n        Args:\n            *others: The iterable to intersect with\n\n        Returns:\n            A new set containing the intersection.\n        \"\"\"\n        cls = self.__class__\n        if not others:\n            return cls(self)\n        common = set.intersection(*(set(other) for other in others))\n        return cls(item for item in self if item in common)\n\n    def difference(self, *others: Iterable[T]) -> Self:\n        \"\"\"Returns all elements that are in this set but not the others.\n\n        Args:\n            *others: The iterable to intersect with\n\n        Returns:\n            A new set containing the difference.\n        \"\"\"\n        cls = self.__class__\n        if not others:\n            return cls(self)\n        other = set.union(*(set(other) for other in others))\n        return cls(item for item in self if item not in other)\n\n    def issubset(self, other: Iterable[T]) -> bool:\n        \"\"\"Report whether another set contains this set.\n\n        Args:\n            other: The set to check\n\n        Returns:\n            True, if this is a subset of other.\n        \"\"\"\n        try:\n            # Fast check for obvious cases\n            if len(self) > len(other):  # type: ignore[arg-type]\n                return False\n        except TypeError:\n            pass\n        return all(item in other for item in self)\n\n    def issuperset(self, other: Iterable[T]) -> bool:\n        \"\"\"Report whether this set contains another set.\n\n        Args:\n            other: The set to check\n\n        Returns:\n            True, if this is a superset of other.\n        \"\"\"\n        try:\n            # Fast check for obvious cases\n            if len(self) < len(other):  # type: ignore[arg-type]\n                return False\n        except TypeError:\n            pass\n        return all(item in self for item in other)\n\n    def __xor__(self, other: Iterable[T]) -> Self:  # type: ignore[override]\n        return self.symmetric_difference(other)\n\n    def symmetric_difference(self, other: Iterable[T]) -> Self:\n        \"\"\"Computes the symmetric difference.\n\n        Return the symmetric difference of this OrderedSet and another set as a new\n        OrderedSet. That is, the new set will contain all elements that are in exactly\n        one of the sets. Their order will be preserved, with elements from `self`\n        preceding elements from `other`.\n\n        Args:\n            other: The other set\n\n        Returns:\n            The symmetric difference.\n        \"\"\"\n        cls = self.__class__\n        diff1 = cls(self).difference(other)\n        diff2 = cls(other).difference(self)\n        return diff1.union(diff2)\n\n\nclass OrderedSet(_AbstractOrderedSet[T], MutableSet[T]):\n    \"\"\"A mutable set that retains its order.\n\n    This is not safe to use with the V2 engine.\n    \"\"\"\n\n    def add(self, value: T) -> None:\n        \"\"\"Add `value` as an item to this OrderedSet.\n\n        Args:\n            value: the object to add\n        \"\"\"\n        self._items[value] = None\n\n    def update(self, iterable: Iterable[T]) -> None:\n        \"\"\"Update the set with the given iterable sequence.\n\n        Args:\n            iterable: The iterable to insert into this set\n        \"\"\"\n        for item in iterable:\n            self._items[item] = None\n\n    def discard(self, value: T) -> None:  # noqa: D102\n        self._items.pop(value, None)\n\n    def clear(self) -> None:  # noqa: D102\n        self._items.clear()\n\n    def difference_update(self, *others: Iterable[T]) -> None:\n        \"\"\"Update this OrderedSet to remove items from one or more other sets.\n\n        Args:\n            *others: The sets to difference update with.\n        \"\"\"\n        items_to_remove: set[T] = set()\n        for other in others:\n            items_as_set = set(other)\n            items_to_remove |= items_as_set\n        self._items = {item: None for item in self._items if item not in items_to_remove}\n\n    def intersection_update(self, other: Iterable[T]) -> None:\n        \"\"\"Computes the intersection inplace.\n\n        Update this OrderedSet to keep only items in another set, preserving their\n        order in this set.\n\n        Args:\n            other: The set to intersection update with.\n        \"\"\"\n        other = set(other)\n        self._items = {item: None for item in self._items if item in other}\n\n    def symmetric_difference_update(self, other: Iterable[T]) -> None:\n        \"\"\"Computes the symmetric difference inplace.\n\n        Update this OrderedSet to remove items from another set, then add items from\n        the other set that were not present in this set.\n\n        Args:\n            other: The other set.\n        \"\"\"\n        items_to_add = [item for item in other if item not in self]\n        items_to_remove = cast(\"set[T]\", set(other))\n        self._items = {item: None for item in self._items if item not in items_to_remove}\n        for item in items_to_add:\n            self._items[item] = None\n\n\nclass FrozenOrderedSet(_AbstractOrderedSet[T_co], Hashable):  # type: ignore[type-var]\n    \"\"\"A frozen (i.e. immutable) set that retains its order.\n\n    This is safe to use with the V2 engine.\n    \"\"\"\n\n    def __init__(self, iterable: Iterable[T_co] | None = None) -> None:  # noqa: D107\n        super().__init__(iterable)\n        self.__hash: int | None = None\n\n    def __hash__(self) -> int:\n        if self.__hash is None:\n            self.__hash = 0\n            for item in self._items:\n                self.__hash ^= hash(item)\n        return self.__hash\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:32"}, {"role": "assistant", "content": "```python\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\nfrom unittest.mock import MagicMock\n\nimport pytest\n\nfrom bytecode import BasicBlock\n\nfrom pynguin.analyses.controlflow import ProgramGraph\nfrom pynguin.analyses.controlflow import ProgramGraphNode\n\n\n@pytest.fixture\ndef mock_basic_block() -> BasicBlock:\n    return MagicMock(BasicBlock)\n\n\n@pytest.fixture\ndef node() -> ProgramGraphNode:\n    return ProgramGraphNode(index=42)\n\n\n@pytest.fixture\ndef second_node() -> ProgramGraphNode:\n    return ProgramGraphNode(index=23)\n\n\n@pytest.fixture\ndef third_node() -> ProgramGraphNode:\n    return ProgramGraphNode(index=21)\n\n\n@pytest.fixture\ndef fourth_node() -> ProgramGraphNode:\n    return ProgramGraphNode(index=24)\n\n\n@pytest.fixture\ndef graph() -> ProgramGraph:\n    return ProgramGraph()\n\n\ndef test_node_index(node):\n    assert node.index == 42\n\n\ndef test_node_basic_block(mock_basic_block):\n    node = ProgramGraphNode(index=42, basic_block=mock_basic_block)\n    assert node.basic_block == mock_basic_block\n\n\ndef test_node_hash(node):\n    assert (hash(node)) != 0\n\n\ndef test_node_equals_other(node):\n    assert node != \"foo\"\n\n\ndef test_node_equals_self(node):\n    assert node == node  # noqa: PLR0124\n\n\ndef test_node_equals_other_node(node):\n    other = ProgramGraphNode(index=42)\n    assert node == other\n\n\ndef test_add_graph(graph, node):\n    graph.add_node(node)\n    assert len(graph.nodes) == 1\n\n\ndef test_graph_nodes_set(graph, node, second_node, third_node, fourth_node):\n    graph.add_node(node)\n    graph.add_node(second_node)\n    graph.add_node(third_node)\n    graph.add_node(fourth_node)\n    nodes = graph.nodes\n    assert nodes == {node, second_node, third_node, fourth_node}\n\n\ndef test_add_edge(graph, node, second_node):\n    graph.add_node(node)\n    graph.add_node(second_node)\n    graph.add_edge(node, second_node)\n    assert graph.entry_node == node\n    assert graph.exit_nodes == {second_node}\n\n\ndef test_entry_exit_node_without_nodes(graph):\n    assert graph.entry_node is None\n    assert graph.exit_nodes == set()\n\n\ndef test_get_transitive_successors(graph, node, second_node, third_node, fourth_node):\n    graph.add_node(fourth_node)\n    graph.add_node(node)\n    graph.add_node(second_node)\n    graph.add_node(third_node)\n    graph.add_edge(node, second_node)\n    graph.add_edge(second_node, third_node)\n    graph.add_edge(third_node, fourth_node)\n    result = graph.get_transitive_successors(second_node)\n    assert result == {third_node, fourth_node}\n\n\ndef test_get_least_common_ancestor(graph, node, second_node, third_node):\n    graph.add_node(node)\n    graph.add_node(second_node)\n    graph.add_node(third_node)\n    graph.add_edge(node, second_node)\n    graph.add_edge(node, third_node)\n    result = graph.get_least_common_ancestor(second_node, third_node)\n    assert result == node\n\n\ndef test_to_dot(graph, node, second_node):\n    graph.add_node(node)\n    graph.add_node(second_node)\n    graph.add_edge(node, second_node)\n    result = graph.dot\n    assert result != \"\"  # noqa: PLC1901\n\n\ndef test_get_predecessors(graph, node, second_node):\n    graph.add_node(node)\n    graph.add_node(second_node)\n    graph.add_edge(node, second_node)\n    result = graph.get_predecessors(second_node)\n    assert result == {node}\n\n\ndef test_is_artificial():\n    node = ProgramGraphNode(index=42, is_artificial=True)\n    assert node.is_artificial\n\n\ndef test_predicate_id_none():\n    node = ProgramGraphNode(index=42)\n    assert node.predicate_id is None\n\n\ndef test_predicate_id_set():\n    node = ProgramGraphNode(index=42)\n    node.predicate_id = 1337\n    assert node.predicate_id == 1337\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pynguin\nTest File Path: pynguin\\test_stack_effect\\test_stack_effect.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n\n### Source File Dependency Files Content\n### Dependency File: opcodes.py\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\n# Idea and structure are taken from the pyChecco project, see:\n# https://github.com/ipsw1/pychecco\n\n\"\"\"Provides enums for opcode numbers of instructions in bytecode.\"\"\"\n\nPOP_TOP = 1\nROT_TWO = 2\nROT_THREE = 3\nDUP_TOP = 4\nDUP_TOP_TWO = 5\nROT_FOUR = 6\n\nNOP = 9\nUNARY_POSITIVE = 10\nUNARY_NEGATIVE = 11\nUNARY_NOT = 12\n\nUNARY_INVERT = 15\n\nBINARY_MATRIX_MULTIPLY = 16\nINPLACE_MATRIX_MULTIPLY = 17\n\nBINARY_POWER = 19\nBINARY_MULTIPLY = 20\n\nBINARY_MODULO = 22\nBINARY_ADD = 23\nBINARY_SUBTRACT = 24\nBINARY_SUBSCR = 25\nBINARY_FLOOR_DIVIDE = 26\nBINARY_TRUE_DIVIDE = 27\nINPLACE_FLOOR_DIVIDE = 28\nINPLACE_TRUE_DIVIDE = 29\n\nGET_LEN = 30\nMATCH_MAPPING = 31\nMATCH_SEQUENCE = 32\nMATCH_KEYS = 33\nCOPY_DICT_WITHOUT_KEYS = 34\n\nWITH_EXCEPT_START = 49\nGET_AITER = 50\nGET_ANEXT = 51\nBEFORE_ASYNC_WITH = 52\n\nEND_ASYNC_FOR = 54\nINPLACE_ADD = 55\nINPLACE_SUBTRACT = 56\nINPLACE_MULTIPLY = 57\n\nINPLACE_MODULO = 59\nSTORE_SUBSCR = 60\nDELETE_SUBSCR = 61\nBINARY_LSHIFT = 62\nBINARY_RSHIFT = 63\nBINARY_AND = 64\nBINARY_XOR = 65\nBINARY_OR = 66\nINPLACE_POWER = 67\nGET_ITER = 68\nGET_YIELD_FROM_ITER = 69\n\nPRINT_EXPR = 70\nLOAD_BUILD_CLASS = 71\nYIELD_FROM = 72\nGET_AWAITABLE = 73\n\nLOAD_ASSERTION_ERROR = 74\n\nINPLACE_LSHIFT = 75\nINPLACE_RSHIFT = 76\nINPLACE_AND = 77\nINPLACE_XOR = 78\nINPLACE_OR = 79\n\nRETURN_VALUE = 83\nIMPORT_STAR = 84\nSETUP_ANNOTATIONS = 85\nYIELD_VALUE = 86\nPOP_BLOCK = 87\n\nPOP_EXCEPT = 89\n\n# opcodes below 90 ignore their argument\n\nSTORE_NAME = 90\nDELETE_NAME = 91\nUNPACK_SEQUENCE = 92\nFOR_ITER = 93\nUNPACK_EX = 94\nSTORE_ATTR = 95\nDELETE_ATTR = 96\nSTORE_GLOBAL = 97\nDELETE_GLOBAL = 98\nROT_N = 99\nLOAD_CONST = 100\n\nLOAD_NAME = 101\nBUILD_TUPLE = 102\nBUILD_LIST = 103\nBUILD_SET = 104\nBUILD_MAP = 105\nLOAD_ATTR = 106\nCOMPARE_OP = 107\nIMPORT_NAME = 108\nIMPORT_FROM = 109\n\nJUMP_FORWARD = 110\nJUMP_IF_FALSE_OR_POP = 111\nJUMP_IF_TRUE_OR_POP = 112\nJUMP_ABSOLUTE = 113\nPOP_JUMP_IF_FALSE = 114\nPOP_JUMP_IF_TRUE = 115\n\nLOAD_GLOBAL = 116\n\nIS_OP = 117\nCONTAINS_OP = 118\n\nRERAISE = 119\n\nJUMP_IF_NOT_EXC_MATCH = 121\n\nSETUP_FINALLY = 122\n\nLOAD_FAST = 124\nSTORE_FAST = 125\nDELETE_FAST = 126\n\nGEN_START = 129\n\nRAISE_VARARGS = 130\nCALL_FUNCTION = 131\nMAKE_FUNCTION = 132\nBUILD_SLICE = 133\nLOAD_CLOSURE = 135\nLOAD_DEREF = 136\nSTORE_DEREF = 137\nDELETE_DEREF = 138\n\nCALL_FUNCTION_KW = 141\nCALL_FUNCTION_EX = 142\n\nSETUP_WITH = 143\n\nLIST_APPEND = 145\nSET_ADD = 146\nMAP_ADD = 147\n\nLOAD_CLASSDEREF = 148\n\nEXTENDED_ARG = 144\n\nMATCH_CLASS = 152\n\nSETUP_ASYNC_WITH = 154\n\nFORMAT_VALUE = 155\nBUILD_CONST_KEY_MAP = 156\nBUILD_STRING = 157\n\nLOAD_METHOD = 160\nCALL_METHOD = 161\nLIST_EXTEND = 162\nSET_UPDATE = 163\nDICT_MERGE = 164\nDICT_UPDATE = 165\n\n\nOP_UNARY = (\n    UNARY_POSITIVE,\n    UNARY_NEGATIVE,\n    UNARY_NOT,\n    UNARY_INVERT,\n    GET_ITER,\n    GET_YIELD_FROM_ITER,\n)\nOP_BINARY = (\n    BINARY_POWER,\n    BINARY_MULTIPLY,\n    BINARY_MATRIX_MULTIPLY,\n    BINARY_FLOOR_DIVIDE,\n    BINARY_TRUE_DIVIDE,\n    BINARY_MODULO,\n    BINARY_ADD,\n    BINARY_SUBTRACT,\n    BINARY_LSHIFT,\n    BINARY_RSHIFT,\n    BINARY_AND,\n    BINARY_XOR,\n    BINARY_OR,\n)\nOP_INPLACE = (\n    INPLACE_POWER,\n    INPLACE_MULTIPLY,\n    INPLACE_MATRIX_MULTIPLY,\n    INPLACE_FLOOR_DIVIDE,\n    INPLACE_TRUE_DIVIDE,\n    INPLACE_MODULO,\n    INPLACE_ADD,\n    INPLACE_SUBTRACT,\n    INPLACE_LSHIFT,\n    INPLACE_RSHIFT,\n    INPLACE_AND,\n    INPLACE_XOR,\n    INPLACE_OR,\n)\nOP_COMPARE = (COMPARE_OP, IS_OP, CONTAINS_OP)\nOP_LOCAL_ACCESS = (STORE_FAST, LOAD_FAST, DELETE_FAST)\nOP_NAME_ACCESS = (STORE_NAME, LOAD_NAME, DELETE_NAME)\nOP_GLOBAL_ACCESS = (STORE_GLOBAL, LOAD_GLOBAL, DELETE_GLOBAL)\nOP_DEREF_ACCESS = (STORE_DEREF, LOAD_DEREF, DELETE_DEREF, LOAD_CLASSDEREF)\nOP_ATTR_ACCESS = (STORE_ATTR, LOAD_ATTR, DELETE_ATTR, IMPORT_FROM, LOAD_METHOD)\nOP_SUBSCR_ACCESS = (STORE_SUBSCR, DELETE_SUBSCR, BINARY_SUBSCR)\nOP_IMPORT_NAME = (IMPORT_NAME,)\nOP_ABSOLUTE_JUMP = (\n    JUMP_IF_FALSE_OR_POP,\n    JUMP_IF_TRUE_OR_POP,\n    JUMP_ABSOLUTE,\n    POP_JUMP_IF_FALSE,\n    POP_JUMP_IF_TRUE,\n    JUMP_IF_NOT_EXC_MATCH,\n)\nOP_RELATIVE_JUMP = (\n    FOR_ITER,\n    JUMP_FORWARD,\n    SETUP_FINALLY,\n    SETUP_WITH,\n    SETUP_ASYNC_WITH,\n)\nOP_CALL = (CALL_FUNCTION, CALL_FUNCTION_KW, CALL_FUNCTION_EX, CALL_METHOD, YIELD_FROM)\nOP_RETURN = (RETURN_VALUE, YIELD_VALUE)\n\nTRACED_INSTRUCTIONS = (\n    OP_UNARY\n    + OP_BINARY\n    + OP_INPLACE\n    + OP_COMPARE\n    + OP_LOCAL_ACCESS\n    + OP_NAME_ACCESS\n    + OP_GLOBAL_ACCESS\n    + OP_DEREF_ACCESS\n    + OP_ATTR_ACCESS\n    + OP_SUBSCR_ACCESS\n    + OP_IMPORT_NAME\n    + OP_ABSOLUTE_JUMP\n    + OP_RELATIVE_JUMP\n    + OP_CALL\n    + OP_RETURN\n)\n\nMEMORY_USE_INSTRUCTIONS = (\n    LOAD_FAST,\n    LOAD_NAME,\n    LOAD_GLOBAL,\n    LOAD_ATTR,\n    LOAD_DEREF,\n    BINARY_SUBSCR,\n    LOAD_METHOD,\n    IMPORT_FROM,\n    LOAD_CLOSURE,\n    LOAD_CLASSDEREF,\n)\nMEMORY_DEF_INSTRUCTIONS = (\n    STORE_FAST,\n    STORE_NAME,\n    STORE_GLOBAL,\n    STORE_DEREF,\n    STORE_ATTR,\n    STORE_SUBSCR,\n    BINARY_SUBSCR,\n    DELETE_FAST,\n    DELETE_NAME,\n    DELETE_GLOBAL,\n    DELETE_ATTR,\n    DELETE_SUBSCR,\n    DELETE_DEREF,\n    IMPORT_NAME,\n)  # compensate incorrect stack effect for IMPORT_NAME\nCOND_BRANCH_INSTRUCTIONS = (\n    POP_JUMP_IF_TRUE,\n    POP_JUMP_IF_FALSE,\n    JUMP_IF_TRUE_OR_POP,\n    JUMP_IF_FALSE_OR_POP,\n    JUMP_IF_NOT_EXC_MATCH,\n    FOR_ITER,\n)\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:32"}, {"role": "assistant", "content": "```python\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\nimport dis\n\nfrom itertools import starmap\n\nimport pytest\n\nimport pynguin.slicer.stack.stackeffect as se\n\nfrom pynguin.utils import opcodes\n\n\n@pytest.mark.parametrize(\n    \"op\",\n    [pytest.param(op) for op in range(90)],  # opcodes up to 90 ignore their argument\n)\ndef test_argument_less_opcodes(op):\n    \"\"\"Test argument less opcode stack effects.\"\"\"\n    if op in opcodes.__dict__.values():\n        pops, pushes = se.StackEffect.stack_effect(op, None)\n        expected = dis.stack_effect(op)\n\n        assert expected == (pushes - pops)\n\n\ndef _conditional_combinations() -> list[tuple[int, int, bool]]:\n    \"\"\"Create a list of all combinations to call a conditional opcode's stack effect.\"\"\"\n    args = [0, 1]\n    conditional_opcodes = range(90, 166)\n\n    # (opcode, argument, jump)  # noqa: ERA001\n    combinations: list[tuple[int, int, bool]] = []\n    for op in conditional_opcodes:\n        if op is opcodes.SETUP_ASYNC_WITH:\n            continue  # async is not supported\n        for arg in args:\n            combinations.extend(((op, arg, True), (op, arg, False)))\n    return combinations\n\n\n@pytest.mark.parametrize(\n    \"op, arg, jump\",\n    list(starmap(pytest.param, _conditional_combinations())),\n)\ndef test_conditional_opcodes(op, arg, jump):\n    \"\"\"Test opcodes with arguments and jumps.\"\"\"\n    if op in opcodes.__dict__.values():\n        pops, pushes = se.StackEffect.stack_effect(op, arg, jump=jump)\n        expected = dis.stack_effect(op, arg, jump=jump)\n\n        assert expected == (pushes - pops)\n\n\ndef test_async_setup_throws_exception():\n    with pytest.raises(AssertionError):\n        se.StackEffect.stack_effect(opcodes.SETUP_ASYNC_WITH, 0)\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pynguin\nTest File Path: pynguin\\test_syntaxtree\\test_syntaxtree.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Provides analyses implemented on the abstract syntax tree.\n\nThe implementation of this module contains some code adopted from the ``darglint``\nlibrary (https://github.com/terrencepreilly/darglint), which was released by Terrence\nReilly under MIT license.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nimport dataclasses\nimport logging\n\nfrom collections import deque\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import TypeAlias\n\nimport astroid\n\nfrom astroid.nodes.as_string import to_code\n\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n\n_LOGGER = logging.getLogger(__name__)\nAstroidFunctionDef: TypeAlias = astroid.AsyncFunctionDef | astroid.FunctionDef\nASTFunctionDef: TypeAlias = ast.AsyncFunctionDef | ast.FunctionDef\n\n\ndef has_decorator(\n    func: ASTFunctionDef,\n    decorators: str | Iterable[str],\n) -> bool:\n    \"\"\"Checks whether a function has one or more decorators.\n\n    Args:\n        func: The function node from the AST\n        decorators: The name or a list of names of decorators to check\n\n    Returns:\n        Whether the function has the decorators\n    \"\"\"\n    if isinstance(decorators, str):\n        decorators = (decorators,)\n\n    for decorator in func.decorator_list:\n        if isinstance(decorator, ast.Name) and decorator.id in decorators:\n            return True\n    return False\n\n\nclass _FunctionScopedVisitorMixin(ast.NodeVisitor):\n    def __init__(self) -> None:\n        super().__init__()\n        self.in_function: bool = False\n\n    def visit_AsyncFunctionDef(  # noqa: N802\n        self, node: ast.AsyncFunctionDef\n    ) -> ast.AST:\n        if not self.in_function:\n            self.in_function = True\n            return getattr(super(), \"visit_AsyncFunctionDef\", super().generic_visit)(node)\n        return ast.Pass()\n\n    def visit_FunctionDef(self, node: ast.FunctionDef) -> ast.AST:  # noqa: N802\n        if not self.in_function:\n            self.in_function = True\n            return getattr(super(), \"visit_FunctionDef\", super().generic_visit)(node)\n        return ast.Pass()\n\n    def visit_Lambda(self, node: ast.Lambda) -> ast.AST:  # noqa: N802\n        if not self.in_function:\n            self.in_function = True\n            return getattr(super(), \"visit_Lambda\", super().generic_visit)(node)\n        return ast.Pass()\n\n\nclass _Context:\n    \"\"\"A context tracking exceptions and symbols.\"\"\"\n\n    def __init__(self) -> None:\n        self.exceptions: set[str] = set()\n\n        # If we are in a bare handler, we have to capture new exceptions raised\n        # separately from the existing ones.  So, we copy the existing exceptions\n        # over here.  This complicates the logic, for the calling class (as\n        # contextual operations have to account for two cases), but it does not seem\n        # avoidable.\n        self.bare_handler_exceptions: set[str] | None = None\n\n        # A lookup from variable names to AST nodes.  If the variable name occurs in\n        # a raise exceptions, then the exception will be added using this lookup.\n        self.variables: dict[str, str | list[str]] = {}\n\n        # The error(s) which the current exception block is handling.\n        self.handling: list[str] | None = None\n\n    def set_in_bare_handler(self) -> None:\n        \"\"\"Move exceptions to bare handler.\"\"\"\n        self.bare_handler_exceptions = set(self.exceptions)\n        self.remove_all_exceptions()\n\n    def __get_attr_name(self, attr: ast.Attribute | ast.Name | ast.Tuple) -> list[str]:\n        curr: Any = attr\n        parts: list[str] = []\n\n        # Assume finite depth of AST here\n        while curr:\n            if isinstance(curr, ast.Attribute):\n                parts.append(curr.attr)\n                curr = curr.value\n            elif isinstance(curr, ast.Name):\n                parts.append(curr.id)\n                curr = None\n            elif isinstance(curr, ast.Tuple):\n                names: list[str] = []\n                for node in curr.elts:\n                    if isinstance(node, ast.Attribute | ast.Name):\n                        names.extend(self.__get_attr_name(node))\n                    else:\n                        _LOGGER.error(\n                            \"While getting the names from a caught tuple of \"\n                            \"exceptions, encountered something other than an \"\n                            \"ast.Name: %s\",\n                            node.__class__.__name__,\n                        )\n                return names\n            else:\n                _LOGGER.error(\n                    \"While getting ast.Attribute representation a node had an unexpected type %s\",\n                    curr.__class__.__name__,\n                )\n                curr = None\n\n        parts.reverse()\n        return [\".\".join(parts)]\n\n    @staticmethod\n    def __get_name_name(name: ast.Name | ast.Tuple) -> str | list[str]:\n        assert isinstance(name, ast.Name | ast.Tuple)\n        if isinstance(name, ast.Name):\n            return name.id\n        return [node.id for node in name.elts if isinstance(node, ast.Name)]\n\n    def __get_exception_name(self, raises: ast.Raise) -> str | list[str]:  # noqa: C901\n        if isinstance(raises, str):\n            return raises\n\n        if isinstance(raises.exc, ast.Name):\n            name = raises.exc.id\n            if name in self.variables:\n                return self.variables[name]\n            return name\n        if isinstance(raises.exc, ast.Call):\n            if hasattr(raises.exc.func, \"id\"):\n                return raises.exc.func.id\n            if hasattr(raises.exc.func, \"attr\"):\n                return raises.exc.func.attr\n            _LOGGER.debug(\n                \"Raises function call has neither id nor attr, has only %s\",\n                str(dir(raises.exc.func)),\n            )\n        elif isinstance(raises.exc, ast.Attribute):\n            return raises.exc.attr\n        elif isinstance(raises.exc, ast.Subscript):\n            id_repr = \"\"\n            if hasattr(raises.exc.value, \"id\"):\n                id_repr = raises.exc.value.id\n            n_repr = \"\"\n            if hasattr(raises.exc.slice, \"value\"):\n                value = raises.exc.slice.value\n                if hasattr(value, \"n\"):\n                    n_repr = value.n\n            return f\"{id_repr}[{n_repr}]\"\n        elif raises.exc is None:\n            if not self.handling:\n                return \"\"\n            if len(self.handling) == 1:\n                return self.handling[0]\n            return self.handling\n        else:\n            _LOGGER.debug(\"Unexpected type in raises expression: %s\", raises.exc)\n        return \"\"\n\n    def add_exception(self, node: ast.Raise) -> set[str]:\n        \"\"\"Add an exception to the context.\n\n        If the exception(s) do not have a name and do not have more children,\n        then it is a bare raise.  In that case, we return the exception(s) to the\n        parent context.\n\n        Args:\n            node: A raise AST node\n\n        Returns:\n            A list of exceptions to be passed up to the parent context\n        \"\"\"\n        name = self.__get_exception_name(node)\n        if not name:  # string `name` is empty\n            if self.bare_handler_exceptions is not None:\n                return self.bare_handler_exceptions | self.exceptions\n            if self.exceptions:\n                return self.exceptions\n            if self.variables:\n                values: set[str] = set()\n                for value in self.variables.values():\n                    if isinstance(value, list):\n                        values |= set(value)\n                    else:\n                        values.add(value)\n                return values\n            _LOGGER.warning(\n                \"Unexpectedly had no exception name raised and no exception in context.\"\n            )\n\n        if isinstance(name, str):\n            self.exceptions.add(name)\n        elif isinstance(name, list):\n            for part in name:\n                self.exceptions.add(part)\n        else:\n            _LOGGER.warning(\"Node name extraction failed: %s\", node)\n        return set()\n\n    def remove_exception(self, node: ast.Raise) -> None:\n        \"\"\"Remove an exception from the context.\n\n        Args:\n            node: The raise node\n        \"\"\"\n        name = self.__get_exception_name(node)\n        if isinstance(name, str) and name in self.exceptions:\n            self.exceptions.remove(name)\n            self.handling = [name]\n        elif isinstance(name, list):\n            self.handling = []\n            for part in name:\n                self.exceptions.remove(part)\n                self.handling.append(part)\n\n    def remove_all_exceptions(self) -> None:\n        \"\"\"Removes all exceptions.\"\"\"\n        self.exceptions.clear()\n\n    def add_variable(self, variable: str, exception: ast.Name | ast.Tuple) -> None:\n        \"\"\"Add a variable to the context.\n\n        Args:\n            variable: The variable\n            exception: The bound exception\n        \"\"\"\n        self.variables[variable] = self.__get_name_name(exception)\n\n    def set_handling(self, attr: ast.Attribute | ast.Name | ast.Tuple) -> None:\n        \"\"\"Set the handling.\n\n        Args:\n            attr: An attribute\n        \"\"\"\n        self.handling = self.__get_attr_name(attr)\n\n    def remove_variable(self, variable: str) -> None:\n        \"\"\"Remove a variable from this context.\n\n        Args:\n            variable: The variable\n        \"\"\"\n        del self.variables[variable]\n\n    def extend(self, other: _Context) -> None:\n        \"\"\"Merge this context with another context.\n\n        Args:\n            other: The other context\n        \"\"\"\n        self.exceptions |= other.exceptions\n\n    def finish_handling(self) -> None:\n        \"\"\"Finish the handling of exceptions in this context.\"\"\"\n        self.handling = None\n\n\nclass _RaiseVisitor(ast.NodeVisitor):\n    \"\"\"Inspects raised exceptions to figure out which get exposed outside the func.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.contexts = deque([_Context()])\n\n    @property\n    def exceptions(self) -> set[str]:\n        \"\"\"Provides the set of exceptions that are not handled.\n\n        Returns:\n            Provides the set of exceptions that are not handled\n        \"\"\"\n        return self.contexts[0].exceptions\n\n    @property\n    def context(self) -> _Context:\n        \"\"\"Provides the outermost context.\n\n        Returns:\n            The outermost context\n        \"\"\"\n        return self.contexts[-1]\n\n    def visit_Raise(self, node: ast.Raise) -> ast.AST:  # noqa: N802\n        bubbles = self.context.add_exception(node)\n        if bubbles:\n            assert len(self.contexts) >= 1\n            if len(self.contexts) < 2:\n                return self.generic_visit(node)\n            parent_context = self.contexts[-2]\n            parent_context.exceptions |= bubbles\n\n        return self.generic_visit(node)\n\n    def visit_Try(self, node: ast.Try) -> None:  # noqa: N802\n        self.contexts.append(_Context())\n        for child in node.body:\n            self.visit(child)\n        for handler in node.handlers:\n            if handler.type:\n                if handler.name and (isinstance(handler.type, ast.Name | ast.Tuple)):\n                    self.context.add_variable(handler.name, handler.type)\n                elif isinstance(handler.type, ast.Attribute | ast.Name | ast.Tuple):\n                    self.context.set_handling(handler.type)\n                else:\n                    _LOGGER.error(\n                        \"While getting the types of exceptions in the handler, \"\n                        \"expected to find an ast.Name, ast.Tuple, or ast.Attribute,\"\n                        \"but got %s\",\n                        handler.type,\n                    )\n                identifier = getattr(handler.type, \"id\", None)\n                if identifier:\n                    self.context.remove_exception(identifier)\n\n                self.generic_visit(handler)\n                self.context.finish_handling()\n            else:\n                # Handle a bare exception.\n                #\n                # Since the bare exception handles all exceptions, we have to clear\n                # all exceptions from the context.  However, exceptions could also be\n                # raised from this handler.  So we cannot clear the exceptions first.\n                # But if we clear the exceptions second, then remove any new\n                # exceptions raised in the handler.  What we need, then, is to know\n                # which new exceptions are raised, and clear all but them.  For that,\n                # we use a temporary context.\n                self.context.set_in_bare_handler()\n                self.generic_visit(handler)\n\n        for child in node.finalbody:\n            self.visit(child)\n        for child in node.orelse:\n            self.visit(child)\n\n        context = self.contexts.pop()\n        self.context.extend(context)\n\n    def visit_Assert(self, node: ast.Assert) -> ast.AST:  # noqa: N802\n        # If we see an assert statement in the subject under test we expect that the\n        # assertion can also be failing, thus it is legitimate to raise an\n        # AssertionError.  Hence, we add the AssertionError to the set of raised\n        # exceptions.\n        self.visit_Raise(\n            ast.Raise(\n                exc=ast.Call(\n                    func=ast.Name(id=\"AssertionError\", ctx=ast.Load()),\n                    args=[],\n                    keywords=[],\n                ),\n            )\n        )\n        # Make sure that we also execute a visit_Assert method in another analysis\n        # visitor class.\n        return getattr(super(), \"visit_Assert\", super().generic_visit)(node)\n\n\nclass _YieldVisitor(ast.NodeVisitor):\n    \"\"\"A visitor checking for ``yield`` nodes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.yields: list[ast.Yield | ast.YieldFrom] = []\n\n    def visit_Yield(self, node: ast.Yield) -> ast.AST:  # noqa: N802\n        self.yields.append(node)\n        return self.generic_visit(node)\n\n    def visit_YieldFrom(self, node: ast.YieldFrom) -> ast.AST:  # noqa: N802\n        self.yields.append(node)\n        return self.generic_visit(node)\n\n\nclass _ReturnVisitor(ast.NodeVisitor):\n    \"\"\"A visitor checking for ``return`` nodes.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.returns: list[ast.Return | None] = []\n\n    def visit_Return(self, node: ast.Return) -> ast.AST:  # noqa: N802\n        self.returns.append(node)\n        return self.generic_visit(node)\n\n\nclass _AssertVisitor(ast.NodeVisitor):\n    \"\"\"A visitor checking for ``assert`` statements.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.asserts: list[ast.Assert] = []\n\n    def visit_Assert(self, node: ast.Assert) -> ast.AST:  # noqa: N802\n        self.asserts.append(node)\n        # Make sure that we also execute a visit_Assert method in another analysis\n        # visitor class.\n        return getattr(super(), \"visit_Assert\", super().generic_visit)(node)\n\n\nclass FunctionAnalysisVisitor(\n    _FunctionScopedVisitorMixin,  # needs to be first in order!\n    _RaiseVisitor,\n    _YieldVisitor,\n    _ReturnVisitor,\n    _AssertVisitor,\n):\n    \"\"\"A visitor that analyses functions.\n\n    It assumes that it will be only called on ``ast.FunctionDef`` or\n    ``ast.AsyncFunctionDef`` nodes.\n    \"\"\"\n\n\n@dataclasses.dataclass\nclass FunctionDescription:\n    \"\"\"Describes a function or method in the subject under test.\n\n    Attributes:\n        end_line_no: The last line number of the function (or -1)\n        func: The AST node of the function\n        has_empty_return: Whether the function has an empty ``return`` statement\n        has_return: Whether the function has a ``return`` statement\n        has_yield: Whether there is a ``yield`` statement in the function's body\n        name: The name of the function\n        raises: The (potentially empty) set of exceptions the function raises\n        raises_assert: Whether the function raises any exceptions\n        start_line_no: The first line number of the function\n    \"\"\"\n\n    end_line_no: int\n    func: AstroidFunctionDef\n    has_empty_return: bool\n    has_return: bool\n    has_yield: bool\n    name: str\n    raises: set[str]\n    raises_assert: bool\n    start_line_no: int\n\n\ndef astroid_to_ast(astroid_in: AstroidFunctionDef) -> ASTFunctionDef:\n    \"\"\"Some part of the analysis only works with Pythons AST (for now).\n\n    So it is necessary to convert astroid to AST.\n\n    Args:\n        astroid_in: The astroid function def\n\n    Returns:\n        The ast function def\n    \"\"\"\n    # TODO(fk) port all of the analysis to astroid so this is no longer necessary.\n    return ast.parse(to_code(astroid_in)).body[0]  # type: ignore[return-value]\n\n\ndef get_function_node_from_ast(\n    tree: astroid.Module | astroid.ClassDef | None, name: str\n) -> AstroidFunctionDef | None:\n    \"\"\"Get the AST Node that represents the function with the given name.\n\n    Args:\n        tree: The AST to search\n        name: The name of the function.\n\n    Returns:\n        The (Async)FunctionDef Node, if any.\n    \"\"\"\n    if tree is None:\n        return None\n    if name not in tree.locals:\n        return None\n    maybe_function = tree.locals[name][0]\n    if isinstance(maybe_function, astroid.FunctionDef | astroid.AsyncFunctionDef):\n        return maybe_function\n    return None\n\n\ndef get_class_node_from_ast(tree: astroid.Module | None, name: str) -> astroid.ClassDef | None:\n    \"\"\"Get the AST Node that represents the class with the given name.\n\n    Args:\n        tree: The AST to search\n        name: The name of the class.\n\n    Returns:\n        The ClassDef Node, if any.\n    \"\"\"\n    if tree is None:\n        return None\n    if name not in tree.locals:\n        return None\n    maybe_class = tree.locals[name][0]\n    if isinstance(maybe_class, astroid.ClassDef):\n        return maybe_class\n    return None\n\n\ndef get_function_description(\n    func: AstroidFunctionDef | None,\n) -> FunctionDescription | None:\n    \"\"\"Get a description of the given function, if any.\n\n    Args:\n        func: The function to describe.\n\n    Returns:\n        A description of the function.\n    \"\"\"\n    if func is None:\n        return None\n\n    function_analysis = FunctionAnalysisVisitor()\n    try:\n        function_analysis.visit(astroid_to_ast(func))\n    except SyntaxError:\n        _LOGGER.debug(\"Analysis of %s failed\", func.name)\n        return None\n\n    has_return = bool(function_analysis.returns)\n    has_empty_return = False\n    if has_return:\n        return_value = function_analysis.returns[0]\n        has_empty_return = return_value is not None and return_value.value is None\n\n    return FunctionDescription(\n        end_line_no=func.tolineno,\n        func=func,\n        has_empty_return=has_empty_return,\n        has_return=has_return,\n        has_yield=bool(function_analysis.yields),\n        name=func.name,\n        raises=function_analysis.exceptions,\n        raises_assert=bool(function_analysis.asserts),\n        start_line_no=func.fromlineno,\n    )\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:14:32"}, {"role": "assistant", "content": "```python\n#  This file is part of Pynguin.\n#\n#  SPDX-FileCopyrightText: 2019\u20132025 Pynguin Contributors\n#\n#  SPDX-License-Identifier: MIT\n#\n\"\"\"Analyses for the syntax tree.\n\nThe implementation of this module contains some code adopted from the ``darglint``\nlibrary (https://github.com/terrencepreilly/darglint), which was released by Terrence\nReilly under MIT license.\n\"\"\"\n\nimport ast\nimport importlib\nimport inspect\n\nimport astroid\nimport pytest\n\nfrom pynguin.analyses.syntaxtree import FunctionAnalysisVisitor\nfrom pynguin.analyses.syntaxtree import astroid_to_ast\nfrom pynguin.analyses.syntaxtree import get_class_node_from_ast\nfrom pynguin.analyses.syntaxtree import get_function_description\nfrom pynguin.analyses.syntaxtree import get_function_node_from_ast\nfrom pynguin.analyses.syntaxtree import has_decorator\n\n\n@pytest.fixture(scope=\"module\")\ndef comments_tree() -> astroid.Module:\n    module = importlib.import_module(\"tests.fixtures.cluster.comments\")\n    return astroid.parse(inspect.getsource(module), path=\"comments.py\")\n\n\n@pytest.fixture\ndef function_analysis() -> FunctionAnalysisVisitor:\n    return FunctionAnalysisVisitor()\n\n\ndef __parse_ast(code: str) -> ast.Module:\n    return ast.parse(code, filename=\"dummy.py\", type_comments=True, feature_version=(3, 8))\n\n\n@pytest.mark.parametrize(\n    \"decorators, expected\",\n    [\n        pytest.param(\"property\", False),\n        pytest.param([\"property\", \"contextmanager\"], True),\n    ],\n)\ndef test__has_decorator(comments_tree, decorators, expected):\n    public_function = get_function_node_from_ast(comments_tree, \"public_function\")\n    assert has_decorator(astroid_to_ast(public_function), decorators) == expected\n\n\n@pytest.mark.parametrize(\n    \"function\",\n    [\n        \"public_function\",\n    ],\n)\ndef test_get_function_description(comments_tree, function):\n    descriptions = get_function_description(get_function_node_from_ast(comments_tree, function))\n    assert descriptions.name == function\n\n\n@pytest.mark.parametrize(\n    \"function\",\n    [\n        \"__init__\",\n        \"public_method\",\n        \"b\",\n        \"asynchronous_method\",\n        \"_protected_method\",\n        \"__private_method\",\n        \"__twice\",\n    ],\n)\ndef test_get_method_description(comments_tree, function):\n    descriptions = get_function_description(\n        get_function_node_from_ast(get_class_node_from_ast(comments_tree, \"AClass\"), function)\n    )\n    assert descriptions.name == function\n\n\ndef test_get_function_description_nested():\n    module = astroid.parse(\n        \"\"\"\ndef foo():\n    def bar():\n        return False\n    yield 5\"\"\"\n    )\n    description = get_function_description(get_function_node_from_ast(module, \"foo\"))\n    assert description.has_return is False\n    assert description.has_yield is True\n\n\ndef __assert_found(program: str, *exceptions):\n    func = ast.parse(program).body[0]\n    visitor = FunctionAnalysisVisitor()\n    visitor.visit(func)\n    for exception in exceptions:\n        assert exception in visitor.exceptions\n\n\ndef __assert_none_found(program: str):\n    func = ast.parse(program).body[0]\n    visitor = FunctionAnalysisVisitor()\n    visitor.visit(func)\n    assert visitor.exceptions == set()\n\n\ndef test_identifies_one_exception():\n    program = \"\"\"def f():\n    raise Exception(\"Something\")\n\"\"\"\n    __assert_found(program, \"Exception\")\n\n\ndef test_ignores_caught_exception():\n    program = \"\"\"def f():\n    try:\n        raise Exception(\"Something\")\n    except Exception as e:\n        pass\n\"\"\"\n    __assert_none_found(program)\n\n\ndef test_ignores_caught_exception_unnamed():\n    program = \"\"\"def f():\n    try:\n        raise Exception(\"Something\")\n    except:\n        pass\n\"\"\"\n    __assert_none_found(program)\n\n\ndef test_identifies_exception_in_catch():\n    program = \"\"\"def f():\n    try:\n        something_dangerous()\n    except:\n        raise Exception(\"Something\")\n\"\"\"\n    __assert_found(program, \"Exception\")\n\n\ndef test_identifies_uncaught_in_try():\n    program = \"\"\"def f():\n    try:\n        raise SyntaxError(\"Problematic\")\n    except IOException:\n        print(\"Not gonna happen.\")\n\"\"\"\n    __assert_found(program, \"SyntaxError\")\n\n\ndef test_caught_in_outer_try():\n    program = \"\"\"def f():\n    try:\n        try:\n            raise SyntaxError(\"Here!\")\n        except IOException:\n            pass\n    except SyntaxError as e:\n        pass\n\"\"\"\n    __assert_none_found(program)\n\n\ndef test_uncaught_in_nested_try():\n    program = \"\"\"def f():\n    try:\n        try:\n            raise InterruptedException()\n        except MathError:\n            pass\n    except IOError:\n        pass\n\"\"\"\n    __assert_found(program, \"InterruptedException\")\n\n\ndef test_caught_in_inner_catch():\n    program = \"\"\"def f():\n    try:\n        try:\n            raise SyntaxError(\">\")\n        except:\n            pass\n    except IOError:\n        pass\n\"\"\"\n    __assert_none_found(program)\n\n\ndef test_caught_multiple_exceptions():\n    program = \"\"\"def f(x):\n    try:\n        y = int(x)\n        return 2 / y\n    except (ValueError, ZeroDivisionError) as e:\n        pass\n\"\"\"\n    __assert_none_found(program)\n\n\ndef test_reraise_on_of_multiple_exceptions():\n    program = \"\"\"def f(x):\n    try:\n        y = int(x)\n        return 2 / y\n    except (ValueError, ZeroDivisionError) as e:\n        raise e\n\"\"\"\n    __assert_found(program, \"ValueError\", \"ZeroDivisionError\")\n\n\ndef test_bare_reraise_with_as():\n    program = \"\"\"def f(x):\n    try:\n        return 1 / x\n    except ZeroDivisionError as e:\n        raise\n\"\"\"\n    __assert_found(program, \"ZeroDivisionError\")\n\n\ndef test_bare_reraise_single_exception():\n    program = \"\"\"def f(x):\n    try:\n        return 1 / x\n    except ZeroDivisionError:\n        raise\n\"\"\"\n    __assert_found(program, \"ZeroDivisionError\")\n\n\ndef test_bare_reraise_one_of_multiple_exceptions():\n    program = \"\"\"def f(x):\n    try:\n        y = int(x)\n        return 2 / y\n    except (ValueError, ZeroDivisionError):\n        raise\n\"\"\"\n    __assert_found(program, \"ValueError\", \"ZeroDivisionError\")\n\n\ndef test_capture_tuple():\n    program = \"\"\"def f(x):\n    try:\n        risky()\n    except (a.AError, b.BError):\n        raise\n\"\"\"\n    __assert_found(program, \"a.AError\", \"b.BError\")\n\n\ndef test_bare_reraise_in_multiple_handlers():\n    program = \"\"\"def f(x):\n    try:\n        risky.attempt(x)\n    except risky.Failed:\n        raise\n    except Exception:\n        logger.log(\"Something else went wrong!\")\n\"\"\"\n    __assert_found(program, \"risky.Failed\")\n\n\ndef test_reraise_any_exception_in_bare_handler():\n    program = \"\"\"def f(x):\n    try:\n        if x == \"Racoon\":\n            raise Rabies()\n        elif x == \"Bird\":\n            raise H1N1()\n    except:\n        raise\n\"\"\"\n    __assert_found(program, \"Rabies\", \"H1N1\")\n\n\ndef test_reraise_any_exception_in_bare_handler_2():\n    program = \"\"\"def f(x):\n    try:\n        if x == \"Racoon\":\n            raise Rabies()\n        elif x == \"Bird\":\n            raise H1N1()\n    except Rabies:\n        raise\n    except H1N1:\n        raise Unexpected()\n\"\"\"\n    __assert_found(program, \"Rabies\", \"Unexpected\")\n\n\ndef test_visits_finally_block():\n    program = \"\"\"def f():\n    try:\n        dangerous_operation()\n    finally:\n        raise AnException()\n\"\"\"\n    __assert_found(program, \"AnException\")\n\n\ndef test_visits_or_else_block():\n    program = \"\"\"def f():\n    try:\n        pass\n    except Exception:\n        pass\n    else:\n        raise MyException()\n\"\"\"\n    __assert_found(program, \"MyException\")\n\n\ndef test_visit_nested_async_functions(function_analysis):\n    program = \"\"\"async def foo():\n    async def bar():\n        pass\n\n    return bar\n\"\"\"\n    tree = __parse_ast(program)\n    function_analysis.visit(tree.body[0])\n    assert function_analysis.in_function\n\n\ndef test_visit_nested_functions(function_analysis):\n    program = \"\"\"def foo():\n    def bar():\n        pass\n\n    return bar\n\"\"\"\n    tree = __parse_ast(program)\n    function_analysis.visit(tree.body[0])\n    assert function_analysis.in_function\n\n\ndef test_visit_nested_lambdas(function_analysis):\n    program = \"lambda a = 2, b = 3: lambda c: a+b+c\"\n    tree = __parse_ast(program)\n    function_analysis.visit(tree.body[0])\n    assert function_analysis.in_function\n\n\ndef test_yield_from(function_analysis):\n    program = \"\"\"def f(g):\n    yield from g\n\"\"\"\n    tree = __parse_ast(program)\n    function_analysis.visit(tree.body[0])\n    assert len(function_analysis.yields) == 1\n    assert isinstance(function_analysis.yields[0], ast.YieldFrom)\n    assert function_analysis.yields[0].value.id == \"g\"\n\n\ndef test_visit_assert(function_analysis):\n    program = \"\"\"def f():\n    assert True\n\"\"\"\n    tree = __parse_ast(program)\n    function_analysis.visit(tree.body[0])\n    assert len(function_analysis.asserts) == 1\n    assert function_analysis.exceptions.pop() == \"AssertionError\"\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: http-prompt\nTest File Path: http-prompt\\test_completer\\test_completer.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n# -*- coding: utf-8 -*-\nimport re\n\nfrom collections import OrderedDict\n\nfrom itertools import chain\nfrom urllib.parse import urlparse\n\nfrom prompt_toolkit.completion import Completer, Completion\n\nfrom .completion import (ROOT_COMMANDS, ACTIONS, OPTION_NAMES, HEADER_NAMES,\n                         HEADER_VALUES)\n\n\nRULES = [\n    # (regex pattern, a method name in CompletionGenerator)\n    (r'((?:[^\\s\\'\"\\\\=:]|(?:\\\\.))+):((?:[^\\s\\'\"\\\\]|(?:\\\\.))*)$',\n     'header_values'),\n\n    (r'(get|head|post|put|patch|delete|connect)\\s+', 'concat_mutations'),\n    (r'(httpie|curl)\\s+', 'preview'),\n    (r'rm\\s+\\-b\\s+', 'existing_body_params'),\n    (r'rm\\s+\\-h\\s+', 'existing_header_names'),\n    (r'rm\\s+\\-o\\s+', 'existing_option_names'),\n    (r'rm\\s+\\-q\\s+', 'existing_querystring_params'),\n\n    # The last two captures are full URL path and the last part of the URL\n    # path. For example:\n    # '/foo/bar' => ('/foo/bar', 'bar')\n    # '/foo/bar/' => ('/foo/bar/', '')\n    # 'foo/bar' => ('foo/bar', 'bar')\n    (r'(ls|cd)\\s+(/?(?:[^/]+/)*([^/]*)/?)$', 'urlpaths'),\n    (r'^\\s*[^\\s]*$', 'root_commands')\n]\n\n\ndef compile_rules(rules):\n    compiled_rules = []\n    for pattern, meta_dict in rules:\n        regex = re.compile(pattern)\n        compiled_rules.append((regex, meta_dict))\n    return compiled_rules\n\n\nRULES = compile_rules(RULES)\n\n\ndef fuzzyfinder(text, collection):\n    \"\"\"https://github.com/amjith/fuzzyfinder\"\"\"\n    suggestions = []\n    if not isinstance(text, str):\n        text = str(text)\n    pat = '.*?'.join(map(re.escape, text))\n    regex = re.compile(pat, flags=re.IGNORECASE)\n    for item in collection:\n        r = regex.search(item)\n        if r:\n            suggestions.append((len(r.group()), r.start(), item))\n\n    return (z for _, _, z in sorted(suggestions))\n\n\ndef match_completions(cur_word, word_dict):\n    words = word_dict.keys()\n    suggestions = fuzzyfinder(cur_word, words)\n    for word in suggestions:\n        desc = word_dict.get(word, '')\n        yield Completion(word, -len(cur_word), display_meta=desc)\n\n\nclass CompletionGenerator(object):\n\n    def root_commands(self, context, match):\n        return chain(\n            self._generic_generate(ROOT_COMMANDS.keys(), {}, ROOT_COMMANDS),\n            self.actions(context, match),\n            self.concat_mutations(context, match)\n        )\n\n    def header_values(self, context, match):\n        header_name = match.group(1)\n        header_values = HEADER_VALUES.get(header_name)\n        if header_values:\n            for value in header_values:\n                yield value, header_name\n\n    def preview(self, context, match):\n        return chain(\n            self.actions(context, match),\n            self.concat_mutations(context, match)\n        )\n\n    def actions(self, context, match):\n        return self._generic_generate(ACTIONS.keys(), {}, ACTIONS)\n\n    def concat_mutations(self, context, match):\n        return chain(\n            self._generic_generate(context.body_params.keys(),\n                                   context.body_params, 'Body parameter'),\n            self._generic_generate(context.querystring_params.keys(),\n                                   context.querystring_params,\n                                   'Querystring parameter'),\n            self._generic_generate(HEADER_NAMES.keys(),\n                                   context.headers, HEADER_NAMES),\n            self._generic_generate(OPTION_NAMES.keys(),\n                                   context.options, OPTION_NAMES)\n        )\n\n    def existing_body_params(self, context, match):\n        params = context.body_params.copy()\n        params.update(context.body_json_params)\n        return self._generic_generate(params.keys(), params, 'Body parameter')\n\n    def existing_querystring_params(self, context, match):\n        return self._generic_generate(\n            context.querystring_params.keys(),\n            context.querystring_params, 'Querystring parameter')\n\n    def existing_header_names(self, context, match):\n        return self._generic_generate(context.headers.keys(),\n                                      context.headers, HEADER_NAMES)\n\n    def existing_option_names(self, context, match):\n        return self._generic_generate(context.options.keys(),\n                                      context.options, OPTION_NAMES)\n\n    def urlpaths(self, context, match):\n        path = urlparse(context.url).path.split('/')\n        overrided_path = match.group(2)\n        if overrided_path:\n            if overrided_path.startswith('/'):\n                # Absolute path\n                path = []\n            path += overrided_path.split('/')[:-1]\n        names = [\n            node.name for node in context.root.ls(*path)\n            if node.data.get('type') == 'dir'\n        ]\n        return self._generic_generate(names, {}, 'Endpoint')\n\n    def _generic_generate(self, names, values, descs):\n        for name in sorted(names):\n            if isinstance(descs, str):\n                desc = descs\n            else:\n                desc = descs.get(name, '')\n            if name in values:\n                value = values[name]\n                if value is None:\n                    desc += ' (on)'\n                else:\n                    value = str(value)\n                    if len(value) > 16:\n                        value = value[:13] + '...'\n                    desc += ' (=%s)' % value\n            yield name, desc\n\n\nclass HttpPromptCompleter(Completer):\n\n    def __init__(self, context):\n        self.context = context\n        self.comp_gen = CompletionGenerator()\n\n    def get_completions(self, document, complete_event):\n        cur_text = document.text_before_cursor\n        cur_word = None\n        word_dict = None\n\n        for regex, method_name in RULES:\n            match = regex.search(cur_text)\n            if match:\n                gen_completions = getattr(self.comp_gen, method_name)\n                completions = gen_completions(self.context, match)\n                word_dict = OrderedDict(completions)\n\n                groups = match.groups()\n                if len(groups) > 1:\n                    cur_word = groups[-1]\n                else:\n                    cur_word = document.get_word_before_cursor(WORD=True)\n\n                break\n\n        if word_dict:\n            for comp in match_completions(cur_word, word_dict):\n                yield comp\n\n### Source File Dependency Files Content\n### Dependency File: completion.py\n\"\"\"Meta data for autocomplete.\"\"\"\n\nfrom collections import OrderedDict\n\nfrom . import options as opt\n\n\nROOT_COMMANDS = OrderedDict([\n    ('cd', 'Change URL/path'),\n    ('clear', 'Clear console screen'),\n    ('curl', 'Preview curl command'),\n    ('env', 'Print environment'),\n    ('exec', 'Clear and load environment from a file'),\n    ('exit', 'Exit HTTP Prompt'),\n    ('help', 'List commands, actions, and HTTPie options'),\n    ('httpie', 'Preview HTTPie command'),\n    ('rm *', 'Remove all options and parameters'),\n    ('rm -b', 'Remove body parameter'),\n    ('rm -b *', 'Remove all body parameters'),\n    ('rm -h', 'Remove header'),\n    ('rm -h *', 'Remove all headers'),\n    ('rm -o', 'Remove HTTPie option'),\n    ('rm -o *', 'Remove all HTTPie options'),\n    ('rm -q', 'Remove querystring parameter'),\n    ('rm -q *', 'Remove all querystring parameters'),\n    ('source', 'Load environment from a file'),\n])\n\nACTIONS = OrderedDict([\n    ('connect', 'CONNECT request'),\n    ('delete', 'DELETE request'),\n    ('get', 'GET request'),\n    ('head', 'HEAD request'),\n    ('options', 'OPTIONS request'),\n    ('patch', 'GET request'),\n    ('post', 'POST request'),\n    ('put', 'PUT request'),\n])\n\n# http://www.iana.org/assignments/message-headers/message-headers.xhtml\n# https://en.wikipedia.org/wiki/List_of_HTTP_header_fields\nHEADER_NAMES = OrderedDict([\n    ('Accept', 'Acceptable response media type'),\n    ('Accept-Charset', 'Acceptable response charsets'),\n    ('Accept-Encoding', 'Acceptable response content codings'),\n    ('Accept-Language', 'Preferred natural languages in response'),\n    ('ALPN', 'Application-layer protocol negotiation to use'),\n    ('Alt-Used', 'Alternative host in use'),\n    ('Authorization', 'Authentication information'),\n    ('Cache-Control', 'Directives for caches'),\n    ('Connection', 'Connection options'),\n    ('Content-Encoding', 'Content codings'),\n    ('Content-Language', 'Natural languages for content'),\n    ('Content-Length', 'Anticipated size for payload body'),\n    ('Content-Location', 'Where content was obtained'),\n    ('Content-MD5', 'Base64-encoded MD5 sum of content'),\n    ('Content-Type', 'Content media type'),\n    ('Cookie', 'Stored cookies'),\n    ('Date', 'Datetime when message was originated'),\n    ('Depth', 'Applied only to resource or its members'),\n    ('DNT', 'Do not track user'),\n    ('Expect', 'Expected behaviors supported by server'),\n    ('Forwarded', 'Proxies involved'),\n    ('From', 'Sender email address'),\n    ('Host', 'Target URI'),\n    ('HTTP2-Settings', 'HTTP/2 connection parameters'),\n    ('If', 'Request condition on state tokens and ETags'),\n    ('If-Match', 'Request condition on target resource'),\n    ('If-Modified-Since', 'Request condition on modification date'),\n    ('If-None-Match', 'Request condition on target resource'),\n    ('If-Range', 'Request condition on Range'),\n    ('If-Schedule-Tag-Match', 'Request condition on Schedule-Tag'),\n    ('If-Unmodified-Since', 'Request condition on modification date'),\n    ('Max-Forwards', 'Max number of times forwarded by proxies'),\n    ('MIME-Version', 'Version of MIME protocol'),\n    ('Origin', 'Origin(s) issuing the request'),\n    ('Pragma', 'Implementation-specific directives'),\n    ('Prefer', 'Preferred server behaviors'),\n    ('Proxy-Authorization', 'Proxy authorization credentials'),\n    ('Proxy-Connection', 'Proxy connection options'),\n    ('Range', 'Request transfer of only part of data'),\n    ('Referer', 'Previous web page'),\n    ('TE', 'Transfer codings willing to accept'),\n    ('Transfer-Encoding', 'Transfer codings applied to payload body'),\n    ('Upgrade', 'Invite server to upgrade to another protocol'),\n    ('User-Agent', 'User agent string'),\n    ('Via', 'Intermediate proxies'),\n    ('Warning', 'Possible incorrectness with payload body'),\n    ('WWW-Authenticate', 'Authentication scheme'),\n    ('X-Csrf-Token', 'Prevent cross-site request forgery'),\n    ('X-CSRFToken', 'Prevent cross-site request forgery'),\n    ('X-Forwarded-For', 'Originating client IP address'),\n    ('X-Forwarded-Host', 'Original host requested by client'),\n    ('X-Forwarded-Proto', 'Originating protocol'),\n    ('X-Http-Method-Override', 'Request method override'),\n    ('X-Requested-With', 'Used to identify Ajax requests'),\n    ('X-XSRF-TOKEN', 'Prevent cross-site request forgery'),\n])\n\nCONTENT_TYPES = [\n    'application/json',\n    'application/x-www-form-urlencoded',\n    'multipart/form-data',\n    'text/html',\n]\n\n# TODO: Include more common header values\nHEADER_VALUES = {\n    'Accept': CONTENT_TYPES,\n    'Content-Type': CONTENT_TYPES,\n}\n\nOPTION_NAMES = sorted(opt.FLAG_OPTIONS + opt.VALUE_OPTIONS)\nOPTION_NAMES = OrderedDict(OPTION_NAMES)\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:16:47"}, {"role": "assistant", "content": "```python\n# -*- coding: utf-8 -*-\nimport unittest\n\nfrom prompt_toolkit.document import Document\n\nfrom http_prompt.completer import HttpPromptCompleter\nfrom http_prompt.context import Context\n\n\nclass TestCompleter(unittest.TestCase):\n\n    def setUp(self):\n        self.context = Context('http://localhost', spec={\n            'paths': {\n                '/users': {},\n                '/users/{username}': {},\n                '/users/{username}/events': {},\n                '/users/{username}/orgs': {},\n                '/orgs': {},\n                '/orgs/{org}': {},\n                '/orgs/{org}/events': {},\n                '/orgs/{org}/members': {}\n            }\n        })\n        self.completer = HttpPromptCompleter(self.context)\n        self.completer_event = None\n\n    def get_completions(self, command):\n        if not isinstance(command, str):\n            command = command.decode()\n        position = len(command)\n        completions = self.completer.get_completions(\n            Document(text=command, cursor_position=position),\n            self.completer_event)\n        return [c.text for c in completions]\n\n    def test_header_name(self):\n        result = self.get_completions('ctype')\n        self.assertEqual(result[0], 'Content-Type')\n\n    def test_header_value(self):\n        result = self.get_completions('Content-Type:json')\n        self.assertEqual(result[0], 'application/json')\n\n    def test_verify_option(self):\n        result = self.get_completions('--vfy')\n        self.assertEqual(result[0], '--verify')\n\n    def test_preview_then_action(self):\n        result = self.get_completions('httpie po')\n        self.assertEqual(result[0], 'post')\n\n    def test_rm_body_param(self):\n        self.context.body_params['my_name'] = 'dont_care'\n        result = self.get_completions('rm -b ')\n        self.assertEqual(result[0], 'my_name')\n\n    def test_rm_body_json_param(self):\n        self.context.body_json_params['number'] = 2\n        result = self.get_completions('rm -b ')\n        self.assertEqual(result[0], 'number')\n\n    def test_rm_querystring_param(self):\n        self.context.querystring_params['my_name'] = 'dont_care'\n        result = self.get_completions('rm -q ')\n        self.assertEqual(result[0], 'my_name')\n\n    def test_rm_header(self):\n        self.context.headers['Accept'] = 'dont_care'\n        result = self.get_completions('rm -h ')\n        self.assertEqual(result[0], 'Accept')\n\n    def test_rm_option(self):\n        self.context.options['--form'] = None\n        result = self.get_completions('rm -o ')\n        self.assertEqual(result[0], '--form')\n\n    def test_querystring_with_chinese(self):\n        result = self.get_completions('name==\u738b')\n        self.assertFalse(result)\n\n    def test_header_with_spanish(self):\n        result = self.get_completions('X-Custom-Header:Jes\u00fa')\n        self.assertFalse(result)\n\n    def test_options_method(self):\n        result = self.get_completions('opt')\n        self.assertEqual(result[0], 'options')\n\n    def test_ls_no_path(self):\n        result = self.get_completions('ls ')\n        self.assertEqual(result, ['orgs', 'users'])\n\n    def test_ls_no_path_substring(self):\n        result = self.get_completions('ls o')\n        self.assertEqual(result, ['orgs'])\n\n    def test_ls_absolute_path(self):\n        result = self.get_completions('ls /users/1/')\n        self.assertEqual(result, ['events', 'orgs'])\n\n    def test_ls_absolute_path_substring(self):\n        result = self.get_completions('ls /users/1/e')\n        self.assertEqual(result, ['events'])\n\n    def test_ls_relative_path(self):\n        self.context.url = 'http://localhost/orgs'\n        result = self.get_completions('ls 1/')\n        self.assertEqual(result, ['events', 'members'])\n\n    def test_cd_no_path(self):\n        result = self.get_completions('cd ')\n        self.assertEqual(result, ['orgs', 'users'])\n\n    def test_cd_no_path_substring(self):\n        result = self.get_completions('cd o')\n        self.assertEqual(result, ['orgs'])\n\n    def test_cd_absolute_path(self):\n        result = self.get_completions('cd /users/1/')\n        self.assertEqual(result, ['events', 'orgs'])\n\n    def test_cd_absolute_path_substring(self):\n        result = self.get_completions('cd /users/1/e')\n        self.assertEqual(result, ['events'])\n\n    def test_cd_relative_path(self):\n        self.context.url = 'http://localhost/orgs'\n        result = self.get_completions('cd 1/')\n        self.assertEqual(result, ['events', 'members'])\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: http-prompt\nTest File Path: http-prompt\\test_contextio\\test_contextio.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"Serialization and deserialization of a Context object.\"\"\"\n\nimport io\nimport os\n\nfrom . import xdg\nfrom .context.transform import format_to_http_prompt\nfrom .execution import execute\n\n\n# Don't save these HTTPie options to avoid collision with user config file\nEXCLUDED_OPTIONS = ['--style']\n\n# Filename the current environment context will be saved to\nCONTEXT_FILENAME = 'context.hp'\n\n\ndef _get_context_filepath():\n    dir_path = xdg.get_data_dir()\n    return os.path.join(dir_path, CONTEXT_FILENAME)\n\n\ndef load_context(context, file_path=None):\n    \"\"\"Load a Context object in place from user data directory.\"\"\"\n    if not file_path:\n        file_path = _get_context_filepath()\n    if os.path.exists(file_path):\n        with open(file_path, encoding='utf-8') as f:\n            for line in f:\n                execute(line, context)\n\n\ndef save_context(context):\n    \"\"\"Save a Context object to user data directory.\"\"\"\n    file_path = _get_context_filepath()\n    content = format_to_http_prompt(context, excluded_options=EXCLUDED_OPTIONS)\n    with open(file_path, 'w', encoding='utf-8') as f:\n        f.write(content)\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:16:47"}, {"role": "assistant", "content": "```python\n# -*- coding: utf-8 -*-\nfrom .base import TempAppDirTestCase\nfrom http_prompt.context import Context\nfrom http_prompt.contextio import save_context, load_context\n\n\nclass TestContextIO(TempAppDirTestCase):\n\n    def test_save_and_load_context_non_ascii(self):\n        c = Context('http://localhost')\n        c.headers.update({\n            'User-Agent': '\u00d6',\n            'Authorization': '\u4e2d\u6587'\n        })\n        save_context(c)\n\n        c = Context('http://0.0.0.0')\n        load_context(c)\n\n        self.assertEqual(c.url, 'http://localhost')\n        self.assertEqual(c.headers, {\n            'User-Agent': '\u00d6',\n            'Authorization': '\u4e2d\u6587'\n        })\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: http-prompt\nTest File Path: http-prompt\\test_lexer\\test_lexer.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom pygments.lexer import (RegexLexer, bygroups, words, using, include,\n                            combined)\nfrom pygments.lexers import BashLexer\n\nfrom pygments.token import Text, String, Keyword, Name, Operator\n\nfrom . import options as opt\n\n\n__all__ = ['HttpPromptLexer']\n\n\nFLAG_OPTIONS = [name for name, _ in opt.FLAG_OPTIONS]\nVALUE_OPTIONS = [name for name, _ in opt.VALUE_OPTIONS]\nHTTP_METHODS = ('get', 'head', 'post', 'put', 'patch',\n                'delete', 'options', 'connect')\n\n\ndef string_rules(state):\n    return [\n        (r'(\")((?:[^\\r\\n\"\\\\]|(?:\\\\.))+)(\")',\n         bygroups(Text, String, Text), state),\n\n        (r'(\")((?:[^\\r\\n\"\\\\]|(?:\\\\.))+)', bygroups(Text, String), state),\n\n        (r\"(')((?:[^\\r\\n'\\\\]|(?:\\\\.))+)(')\",\n         bygroups(Text, String, Text), state),\n\n        (r\"(')((?:[^\\r\\n'\\\\]|(?:\\\\.))+)\", bygroups(Text, String), state),\n\n        (r'([^\\s\\'\\\\]|(\\\\.))+', String, state)\n    ]\n\n\nclass HttpPromptLexer(RegexLexer):\n\n    name = 'HttpPrompt'\n    aliases = ['http-prompt']\n    filenames = ['*.http-prompt']\n\n    tokens = {\n        'root': [\n            (r'\\s+', Text),\n            (r'(cd)(\\s*)', bygroups(Keyword, Text), 'cd'),\n            (r'(rm)(\\s*)', bygroups(Keyword, Text), 'rm_option'),\n            (r'(httpie|curl)(\\s*)', bygroups(Keyword, Text), 'action'),\n\n            (words(HTTP_METHODS, prefix='(?i)', suffix=r'(?!\\S)(\\s*)'),\n             bygroups(Keyword, Text), combined('redir_out', 'urlpath')),\n\n            (r'(clear)(\\s*)', bygroups(Keyword, Text), 'end'),\n            (r'(exit)(\\s*)', bygroups(Keyword, Text), 'end'),\n            (r'(help)(\\s)*', bygroups(Keyword, Text), 'end'),\n            (r'(env)(\\s*)', bygroups(Keyword, Text),\n             combined('redir_out', 'pipe')),\n            (r'(source)(\\s*)', bygroups(Keyword, Text), 'file_path'),\n            (r'(exec)(\\s*)', bygroups(Keyword, Text), 'file_path'),\n            (r'(ls)(\\s*)', bygroups(Keyword, Text),\n             combined('redir_out', 'urlpath')),\n            (r'', Text, 'concat_mut')\n        ],\n\n        'cd': string_rules('end'),\n\n        'rm_option': [\n            (r'(\\-(?:h|o|b|q))(\\s*)', bygroups(Name, Text), 'rm_name'),\n            (r'(\\*)(\\s*)', bygroups(Name, Text), 'end')\n        ],\n        'rm_name': string_rules('end'),\n\n        'shell_command': [\n            (r'(`)([^`]*)(`)', bygroups(Text, using(BashLexer), Text)),\n        ],\n        'pipe': [\n            (r'(\\s*)(\\|)(.*)', bygroups(Text, Operator, using(BashLexer))),\n        ],\n        'concat_mut': [\n            (r'$', Text, 'end'),\n            (r'\\s+', Text),\n\n            # Flag options, such as (--form) and (--json)\n            (words(FLAG_OPTIONS, suffix=r'\\b'), Name, 'concat_mut'),\n\n            # Options with values, such as (--style=default) and (--pretty all)\n            (words(VALUE_OPTIONS, suffix=r'\\b'), Name,\n             combined('shell_command', 'option_op')),\n\n            include('shell_command'),\n\n            # Unquoted or value-quoted request mutation,\n            # such as (name=\"John Doe\") and (name=John\\ Doe)\n            (r'((?:[^\\s\\'\"\\\\=:]|(?:\\\\.))*)(:=|:|==|=)',\n             bygroups(Name, Operator),\n             combined('shell_command', 'unquoted_mut')),\n\n            # Full single-quoted request mutation, such as ('name=John Doe')\n            (r\"(')((?:[^\\r\\n'\\\\=:]|(?:\\\\.))+)(:=|:|==|=)\",\n             bygroups(Text, Name, Operator),\n             combined('shell_command', 'squoted_mut')),\n\n            # Full double-quoted request mutation, such as (\"name=John Doe\")\n            (r'(\")((?:[^\\r\\n\"\\\\=:]|(?:\\\\.))+)(:=|:|==|=)',\n             bygroups(Text, Name, Operator),\n             combined('shell_command', 'dquoted_mut'))\n        ],\n\n        'option_op': [\n            (r'(\\s+|=)', Operator, 'option_value'),\n        ],\n        'option_value': string_rules('#pop:2'),\n        'file_path': string_rules('end'),\n        'redir_out': [\n            (r'(?i)(>>?)(\\s*)', bygroups(Operator, Text), 'file_path')\n        ],\n\n        'unquoted_mut': string_rules('#pop'),\n        'squoted_mut': [\n            (r\"((?:[^\\r\\n'\\\\]|(?:\\\\.))+)(')\", bygroups(String, Text), '#pop'),\n            (r\"([^\\r\\n'\\\\]|(\\\\.))+\", String, '#pop')\n        ],\n        'dquoted_mut': [\n            (r'((?:[^\\r\\n\"\\\\]|(?:\\\\.))+)(\")', bygroups(String, Text), '#pop'),\n            (r'([^\\r\\n\"\\\\]|(\\\\.))+', String, '#pop')\n        ],\n\n        'action': [\n            (words(HTTP_METHODS, prefix='(?i)', suffix=r'(\\s*)'),\n             bygroups(Keyword, Text),\n             combined('redir_out', 'pipe', 'urlpath')),\n            (r'', Text, combined('redir_out', 'pipe', 'urlpath'))\n        ],\n        'urlpath': [\n            (r'https?://([^\\s\"\\'\\\\]|(\\\\.))+', String,\n             combined('concat_mut', 'redir_out', 'pipe')),\n\n            (r'(\")(https?://(?:[^\\r\\n\"\\\\]|(?:\\\\.))+)(\")',\n             bygroups(Text, String, Text),\n             combined('concat_mut', 'redir_out', 'pipe')),\n\n            (r'(\")(https?://(?:[^\\r\\n\"\\\\]|(?:\\\\.))+)',\n             bygroups(Text, String)),\n\n            (r\"(')(https?://(?:[^\\r\\n'\\\\]|(?:\\\\.))+)(')\",\n             bygroups(Text, String, Text),\n             combined('concat_mut', 'redir_out', 'pipe')),\n\n            (r\"(')(https?://(?:[^\\r\\n'\\\\]|(?:\\\\.))+)\",\n             bygroups(Text, String)),\n\n            (r'(\")((?:[^\\r\\n\"\\\\=:]|(?:\\\\.))+)(\")',\n             bygroups(Text, String, Text),\n             combined('concat_mut', 'redir_out', 'pipe')),\n\n            (r'(\")((?:[^\\r\\n\"\\\\=:]|(?:\\\\.))+)', bygroups(Text, String)),\n\n            (r\"(')((?:[^\\r\\n'\\\\=:]|(?:\\\\.))+)(')\",\n             bygroups(Text, String, Text),\n             combined('concat_mut', 'redir_out', 'pipe')),\n\n            (r\"(')((?:[^\\r\\n'\\\\=:]|(?:\\\\.))+)\", bygroups(Text, String)),\n\n            (r'([^\\-](?:[^\\s\"\\'\\\\=:]|(?:\\\\.))+)(\\s+|$)',\n             bygroups(String, Text),\n             combined('concat_mut', 'redir_out', 'pipe')),\n\n            (r'', Text,\n             combined('concat_mut', 'redir_out', 'pipe'))\n        ],\n\n        'end': [\n            (r'\\n', Text, 'root')\n        ]\n    }\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:16:47"}, {"role": "assistant", "content": "```python\nimport unittest\n\nfrom pygments.token import Keyword, String, Text, Error, Name, Operator\n\nfrom http_prompt.lexer import HttpPromptLexer\n\n\nclass LexerTestCase(unittest.TestCase):\n\n    def setUp(self):\n        self.lexer = HttpPromptLexer()\n\n    def get_tokens(self, text, filter_spaces=True):\n        tokens = self.lexer.get_tokens(text)\n        tokens = filter(lambda t: t[1], tokens)\n        if filter_spaces:\n            tokens = filter(lambda t: t[1].strip(), tokens)\n        return list(tokens)\n\n\nclass TestLexer_mutation(LexerTestCase):\n\n    def test_querystring(self):\n        self.assertEqual(self.get_tokens('foo==bar'), [\n            (Name, 'foo'),\n            (Operator, '=='),\n            (String, 'bar')\n        ])\n\n    def test_body_param(self):\n        self.assertEqual(self.get_tokens('foo=bar'), [\n            (Name, 'foo'),\n            (Operator, '='),\n            (String, 'bar')\n        ])\n\n    def test_header(self):\n        self.assertEqual(self.get_tokens('Accept:application/json'), [\n            (Name, 'Accept'),\n            (Operator, ':'),\n            (String, 'application/json')\n        ])\n\n    def test_json_integer(self):\n        self.assertEqual(self.get_tokens('number:=1'), [\n            (Name, 'number'),\n            (Operator, ':='),\n            (String, '1')\n        ])\n\n    def test_json_boolean(self):\n        self.assertEqual(self.get_tokens('enabled:=true'), [\n            (Name, 'enabled'),\n            (Operator, ':='),\n            (String, 'true')\n        ])\n\n    def test_json_string(self):\n        self.assertEqual(self.get_tokens('name:=\"foo bar\"'), [\n            (Name, 'name'),\n            (Operator, ':='),\n            (Text, '\"'),\n            (String, 'foo bar'),\n            (Text, '\"')\n        ])\n\n    def test_json_array(self):\n        self.assertEqual(self.get_tokens('list:=[1,\"two\"]'), [\n            (Name, 'list'),\n            (Operator, ':='),\n            (String, '[1,\"two\"]'),\n        ])\n\n    def test_json_array_quoted(self):\n        self.assertEqual(self.get_tokens(\"\"\"list:='[1,\"two\"]'\"\"\"), [\n            (Name, 'list'),\n            (Operator, ':='),\n            (Text, \"'\"),\n            (String, '[1,\"two\"]'),\n            (Text, \"'\"),\n        ])\n\n    def test_json_object(self):\n        self.assertEqual(self.get_tokens('object:={\"id\":123,\"name\":\"foo\"}'), [\n            (Name, 'object'),\n            (Operator, ':='),\n            (String, '{\"id\":123,\"name\":\"foo\"}'),\n        ])\n\n    def test_json_object_quoted(self):\n        self.assertEqual(self.get_tokens(\"\"\"object:='{\"id\": 123}'\"\"\"), [\n            (Name, 'object'),\n            (Operator, ':='),\n            (Text, \"'\"),\n            (String, '{\"id\": 123}'),\n            (Text, \"'\")\n        ])\n\n    def test_json_escaped_colon(self):\n        self.assertEqual(self.get_tokens(r'where[id\\:gt]:=2'), [\n            (Name, r'where[id\\:gt]'),\n            (Operator, ':='),\n            (String, '2')\n        ])\n\n    def test_body_param_escaped_equal(self):\n        self.assertEqual(self.get_tokens(r'foo\\=bar=hello'), [\n            (Name, r'foo\\=bar'),\n            (Operator, '='),\n            (String, 'hello')\n        ])\n\n    def test_parameter_name_including_http_method_name(self):\n        self.assertEqual(self.get_tokens('heading==hello'), [\n            (Name, 'heading'),\n            (Operator, '=='),\n            (String, 'hello')\n        ])\n\n\nclass TestLexer_cd(LexerTestCase):\n\n    def test_simple(self):\n        self.assertEqual(self.get_tokens('cd api/v1'), [\n            (Keyword, 'cd'),\n            (String, 'api/v1')\n        ])\n\n    def test_double_quoted(self):\n        self.assertEqual(self.get_tokens('cd \"api/v 1\"'), [\n            (Keyword, 'cd'),\n            (Text, '\"'),\n            (String, 'api/v 1'),\n            (Text, '\"')\n        ])\n\n    def test_single_quoted(self):\n        self.assertEqual(self.get_tokens(\"cd 'api/v 1'\"), [\n            (Keyword, 'cd'),\n            (Text, \"'\"),\n            (String, 'api/v 1'),\n            (Text, \"'\")\n        ])\n\n    def test_escape(self):\n        self.assertEqual(self.get_tokens(r\"cd api/v\\ 1\"), [\n            (Keyword, 'cd'),\n            (String, r'api/v\\ 1')\n        ])\n\n    def test_second_path(self):\n        self.assertEqual(self.get_tokens(r\"cd api v1\"), [\n            (Keyword, 'cd'),\n            (String, 'api'),\n            (Error, 'v'),\n            (Error, '1')\n        ])\n\n    def test_leading_trailing_spaces(self):\n        self.assertEqual(self.get_tokens('   cd   api/v1  '), [\n            (Keyword, 'cd'),\n            (String, 'api/v1')\n        ])\n\n\nclass TestLexer_ls(LexerTestCase):\n\n    def test_no_path(self):\n        self.assertEqual(self.get_tokens('ls'), [\n            (Keyword, 'ls')\n        ])\n\n    def test_path(self):\n        self.assertEqual(self.get_tokens('ls api/v1'), [\n            (Keyword, 'ls'),\n            (String, 'api/v1')\n        ])\n\n    def test_second_path(self):\n        self.assertEqual(self.get_tokens(r\"ls api v1\"), [\n            (Keyword, 'ls'),\n            (String, 'api'),\n            (Error, 'v'),\n            (Error, '1')\n        ])\n\n    def test_leading_trailing_spaces(self):\n        self.assertEqual(self.get_tokens('   ls   api/v1  '), [\n            (Keyword, 'ls'),\n            (String, 'api/v1')\n        ])\n\n    def test_redirect(self):\n        self.assertEqual(self.get_tokens('ls api/v1 > endpoints.txt'), [\n            (Keyword, 'ls'),\n            (String, 'api/v1'),\n            (Operator, '>'),\n            (String, 'endpoints.txt')\n        ])\n\n\nclass TestLexer_env(LexerTestCase):\n\n    def test_env_simple(self):\n        self.assertEqual(self.get_tokens('env'), [\n            (Keyword, 'env'),\n        ])\n\n    def test_env_with_spaces(self):\n        self.assertEqual(self.get_tokens('   env    '), [\n            (Keyword, 'env'),\n        ])\n\n    def test_env_write(self):\n        self.assertEqual(self.get_tokens('env > /tmp/file.txt'), [\n            (Keyword, 'env'), (Operator, '>'),\n            (String, '/tmp/file.txt')\n        ])\n\n    def test_env_append(self):\n        self.assertEqual(self.get_tokens('env >> /tmp/file.txt'), [\n            (Keyword, 'env'), (Operator, '>>'),\n            (String, '/tmp/file.txt')\n        ])\n\n    def test_env_write_quoted_filename(self):\n        self.assertEqual(self.get_tokens('env > \"/tmp/my file.txt\"'), [\n            (Keyword, 'env'), (Operator, '>'),\n            (Text, '\"'), (String, '/tmp/my file.txt'), (Text, '\"')\n        ])\n\n    def test_env_append_escaped_filename(self):\n        self.assertEqual(self.get_tokens(r'env >> /tmp/my\\ file.txt'), [\n            (Keyword, 'env'), (Operator, '>>'),\n            (String, r'/tmp/my\\ file.txt')\n        ])\n\n    def test_env_pipe(self):\n        self.assertEqual(self.get_tokens('env | grep name'), [\n            (Keyword, 'env'), (Operator, '|'),\n            (Text, 'grep'), (Text, 'name')\n        ])\n\n\nclass TestLexer_rm(LexerTestCase):\n\n    def test_header(self):\n        self.assertEqual(self.get_tokens('rm -h Accept'), [\n            (Keyword, 'rm'),\n            (Name, '-h'),\n            (String, 'Accept')\n        ])\n\n    def test_header_escaped(self):\n        self.assertEqual(self.get_tokens(r'rm -h Custom\\ Header'), [\n            (Keyword, 'rm'),\n            (Name, '-h'),\n            (String, r'Custom\\ Header')\n        ])\n\n    def test_querystring(self):\n        self.assertEqual(self.get_tokens('rm -q page'), [\n            (Keyword, 'rm'),\n            (Name, '-q'),\n            (String, 'page')\n        ])\n\n    def test_querystring_double_quoted(self):\n        self.assertEqual(self.get_tokens('rm -q \"page size\"'), [\n            (Keyword, 'rm'),\n            (Name, '-q'),\n            (Text, '\"'),\n            (String, 'page size'),\n            (Text, '\"')\n        ])\n\n    def test_body_param(self):\n        self.assertEqual(self.get_tokens('rm -b name'), [\n            (Keyword, 'rm'),\n            (Name, '-b'),\n            (String, 'name')\n        ])\n\n    def test_body_param_single_quoted(self):\n        self.assertEqual(self.get_tokens(\"rm -b 'first name'\"), [\n            (Keyword, 'rm'),\n            (Name, '-b'),\n            (Text, \"'\"),\n            (String, 'first name'),\n            (Text, \"'\")\n        ])\n\n    def test_option(self):\n        self.assertEqual(self.get_tokens('rm -o --json'), [\n            (Keyword, 'rm'),\n            (Name, '-o'),\n            (String, '--json')\n        ])\n\n    def test_reset(self):\n        self.assertEqual(self.get_tokens('rm *'), [\n            (Keyword, 'rm'),\n            (Name, '*')\n        ])\n\n    def test_option_leading_trailing_spaces(self):\n        self.assertEqual(self.get_tokens('  rm  -o    --json   '), [\n            (Keyword, 'rm'),\n            (Name, '-o'),\n            (String, '--json')\n        ])\n\n    def test_invalid_type(self):\n        self.assertEqual(self.get_tokens('rm -a foo'), [\n            (Keyword, 'rm'),\n            (Error, '-'), (Error, 'a'),\n            (Error, 'f'), (Error, 'o'), (Error, 'o')\n        ])\n\n\nclass TestLexer_help(LexerTestCase):\n\n    def test_help_simple(self):\n        self.assertEqual(self.get_tokens('help'), [\n            (Keyword, 'help')\n        ])\n\n    def test_help_with_spaces(self):\n        self.assertEqual(self.get_tokens('  help   '), [\n            (Keyword, 'help')\n        ])\n\n\nclass TestLexer_source(LexerTestCase):\n\n    def test_source_simple_filename(self):\n        self.assertEqual(self.get_tokens('source file.txt'), [\n            (Keyword, 'source'), (String, 'file.txt')\n        ])\n\n    def test_source_with_spaces(self):\n        self.assertEqual(self.get_tokens('  source    file.txt    '), [\n            (Keyword, 'source'), (String, 'file.txt')\n        ])\n\n    def test_source_quoted_filename(self):\n        self.assertEqual(self.get_tokens(\"source '/tmp/my file.txt'\"), [\n            (Keyword, 'source'),\n            (Text, \"'\"), (String, '/tmp/my file.txt'), (Text, \"'\")\n        ])\n\n    def test_source_escaped_filename(self):\n        self.assertEqual(self.get_tokens(r\"source /tmp/my\\ file.txt\"), [\n            (Keyword, 'source'), (String, r'/tmp/my\\ file.txt')\n        ])\n\n\nclass TestLexer_exec(LexerTestCase):\n\n    def test_exec_simple_filename(self):\n        self.assertEqual(self.get_tokens('exec file.txt'), [\n            (Keyword, 'exec'), (String, 'file.txt')\n        ])\n\n    def test_exec_with_spaces(self):\n        self.assertEqual(self.get_tokens('  exec    file.txt    '), [\n            (Keyword, 'exec'), (String, 'file.txt')\n        ])\n\n    def test_exec_quoted_filename(self):\n        self.assertEqual(self.get_tokens(\"exec '/tmp/my file.txt'\"), [\n            (Keyword, 'exec'),\n            (Text, \"'\"), (String, '/tmp/my file.txt'), (Text, \"'\")\n        ])\n\n    def test_exec_escaped_filename(self):\n        self.assertEqual(self.get_tokens(r\"exec /tmp/my\\ file.txt\"), [\n            (Keyword, 'exec'), (String, r'/tmp/my\\ file.txt')\n        ])\n\n\nclass TestLexer_exit(LexerTestCase):\n\n    def test_exit_simple(self):\n        self.assertEqual(self.get_tokens('exit'), [\n            (Keyword, 'exit')\n        ])\n\n    def test_exit_with_spaces(self):\n        self.assertEqual(self.get_tokens('  exit   '), [\n            (Keyword, 'exit')\n        ])\n\n\nclass TestLexerPreview(LexerTestCase):\n\n    def test_httpie_without_action(self):\n        cmd = 'httpie http://example.com name=jack'\n        self.assertEqual(self.get_tokens(cmd), [\n            (Keyword, 'httpie'),\n            (String, 'http://example.com'),\n            (Name, 'name'), (Operator, '='), (String, 'jack')\n        ])\n\n    def test_httpie_without_action_and_url(self):\n        cmd = 'httpie name=jack Accept:*/*'\n        self.assertEqual(self.get_tokens(cmd), [\n            (Keyword, 'httpie'),\n            (Name, 'name'), (Operator, '='), (String, 'jack'),\n            (Name, 'Accept'), (Operator, ':'), (String, '*/*')\n        ])\n\n    def test_httpie_absolute_url(self):\n        cmd = 'httpie post http://example.com name=jack'\n        self.assertEqual(self.get_tokens(cmd), [\n            (Keyword, 'httpie'), (Keyword, 'post'),\n            (String, 'http://example.com'),\n            (Name, 'name'), (Operator, '='), (String, 'jack')\n        ])\n\n    def test_httpie_option_first(self):\n        self.assertEqual(self.get_tokens('httpie post --form name=jack'), [\n            (Keyword, 'httpie'), (Keyword, 'post'),\n            (Name, '--form'),\n            (Name, 'name'), (Operator, '='), (String, 'jack')\n        ])\n\n    def test_httpie_body_param_first(self):\n        self.assertEqual(self.get_tokens('httpie post name=jack --form'), [\n            (Keyword, 'httpie'), (Keyword, 'post'),\n            (Name, 'name'), (Operator, '='), (String, 'jack'),\n            (Name, '--form')\n        ])\n\n    def test_httpie_options(self):\n        self.assertEqual(self.get_tokens('httpie options test --body'), [\n            (Keyword, 'httpie'), (Keyword, 'options'),\n            (String, 'test'), (Name, '--body')\n        ])\n\n    def test_httpie_relative_path(self):\n        tokens = self.get_tokens('httpie /api/test name==foo',\n                                 filter_spaces=False)\n        self.assertEqual(tokens, [\n            (Keyword, 'httpie'), (Text, ' '),\n            (String, '/api/test'), (Text, ' '),\n            (Name, 'name'), (Operator, '=='), (String, 'foo'),\n            (Text, '\\n')\n        ])\n\n\nclass TestShellCode(LexerTestCase):\n\n    def test_unquoted_querystring(self):\n        self.assertEqual(self.get_tokens('`echo name`==john'), [\n            (Text, '`'),\n            (Name.Builtin, 'echo'),\n            (Text, 'name'),\n            (Text, '`'),\n            (Operator, '=='),\n            (String, 'john')\n        ])\n        self.assertEqual(self.get_tokens('name==`echo john`'), [\n            (Name, 'name'),\n            (Operator, '=='),\n            (Text, '`'),\n            (Name.Builtin, 'echo'),\n            (Text, 'john'),\n            (Text, '`')\n        ])\n\n    def test_unquoted_bodystring(self):\n        self.assertEqual(self.get_tokens('`echo name`=john'), [\n            (Text, '`'),\n            (Name.Builtin, 'echo'),\n            (Text, 'name'),\n            (Text, '`'),\n            (Operator, '='),\n            (String, 'john')\n        ])\n        self.assertEqual(self.get_tokens('name=`echo john`'), [\n            (Name, 'name'),\n            (Operator, '='),\n            (Text, '`'),\n            (Name.Builtin, 'echo'),\n            (Text, 'john'),\n            (Text, '`')\n        ])\n\n    def test_header_option_value(self):\n        self.assertEqual(self.get_tokens('Accept:`echo \"application/json\"`'), [\n            (Name, 'Accept'),\n            (Operator, ':'),\n            (Text, '`'),\n            (Name.Builtin, 'echo'),\n            (String.Double, '\"application/json\"'),\n            (Text, '`'),\n        ])\n\n    def test_httpie_body_param(self):\n        self.assertEqual(self.get_tokens('httpie post name=`echo john`'), [\n            (Keyword, 'httpie'),\n            (Keyword, 'post'),\n            (Name, 'name'),\n            (Operator, '='),\n            (Text, '`'),\n            (Name.Builtin, 'echo'),\n            (Text, 'john'),\n            (Text, '`'),\n        ])\n\n    def test_httpie_post_pipe(self):\n        self.assertEqual(self.get_tokens('httpie post | tee \"/tmp/test\"'), [\n            (Keyword, 'httpie'),\n            (Keyword, 'post'),\n            (Operator, '|'),\n            (Text, 'tee'),\n            (String.Double, '\"/tmp/test\"'),\n        ])\n\n    def test_post_pipe(self):\n        self.assertEqual(self.get_tokens('post | tee \"/tmp/test\"'), [\n            (Keyword, 'post'),\n            (Operator, '|'),\n            (Text, 'tee'),\n            (String.Double, '\"/tmp/test\"'),\n        ])\n\n\nclass TestLexerPreviewRedirection(LexerTestCase):\n\n    def test_httpie_write(self):\n        self.assertEqual(self.get_tokens('httpie > file.txt'), [\n            (Keyword, 'httpie'),\n            (Operator, '>'), (String, 'file.txt')\n        ])\n\n    def test_httpie_write_without_spaces(self):\n        self.assertEqual(self.get_tokens('httpie>file.txt'), [\n            (Keyword, 'httpie'),\n            (Operator, '>'), (String, 'file.txt')\n        ])\n\n    def test_httpie_append(self):\n        self.assertEqual(self.get_tokens('httpie >> file.txt'), [\n            (Keyword, 'httpie'),\n            (Operator, '>>'), (String, 'file.txt')\n        ])\n\n    def test_httpie_append_without_spaces(self):\n        self.assertEqual(self.get_tokens('httpie>>file.txt'), [\n            (Keyword, 'httpie'),\n            (Operator, '>>'), (String, 'file.txt')\n        ])\n\n    def test_httpie_write_with_post_param(self):\n        self.assertEqual(self.get_tokens('httpie post name=jack > file.txt'), [\n            (Keyword, 'httpie'), (Keyword, 'post'),\n            (Name, 'name'), (Operator, '='), (String, 'jack'),\n            (Operator, '>'), (String, 'file.txt')\n        ])\n\n    def test_httpie_append_with_post_param(self):\n        self.assertEqual(self.get_tokens('httpie post name=doe >> file.txt'), [\n            (Keyword, 'httpie'), (Keyword, 'post'),\n            (Name, 'name'), (Operator, '='), (String, 'doe'),\n            (Operator, '>>'), (String, 'file.txt')\n        ])\n\n    def test_httpie_write_quoted_filename(self):\n        self.assertEqual(self.get_tokens(\"httpie > 'my file.txt'\"), [\n            (Keyword, 'httpie'), (Operator, '>'),\n            (Text, \"'\"), (String, 'my file.txt'), (Text, \"'\")\n        ])\n\n    def test_httpie_append_quoted_filename(self):\n        self.assertEqual(self.get_tokens('httpie >> \"my file.txt\"'), [\n            (Keyword, 'httpie'), (Operator, '>>'),\n            (Text, '\"'), (String, 'my file.txt'), (Text, '\"')\n        ])\n\n    def test_httpie_append_with_many_params(self):\n        command = (\"httpie post --auth user:pass --verify=no  \"\n                   \"name='john doe'  page==2 >> file.txt\")\n        self.assertEqual(self.get_tokens(command), [\n            (Keyword, 'httpie'), (Keyword, 'post'),\n            (Name, '--auth'), (String, 'user:pass'),\n            (Name, '--verify'), (Operator, '='), (String, 'no'),\n            (Name, 'name'), (Operator, '='),\n            (Text, \"'\"), (String, 'john doe'), (Text, \"'\"),\n            (Name, 'page'), (Operator, '=='), (String, '2'),\n            (Operator, '>>'), (String, 'file.txt')\n        ])\n\n    def test_curl_write(self):\n        self.assertEqual(self.get_tokens('curl > file.txt'), [\n            (Keyword, 'curl'),\n            (Operator, '>'), (String, 'file.txt')\n        ])\n\n    def test_curl_write_without_spaces(self):\n        self.assertEqual(self.get_tokens('curl>file.txt'), [\n            (Keyword, 'curl'),\n            (Operator, '>'), (String, 'file.txt')\n        ])\n\n    def test_curl_append(self):\n        self.assertEqual(self.get_tokens('curl >> file.txt'), [\n            (Keyword, 'curl'),\n            (Operator, '>>'), (String, 'file.txt')\n        ])\n\n    def test_curl_append_without_spaces(self):\n        self.assertEqual(self.get_tokens('curl>>file.txt'), [\n            (Keyword, 'curl'),\n            (Operator, '>>'), (String, 'file.txt')\n        ])\n\n    def test_curl_write_with_post_param(self):\n        self.assertEqual(self.get_tokens('curl post name=jack > file.txt'), [\n            (Keyword, 'curl'), (Keyword, 'post'),\n            (Name, 'name'), (Operator, '='), (String, 'jack'),\n            (Operator, '>'), (String, 'file.txt')\n        ])\n\n    def test_curl_append_with_post_param(self):\n        self.assertEqual(self.get_tokens('curl post name=doe >> file.txt'), [\n            (Keyword, 'curl'), (Keyword, 'post'),\n            (Name, 'name'), (Operator, '='), (String, 'doe'),\n            (Operator, '>>'), (String, 'file.txt')\n        ])\n\n    def test_curl_write_quoted_filename(self):\n        self.assertEqual(self.get_tokens(\"curl > 'my file.txt'\"), [\n            (Keyword, 'curl'), (Operator, '>'),\n            (Text, \"'\"), (String, 'my file.txt'), (Text, \"'\")\n        ])\n\n    def test_curl_append_quoted_filename(self):\n        self.assertEqual(self.get_tokens('curl >> \"my file.txt\"'), [\n            (Keyword, 'curl'), (Operator, '>>'),\n            (Text, '\"'), (String, 'my file.txt'), (Text, '\"')\n        ])\n\n    def test_curl_append_with_many_params(self):\n        command = (\"curl post --auth user:pass --verify=no  \"\n                   \"name='john doe'  page==2 >> file.txt\")\n        self.assertEqual(self.get_tokens(command), [\n            (Keyword, 'curl'), (Keyword, 'post'),\n            (Name, '--auth'), (String, 'user:pass'),\n            (Name, '--verify'), (Operator, '='), (String, 'no'),\n            (Name, 'name'), (Operator, '='),\n            (Text, \"'\"), (String, 'john doe'), (Text, \"'\"),\n            (Name, 'page'), (Operator, '=='), (String, '2'),\n            (Operator, '>>'), (String, 'file.txt')\n        ])\n\n\nclass TestLexerAction(LexerTestCase):\n\n    def test_get(self):\n        self.assertEqual(self.get_tokens('get'), [\n            (Keyword, 'get')\n        ])\n\n    def test_post_with_spaces(self):\n        self.assertEqual(self.get_tokens('   post  '), [\n            (Keyword, 'post')\n        ])\n\n    def test_capital_head(self):\n        self.assertEqual(self.get_tokens('HEAD'), [\n            (Keyword, 'HEAD')\n        ])\n\n    def test_delete_random_capitals(self):\n        self.assertEqual(self.get_tokens('dElETe'), [\n            (Keyword, 'dElETe')\n        ])\n\n    def test_patch(self):\n        self.assertEqual(self.get_tokens('patch'), [\n            (Keyword, 'patch')\n        ])\n\n    def test_get_with_querystring_params(self):\n        command = 'get page==10 id==200'\n        self.assertEqual(self.get_tokens(command), [\n            (Keyword, 'get'),\n            (Name, 'page'), (Operator, '=='), (String, '10'),\n            (Name, 'id'), (Operator, '=='), (String, '200')\n        ])\n\n    def test_capital_get_with_querystring_params(self):\n        command = 'GET page==10 id==200'\n        self.assertEqual(self.get_tokens(command), [\n            (Keyword, 'GET'),\n            (Name, 'page'), (Operator, '=='), (String, '10'),\n            (Name, 'id'), (Operator, '=='), (String, '200')\n        ])\n\n    def test_post_with_body_params(self):\n        command = 'post name=\"john doe\" username=john'\n        self.assertEqual(self.get_tokens(command), [\n            (Keyword, 'post'), (Name, 'name'), (Operator, '='),\n            (Text, '\"'), (String, 'john doe'), (Text, '\"'),\n            (Name, 'username'), (Operator, '='), (String, 'john')\n        ])\n\n    def test_post_with_spaces_and_body_params(self):\n        command = '  post   name=\"john doe\"     username=john  '\n        self.assertEqual(self.get_tokens(command), [\n            (Keyword, 'post'), (Name, 'name'), (Operator, '='),\n            (Text, '\"'), (String, 'john doe'), (Text, '\"'),\n            (Name, 'username'), (Operator, '='), (String, 'john')\n        ])\n\n    def test_options(self):\n        self.assertEqual(self.get_tokens('options'), [\n            (Keyword, 'options')\n        ])\n\n    def test_post_relative_path(self):\n        tokens = self.get_tokens('post /api/test name=foo',\n                                 filter_spaces=False)\n        self.assertEqual(tokens, [\n            (Keyword, 'post'), (Text, ' '),\n            (String, '/api/test'), (Text, ' '),\n            (Name, 'name'), (Operator, '='), (String, 'foo'),\n            (Text, '\\n')\n        ])\n\n\nclass TestLexerActionRedirection(LexerTestCase):\n\n    def test_get_write(self):\n        self.assertEqual(self.get_tokens('get > file.txt'), [\n            (Keyword, 'get'), (Operator, '>'), (String, 'file.txt')\n        ])\n\n    def test_get_write_quoted_filename(self):\n        self.assertEqual(self.get_tokens('get > \"/tmp/my file.txt\"'), [\n            (Keyword, 'get'), (Operator, '>'),\n            (Text, '\"'), (String, '/tmp/my file.txt'), (Text, '\"')\n        ])\n\n    def test_get_append(self):\n        self.assertEqual(self.get_tokens('get >> file.txt'), [\n            (Keyword, 'get'), (Operator, '>>'), (String, 'file.txt')\n        ])\n\n    def test_get_append_escaped_filename(self):\n        self.assertEqual(self.get_tokens(r'get >> /tmp/my\\ file.txt'), [\n            (Keyword, 'get'), (Operator, '>>'),\n            (String, r'/tmp/my\\ file.txt')\n        ])\n\n    def test_post_append_with_spaces(self):\n        self.assertEqual(self.get_tokens('   post  >>   file.txt'), [\n            (Keyword, 'post'), (Operator, '>>'), (String, 'file.txt')\n        ])\n\n    def test_capital_head_write(self):\n        self.assertEqual(self.get_tokens('HEAD > file.txt'), [\n            (Keyword, 'HEAD'), (Operator, '>'), (String, 'file.txt')\n        ])\n\n    def test_get_append_with_querystring_params(self):\n        command = 'get page==10 id==200 >> /tmp/file.txt'\n        self.assertEqual(self.get_tokens(command), [\n            (Keyword, 'get'),\n            (Name, 'page'), (Operator, '=='), (String, '10'),\n            (Name, 'id'), (Operator, '=='), (String, '200'),\n            (Operator, '>>'), (String, '/tmp/file.txt')\n        ])\n\n    def test_post_write_escaped_filename_with_body_params(self):\n        command = r'post name=\"john doe\" username=john > /tmp/my\\ file.txt'\n        self.assertEqual(self.get_tokens(command), [\n            (Keyword, 'post'), (Name, 'name'), (Operator, '='),\n            (Text, '\"'), (String, 'john doe'), (Text, '\"'),\n            (Name, 'username'), (Operator, '='), (String, 'john'),\n            (Operator, '>'), (String, r'/tmp/my\\ file.txt')\n        ])\n\n    def test_post_append_with_spaces_and_body_params(self):\n        command = ' post    name=\"john doe\"  username=john  >> /tmp/file.txt  '\n        self.assertEqual(self.get_tokens(command), [\n            (Keyword, 'post'), (Name, 'name'), (Operator, '='),\n            (Text, '\"'), (String, 'john doe'), (Text, '\"'),\n            (Name, 'username'), (Operator, '='), (String, 'john'),\n            (Operator, '>>'), (String, '/tmp/file.txt')\n        ])\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: http-prompt\nTest File Path: http-prompt\\test_tree\\test_tree.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"Tree data structure for ls command to work with OpenAPI specification.\"\"\"\n\n\nclass Node(object):\n\n    def __init__(self, name, data=None, parent=None):\n        if name in ('.', '..'):\n            raise ValueError(\"name cannot be '.' or '..'\")\n\n        self.name = name\n        self.data = data or {}\n        self.parent = parent\n        self.children = set()\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        return \"Node('{}', '{}')\".format(self.name, self.data.get('type'))\n\n    def __lt__(self, other):\n        ta = self.data.get('type')\n        tb = other.data.get('type')\n        if ta != tb:\n            return ta < tb\n        return self.name < other.name\n\n    def __eq__(self, other):\n        return self.name == other.name and self.data == other.data\n\n    def __hash__(self):\n        return hash((self.name, self.data.get('type')))\n\n    def add_path(self, *path, **kwargs):\n        node_type = kwargs.get('node_type', 'dir')\n        name = path[0]\n        tail = path[1:]\n        child = self.find_child(name, wildcard=False)\n        if not child:\n            data = {'type': 'dir' if tail else node_type}\n            child = Node(name, data=data, parent=self)\n            self.children.add(child)\n\n        if tail:\n            child.add_path(*tail, node_type=node_type)\n\n    def find_child(self, name, wildcard=True):\n        for child in self.children:\n            if child.name == name:\n                return child\n\n        # Attempt to match wildcard like /users/{user_id}\n        if wildcard:\n            for child in self.children:\n                if child.name.startswith('{') and child.name.endswith('}'):\n                    return child\n\n        return None\n\n    def ls(self, *path):\n        success = True\n        cur = self\n        for name in path:\n            if not name or name == '.':\n                continue\n            elif name == '..':\n                if cur.parent:\n                    cur = cur.parent\n            else:\n                child = cur.find_child(name)\n                if child:\n                    cur = child\n                else:\n                    success = False\n                    break\n        if success:\n            for node in sorted(cur.children):\n                yield node\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:16:47"}, {"role": "assistant", "content": "```python\nimport unittest\n\nfrom http_prompt.tree import Node\n\n\nclass TestNode(unittest.TestCase):\n\n    def setUp(self):\n        # Make a tree like this:\n        #          root\n        #     a             h\n        #  b     d        i   n\n        # c f   e g     k     o\n        #             l m p\n        self.root = Node('root')\n        self.root.add_path('a', 'b', 'c')\n        self.root.add_path('a', 'b', 'f')\n        self.root.add_path('a', 'd', 'e')\n        self.root.add_path('a', 'd', 'g')\n        self.root.add_path('h', 'i', 'k', 'l')\n        self.root.add_path('h', 'i', 'k', 'm')\n        self.root.add_path('h', 'i', 'k', 'p')\n        self.root.add_path('h', 'n', 'o')\n\n    def test_illegal_name(self):\n        self.assertRaises(ValueError, Node, '.')\n        self.assertRaises(ValueError, Node, '..')\n\n    def test_str(self):\n        node = Node('my node')\n        self.assertEqual(str(node), 'my node')\n\n    def test_cmp_same_type(self):\n        a = Node('a', data={'type': 'dir'})\n        b = Node('b', data={'type': 'dir'})\n        self.assertTrue(a < b)\n\n    def test_cmp_different_type(self):\n        a = Node('a', data={'type': 'file'})\n        b = Node('b', data={'type': 'dir'})\n        self.assertTrue(b < a)\n\n    def test_eq(self):\n        a = Node('a', data={'type': 'file'})\n        b = Node('b', data={'type': 'dir'})\n        self.assertNotEqual(a, b)\n\n        a = Node('a', data={'type': 'file'})\n        b = Node('a', data={'type': 'file'})\n        self.assertEqual(a, b)\n\n    def test_add_path_and_find_child(self):\n        # Level 1 (root)\n        self.assertEqual(set(c.name for c in self.root.children), set('ah'))\n\n        # Level 2\n        node_a = self.root.find_child('a')\n        node_h = self.root.find_child('h')\n        self.assertEqual(set(c.name for c in node_a.children), set('bd'))\n        self.assertEqual(set(c.name for c in node_h.children), set('in'))\n\n        # Level 3\n        node_b = node_a.find_child('b')\n        node_i = node_h.find_child('i')\n        self.assertEqual(set(c.name for c in node_b.children), set('cf'))\n        self.assertEqual(set(c.name for c in node_i.children), set('k'))\n\n        # Level 4\n        node_c = node_b.find_child('c')\n        node_k = node_i.find_child('k')\n        self.assertEqual(set(c.name for c in node_c.children), set())\n        self.assertEqual(set(c.name for c in node_k.children), set('lmp'))\n\n        # Return None if child can't be found\n        self.assertFalse(node_c.find_child('x'))\n\n    def test_find_child_wildcard(self):\n        root = Node('root')\n        root.add_path('a')\n        root.add_path('{b}')\n        root.add_path('c')\n\n        self.assertEqual(root.find_child('a').name, 'a')\n        self.assertEqual(root.find_child('c').name, 'c')\n        self.assertEqual(root.find_child('x').name, '{b}')\n        self.assertFalse(root.find_child('x', wildcard=False))\n\n    def test_ls(self):\n        self.assertEqual([n.name for n in self.root.ls('a')], list('bd'))\n        self.assertEqual([n.name for n in self.root.ls('a', 'b')], list('cf'))\n        self.assertEqual([n.name for n in self.root.ls('a', 'b', 'c')], [])\n        self.assertEqual([n.name for n in self.root.ls('h', 'i', 'k')],\n                         list('lmp'))\n\n    def test_ls_root(self):\n        self.assertEqual([n.name for n in self.root.ls()], list('ah'))\n\n    def test_ls_non_existing(self):\n        self.assertEqual([n.name for n in self.root.ls('x')], [])\n        self.assertEqual([n.name for n in self.root.ls('a', 'b', 'x')], [])\n\n    def test_ls_parent(self):\n        self.assertEqual([n.name for n in self.root.ls('..')], list('ah'))\n        self.assertEqual([n.name for n in self.root.ls('..', '..', '..')],\n                         list('ah'))\n        self.assertEqual([n.name for n in self.root.ls('..', '..', 'h')],\n                         list('in'))\n        self.assertEqual(\n            [n.name for n in self.root.ls('..', '..', 'h', '..', 'a')],\n            list('bd'))\n\n    def test_ls_dot(self):\n        self.assertEqual([n.name for n in self.root.ls('.')], list('ah'))\n        self.assertEqual([n.name for n in self.root.ls('.', '.', '.')],\n                         list('ah'))\n        self.assertEqual([n.name for n in self.root.ls('.', 'a', 'b')],\n                         list('cf'))\n        self.assertEqual([n.name for n in self.root.ls('.', 'h', '.')],\n                         list('in'))\n        self.assertEqual(\n            [n.name for n in self.root.ls('.', 'h', '.', '.', 'n')], ['o'])\n\n    def test_ls_sort_by_types(self):\n        self.root.add_path('q', 'r')\n        self.root.add_path('q', 's', node_type='file')\n        self.root.add_path('q', 't', node_type='file')\n        self.root.add_path('q', 'u')\n        self.root.add_path('q', 'v', node_type='file')\n\n        self.assertEqual([n.name for n in self.root.ls('q')],\n                         list('rustv'))\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: http-prompt\nTest File Path: http-prompt\\test_xdg\\test_xdg.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"XDG Base Directory Specification.\n\nSee:\n    https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html\n    https://github.com/ActiveState/appdirs\n\"\"\"\nimport os\nimport sys\n\nfrom functools import partial\n\n\ndef _get_dir(envvar_name, default_dir, resource_name=None):\n    base_dir = os.getenv(envvar_name) or default_dir\n    app_dir = os.path.join(base_dir, 'http-prompt')\n    if not os.path.exists(app_dir):\n        os.makedirs(app_dir, mode=0o700)\n\n    if resource_name:\n        app_dir = os.path.join(app_dir, resource_name)\n        if not os.path.exists(app_dir):\n            os.mkdir(app_dir)\n\n    return app_dir\n\n\nif sys.platform == 'win32':  # nocover\n    # NOTE: LOCALAPPDATA is not available on Windows XP\n    get_data_dir = partial(_get_dir, 'LOCALAPPDATA',\n                           os.path.expanduser('~/AppData/Local'))\n    get_config_dir = partial(_get_dir, 'LOCALAPPDATA',\n                             os.path.expanduser('~/AppData/Local'))\nelse:\n    get_data_dir = partial(_get_dir, 'XDG_DATA_HOME',\n                           os.path.expanduser('~/.local/share'))\n    get_config_dir = partial(_get_dir, 'XDG_CONFIG_HOME',\n                             os.path.expanduser('~/.config'))\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:16:47"}, {"role": "assistant", "content": "```python\nimport os\nimport stat\nimport sys\n\nfrom .base import TempAppDirTestCase\nfrom http_prompt import xdg\n\n\nclass TestXDG(TempAppDirTestCase):\n\n    def test_get_app_data_home(self):\n        path = xdg.get_data_dir()\n        expected_path = os.path.join(os.environ[self.homes['data']],\n                                     'http-prompt')\n        self.assertEqual(path, expected_path)\n        self.assertTrue(os.path.exists(path))\n\n        if sys.platform != 'win32':\n            # Make sure permission for the directory is 700\n            mask = stat.S_IMODE(os.stat(path).st_mode)\n            self.assertTrue(mask & stat.S_IRWXU)\n            self.assertFalse(mask & stat.S_IRWXG)\n            self.assertFalse(mask & stat.S_IRWXO)\n\n    def test_get_app_config_home(self):\n        path = xdg.get_config_dir()\n        expected_path = os.path.join(os.environ[self.homes['config']],\n                                     'http-prompt')\n        self.assertEqual(path, expected_path)\n        self.assertTrue(os.path.exists(path))\n\n        if sys.platform != 'win32':\n            # Make sure permission for the directory is 700\n            mask = stat.S_IMODE(os.stat(path).st_mode)\n            self.assertTrue(mask & stat.S_IRWXU)\n            self.assertFalse(mask & stat.S_IRWXG)\n            self.assertFalse(mask & stat.S_IRWXO)\n\n    def test_get_resource_data_dir(self):\n        path = xdg.get_data_dir('something')\n        expected_path = os.path.join(\n            os.environ[self.homes['data']], 'http-prompt', 'something')\n        self.assertEqual(path, expected_path)\n        self.assertTrue(os.path.exists(path))\n\n        # Make sure we can write a file to the directory\n        with open(os.path.join(path, 'test'), 'wb') as f:\n            f.write(b'hello')\n\n    def test_get_resource_config_dir(self):\n        path = xdg.get_config_dir('something')\n        expected_path = os.path.join(\n            os.environ[self.homes['config']], 'http-prompt', 'something')\n        self.assertEqual(path, expected_path)\n        self.assertTrue(os.path.exists(path))\n\n        # Make sure we can write a file to the directory\n        with open(os.path.join(path, 'test'), 'wb') as f:\n            f.write(b'hello')\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: distributed\nTest File Path: distributed\\test_asyncio\\test_asyncio.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport asyncio\nfrom collections.abc import AsyncIterator\nfrom contextlib import asynccontextmanager\n\n\nclass RLock:\n    \"\"\"asyncio reentrant lock, which allows the same owner (or two owners that compare\n    as equal) to be inside the critical section at the same time.\n\n    Note that here the owner is an explicit, generic object, whereas in\n    :func:`threading.RLock` and :func:`multiprocessing.RLock` it's hardcoded\n    respectively to the thread ID and process ID.\n\n    **Usage**::\n\n        lock = RLock()\n        async with lock(\"my-owner\"):\n            ...\n\n    **Tip**\n\n    You can mix reentrant and non-reentrant owners; all you need to do is create an\n    owner that doesn't compare as equal to other instances of itself::\n\n        lock = RLock()\n\n        async def non_reentrant():\n            async with lock(object()):\n                ...\n\n        async def reentrant():\n            async with lock(\"foo\"):\n                ...\n\n    In the above example, at any time you may have inside the critical section\n    at most one call to ``non_reentrant`` and no calls to ``reentrant``, or any number\n    of calls to ``reentrant`` but no calls to ``non_reentrant``.\n    \"\"\"\n\n    _owner: object\n    _count: int\n    _lock: asyncio.Lock\n\n    def __init__(self):\n        self._owner = None\n        self._count = 0\n        self._lock = asyncio.Lock()\n\n    async def acquire(self, owner: object) -> None:\n        if self._count == 0 or self._owner != owner:\n            await self._lock.acquire()\n            self._owner = owner\n        self._count += 1\n\n    def release(self, owner: object) -> None:\n        if self._count == 0 or self._owner != owner:\n            raise RuntimeError(\"release unlocked lock or mismatched owner\")\n        self._count -= 1\n        if self._count == 0:\n            self._owner = None\n            self._lock.release()\n\n    @asynccontextmanager\n    async def __call__(self, owner: object) -> AsyncIterator[None]:\n        await self.acquire(owner)\n        try:\n            yield\n        finally:\n            self.release(owner)\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:18:01"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport asyncio\n\nimport pytest\n\nfrom distributed._asyncio import RLock\nfrom distributed.utils_test import gen_test\n\n\n@gen_test()\nasync def test_rlock():\n    l = RLock()\n\n    # Owners are treated as reentrant as long as they compare as equal.\n    # Make sure that the comparison uses '==' and not 'is'.\n    x1 = [\"x\"]\n    x2 = [\"x\"]\n    assert x1 is not x2, \"internalized objects\"\n\n    async with l(x1), l(x2):\n        assert l._owner == [\"x\"]\n        assert l._count == 2\n\n        fut = asyncio.create_task(l.acquire(\"y\"))\n        await asyncio.sleep(0.01)\n        assert not fut.done()\n\n        with pytest.raises(RuntimeError):\n            l.release(\"z\")\n\n    await fut\n    l.release(\"y\")\n\n    with pytest.raises(RuntimeError):\n        l.release(\"w\")\n\n    assert l._owner is None\n    assert l._count == 0\n    assert not l._lock.locked()\n\n\n@gen_test()\nasync def test_rlock_nonreentrant():\n    \"\"\"Use the RLock with non-reentrant owners\"\"\"\n    l = RLock()\n    x = object()\n    y = object()\n    assert x != y\n\n    async with l(x):\n        assert l._owner is x\n        fut = asyncio.create_task(l.acquire(y))\n        await asyncio.sleep(0.01)\n        assert not fut.done()\n    await fut\n    assert l._owner is y\n    l.release(y)\n\n    assert l._owner is None\n    assert l._count == 0\n    assert not l._lock.locked()\n\n\n@gen_test()\nasync def test_rlock_none():\n    \"\"\"None is a valid owner\"\"\"\n    l = RLock()\n\n    async with l(None), l(None):\n        assert l._owner is None\n        assert l._count == 2\n        assert l._lock.locked()\n\n        fut = asyncio.create_task(l.acquire(\"x\"))\n        await asyncio.sleep(0.01)\n        assert not fut.done()\n\n    await fut\n    assert l._owner == \"x\"\n    l.release(\"x\")\n\n    assert l._owner is None\n    assert l._count == 0\n    assert not l._lock.locked()\n\n\n@gen_test()\nasync def test_rlock_release_on_raise():\n    \"\"\"None is a valid owner\"\"\"\n    l = RLock()\n    with pytest.raises(ZeroDivisionError):\n        async with l(\"x\"):\n            1 / 0\n\n    assert l._owner is None\n    assert l._count == 0\n    assert not l._lock.locked()\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: distributed\nTest File Path: distributed\\test_async_task_group\\test_async_task_group.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport asyncio\nimport threading\nfrom collections.abc import Callable, Coroutine\nfrom typing import TYPE_CHECKING, Any, TypeVar\n\nif TYPE_CHECKING:\n    from typing_extensions import ParamSpec\n\n    P = ParamSpec(\"P\")\n    R = TypeVar(\"R\")\n    T = TypeVar(\"T\")\n    Coro = Coroutine[Any, Any, T]\n\n\nclass _LoopBoundMixin:\n    \"\"\"Backport of the private asyncio.mixins._LoopBoundMixin from 3.11\"\"\"\n\n    _global_lock = threading.Lock()\n\n    _loop = None\n\n    def _get_loop(self):\n        loop = asyncio.get_running_loop()\n\n        if self._loop is None:\n            with self._global_lock:\n                if self._loop is None:\n                    self._loop = loop\n        if loop is not self._loop:\n            raise RuntimeError(f\"{self!r} is bound to a different event loop\")\n        return loop\n\n\nclass AsyncTaskGroupClosedError(RuntimeError):\n    pass\n\n\ndef _delayed(corofunc: Callable[P, Coro[T]], delay: float) -> Callable[P, Coro[T]]:\n    \"\"\"Decorator to delay the evaluation of a coroutine function by the given delay in seconds.\"\"\"\n\n    async def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\n        await asyncio.sleep(delay)\n        return await corofunc(*args, **kwargs)\n\n    return wrapper\n\n\nclass AsyncTaskGroup(_LoopBoundMixin):\n    \"\"\"Collection tracking all currently running asynchronous tasks within a group\"\"\"\n\n    #: If True, the group is closed and does not allow adding new tasks.\n    closed: bool\n\n    def __init__(self) -> None:\n        self.closed = False\n        self._ongoing_tasks: set[asyncio.Task[None]] = set()\n\n    def call_soon(\n        self, afunc: Callable[P, Coro[None]], /, *args: P.args, **kwargs: P.kwargs\n    ) -> None:\n        \"\"\"Schedule a coroutine function to be executed as an `asyncio.Task`.\n\n        The coroutine function `afunc` is scheduled with `args` arguments and `kwargs` keyword arguments\n        as an `asyncio.Task`.\n\n        Parameters\n        ----------\n        afunc\n            Coroutine function to schedule.\n        *args\n            Arguments to be passed to `afunc`.\n        **kwargs\n            Keyword arguments to be passed to `afunc`\n\n        Returns\n        -------\n            None\n\n        Raises\n        ------\n        AsyncTaskGroupClosedError\n            If the task group is closed.\n        \"\"\"\n        if self.closed:  # Avoid creating a coroutine\n            raise AsyncTaskGroupClosedError(\n                \"Cannot schedule a new coroutine function as the group is already closed.\"\n            )\n        task = self._get_loop().create_task(afunc(*args, **kwargs))\n        task.add_done_callback(self._ongoing_tasks.remove)\n        self._ongoing_tasks.add(task)\n        return None\n\n    def call_later(\n        self,\n        delay: float,\n        afunc: Callable[P, Coro[None]],\n        /,\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -> None:\n        \"\"\"Schedule a coroutine function to be executed after `delay` seconds as an `asyncio.Task`.\n\n        The coroutine function `afunc` is scheduled with `args` arguments and `kwargs` keyword arguments\n        as an `asyncio.Task` that is executed after `delay` seconds.\n\n        Parameters\n        ----------\n        delay\n            Delay in seconds.\n        afunc\n            Coroutine function to schedule.\n        *args\n            Arguments to be passed to `afunc`.\n        **kwargs\n            Keyword arguments to be passed to `afunc`\n\n        Returns\n        -------\n            The None\n\n        Raises\n        ------\n        AsyncTaskGroupClosedError\n            If the task group is closed.\n        \"\"\"\n        self.call_soon(_delayed(afunc, delay), *args, **kwargs)\n\n    def close(self) -> None:\n        \"\"\"Closes the task group so that no new tasks can be scheduled.\n\n        Existing tasks continue to run.\n        \"\"\"\n        self.closed = True\n\n    async def stop(self) -> None:\n        \"\"\"Close the group and stop all currently running tasks.\n\n        Closes the task group and cancels all tasks. All tasks are cancelled\n        an additional time for each time this task is cancelled.\n        \"\"\"\n        self.close()\n\n        current_task = asyncio.current_task(self._get_loop())\n        err = None\n        while tasks_to_stop := (self._ongoing_tasks - {current_task}):\n            for task in tasks_to_stop:\n                task.cancel()\n            try:\n                await asyncio.wait(tasks_to_stop)\n            except asyncio.CancelledError as e:\n                err = e\n\n        if err is not None:\n            raise err\n\n    def __len__(self):\n        return len(self._ongoing_tasks)\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:18:01"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport asyncio\nimport time as timemod\n\nimport pytest\n\nfrom distributed._async_taskgroup import AsyncTaskGroup, AsyncTaskGroupClosedError\nfrom distributed.utils_test import gen_test\n\n\nasync def _wait_for_n_loop_cycles(n):\n    for _ in range(n):\n        await asyncio.sleep(0)\n\n\ndef test_async_task_group_initialization():\n    group = AsyncTaskGroup()\n    assert not group.closed\n    assert len(group) == 0\n\n\n@gen_test()\nasync def test_async_task_group_call_soon_executes_task_in_background():\n    group = AsyncTaskGroup()\n    ev = asyncio.Event()\n    flag = False\n\n    async def set_flag():\n        nonlocal flag\n        await ev.wait()\n        flag = True\n\n    assert group.call_soon(set_flag) is None\n    assert len(group) == 1\n    ev.set()\n    await _wait_for_n_loop_cycles(2)\n    assert len(group) == 0\n    assert flag\n\n\n@gen_test()\nasync def test_async_task_group_call_later_executes_delayed_task_in_background():\n    group = AsyncTaskGroup()\n    ev = asyncio.Event()\n\n    start = timemod.monotonic()\n    assert group.call_later(1, ev.set) is None\n    assert len(group) == 1\n    await ev.wait()\n    end = timemod.monotonic()\n    # the task must be removed in exactly 1 event loop cycle\n    await _wait_for_n_loop_cycles(2)\n    assert len(group) == 0\n    assert end - start > 1 - timemod.get_clock_info(\"monotonic\").resolution\n\n\ndef test_async_task_group_close_closes():\n    group = AsyncTaskGroup()\n    group.close()\n    assert group.closed\n\n    # Test idempotency\n    group.close()\n    assert group.closed\n\n\n@gen_test()\nasync def test_async_task_group_close_does_not_cancel_existing_tasks():\n    group = AsyncTaskGroup()\n\n    ev = asyncio.Event()\n    flag = False\n\n    async def set_flag():\n        nonlocal flag\n        await ev.wait()\n        flag = True\n        return None\n\n    assert group.call_soon(set_flag) is None\n\n    group.close()\n\n    assert len(group) == 1\n\n    ev.set()\n    await _wait_for_n_loop_cycles(2)\n    assert len(group) == 0\n\n\n@gen_test()\nasync def test_async_task_group_close_prohibits_new_tasks():\n    group = AsyncTaskGroup()\n    group.close()\n\n    ev = asyncio.Event()\n    flag = False\n\n    async def set_flag():\n        nonlocal flag\n        await ev.wait()\n        flag = True\n        return True\n\n    with pytest.raises(AsyncTaskGroupClosedError):\n        group.call_soon(set_flag)\n    assert len(group) == 0\n\n    with pytest.raises(AsyncTaskGroupClosedError):\n        group.call_later(1, set_flag)\n    assert len(group) == 0\n\n    await asyncio.sleep(0.01)\n    assert not flag\n\n\n@gen_test()\nasync def test_async_task_group_stop_disallows_shutdown():\n    group = AsyncTaskGroup()\n\n    task = None\n\n    async def set_flag():\n        nonlocal task\n        task = asyncio.current_task()\n\n    assert group.call_soon(set_flag) is None\n    assert len(group) == 1\n    # tasks are not given a grace period, and are not even allowed to start\n    # if the group is closed immediately\n    await group.stop()\n    assert task is None\n\n\n@gen_test()\nasync def test_async_task_group_stop_cancels_long_running():\n    group = AsyncTaskGroup()\n\n    task = None\n    flag = False\n    started = asyncio.Event()\n\n    async def set_flag():\n        nonlocal task\n        task = asyncio.current_task()\n        started.set()\n        await asyncio.sleep(10)\n        nonlocal flag\n        flag = True\n        return True\n\n    assert group.call_soon(set_flag) is None\n    assert len(group) == 1\n    await started.wait()\n    await group.stop()\n    assert task\n    assert task.cancelled()\n    assert not flag\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: distributed\nTest File Path: distributed\\test_batched\\test_batched.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport contextlib\nimport logging\nfrom collections import deque\nfrom typing import Any\n\nfrom tornado import gen, locks\nfrom tornado.ioloop import IOLoop\n\nimport dask\nfrom dask.utils import parse_timedelta\n\nfrom distributed.core import CommClosedError\nfrom distributed.metrics import time\n\nlogger = logging.getLogger(__name__)\n\n\nclass BatchedSend:\n    \"\"\"Batch messages in batches on a stream\n\n    This takes an IOStream and an interval (in ms) and ensures that we send no\n    more than one message every interval milliseconds.  We send lists of\n    messages.\n\n    Batching several messages at once helps performance when sending\n    a myriad of tiny messages.\n\n    Examples\n    --------\n    >>> stream = await connect(address)\n    >>> bstream = BatchedSend(interval='10 ms')\n    >>> bstream.start(stream)\n    >>> bstream.send('Hello,')\n    >>> bstream.send('world!')\n\n    On the other side, the recipient will get a message like the following::\n\n        ['Hello,', 'world!']\n    \"\"\"\n\n    # XXX why doesn't BatchedSend follow either the IOStream or Comm API?\n\n    def __init__(self, interval, loop=None, serializers=None):\n        # XXX is the loop arg useful?\n        self.loop = loop or IOLoop.current()\n        self.interval = parse_timedelta(interval, default=\"ms\")\n        self.waker = locks.Event()\n        self.stopped = locks.Event()\n        self.please_stop = False\n        self.buffer = []\n        self.comm = None\n        self.message_count = 0\n        self.batch_count = 0\n        self.byte_count = 0\n        self.next_deadline = None\n        self.recent_message_log = deque(\n            maxlen=dask.config.get(\"distributed.admin.low-level-log-length\")\n        )\n        self.serializers = serializers\n        self._consecutive_failures = 0\n\n    def start(self, comm):\n        self.comm = comm\n        self.loop.add_callback(self._background_send)\n\n    def closed(self):\n        return self.comm and self.comm.closed()\n\n    def __repr__(self):\n        if self.closed():\n            return \"<BatchedSend: closed>\"\n        else:\n            return \"<BatchedSend: %d in buffer>\" % len(self.buffer)\n\n    __str__ = __repr__\n\n    @gen.coroutine\n    def _background_send(self):\n        while not self.please_stop:\n            try:\n                yield self.waker.wait(self.next_deadline)\n                self.waker.clear()\n            except gen.TimeoutError:\n                pass\n            if not self.buffer:\n                # Nothing to send\n                self.next_deadline = None\n                continue\n            if self.next_deadline is not None and time() < self.next_deadline:\n                # Send interval not expired yet\n                continue\n            payload, self.buffer = self.buffer, []\n            self.batch_count += 1\n            self.next_deadline = time() + self.interval\n            try:\n                # NOTE: Since `BatchedSend` doesn't have a handle on the running\n                # `_background_send` coroutine, the only thing with a reference to this\n                # coroutine is the event loop itself. If the event loop stops while\n                # we're waiting on a `write`, the `_background_send` coroutine object\n                # may be garbage collected. If that happens, the `yield coro` will raise\n                # `GeneratorExit`. But because this is an old-school `gen.coroutine`,\n                # and we're using `yield` and not `await`, the `write` coroutine object\n                # will not actually have been awaited, and it will remain sitting around\n                # for someone to retrieve it. At interpreter exit, this will warn\n                # sommething like `RuntimeWarning: coroutine 'TCP.write' was never\n                # awaited`. By using the `closing` contextmanager, the `write` coroutine\n                # object is always cleaned up, even if `yield` raises `GeneratorExit`.\n                with contextlib.closing(\n                    self.comm.write(\n                        payload, serializers=self.serializers, on_error=\"raise\"\n                    )\n                ) as coro:\n                    nbytes = yield coro\n                if nbytes < 1e6:\n                    self.recent_message_log.append(payload)\n                else:\n                    self.recent_message_log.append(\"large-message\")\n                self.byte_count += nbytes\n            except CommClosedError:\n                logger.info(\"Batched Comm Closed %r\", self.comm, exc_info=True)\n                break\n            except Exception:\n                # We cannot safely retry self.comm.write, as we have no idea\n                # what (if anything) was actually written to the underlying stream.\n                # Re-writing messages could result in complete garbage (e.g. if a frame\n                # header has been written, but not the frame payload), therefore\n                # the only safe thing to do here is to abort the stream without\n                # any attempt to re-try `write`.\n                logger.exception(\"Error in batched write\")\n                break\n            finally:\n                payload = None  # lose ref\n        else:\n            # nobreak. We've been gracefully closed.\n            self.stopped.set()\n            return\n\n        # If we've reached here, it means `break` was hit above and\n        # there was an exception when using `comm`.\n        # We can't close gracefully via `.close()` since we can't send messages.\n        # So we just abort.\n        # This means that any messages in our buffer our lost.\n        # To propagate exceptions, we rely on subsequent `BatchedSend.send`\n        # calls to raise CommClosedErrors.\n        self.stopped.set()\n        self.abort()\n\n    def send(self, *msgs: Any) -> None:\n        \"\"\"Schedule a message for sending to the other side\n\n        This completes quickly and synchronously\n        \"\"\"\n        if self.comm is not None and self.comm.closed():\n            raise CommClosedError(f\"Comm {self.comm!r} already closed.\")\n\n        self.message_count += len(msgs)\n        self.buffer.extend(msgs)\n        # Avoid spurious wakeups if possible\n        if self.next_deadline is None:\n            self.waker.set()\n\n    @gen.coroutine\n    def close(self, timeout=None):\n        \"\"\"Flush existing messages and then close comm\n\n        If set, raises `tornado.util.TimeoutError` after a timeout.\n        \"\"\"\n        if self.comm is None:\n            return\n        self.please_stop = True\n        self.waker.set()\n        yield self.stopped.wait(timeout=timeout)\n        if not self.comm.closed():\n            try:\n                if self.buffer:\n                    self.buffer, payload = [], self.buffer\n                    # See note in `_background_send` for explanation of `closing`.\n                    with contextlib.closing(\n                        self.comm.write(\n                            payload, serializers=self.serializers, on_error=\"raise\"\n                        )\n                    ) as coro:\n                        yield coro\n            except CommClosedError:\n                pass\n            yield self.comm.close()\n\n    def abort(self):\n        if self.comm is None:\n            return\n        self.please_stop = True\n        self.buffer = []\n        self.waker.set()\n        if not self.comm.closed():\n            self.comm.abort()\n\n### Source File Dependency Files Content\n### Dependency File: core.py\nfrom __future__ import annotations\n\nimport asyncio\nimport functools\nimport inspect\nimport logging\nimport math\nimport os\nimport sys\nimport tempfile\nimport threading\nimport traceback\nimport types\nimport uuid\nimport warnings\nimport weakref\nfrom collections import defaultdict, deque\nfrom collections.abc import (\n    Awaitable,\n    Callable,\n    Container,\n    Coroutine,\n    Generator,\n    Hashable,\n)\nfrom enum import Enum\nfrom functools import wraps\nfrom typing import TYPE_CHECKING, Any, ClassVar, Literal, TypedDict, TypeVar, final\n\nimport tblib\nfrom tlz import merge\nfrom tornado.ioloop import IOLoop\n\nimport dask\nfrom dask.utils import parse_timedelta\n\nfrom distributed import profile, protocol\nfrom distributed._async_taskgroup import AsyncTaskGroup, AsyncTaskGroupClosedError\nfrom distributed.comm import (\n    Comm,\n    CommClosedError,\n    connect,\n    get_address_host_port,\n    listen,\n    normalize_address,\n    unparse_host_port,\n)\nfrom distributed.comm.core import Listener\nfrom distributed.compatibility import PeriodicCallback\nfrom distributed.counter import Counter\nfrom distributed.diskutils import WorkDir, WorkSpace\nfrom distributed.metrics import context_meter, time\nfrom distributed.system_monitor import SystemMonitor\nfrom distributed.utils import (\n    NoOpAwaitable,\n    get_traceback,\n    has_keyword,\n    import_file,\n    iscoroutinefunction,\n    offload,\n    recursive_to_dict,\n    truncate_exception,\n    wait_for,\n    warn_on_duration,\n)\n\nif TYPE_CHECKING:\n    from typing_extensions import ParamSpec, Self\n\n    from distributed.counter import Digest\n\n    P = ParamSpec(\"P\")\n    R = TypeVar(\"R\")\n    T = TypeVar(\"T\")\n    Coro = Coroutine[Any, Any, T]\n\n\nclass Status(Enum):\n    \"\"\"\n    This Enum contains the various states a cluster, worker, scheduler and nanny can be\n    in. Some of the status can only be observed in one of cluster, nanny, scheduler or\n    worker but we put them in the same Enum as they are compared with each\n    other.\n    \"\"\"\n\n    undefined = \"undefined\"\n    created = \"created\"\n    init = \"init\"\n    starting = \"starting\"\n    running = \"running\"\n    paused = \"paused\"\n    stopping = \"stopping\"\n    stopped = \"stopped\"\n    closing = \"closing\"\n    closing_gracefully = \"closing_gracefully\"\n    closed = \"closed\"\n    failed = \"failed\"\n    dont_reply = \"dont_reply\"\n\n\nStatus.lookup = {s.name: s for s in Status}  # type: ignore\n\n\nclass RPCClosed(IOError):\n    pass\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef raise_later(exc):\n    def _raise(*args, **kwargs):\n        raise exc\n\n    return _raise\n\n\ntick_maximum_delay = parse_timedelta(\n    dask.config.get(\"distributed.admin.tick.limit\"), default=\"ms\"\n)\n\nLOG_PDB = dask.config.get(\"distributed.admin.pdb-on-err\")\n\n\n@functools.cache\ndef _expects_comm(func: Callable) -> bool:\n    sig = inspect.signature(func)\n    params = list(sig.parameters)\n    if params and params[0] == \"comm\":\n        return True\n    if params and params[0] == \"stream\":\n        warnings.warn(\n            \"Calling the first argument of a RPC handler `stream` is \"\n            \"deprecated. Defining this argument is optional. Either remove the \"\n            f\"argument or rename it to `comm` in {func}.\",\n            FutureWarning,\n        )\n        return True\n    return False\n\n\nclass Server:\n    \"\"\"Dask Distributed Server\n\n    Superclass for endpoints in a distributed cluster, such as Worker\n    and Scheduler objects.\n\n    **Handlers**\n\n    Servers define operations with a ``handlers`` dict mapping operation names\n    to functions.  The first argument of a handler function will be a ``Comm``\n    for the communication established with the client.  Other arguments\n    will receive inputs from the keys of the incoming message which will\n    always be a dictionary.\n\n    >>> def pingpong(comm):\n    ...     return b'pong'\n\n    >>> def add(comm, x, y):\n    ...     return x + y\n\n    >>> handlers = {'ping': pingpong, 'add': add}\n    >>> server = Server(handlers)  # doctest: +SKIP\n    >>> server.listen('tcp://0.0.0.0:8000')  # doctest: +SKIP\n\n    **Message Format**\n\n    The server expects messages to be dictionaries with a special key, `'op'`\n    that corresponds to the name of the operation, and other key-value pairs as\n    required by the function.\n\n    So in the example above the following would be good messages.\n\n    *  ``{'op': 'ping'}``\n    *  ``{'op': 'add', 'x': 10, 'y': 20}``\n\n    \"\"\"\n\n    _is_finalizing: staticmethod[[], bool] = staticmethod(sys.is_finalizing)\n\n    default_ip: ClassVar[str] = \"\"\n    default_port: ClassVar[int] = 0\n\n    id: str\n    blocked_handlers: list[str]\n    handlers: dict[str, Callable]\n    stream_handlers: dict[str, Callable]\n    listeners: list[Listener]\n    counters: defaultdict[str, Counter]\n    deserialize: bool\n\n    local_directory: str\n\n    monitor: SystemMonitor\n    io_loop: IOLoop\n    thread_id: int\n\n    periodic_callbacks: dict[str, PeriodicCallback]\n    digests: defaultdict[Hashable, Digest] | None\n    digests_total: defaultdict[Hashable, float]\n    digests_total_since_heartbeat: defaultdict[Hashable, float]\n    digests_max: defaultdict[Hashable, float]\n\n    _last_tick: float\n    _tick_counter: int\n    _last_tick_counter: int\n    _tick_interval: float\n    _tick_interval_observed: float\n\n    _status: Status\n\n    _address: str | None\n    _listen_address: str | None\n    _host: str | None\n    _port: int | None\n\n    _comms: dict[Comm, str | None]\n\n    _ongoing_background_tasks: AsyncTaskGroup\n    _event_finished: asyncio.Event\n\n    _original_local_dir: str\n    _updated_sys_path: bool\n    _workspace: WorkSpace\n    _workdir: None | WorkDir\n\n    _startup_lock: asyncio.Lock\n    __startup_exc: Exception | None\n\n    def __init__(\n        self,\n        handlers,\n        blocked_handlers=None,\n        stream_handlers=None,\n        connection_limit=512,\n        deserialize=True,\n        serializers=None,\n        deserializers=None,\n        connection_args=None,\n        timeout=None,\n        io_loop=None,\n        local_directory=None,\n        needs_workdir=True,\n    ):\n        if local_directory is None:\n            local_directory = (\n                dask.config.get(\"temporary-directory\") or tempfile.gettempdir()\n            )\n\n        if \"dask-scratch-space\" not in str(local_directory):\n            local_directory = os.path.join(local_directory, \"dask-scratch-space\")\n\n        self._original_local_dir = local_directory\n\n        with warn_on_duration(\n            \"1s\",\n            \"Creating scratch directories is taking a surprisingly long time. ({duration:.2f}s) \"\n            \"This is often due to running workers on a network file system. \"\n            \"Consider specifying a local-directory to point workers to write \"\n            \"scratch data to a local disk.\",\n        ):\n            self._workspace = WorkSpace(local_directory)\n\n            if not needs_workdir:  # eg. Nanny will not need a WorkDir\n                self._workdir = None\n                self.local_directory = self._workspace.base_dir\n            else:\n                name = type(self).__name__.lower()\n                self._workdir = self._workspace.new_work_dir(prefix=f\"{name}-\")\n                self.local_directory = self._workdir.dir_path\n\n        self._updated_sys_path = False\n        if self.local_directory not in sys.path:\n            sys.path.insert(0, self.local_directory)\n            self._updated_sys_path = True\n\n        if io_loop is not None:\n            warnings.warn(\n                \"The io_loop kwarg to Server is ignored and will be deprecated\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        self._status = Status.init\n        self.handlers = {\n            \"identity\": self.identity,\n            \"echo\": self.echo,\n            \"connection_stream\": self.handle_stream,\n            \"dump_state\": self._to_dict,\n        }\n        self.handlers.update(handlers)\n        if blocked_handlers is None:\n            blocked_handlers = dask.config.get(\n                \"distributed.%s.blocked-handlers\" % type(self).__name__.lower(), []\n            )\n        self.blocked_handlers = blocked_handlers\n        self.stream_handlers = {}\n        self.stream_handlers.update(stream_handlers or {})\n\n        self.id = type(self).__name__ + \"-\" + str(uuid.uuid4())\n        self._address = None\n        self._listen_address = None\n        self._port = None\n        self._host = None\n        self._comms = {}\n        self.deserialize = deserialize\n        self.monitor = SystemMonitor()\n        self._ongoing_background_tasks = AsyncTaskGroup()\n        self._event_finished = asyncio.Event()\n\n        self.listeners = []\n        self.io_loop = self.loop = IOLoop.current()\n\n        if not hasattr(self.io_loop, \"profile\"):\n            if (\n                dask.config.get(\"distributed.worker.profile.enabled\")\n                and sys.version_info.minor != 11\n            ):\n                ref = weakref.ref(self.io_loop)\n\n                def stop() -> bool:\n                    loop = ref()\n                    return loop is None or loop.asyncio_loop.is_closed()\n\n                self.io_loop.profile = profile.watch(\n                    omit=(\"profile.py\", \"selectors.py\"),\n                    interval=dask.config.get(\"distributed.worker.profile.interval\"),\n                    cycle=dask.config.get(\"distributed.worker.profile.cycle\"),\n                    stop=stop,\n                )\n            else:\n                self.io_loop.profile = deque()\n\n        self.periodic_callbacks = {}\n\n        # Statistics counters for various events\n        try:\n            from distributed.counter import Digest\n\n            self.digests = defaultdict(Digest)\n        except ImportError:\n            self.digests = None\n\n        # Also log cumulative totals (reset at server restart)\n        # and local maximums (reset by prometheus poll)\n        # Don't cast int metrics to float\n        self.digests_total = defaultdict(int)\n        self.digests_total_since_heartbeat = defaultdict(int)\n        self.digests_max = defaultdict(int)\n\n        self.counters = defaultdict(Counter)\n        pc = PeriodicCallback(self._shift_counters, 5000)\n        self.periodic_callbacks[\"shift_counters\"] = pc\n\n        pc = PeriodicCallback(\n            self.monitor.update,\n            parse_timedelta(\n                dask.config.get(\"distributed.admin.system-monitor.interval\")\n            )\n            * 1000,\n        )\n        self.periodic_callbacks[\"monitor\"] = pc\n\n        self._last_tick = time()\n        self._tick_counter = 0\n        self._last_tick_counter = 0\n        self._last_tick_cycle = time()\n        self._tick_interval = parse_timedelta(\n            dask.config.get(\"distributed.admin.tick.interval\"), default=\"ms\"\n        )\n        self._tick_interval_observed = self._tick_interval\n        self.periodic_callbacks[\"tick\"] = PeriodicCallback(\n            self._measure_tick, self._tick_interval * 1000\n        )\n        self.periodic_callbacks[\"ticks\"] = PeriodicCallback(\n            self._cycle_ticks,\n            parse_timedelta(dask.config.get(\"distributed.admin.tick.cycle\")) * 1000,\n        )\n\n        self.thread_id = 0\n\n        def set_thread_ident():\n            self.thread_id = threading.get_ident()\n\n        self.io_loop.add_callback(set_thread_ident)\n        self._startup_lock = asyncio.Lock()\n        self.__startup_exc = None\n\n        self.rpc = ConnectionPool(\n            limit=connection_limit,\n            deserialize=deserialize,\n            serializers=serializers,\n            deserializers=deserializers,\n            connection_args=connection_args,\n            timeout=timeout,\n            server=self,\n        )\n\n        self.__stopped = False\n\n    async def upload_file(\n        self, filename: str, data: str | bytes, load: bool = True\n    ) -> dict[str, Any]:\n        out_filename = os.path.join(self.local_directory, filename)\n\n        def func(data):\n            if isinstance(data, str):\n                data = data.encode()\n            with open(out_filename, \"wb\") as f:\n                f.write(data)\n                f.flush()\n                os.fsync(f.fileno())\n            return data\n\n        if len(data) < 10000:\n            data = func(data)\n        else:\n            data = await offload(func, data)\n\n        if load:\n            try:\n                import_file(out_filename)\n            except Exception as e:\n                logger.exception(e)\n                raise e\n\n        return {\"status\": \"OK\", \"nbytes\": len(data)}\n\n    def _shift_counters(self):\n        # Copy counters before iterating to avoid concurrent modification\n        for counter in list(self.counters.values()):\n            counter.shift()\n        if self.digests is not None:\n            # Copy digests before iterating to avoid concurrent modification\n            for digest in list(self.digests.values()):\n                digest.shift()\n\n    @property\n    def status(self) -> Status:\n        try:\n            return self._status\n        except AttributeError:\n            return Status.undefined\n\n    @status.setter\n    def status(self, value: Status) -> None:\n        if not isinstance(value, Status):\n            raise TypeError(f\"Expected Status; got {value!r}\")\n        self._status = value\n\n    @property\n    def incoming_comms_open(self) -> int:\n        \"\"\"The number of total incoming connections listening to remote RPCs\"\"\"\n        return len(self._comms)\n\n    @property\n    def incoming_comms_active(self) -> int:\n        \"\"\"The number of connections currently handling a remote RPC\"\"\"\n        return len([c for c, op in self._comms.items() if op is not None])\n\n    @property\n    def outgoing_comms_open(self) -> int:\n        \"\"\"The number of connections currently open and waiting for a remote RPC\"\"\"\n        return self.rpc.open\n\n    @property\n    def outgoing_comms_active(self) -> int:\n        \"\"\"The number of outgoing connections that are currently used to\n        execute a RPC\"\"\"\n        return self.rpc.active\n\n    def get_connection_counters(self) -> dict[str, int]:\n        \"\"\"A dict with various connection counters\n\n        See also\n        --------\n        Server.incoming_comms_open\n        Server.incoming_comms_active\n        Server.outgoing_comms_open\n        Server.outgoing_comms_active\n        \"\"\"\n        return {\n            attr: getattr(self, attr)\n            for attr in [\n                \"incoming_comms_open\",\n                \"incoming_comms_active\",\n                \"outgoing_comms_open\",\n                \"outgoing_comms_active\",\n            ]\n        }\n\n    async def finished(self):\n        \"\"\"Wait until the server has finished\"\"\"\n        await self._event_finished.wait()\n\n    def __await__(self):\n        return self.start().__await__()\n\n    async def start_unsafe(self):\n        \"\"\"Attempt to start the server. This is not idempotent and not protected against concurrent startup attempts.\n\n        This is intended to be overwritten or called by subclasses. For a safe\n        startup, please use ``Server.start`` instead.\n\n        If ``death_timeout`` is configured, we will require this coroutine to\n        finish before this timeout is reached. If the timeout is reached we will\n        close the instance and raise an ``asyncio.TimeoutError``\n        \"\"\"\n        await self.rpc.start()\n        return self\n\n    @final\n    async def start(self):\n        async with self._startup_lock:\n            if self.status == Status.failed:\n                assert self.__startup_exc is not None\n                raise self.__startup_exc\n            elif self.status != Status.init:\n                return self\n            timeout = getattr(self, \"death_timeout\", None)\n\n            async def _close_on_failure(exc: Exception) -> None:\n                await self.close(reason=f\"failure-to-start-{str(type(exc))}\")\n                self.status = Status.failed\n                self.__startup_exc = exc\n\n            try:\n                await wait_for(self.start_unsafe(), timeout=timeout)\n            except asyncio.TimeoutError as exc:\n                await _close_on_failure(exc)\n                raise asyncio.TimeoutError(\n                    f\"{type(self).__name__} start timed out after {timeout}s.\"\n                ) from exc\n            except Exception as exc:\n                await _close_on_failure(exc)\n                raise RuntimeError(f\"{type(self).__name__} failed to start.\") from exc\n            if self.status == Status.init:\n                self.status = Status.running\n        return self\n\n    async def __aenter__(self):\n        await self\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        await self.close()\n\n    def start_periodic_callbacks(self):\n        \"\"\"Start Periodic Callbacks consistently\n\n        This starts all PeriodicCallbacks stored in self.periodic_callbacks if\n        they are not yet running. It does this safely by checking that it is using the\n        correct event loop.\n        \"\"\"\n        if self.io_loop.asyncio_loop is not asyncio.get_running_loop():\n            raise RuntimeError(f\"{self!r} is bound to a different event loop\")\n\n        self._last_tick = time()\n        for pc in self.periodic_callbacks.values():\n            if not pc.is_running():\n                pc.start()\n\n    def _stop_listeners(self) -> asyncio.Future:\n        listeners_to_stop: set[Awaitable] = set()\n\n        for listener in self.listeners:\n            future = listener.stop()\n            if inspect.isawaitable(future):\n                warnings.warn(\n                    f\"{type(listener)} is using an asynchronous `stop` method. \"\n                    \"Support for asynchronous `Listener.stop` has been deprecated and \"\n                    \"will be removed in a future version\",\n                    DeprecationWarning,\n                )\n                listeners_to_stop.add(future)\n            elif hasattr(listener, \"abort_handshaking_comms\"):\n                listener.abort_handshaking_comms()\n\n        return asyncio.gather(*listeners_to_stop)\n\n    def stop(self) -> None:\n        if self.__stopped:\n            return\n        self.__stopped = True\n        self.monitor.close()\n        if not (stop_listeners := self._stop_listeners()).done():\n            self._ongoing_background_tasks.call_soon(\n                asyncio.wait_for(stop_listeners, timeout=None)  # type: ignore[arg-type]\n            )\n        if self._workdir is not None:\n            self._workdir.release()\n\n    @property\n    def listener(self) -> Listener | None:\n        if self.listeners:\n            return self.listeners[0]\n        else:\n            return None\n\n    def _measure_tick(self):\n        now = time()\n        tick_duration = now - self._last_tick\n        self._last_tick = now\n        self._tick_counter += 1\n        # This metric is exposed in Prometheus and is reset there during\n        # collection\n        if tick_duration > tick_maximum_delay:\n            logger.info(\n                \"Event loop was unresponsive in %s for %.2fs.  \"\n                \"This is often caused by long-running GIL-holding \"\n                \"functions or moving large chunks of data. \"\n                \"This can cause timeouts and instability.\",\n                type(self).__name__,\n                tick_duration,\n            )\n        self.digest_metric(\"tick-duration\", tick_duration)\n\n    def _cycle_ticks(self):\n        if not self._tick_counter:\n            return\n        now = time()\n        last_tick_cycle, self._last_tick_cycle = self._last_tick_cycle, now\n        count = self._tick_counter - self._last_tick_counter\n        self._last_tick_counter = self._tick_counter\n        self._tick_interval_observed = (now - last_tick_cycle) / (count or 1)\n\n    @property\n    def address(self) -> str:\n        \"\"\"\n        The address this Server can be contacted on.\n        If the server is not up, yet, this raises a ValueError.\n        \"\"\"\n        if not self._address:\n            if self.listener is None:\n                raise ValueError(\"cannot get address of non-running Server\")\n            self._address = self.listener.contact_address\n            assert self._address\n        return self._address\n\n    @property\n    def address_safe(self) -> str:\n        \"\"\"\n        The address this Server can be contacted on.\n        If the server is not up, yet, this returns a ``\"not-running\"``.\n        \"\"\"\n        try:\n            return self.address\n        except ValueError:\n            return \"not-running\"\n\n    @property\n    def listen_address(self):\n        \"\"\"\n        The address this Server is listening on.  This may be a wildcard\n        address such as `tcp://0.0.0.0:1234`.\n        \"\"\"\n        if not self._listen_address:\n            if self.listener is None:\n                raise ValueError(\"cannot get listen address of non-running Server\")\n            self._listen_address = self.listener.listen_address\n        return self._listen_address\n\n    @property\n    def host(self):\n        \"\"\"\n        The host this Server is running on.\n\n        This will raise ValueError if the Server is listening on a\n        non-IP based protocol.\n        \"\"\"\n        if not self._host:\n            self._host, self._port = get_address_host_port(self.address)\n        return self._host\n\n    @property\n    def port(self):\n        \"\"\"\n        The port number this Server is listening on.\n\n        This will raise ValueError if the Server is listening on a\n        non-IP based protocol.\n        \"\"\"\n        if not self._port:\n            self._host, self._port = get_address_host_port(self.address)\n        return self._port\n\n    def identity(self) -> dict[str, str]:\n        return {\"type\": type(self).__name__, \"id\": self.id}\n\n    def _to_dict(self, *, exclude: Container[str] = ()) -> dict[str, Any]:\n        \"\"\"Dictionary representation for debugging purposes.\n        Not type stable and not intended for roundtrips.\n\n        See also\n        --------\n        Server.identity\n        Client.dump_cluster_state\n        distributed.utils.recursive_to_dict\n        \"\"\"\n        info: dict[str, Any] = self.identity()\n        extra = {\n            \"address\": self.address,\n            \"status\": self.status.name,\n            \"thread_id\": self.thread_id,\n        }\n        info.update(extra)\n        info = {k: v for k, v in info.items() if k not in exclude}\n        return recursive_to_dict(info, exclude=exclude)\n\n    def echo(self, data=None):\n        return data\n\n    async def listen(self, port_or_addr=None, allow_offload=True, **kwargs):\n        if port_or_addr is None:\n            port_or_addr = self.default_port\n        if isinstance(port_or_addr, int):\n            addr = unparse_host_port(self.default_ip, port_or_addr)\n        elif isinstance(port_or_addr, tuple):\n            addr = unparse_host_port(*port_or_addr)\n        else:\n            addr = port_or_addr\n            assert isinstance(addr, str)\n        listener = await listen(\n            addr,\n            self.handle_comm,\n            deserialize=self.deserialize,\n            allow_offload=allow_offload,\n            **kwargs,\n        )\n        self.listeners.append(listener)\n\n    def handle_comm(self, comm: Comm) -> NoOpAwaitable:\n        \"\"\"Start a background task that dispatches new communications to coroutine-handlers\"\"\"\n        try:\n            self._ongoing_background_tasks.call_soon(self._handle_comm, comm)\n        except AsyncTaskGroupClosedError:\n            comm.abort()\n        return NoOpAwaitable()\n\n    async def _handle_comm(self, comm: Comm) -> None:\n        \"\"\"Dispatch new communications to coroutine-handlers\n\n        Handlers is a dictionary mapping operation names to functions or\n        coroutines.\n\n            {'get_data': get_data,\n             'ping': pingpong}\n\n        Coroutines should expect a single Comm object.\n        \"\"\"\n        if self.__stopped:\n            comm.abort()\n            return\n        address = comm.peer_address\n        op = None\n\n        logger.debug(\"Connection from %r to %s\", address, type(self).__name__)\n        self._comms[comm] = op\n\n        await self\n        try:\n            while not self.__stopped:\n                try:\n                    msg = await comm.read()\n                    logger.debug(\"Message from %r: %s\", address, msg)\n                except OSError as e:\n                    if not self._is_finalizing():\n                        logger.debug(\n                            \"Lost connection to %r while reading message: %s.\"\n                            \" Last operation: %s\",\n                            address,\n                            e,\n                            op,\n                        )\n                    break\n                except Exception as e:\n                    logger.exception(\"Exception while reading from %s\", address)\n                    if comm.closed():\n                        raise\n                    else:\n                        await comm.write(error_message(e, status=\"uncaught-error\"))\n                        continue\n                if not isinstance(msg, dict):\n                    raise TypeError(\n                        \"Bad message type.  Expected dict, got\\n  \" + str(msg)\n                    )\n\n                try:\n                    op = msg.pop(\"op\")\n                except KeyError as e:\n                    raise ValueError(\n                        \"Received unexpected message without 'op' key: \" + str(msg)\n                    ) from e\n                if self.counters is not None:\n                    self.counters[\"op\"].add(op)\n                self._comms[comm] = op\n                serializers = msg.pop(\"serializers\", None)\n                close_desired = msg.pop(\"close\", False)\n                reply = msg.pop(\"reply\", True)\n                if op == \"close\":\n                    if reply:\n                        await comm.write(\"OK\")\n                    break\n\n                result = None\n                try:\n                    if op in self.blocked_handlers:\n                        _msg = (\n                            \"The '{op}' handler has been explicitly disallowed \"\n                            \"in {obj}, possibly due to security concerns.\"\n                        )\n                        exc = ValueError(_msg.format(op=op, obj=type(self).__name__))\n                        handler = raise_later(exc)\n                    else:\n                        handler = self.handlers[op]\n                except KeyError:\n                    logger.warning(\n                        \"No handler %s found in %s\",\n                        op,\n                        type(self).__name__,\n                        exc_info=True,\n                    )\n                else:\n                    if serializers is not None and has_keyword(handler, \"serializers\"):\n                        msg[\"serializers\"] = serializers  # add back in\n\n                    logger.debug(\"Calling into handler %s\", handler.__name__)\n                    try:\n                        if _expects_comm(handler):\n                            result = handler(comm, **msg)\n                        else:\n                            result = handler(**msg)\n                        if inspect.iscoroutine(result):\n                            result = await result\n                        elif inspect.isawaitable(result):\n                            raise RuntimeError(\n                                f\"Comm handler returned unknown awaitable. Expected coroutine, instead got {type(result)}\"\n                            )\n                    except CommClosedError:\n                        if self.status == Status.running:\n                            logger.info(\"Lost connection to %r\", address, exc_info=True)\n                        break\n                    except Exception as e:\n                        logger.exception(\"Exception while handling op %s\", op)\n                        if comm.closed():\n                            raise\n                        else:\n                            result = error_message(e, status=\"uncaught-error\")\n\n                if reply and result != Status.dont_reply:\n                    try:\n                        await comm.write(result, serializers=serializers)\n                    except (OSError, TypeError) as e:\n                        logger.debug(\n                            \"Lost connection to %r while sending result for op %r: %s\",\n                            address,\n                            op,\n                            e,\n                        )\n                        break\n\n                self._comms[comm] = None\n                msg = result = None\n                if close_desired:\n                    await comm.close()\n                if comm.closed():\n                    break\n\n        finally:\n            del self._comms[comm]\n            if not self._is_finalizing() and not comm.closed():\n                try:\n                    comm.abort()\n                except Exception as e:\n                    logger.error(\n                        \"Failed while closing connection to %r: %s\", address, e\n                    )\n\n    async def handle_stream(\n        self, comm: Comm, extra: dict[str, Any] | None = None\n    ) -> None:\n        extra = extra or {}\n        logger.info(\"Starting established connection to %s\", comm.peer_address)\n\n        closed = False\n        try:\n            while not closed:\n                try:\n                    msgs = await comm.read()\n                # If another coroutine has closed the comm, stop handling the stream.\n                except CommClosedError:\n                    closed = True\n                    logger.info(\n                        \"Connection to %s has been closed.\",\n                        comm.peer_address,\n                    )\n                    break\n                if not isinstance(msgs, (tuple, list)):\n                    msgs = (msgs,)\n\n                for msg in msgs:\n                    if msg == \"OK\":\n                        break\n                    op = msg.pop(\"op\")\n                    if op:\n                        if op == \"close-stream\":\n                            closed = True\n                            logger.info(\n                                \"Received 'close-stream' from %s; closing.\",\n                                comm.peer_address,\n                            )\n                            break\n                        handler = self.stream_handlers[op]\n                        if iscoroutinefunction(handler):\n                            await handler(**merge(extra, msg))\n                        else:\n                            handler(**merge(extra, msg))\n                    else:\n                        logger.error(\"odd message %s\", msg)\n                await asyncio.sleep(0)\n        except Exception:\n            if LOG_PDB:\n                import pdb\n\n                pdb.set_trace()\n            raise\n        finally:\n            await comm.close()\n            assert comm.closed()\n\n    async def close(self, timeout: float | None = None, reason: str = \"\") -> None:\n        try:\n            for pc in self.periodic_callbacks.values():\n                pc.stop()\n\n            self.__stopped = True\n            self.monitor.close()\n            await self._stop_listeners()\n\n            # TODO: Deal with exceptions\n            await self._ongoing_background_tasks.stop()\n\n            await self.rpc.close()\n            await asyncio.gather(*[comm.close() for comm in list(self._comms)])\n\n            # Remove scratch directory from global sys.path\n            if self._updated_sys_path and sys.path[0] == self.local_directory:\n                sys.path.remove(self.local_directory)\n        finally:\n            self._event_finished.set()\n\n    def digest_metric(self, name: Hashable, value: float) -> None:\n        # Granular data (requires crick)\n        if self.digests is not None:\n            self.digests[name].add(value)\n        # Cumulative data (reset by server restart)\n        self.digests_total[name] += value\n        # Cumulative data sent to scheduler and reset on heartbeat\n        self.digests_total_since_heartbeat[name] += value\n        # Local maximums (reset by Prometheus poll)\n        self.digests_max[name] = max(self.digests_max[name], value)\n\n\ndef context_meter_to_server_digest(digest_tag: str) -> Callable:\n    \"\"\"Decorator for an async method of a Server subclass that calls\n    ``distributed.metrics.context_meter.meter`` and/or ``digest_metric``.\n    It routes the calls from ``context_meter.digest_metric(label, value, unit)`` to\n    ``Server.digest_metric((digest_tag, label, unit), value)``.\n    \"\"\"\n\n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        async def wrapper(self: Server, *args: Any, **kwargs: Any) -> Any:\n            def metrics_callback(label: Hashable, value: float, unit: str) -> None:\n                if not isinstance(label, tuple):\n                    label = (label,)\n                name = (digest_tag, *label, unit)\n                self.digest_metric(name, value)\n\n            with context_meter.add_callback(metrics_callback, allow_offload=True):\n                return await func(self, *args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\ndef pingpong(comm):\n    return b\"pong\"\n\n\nasync def send_recv(  # type: ignore[no-untyped-def]\n    comm: Comm,\n    *,\n    reply: bool = True,\n    serializers=None,\n    deserializers=None,\n    **kwargs,\n):\n    \"\"\"Send and recv with a Comm.\n\n    Keyword arguments turn into the message\n\n    response = await send_recv(comm, op='ping', reply=True)\n    \"\"\"\n    msg = kwargs\n    msg[\"reply\"] = reply\n    please_close = kwargs.get(\"close\", False)\n    force_close = False\n    if deserializers is None:\n        deserializers = serializers\n    if deserializers is not None:\n        msg[\"serializers\"] = deserializers\n\n    try:\n        await comm.write(msg, serializers=serializers, on_error=\"raise\")\n        if reply:\n            response = await comm.read(deserializers=deserializers)\n        else:\n            response = None\n    except (asyncio.TimeoutError, OSError):\n        # On communication errors, we should simply close the communication\n        # Note that OSError includes CommClosedError and socket timeouts\n        force_close = True\n        raise\n    except asyncio.CancelledError:\n        # Do not reuse the comm to prevent the next call of send_recv from receiving\n        # data from this call and/or accidentally putting multiple waiters on read().\n        # Note that this relies on all Comm implementations to allow a write() in the\n        # middle of a read().\n        please_close = True\n        raise\n    finally:\n        if force_close:\n            comm.abort()\n        elif please_close:\n            await comm.close()\n\n    if isinstance(response, dict) and response.get(\"status\") == \"uncaught-error\":\n        if comm.deserialize:\n            _, exc, tb = clean_exception(**response)\n            assert exc\n            raise exc.with_traceback(tb)\n        else:\n            raise Exception(response[\"exception_text\"])\n    return response\n\n\ndef addr_from_args(\n    addr: str | tuple[str, int | None] | None = None,\n    ip: str | None = None,\n    port: int | None = None,\n) -> str:\n    if addr is None:\n        assert ip is not None\n        return normalize_address(unparse_host_port(ip, port))\n\n    assert ip is None and port is None\n    if isinstance(addr, tuple):\n        return normalize_address(unparse_host_port(*addr))\n\n    return normalize_address(addr)\n\n\nclass rpc:\n    \"\"\"Conveniently interact with a remote server\n\n    >>> remote = rpc(address)  # doctest: +SKIP\n    >>> response = await remote.add(x=10, y=20)  # doctest: +SKIP\n\n    One rpc object can be reused for several interactions.\n    Additionally, this object creates and destroys many comms as necessary\n    and so is safe to use in multiple overlapping communications.\n\n    When done, close comms explicitly.\n\n    >>> remote.close_comms()  # doctest: +SKIP\n    \"\"\"\n\n    active: ClassVar[weakref.WeakSet[rpc]] = weakref.WeakSet()\n    comms = ()\n    address = None\n\n    def __init__(\n        self,\n        arg=None,\n        comm=None,\n        deserialize=True,\n        timeout=None,\n        connection_args=None,\n        serializers=None,\n        deserializers=None,\n    ):\n        self.comms = {}\n        self.address = coerce_to_address(arg)\n        self.timeout = timeout\n        self.status = Status.running\n        self.deserialize = deserialize\n        self.serializers = serializers\n        self.deserializers = deserializers if deserializers is not None else serializers\n        self.connection_args = connection_args or {}\n        self._created = weakref.WeakSet()\n        rpc.active.add(self)\n\n    async def live_comm(self):\n        \"\"\"Get an open communication\n\n        Some comms to the ip/port target may be in current use by other\n        coroutines.  We track this with the `comms` dict\n\n            :: {comm: True/False if open and ready for use}\n\n        This function produces an open communication, either by taking one\n        that we've already made or making a new one if they are all taken.\n        This also removes comms that have been closed.\n\n        When the caller is done with the stream they should set\n\n            self.comms[comm] = True\n\n        As is done in __getattr__ below.\n        \"\"\"\n        if self.status == Status.closed:\n            raise RPCClosed(\"RPC Closed\")\n        to_clear = set()\n        open = False\n        for comm, open in self.comms.items():\n            if comm.closed():\n                to_clear.add(comm)\n            if open:\n                break\n        for s in to_clear:\n            del self.comms[s]\n        if not open or comm.closed():\n            comm = await connect(\n                self.address,\n                self.timeout,\n                deserialize=self.deserialize,\n                **self.connection_args,\n            )\n            comm.name = \"rpc\"\n        self.comms[comm] = False  # mark as taken\n        return comm\n\n    def close_comms(self):\n        async def _close_comm(comm):\n            # Make sure we tell the peer to close\n            try:\n                if not comm.closed():\n                    await comm.write({\"op\": \"close\", \"reply\": False})\n                    await comm.close()\n            except OSError:\n                comm.abort()\n\n        tasks = []\n        for comm in list(self.comms):\n            if comm and not comm.closed():\n                task = asyncio.ensure_future(_close_comm(comm))\n                tasks.append(task)\n        for comm in list(self._created):\n            if comm and not comm.closed():\n                task = asyncio.ensure_future(_close_comm(comm))\n                tasks.append(task)\n\n        self.comms.clear()\n        return tasks\n\n    def __getattr__(self, key):\n        async def send_recv_from_rpc(**kwargs):\n            if self.serializers is not None and kwargs.get(\"serializers\") is None:\n                kwargs[\"serializers\"] = self.serializers\n            if self.deserializers is not None and kwargs.get(\"deserializers\") is None:\n                kwargs[\"deserializers\"] = self.deserializers\n            comm = None\n            try:\n                comm = await self.live_comm()\n                comm.name = \"rpc.\" + key\n                result = await send_recv(comm=comm, op=key, **kwargs)\n            except (RPCClosed, CommClosedError) as e:\n                if comm:\n                    raise type(e)(\n                        f\"Exception while trying to call remote method {key!r} using comm {comm!r}.\"\n                    ) from e\n                else:\n                    raise type(e)(\n                        f\"Exception while trying to call remote method {key!r} before comm was established.\"\n                    ) from e\n\n            self.comms[comm] = True  # mark as open\n            return result\n\n        return send_recv_from_rpc\n\n    async def close_rpc(self):\n        if self.status != Status.closed:\n            rpc.active.discard(self)\n        self.status = Status.closed\n        return await asyncio.gather(*self.close_comms())\n\n    def __enter__(self):\n        warnings.warn(\n            \"the rpc synchronous context manager is deprecated\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        asyncio.ensure_future(self.close_rpc())\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        await self.close_rpc()\n\n    def __del__(self):\n        if self.status != Status.closed:\n            rpc.active.discard(self)\n            self.status = Status.closed\n            still_open = [comm for comm in self.comms if not comm.closed()]\n            if still_open:\n                logger.warning(\n                    \"rpc object %s deleted with %d open comms\", self, len(still_open)\n                )\n                for comm in still_open:\n                    comm.abort()\n\n    def __repr__(self):\n        return \"<rpc to %r, %d comms>\" % (self.address, len(self.comms))\n\n\nclass PooledRPCCall:\n    \"\"\"The result of ConnectionPool()('host:port')\n\n    See Also:\n        ConnectionPool\n    \"\"\"\n\n    def __init__(self, addr, pool, serializers=None, deserializers=None):\n        self.addr = addr\n        self.pool = pool\n        self.serializers = serializers\n        self.deserializers = deserializers if deserializers is not None else serializers\n\n    @property\n    def address(self):\n        return self.addr\n\n    def __getattr__(self, key):\n        async def send_recv_from_rpc(**kwargs):\n            if self.serializers is not None and kwargs.get(\"serializers\") is None:\n                kwargs[\"serializers\"] = self.serializers\n            if self.deserializers is not None and kwargs.get(\"deserializers\") is None:\n                kwargs[\"deserializers\"] = self.deserializers\n            comm = await self.pool.connect(self.addr)\n            prev_name, comm.name = comm.name, \"ConnectionPool.\" + key\n            try:\n                return await send_recv(comm=comm, op=key, **kwargs)\n            finally:\n                self.pool.reuse(self.addr, comm)\n                comm.name = prev_name\n\n        return send_recv_from_rpc\n\n    async def close_rpc(self):\n        pass\n\n    # For compatibility with rpc()\n    def __enter__(self):\n        warnings.warn(\n            \"the rpc synchronous context manager is deprecated\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *args):\n        pass\n\n    def __repr__(self):\n        return f\"<pooled rpc to {self.addr!r}>\"\n\n\nclass ConnectionPool:\n    \"\"\"A maximum sized pool of Comm objects.\n\n    This provides a connect method that mirrors the normal distributed.connect\n    method, but provides connection sharing and tracks connection limits.\n\n    This object provides an ``rpc`` like interface::\n\n        >>> rpc = ConnectionPool(limit=512)\n        >>> scheduler = rpc('127.0.0.1:8786')\n        >>> workers = [rpc(address) for address in ...]\n\n        >>> info = await scheduler.identity()\n\n    It creates enough comms to satisfy concurrent connections to any\n    particular address::\n\n        >>> a, b = await asyncio.gather(scheduler.who_has(), scheduler.has_what())\n\n    It reuses existing comms so that we don't have to continuously reconnect.\n\n    It also maintains a comm limit to avoid \"too many open file handle\"\n    issues.  Whenever this maximum is reached we clear out all idling comms.\n    If that doesn't do the trick then we wait until one of the occupied comms\n    closes.\n\n    Parameters\n    ----------\n    limit: int\n        The number of open comms to maintain at once\n    deserialize: bool\n        Whether or not to deserialize data by default or pass it through\n    \"\"\"\n\n    _instances: ClassVar[weakref.WeakSet[ConnectionPool]] = weakref.WeakSet()\n\n    def __init__(\n        self,\n        limit: int = 512,\n        deserialize: bool = True,\n        serializers: list[str] | None = None,\n        allow_offload: bool = True,\n        deserializers: list[str] | None = None,\n        connection_args: dict[str, object] | None = None,\n        timeout: float | None = None,\n        server: object = None,\n    ) -> None:\n        self.limit = limit  # Max number of open comms\n        # Invariant: len(available) == open - active\n        self.available: defaultdict[str, set[Comm]] = defaultdict(set)\n        # Invariant: len(occupied) == active\n        self.occupied: defaultdict[str, set[Comm]] = defaultdict(set)\n        self.allow_offload = allow_offload\n        self.deserialize = deserialize\n        self.serializers = serializers\n        self.deserializers = deserializers if deserializers is not None else serializers\n        self.connection_args = connection_args or {}\n        self.timeout = timeout\n        self.server = weakref.ref(server) if server else None\n        self._created: weakref.WeakSet[Comm] = weakref.WeakSet()\n        self._instances.add(self)\n        # _n_connecting and _connecting have subtle different semantics. The set\n        # _connecting contains futures actively trying to establish a connection\n        # while the _n_connecting also accounts for connection attempts which\n        # are waiting due to the connection limit\n        self._connecting: defaultdict[str, set[Callable[[str], None]]] = defaultdict(\n            set\n        )\n        self._pending_count = 0\n        self._connecting_count = 0\n        self._connecting_close_timeout = 5\n        self.status = Status.init\n\n    def _validate(self) -> None:\n        \"\"\"\n        Validate important invariants of this class\n\n        Used only for testing / debugging\n        \"\"\"\n        assert self.semaphore._value == self.limit - self.open - self._n_connecting\n\n    @property\n    def active(self) -> int:\n        return sum(map(len, self.occupied.values()))\n\n    @property\n    def open(self) -> int:\n        return self.active + sum(map(len, self.available.values()))\n\n    def __repr__(self) -> str:\n        return \"<ConnectionPool: open=%d, active=%d, connecting=%d>\" % (\n            self.open,\n            self.active,\n            len(self._connecting),\n        )\n\n    def __call__(\n        self,\n        addr: str | tuple[str, int | None] | None = None,\n        ip: str | None = None,\n        port: int | None = None,\n    ) -> PooledRPCCall:\n        \"\"\"Cached rpc objects\"\"\"\n        addr = addr_from_args(addr=addr, ip=ip, port=port)\n        return PooledRPCCall(\n            addr, self, serializers=self.serializers, deserializers=self.deserializers\n        )\n\n    def __await__(self) -> Generator[Any, Any, Self]:\n        async def _() -> Self:\n            await self.start()\n            return self\n\n        return _().__await__()\n\n    async def __aenter__(self):\n        await self\n        return self\n\n    async def __aexit__(self, *args):\n        await self.close()\n        return\n\n    async def start(self) -> None:\n        # Invariant: semaphore._value == limit - open - _n_connecting\n        self.semaphore = asyncio.Semaphore(self.limit)\n        self.status = Status.running\n\n    @property\n    def _n_connecting(self) -> int:\n        return self._connecting_count\n\n    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:\n        self._pending_count += 1\n        try:\n            await self.semaphore.acquire()\n            try:\n                self._connecting_count += 1\n                comm = await connect(\n                    addr,\n                    timeout=timeout or self.timeout,\n                    deserialize=self.deserialize,\n                    **self.connection_args,\n                )\n                comm.name = \"ConnectionPool\"\n                comm._pool = weakref.ref(self)\n                comm.allow_offload = self.allow_offload\n                self._created.add(comm)\n\n                self.occupied[addr].add(comm)\n\n                return comm\n            except BaseException:\n                self.semaphore.release()\n                raise\n            finally:\n                self._connecting_count -= 1\n        finally:\n            self._pending_count -= 1\n\n    async def connect(self, addr: str, timeout: float | None = None) -> Comm:\n        \"\"\"\n        Get a Comm to the given address.  For internal use.\n        \"\"\"\n        if self.status != Status.running:\n            raise RuntimeError(\"ConnectionPool is closed\")\n        available = self.available[addr]\n        occupied = self.occupied[addr]\n        while available:\n            comm = available.pop()\n            if comm.closed():\n                self.semaphore.release()\n            else:\n                occupied.add(comm)\n                return comm\n\n        if self.semaphore.locked():\n            self.collect()\n\n        # on 3.11 this uses asyncio.timeout as a cancel scope to avoid having\n        # to track inner and outer CancelledError exceptions and correctly\n        # call .uncancel() when a CancelledError is caught.\n        if sys.version_info >= (3, 11):\n            reason: str | None = None\n            try:\n                async with asyncio.timeout(math.inf) as scope:\n\n                    def cancel_timeout_cb(reason_: str) -> None:\n                        nonlocal reason\n                        reason = reason_\n                        scope.reschedule(-1)\n\n                    self._connecting[addr].add(cancel_timeout_cb)\n                    try:\n                        return await self._connect(addr=addr, timeout=timeout)\n                    finally:\n                        connecting = self._connecting[addr]\n                        connecting.discard(cancel_timeout_cb)\n                        if not connecting:\n                            try:\n                                del self._connecting[addr]\n                            except KeyError:\n                                pass\n            except TimeoutError:\n                if reason is None:\n                    raise\n                raise CommClosedError(reason)\n        else:\n            # This construction is there to ensure that cancellation requests from\n            # the outside can be distinguished from cancellations of our own.\n            # Once the CommPool closes, we'll cancel the connect_attempt which will\n            # raise an OSError\n            # If the ``connect`` is cancelled from the outside, the Event.wait will\n            # be cancelled instead which we'll reraise as a CancelledError and allow\n            # it to propagate\n            connect_attempt = asyncio.create_task(self._connect(addr, timeout))\n            done = asyncio.Event()\n            reason = \"ConnectionPool closing.\"\n\n            def cancel_task_cb(reason_: str) -> None:\n                nonlocal reason\n                reason = reason_\n                connect_attempt.cancel()\n\n            connecting = self._connecting[addr]\n            connecting.add(cancel_task_cb)\n\n            def callback(task: asyncio.Task[Comm]) -> None:\n                done.set()\n                connecting = self._connecting[addr]\n                connecting.discard(cancel_task_cb)\n\n                if not connecting:\n                    try:\n                        del self._connecting[addr]\n                    except KeyError:  # pragma: no cover\n                        pass\n\n            connect_attempt.add_done_callback(callback)\n\n            try:\n                await done.wait()\n            except asyncio.CancelledError:\n                # This is an outside cancel attempt\n                connect_attempt.cancel()\n                await connect_attempt\n                raise\n            try:\n                return connect_attempt.result()\n            except asyncio.CancelledError:\n                if reason:\n                    raise CommClosedError(reason)\n                raise\n\n    def reuse(self, addr: str, comm: Comm) -> None:\n        \"\"\"\n        Reuse an open communication to the given address.  For internal use.\n        \"\"\"\n        # if the pool is asked to re-use a comm it does not know about, ignore\n        # this comm: just close it.\n        if comm not in self.occupied[addr]:\n            IOLoop.current().add_callback(comm.close)\n        else:\n            self.occupied[addr].remove(comm)\n            if comm.closed():\n                # Either the user passed the close=True parameter to send_recv, or\n                # the RPC call raised OSError or CancelledError\n                self.semaphore.release()\n            else:\n                self.available[addr].add(comm)\n                if self.semaphore.locked() and self._pending_count:\n                    self.collect()\n\n    def collect(self) -> None:\n        \"\"\"\n        Collect open but unused communications, to allow opening other ones.\n        \"\"\"\n        logger.info(\n            \"Collecting unused comms.  open: %d, active: %d, connecting: %d\",\n            self.open,\n            self.active,\n            len(self._connecting),\n        )\n        for comms in self.available.values():\n            for comm in comms:\n                IOLoop.current().add_callback(comm.close)\n                self.semaphore.release()\n            comms.clear()\n\n    def remove(self, addr: str, *, reason: str = \"Address removed.\") -> None:\n        \"\"\"\n        Remove all Comms to a given address.\n        \"\"\"\n        logger.debug(\"Removing comms to %s\", addr)\n        if addr in self.available:\n            comms = self.available.pop(addr)\n            for comm in comms:\n                IOLoop.current().add_callback(comm.close)\n                self.semaphore.release()\n        if addr in self.occupied:\n            comms = self.occupied.pop(addr)\n            for comm in comms:\n                IOLoop.current().add_callback(comm.close)\n                self.semaphore.release()\n\n        if addr in self._connecting:\n            cbs = self._connecting[addr]\n            for cb in cbs:\n                cb(reason)\n\n    async def close(self) -> None:\n        \"\"\"\n        Close all communications\n        \"\"\"\n        self.status = Status.closed\n        for cbs in self._connecting.values():\n            for cb in cbs:\n                cb(\"ConnectionPool closing.\")\n        for d in [self.available, self.occupied]:\n            comms = set()\n            while d:\n                comms.update(d.popitem()[1])\n\n            await asyncio.gather(\n                *(comm.close() for comm in comms), return_exceptions=True\n            )\n\n            for _ in comms:\n                self.semaphore.release()\n\n        start = time()\n        while self._connecting:\n            if time() - start > self._connecting_close_timeout:\n                logger.warning(\n                    \"Pending connections refuse to cancel. %d connections pending. Closing anyway.\",\n                    len(self._connecting),\n                )\n                break\n            await asyncio.sleep(0.01)\n\n\ndef coerce_to_address(o):\n    if isinstance(o, (list, tuple)):\n        o = unparse_host_port(*o)\n\n    return normalize_address(o)\n\n\ndef collect_causes(e: BaseException) -> list[BaseException]:\n    causes = []\n    while e.__cause__ is not None:\n        causes.append(e.__cause__)\n        e = e.__cause__\n    return causes\n\n\nclass ErrorMessage(TypedDict):\n    status: str\n    exception: protocol.Serialize\n    traceback: protocol.Serialize | None\n    exception_text: str\n    traceback_text: str\n\n\nclass OKMessage(TypedDict):\n    status: Literal[\"OK\"]\n\n\ndef error_message(e: BaseException, status: str = \"error\") -> ErrorMessage:\n    \"\"\"Produce message to send back given an exception has occurred\n\n    This does the following:\n\n    1.  Gets the traceback\n    2.  Truncates the exception and the traceback\n    3.  Serializes the exception and traceback or\n    4.  If they can't be serialized send string versions\n    5.  Format a message and return\n\n    See Also\n    --------\n    clean_exception : deserialize and unpack message into exception/traceback\n    \"\"\"\n    MAX_ERROR_LEN = dask.config.get(\"distributed.admin.max-error-length\")\n    tblib.pickling_support.install(e, *collect_causes(e))\n    tb = get_traceback()\n    tb_text = \"\".join(traceback.format_tb(tb))\n    e = truncate_exception(e, MAX_ERROR_LEN)\n    try:\n        e_bytes = protocol.pickle.dumps(e)\n        protocol.pickle.loads(e_bytes)\n    except Exception:\n        e_bytes = protocol.pickle.dumps(Exception(repr(e)))\n    e_serialized = protocol.to_serialize(e_bytes)\n\n    try:\n        tb_bytes = protocol.pickle.dumps(tb)\n        protocol.pickle.loads(tb_bytes)\n    except Exception:\n        tb_bytes = protocol.pickle.dumps(tb_text)\n\n    if len(tb_bytes) > MAX_ERROR_LEN:\n        tb_serialized = None\n    else:\n        tb_serialized = protocol.to_serialize(tb_bytes)\n\n    return {\n        \"status\": status,\n        \"exception\": e_serialized,\n        \"traceback\": tb_serialized,\n        \"exception_text\": repr(e),\n        \"traceback_text\": tb_text,\n    }\n\n\ndef clean_exception(\n    exception: BaseException | bytes | bytearray | str | None,\n    traceback: types.TracebackType | bytes | str | None = None,\n    **kwargs: Any,\n) -> tuple[\n    type[BaseException | None], BaseException | None, types.TracebackType | None\n]:\n    \"\"\"Reraise exception and traceback. Deserialize if necessary\n\n    See Also\n    --------\n    error_message : create and serialize errors into message\n    \"\"\"\n    if isinstance(exception, (bytes, bytearray)):\n        try:\n            exception = protocol.pickle.loads(exception)\n        except Exception:\n            exception = Exception(exception)\n    elif isinstance(exception, str):\n        exception = Exception(exception)\n\n    if isinstance(traceback, bytes):\n        try:\n            traceback = protocol.pickle.loads(traceback)\n        except (TypeError, AttributeError):\n            traceback = None\n    elif isinstance(traceback, str):\n        traceback = None  # happens if the traceback failed serializing\n\n    assert isinstance(exception, BaseException) or exception is None\n    assert isinstance(traceback, types.TracebackType) or traceback is None\n    return type(exception), exception, traceback\n\n\n### Dependency File: metrics.py\nfrom __future__ import annotations\n\nimport asyncio\nimport collections\nimport sys\nimport threading\nimport time as timemod\nfrom collections.abc import Callable, Hashable, Iterator\nfrom contextlib import contextmanager\nfrom contextvars import ContextVar\nfrom dataclasses import dataclass\nfrom functools import wraps\nfrom math import nan\nfrom typing import Literal\n\nimport psutil\n\nfrom distributed.compatibility import WINDOWS\n\n_empty_namedtuple = collections.namedtuple(\"_empty_namedtuple\", ())\n\n\ndef _psutil_caller(method_name, default=_empty_namedtuple):\n    \"\"\"\n    Return a function calling the given psutil *method_name*,\n    or returning *default* if psutil fails.\n    \"\"\"\n    meth = getattr(psutil, method_name)\n\n    @wraps(meth)\n    def wrapper():  # pragma: no cover\n        try:\n            return meth()\n        except RuntimeError:\n            # This can happen on some systems (e.g. no physical disk in worker)\n            return default()\n\n    return wrapper\n\n\ndisk_io_counters = _psutil_caller(\"disk_io_counters\")\n\nnet_io_counters = _psutil_caller(\"net_io_counters\")\n\n\nclass _WindowsTime:\n    \"\"\"Combine time.time() or time.monotonic() with time.perf_counter() to get an\n    absolute clock with fine resolution.\n    \"\"\"\n\n    base_timer: Callable[[], float]\n    delta: float\n    previous: float | None\n    next_resync: float\n    resync_every: float\n\n    def __init__(\n        self, base: Callable[[], float], is_monotonic: bool, resync_every: float = 600.0\n    ):\n        self.base_timer = base\n        self.previous = float(\"-inf\") if is_monotonic else None\n        self.next_resync = float(\"-inf\")\n        self.resync_every = resync_every\n\n    def time(self) -> float:\n        cur = timemod.perf_counter()\n        if cur > self.next_resync:\n            self.resync()\n            self.next_resync = cur + self.resync_every\n        cur += self.delta\n        if self.previous is not None:\n            # Monotonic timer\n            if cur <= self.previous:\n                cur = self.previous + 1e-9\n            self.previous = cur\n        return cur\n\n    def resync(self) -> None:\n        _time = self.base_timer\n        _perf_counter = timemod.perf_counter\n        min_samples = 5\n        while True:\n            times = [(_time(), _perf_counter()) for _ in range(min_samples * 2)]\n            abs_times = collections.Counter(t[0] for t in times)\n            first, nfirst = abs_times.most_common()[0]\n            if nfirst < min_samples:\n                # System too noisy? Start again\n                continue\n\n            perf_times = [t[1] for t in times if t[0] == first][:-1]\n            assert len(perf_times) >= min_samples - 1, perf_times\n            self.delta = first - sum(perf_times) / len(perf_times)\n            break\n\n\n# A high-resolution wall clock timer measuring the seconds since Unix epoch\nif WINDOWS and sys.version_info < (3, 13):\n    time = _WindowsTime(timemod.time, is_monotonic=False).time\n    monotonic = _WindowsTime(timemod.monotonic, is_monotonic=True).time\nelse:\n    # Under modern Unixes, time.time() and time.monotonic() should be good enough\n    time = timemod.time\n    monotonic = timemod.monotonic\n\nprocess_time = timemod.process_time\n\n# Get a per-thread CPU timer function if possible, otherwise\n# use a per-process CPU timer function.\ntry:\n    # thread_time is not supported on all platforms\n    thread_time = timemod.thread_time\nexcept (AttributeError, OSError):  # pragma: no cover\n    thread_time = process_time\n\n\n@dataclass\nclass MeterOutput:\n    start: float\n    stop: float\n    delta: float\n    __slots__ = tuple(__annotations__)\n\n\n@contextmanager\ndef meter(\n    func: Callable[[], float] = timemod.perf_counter,\n    floor: float | Literal[False] = 0.0,\n) -> Iterator[MeterOutput]:\n    \"\"\"Convenience context manager which calls func() before and after the wrapped\n    code and calculates the delta.\n\n    Parameters\n    ----------\n    label: str\n        label to pass to the callback\n    func: callable\n        function to call before and after, which must return a number.\n        Besides time, it could return e.g. cumulative network traffic or disk usage.\n        Default: :func:`timemod.perf_counter`\n    floor: float or False, optional\n        Floor the delta to the given value (default: 0). This is useful for strictly\n        cumulative functions that can occasionally glitch and go backwards.\n        Set to False to disable.\n\n    Yields\n    ------\n    :class:`MeterOutput` where the ``start`` attribute is populated straight away, while\n    ``stop`` and ``delta`` are nan until context exit.\n    \"\"\"\n    out = MeterOutput(func(), nan, nan)\n    try:\n        yield out\n    finally:\n        out.stop = func()\n        out.delta = out.stop - out.start\n        if floor is not False:\n            out.delta = max(floor, out.delta)\n\n\nclass ContextMeter:\n    \"\"\"Context-based general purpose meter.\n\n    Usage\n    -----\n    1. In high level code, call :meth:`add_callback` to install a hook that defines an\n       activity\n    2. In low level code, typically many stack levels below, log quantitative events\n       (e.g. elapsed time, transferred bytes, etc.) so that they will be attributed to\n       the high-level code calling it, either with :meth:`meter`,\n       :meth:`meter_function`, or :meth:`digest_metric`.\n\n    Examples\n    --------\n    In the code that e.g. sends a Python object from A to B over the network:\n    >>> from distributed.metrics import context_meter\n    >>> with context_meter.add_callback(partial(print, \"A->B comms:\")):\n    ...     await send_over_the_network(obj)\n\n    In the serialization utilities, called many stack levels below:\n    >>> with context_meter.meter(\"dumps\"):\n    ...     pik = pickle.dumps(obj)\n    >>> with context_meter.meter(\"compress\"):\n    ...     pik = lz4.compress(pik)\n\n    And finally, elsewhere, deep into the TCP stack:\n    >>> with context_meter.meter(\"network-write\"):\n    ...     await comm.write(frames)\n\n    When you call the top-level code, you'll get::\n      A->B comms: dumps 0.012 seconds\n      A->B comms: compress 0.034 seconds\n      A->B comms: network-write 0.567 seconds\n    \"\"\"\n\n    _callbacks: ContextVar[dict[Hashable, Callable[[Hashable, float, str], None]]]\n\n    def __init__(self):\n        self._callbacks = ContextVar(\n            f\"MetricHook<{id(self)}>._callbacks\", default={}  # noqa: B039\n        )\n\n    def __reduce__(self):\n        assert self is context_meter, \"Found copy of singleton\"\n        return self._unpickle_singleton, ()\n\n    @staticmethod\n    def _unpickle_singleton():\n        return context_meter\n\n    @contextmanager\n    def add_callback(\n        self,\n        callback: Callable[[Hashable, float, str], None],\n        *,\n        key: Hashable | None = None,\n        allow_offload: bool = False,\n    ) -> Iterator[None]:\n        \"\"\"Add a callback when entering the context and remove it when exiting it.\n        The callback must accept the same parameters as :meth:`digest_metric`.\n\n        Parameters\n        ----------\n        callback: Callable\n            ``f(label, value, unit)`` to be executed\n        key: Hashable, optional\n            Unique key for the callback. If two nested calls to ``add_callback`` use the\n            same key, suppress the outermost callback.\n        allow_offload: bool, optional\n            If set to True, this context must be executed inside a running asyncio\n            event loop. If a call to :meth:`digest_metric` is performed from a different\n            thread, e.g. from inside :func:`distributed.utils.offload`, ensure that\n            the callback is executed in the event loop's thread instead.\n        \"\"\"\n        if allow_offload:\n            loop = asyncio.get_running_loop()\n            tid = threading.get_ident()\n\n            def safe_cb(label: Hashable, value: float, unit: str, /) -> None:\n                if threading.get_ident() == tid:\n                    callback(label, value, unit)\n                else:  # We're inside offload()\n                    loop.call_soon_threadsafe(callback, label, value, unit)\n\n        else:\n            safe_cb = callback\n\n        if key is None:\n            key = object()\n        cbs = self._callbacks.get()\n        cbs = cbs.copy()\n        cbs[key] = safe_cb\n        tok = self._callbacks.set(cbs)\n        try:\n            yield\n        finally:\n            tok.var.reset(tok)\n\n    @contextmanager\n    def clear_callbacks(self) -> Iterator[None]:\n        \"\"\"Do not trigger any callbacks set outside of this context\"\"\"\n        tok = self._callbacks.set({})\n        try:\n            yield\n        finally:\n            tok.var.reset(tok)\n\n    def digest_metric(self, label: Hashable, value: float, unit: str) -> None:\n        \"\"\"Invoke the currently set context callbacks for an arbitrary quantitative\n        metric.\n        \"\"\"\n        cbs = self._callbacks.get()\n        for cb in cbs.values():\n            cb(label, value, unit)\n\n    @contextmanager\n    def meter(\n        self,\n        label: Hashable,\n        unit: str = \"seconds\",\n        func: Callable[[], float] = timemod.perf_counter,\n        floor: float | Literal[False] = 0.0,\n    ) -> Iterator[MeterOutput]:\n        \"\"\"Convenience context manager or decorator which calls func() before and after\n        the wrapped code, calculates the delta, and finally calls :meth:`digest_metric`.\n\n        If unit=='seconds', it also subtracts any other calls to :meth:`meter` or\n        :meth:`digest_metric` with the same unit performed within the context, so that\n        the total is strictly additive.\n\n        Parameters\n        ----------\n        label: Hashable\n            label to pass to the callback\n        unit: str, optional\n            unit to pass to the callback. Default: seconds\n        func: callable\n            see :func:`meter`\n        floor: bool, optional\n            see :func:`meter`\n\n        Yields\n        ------\n        :class:`MeterOutput` where the ``start`` attribute is populated straight away,\n        while ``stop`` and ``delta`` are nan until context exit. In case of multiple\n        nested calls to :meth:`meter`, then delta (for seconds only) is reduced by the\n        inner metrics, to a minimum of ``floor``.\n        \"\"\"\n        if unit != \"seconds\":\n            try:\n                with meter(func, floor=floor) as m:\n                    yield m\n            finally:\n                self.digest_metric(label, m.delta, unit)\n            return\n\n        # If unit==\"seconds\", subtract time metered from the sub-contexts\n        offsets = []\n\n        def callback(label2: Hashable, value2: float, unit2: str) -> None:\n            if unit2 == unit:\n                # This must be threadsafe to support callbacks invoked from\n                # distributed.utils.offload; '+=' on a float would not be threadsafe!\n                offsets.append(value2)\n\n        try:\n            with self.add_callback(callback), meter(func, floor=False) as m:\n                yield m\n        finally:\n            delta = m.delta - sum(offsets)\n            if floor is not False:\n                delta = max(floor, delta)\n            m.delta = delta\n            self.digest_metric(label, delta, unit)\n\n\ncontext_meter = ContextMeter()\n\n\nclass DelayedMetricsLedger:\n    \"\"\"Add-on to :class:`ContextMeter` that helps in the case where:\n\n    - The code to be metered is not easily expressed as a self-contained code block\n      e.g. you want to measure latency in the asyncio event loop before and after\n      running a task\n    - You want to alter the metrics depending on how the code ends; e.g. you want to\n      post them differently in case of failure.\n\n    Examples\n    --------\n    >>> ledger = DelayedMetricsLedger()  # Metering starts here\n    >>> async def wrapper():\n    ...     with ledger.record():\n    ...         return await metered_function()\n    >>> task = asyncio.create_task(wrapper())\n    >>> # (later, elsewhere)\n    >>> try:\n    ...     await task\n    ...     coarse_time = False\n    ... except Exception:\n    ...     coarse_time = \"failed\"\n    ...     raise\n    ... finally:\n    ...     # Metering stops here\n    ...     for label, value, unit in ledger.finalize(coarse_time):\n    ...         # actually log metrics\n    \"\"\"\n\n    func: Callable[[], float]\n    start: float\n    metrics: list[tuple[Hashable, float, str]]  # (label, value, unit)\n\n    def __init__(self, func: Callable[[], float] = timemod.perf_counter):\n        self.func = func\n        self.start = func()\n        self.metrics = []\n\n    def _callback(self, label: Hashable, value: float, unit: str) -> None:\n        self.metrics.append((label, value, unit))\n\n    @contextmanager\n    def record(self, *, key: Hashable | None = None) -> Iterator[None]:\n        \"\"\"Ingest metrics logged with :meth:`ContextMeter.digest_metric` or\n        :meth:`ContextMeter.meter` and temporarily store them in :ivar:`metrics`.\n\n        Parameters\n        ----------\n        key: Hashable, optional\n            See :meth:`ContextMeter.add_callback`\n        \"\"\"\n        with context_meter.add_callback(self._callback, key=key):\n            yield\n\n    def finalize(\n        self,\n        coarse_time: str | Literal[False] = False,\n        floor: float | Literal[False] = 0.0,\n    ) -> Iterator[tuple[Hashable, float, str]]:\n        \"\"\"The metered code is terminated, and we now know how to log it.\n\n        Parameters\n        ----------\n        coarse_time: str | False, optional\n            False\n                Yield all acquired metrics, plus an extra time metric, labelled \"other\",\n                which is the time between creating the DelayedMetricsLedger and\n                calling this method, minus any time logged in the metrics.\n            label\n                Yield all acquired non-time metrics.\n                Yield a single metric, labelled <coarse_time>, which is the time\n                between creating the DelayedMetricsLedger and calling this method.\n        floor: float | False, optional\n            Floor either the \"other\" or the <coarse_time> metric to this value\n             (default: 0). Set to False to disable.\n        \"\"\"\n        stop = self.func()\n        delta = stop - self.start\n\n        for label, value, unit in self.metrics:\n            if unit != \"seconds\" or not coarse_time:\n                yield label, value, unit\n            if unit == \"seconds\" and not coarse_time:\n                delta -= value\n\n        if floor is not False:\n            delta = max(floor, delta)\n        yield coarse_time or \"other\", delta, \"seconds\"\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:18:01"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport asyncio\nimport random\n\nimport pytest\nfrom tlz import assoc\n\nfrom distributed.batched import BatchedSend\nfrom distributed.core import CommClosedError, connect, listen\nfrom distributed.metrics import time\nfrom distributed.protocol import to_serialize\nfrom distributed.utils import All, wait_for\nfrom distributed.utils_test import captured_logger, gen_test\n\n\nclass EchoServer:\n    count = 0\n\n    async def handle_comm(self, comm):\n        while True:\n            try:\n                msg = await comm.read()\n                self.count += 1\n                await comm.write(msg)\n            except CommClosedError as e:\n                return\n\n    async def listen(self):\n        listener = await listen(\"\", self.handle_comm)\n        self.address = listener.contact_address\n        self.stop = listener.stop\n\n    async def __aenter__(self):\n        await self.listen()\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        self.stop()\n\n\n@gen_test()\nasync def test_BatchedSend():\n    async with EchoServer() as e:\n        comm = await connect(e.address)\n\n        b = BatchedSend(interval=10)\n        assert str(len(b.buffer)) in str(b)\n        assert str(len(b.buffer)) in repr(b)\n        b.start(comm)\n\n        await asyncio.sleep(0.020)\n\n        b.send(\"hello\")\n        b.send(\"hello\")\n        b.send(\"world\")\n        await asyncio.sleep(0.020)\n        b.send(\"HELLO\")\n        b.send(\"HELLO\")\n\n        result = await comm.read()\n        assert result == (\"hello\", \"hello\", \"world\")\n        result = await comm.read()\n        assert result == (\"HELLO\", \"HELLO\")\n\n        assert b.byte_count > 1\n        await comm.close()\n\n\n@gen_test()\nasync def test_send_before_start():\n    async with EchoServer() as e:\n        comm = await connect(e.address)\n\n        b = BatchedSend(interval=10)\n\n        b.send(\"hello\")\n        b.send(\"world\")\n\n        b.start(comm)\n        result = await comm.read()\n        assert result == (\"hello\", \"world\")\n        await comm.close()\n\n\n@gen_test()\nasync def test_send_after_stream_start():\n    async with EchoServer() as e:\n        comm = await connect(e.address)\n\n        b = BatchedSend(interval=10)\n\n        b.start(comm)\n        b.send(\"hello\")\n        b.send(\"world\")\n        result = await comm.read()\n        if len(result) < 2:\n            result += await comm.read()\n        assert result == (\"hello\", \"world\")\n        await comm.close()\n\n\n@gen_test()\nasync def test_send_before_close():\n    async with EchoServer() as e:\n        comm = await connect(e.address)\n\n        b = BatchedSend(interval=10)\n        b.start(comm)\n\n        cnt = int(e.count)\n        b.send(\"hello\")\n        await b.close()  # close immediately after sending\n        assert not b.buffer\n\n        start = time()\n        while e.count != cnt + 1:\n            await asyncio.sleep(0.01)\n            assert time() < start + 5\n\n        with pytest.raises(CommClosedError):\n            b.send(\"123\")\n\n\n@gen_test()\nasync def test_close_closed():\n    async with EchoServer() as e:\n        comm = await connect(e.address)\n\n        b = BatchedSend(interval=10)\n        b.start(comm)\n\n        b.send(123)\n        await comm.close()  # external closing\n\n        await b.close()\n        assert \"closed\" in repr(b)\n        assert \"closed\" in str(b)\n\n\n@gen_test()\nasync def test_close_not_started():\n    b = BatchedSend(interval=10)\n    await b.close()\n\n\n@gen_test()\nasync def test_close_twice():\n    async with EchoServer() as e:\n        comm = await connect(e.address)\n\n        b = BatchedSend(interval=10)\n        b.start(comm)\n        await b.close()\n        await b.close()\n\n\n@pytest.mark.slow\n@gen_test()\nasync def test_stress():\n    async with EchoServer() as e:\n        comm = await connect(e.address)\n        L = []\n\n        async def send():\n            b = BatchedSend(interval=3)\n            b.start(comm)\n            for i in range(0, 10000, 2):\n                b.send(i)\n                b.send(i + 1)\n                await asyncio.sleep(0.00001 * random.randint(1, 10))\n\n        async def recv():\n            while True:\n                result = await wait_for(comm.read(), 1)\n                L.extend(result)\n                if result[-1] == 9999:\n                    break\n\n        await All([send(), recv()])\n\n        assert L == list(range(0, 10000, 1))\n        await comm.close()\n\n\nasync def run_traffic_jam(nsends, nbytes):\n    # This test eats `nsends * nbytes` bytes in RAM\n    np = pytest.importorskip(\"numpy\")\n    from distributed.protocol import to_serialize\n\n    data = bytes(np.random.randint(0, 255, size=(nbytes,)).astype(\"u1\").data)\n    async with EchoServer() as e:\n        comm = await connect(e.address)\n\n        b = BatchedSend(interval=0.01)\n        b.start(comm)\n\n        msg = {\"x\": to_serialize(data)}\n        for i in range(nsends):\n            b.send(assoc(msg, \"i\", i))\n            if np.random.random() > 0.5:\n                await asyncio.sleep(0.001)\n\n        results = []\n        count = 0\n        while len(results) < nsends:\n            # If this times out then I think it's a backpressure issue\n            # Somehow we're able to flood the socket so that the receiving end\n            # loses some of our messages\n            L = await wait_for(comm.read(), 5)\n            count += 1\n            results.extend(r[\"i\"] for r in L)\n\n        assert count == b.batch_count == e.count\n        assert b.message_count == nsends\n\n        assert results == list(range(nsends))\n\n        await comm.close()  # external closing\n        await b.close()\n\n\n@gen_test()\nasync def test_sending_traffic_jam():\n    await run_traffic_jam(50, 300000)\n\n\n@pytest.mark.slow\n@gen_test()\nasync def test_large_traffic_jam():\n    await run_traffic_jam(500, 1500000)\n\n\n@gen_test()\nasync def test_serializers():\n    async with EchoServer() as e:\n        comm = await connect(e.address)\n\n        b = BatchedSend(interval=\"10ms\", serializers=[\"msgpack\"])\n        b.start(comm)\n\n        b.send({\"x\": to_serialize(123)})\n        b.send({\"x\": to_serialize(\"hello\")})\n        await asyncio.sleep(0.100)\n\n        b.send({\"x\": to_serialize(lambda x: x + 1)})\n\n        with captured_logger(\"distributed.protocol\") as sio:\n            await asyncio.sleep(0.100)\n\n        value = sio.getvalue()\n        assert \"serialize\" in value\n        assert \"type\" in value\n        assert \"function\" in value\n\n        assert comm.closed()\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: distributed\nTest File Path: distributed\\test_chaos\\test_chaos.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport asyncio\nimport ctypes\nimport random\nimport sys\nfrom typing import Literal\n\nfrom dask.utils import parse_timedelta\n\nfrom distributed.diagnostics.plugin import WorkerPlugin\n\n\nclass KillWorker(WorkerPlugin):\n    \"\"\"Kill Workers Randomly\n\n    This kills workers in a cluster randomly.  It is intended to be used in\n    stress testing.\n\n    Parameters\n    ----------\n    delay: str\n        The expected amount of time for a worker to live.\n        The actual time will vary, treating worker death as a poisson process.\n    mode: str\n        or \"graceful\" which calls worker.close(...)\n        Either \"sys.exit\" which calls sys.exit(0)\n        or \"segfault\" which triggers a segfault\n    \"\"\"\n\n    def __init__(\n        self,\n        delay: str | int | float = \"100 s\",\n        mode: Literal[\"sys.exit\", \"graceful\", \"segfault\"] = \"sys.exit\",\n    ):\n        self.delay = parse_timedelta(delay)\n        if mode not in (\"sys.exit\", \"graceful\", \"segfault\"):\n            raise ValueError(\n                f\"Three modes supported, 'sys.exit', 'graceful', and 'segfault'. \"\n                f\"got {mode!r}\"\n            )\n        self.mode = mode\n\n    async def setup(self, worker):\n        self.worker = worker\n        if self.mode == \"graceful\":\n            f = self.graceful\n        elif self.mode == \"sys.exit\":\n            f = self.sys_exit\n        elif self.mode == \"segfault\":\n            f = self.segfault\n\n        self.worker.loop.asyncio_loop.call_later(\n            delay=random.expovariate(1 / self.delay),\n            callback=f,\n        )\n\n    def graceful(self):\n        asyncio.create_task(self.worker.close(nanny=False, executor_wait=False))\n\n    def sys_exit(self):\n        sys.exit(0)\n\n    def segfault(self):\n        \"\"\"\n        Magic, from https://gist.github.com/coolreader18/6dbe0be2ae2192e90e1a809f1624c694?permalink_comment_id=3874116#gistcomment-3874116\n        \"\"\"\n        ctypes.string_at(0)\n\n### Source File Dependency Files Content\n### Dependency File: plugin.py\nfrom __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport functools\nimport logging\nimport os\nimport socket\nimport subprocess\nimport sys\nimport tempfile\nimport uuid\nimport zipfile\nfrom collections.abc import Awaitable\nfrom typing import TYPE_CHECKING, Any, Callable, ClassVar\n\nfrom dask.typing import Key\nfrom dask.utils import _deprecated_kwarg, funcname, tmpfile\n\nfrom distributed.protocol.pickle import dumps\n\nif TYPE_CHECKING:\n    # circular imports\n    # Needed to avoid Sphinx WARNING: more than one target found for cross-reference\n    # 'WorkerState'\"\n    # https://github.com/agronholm/sphinx-autodoc-typehints#dealing-with-circular-imports\n    from distributed import scheduler as scheduler_module\n    from distributed.scheduler import Scheduler\n    from distributed.scheduler import TaskStateState as SchedulerTaskStateState\n    from distributed.worker import Worker\n    from distributed.worker_state_machine import TaskStateState as WorkerTaskStateState\n\nlogger = logging.getLogger(__name__)\n\n\nclass SchedulerPlugin:\n    \"\"\"Interface to extend the Scheduler\n\n    A plugin enables custom hooks to run when specific events occur.\n    The scheduler will run the methods of this plugin whenever the corresponding\n    method of the scheduler is run.  This runs user code within the scheduler\n    thread that can perform arbitrary operations in synchrony with the scheduler\n    itself.\n\n    Plugins are often used for diagnostics and measurement, but have full\n    access to the scheduler and could in principle affect core scheduling.\n\n    To implement a plugin:\n\n    1. inherit from this class\n    2. override some of its methods\n    3. register the plugin using :meth:`Client.register_plugin<distributed.Client.register_plugin>`.\n\n    The ``idempotent`` attribute is used to control whether or not the plugin should\n    be ignored upon registration if a scheduler plugin with the same name already exists.\n    If ``True``, the plugin is ignored, otherwise the existing plugin is replaced.\n    Defaults to ``False``.\n\n    Examples\n    --------\n    >>> class Counter(SchedulerPlugin):\n    ...     def __init__(self):\n    ...         self.counter = 0\n    ...\n    ...     def transition(self, key, start, finish, *args, **kwargs):\n    ...         if start == 'processing' and finish == 'memory':\n    ...             self.counter += 1\n    ...\n    ...     def restart(self, scheduler):\n    ...         self.counter = 0\n\n    >>> plugin = Counter()\n    >>> scheduler.add_plugin(plugin)  # doctest: +SKIP\n    \"\"\"\n\n    idempotent: bool = False\n\n    async def start(self, scheduler: Scheduler) -> None:\n        \"\"\"Run when the scheduler starts up\n\n        This runs at the end of the Scheduler startup process\n        \"\"\"\n\n    async def before_close(self) -> None:\n        \"\"\"Runs prior to any Scheduler shutdown logic\"\"\"\n\n    async def close(self) -> None:\n        \"\"\"Run when the scheduler closes down\n\n        This runs at the beginning of the Scheduler shutdown process, but after\n        workers have been asked to shut down gracefully\n        \"\"\"\n\n    def update_graph(\n        self,\n        scheduler: Scheduler,\n        *,\n        client: str,\n        keys: set[Key],\n        tasks: list[Key],\n        annotations: dict[str, dict[Key, Any]],\n        priority: dict[Key, tuple[int | float, ...]],\n        dependencies: dict[Key, set[Key]],\n        stimulus_id: str,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when a new graph / tasks enter the scheduler\n\n        Parameters\n        ----------\n            scheduler:\n                The `Scheduler` instance.\n            client:\n                The unique Client id.\n            keys:\n                The keys the Client is interested in when calling `update_graph`.\n            tasks:\n                The\n            annotations:\n                Fully resolved annotations as applied to the tasks in the format::\n\n                    {\n                        \"annotation\": {\n                            \"key\": \"value,\n                            ...\n                        },\n                        ...\n                    }\n            priority:\n                Task calculated priorities as assigned to the tasks.\n            dependencies:\n                A mapping that maps a key to its dependencies.\n            stimulus_id:\n                ID of the stimulus causing the graph update\n            **kwargs:\n                It is recommended to allow plugins to accept more parameters to\n                ensure future compatibility.\n        \"\"\"\n\n    def restart(self, scheduler: Scheduler) -> None:\n        \"\"\"Run when the scheduler restarts itself\"\"\"\n\n    def transition(\n        self,\n        key: Key,\n        start: SchedulerTaskStateState,\n        finish: SchedulerTaskStateState,\n        *args: Any,\n        stimulus_id: str,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run whenever a task changes state\n\n        For a description of the transition mechanism and the available states,\n        see :ref:`Scheduler task states <scheduler-task-state>`.\n\n        .. warning::\n\n            This is an advanced feature and the transition mechanism and details\n            of task states are subject to change without deprecation cycle.\n\n        Parameters\n        ----------\n        key :\n        start :\n            Start state of the transition.\n            One of released, waiting, processing, memory, error.\n        finish :\n            Final state of the transition.\n        stimulus_id :\n            ID of stimulus causing the transition.\n        *args, **kwargs :\n            More options passed when transitioning\n            This may include worker ID, compute time, etc.\n        \"\"\"\n\n    def add_worker(self, scheduler: Scheduler, worker: str) -> None | Awaitable[None]:\n        \"\"\"Run when a new worker enters the cluster\n\n        If this method is synchronous, it is immediately and synchronously executed\n        without ``Scheduler.add_worker`` ever yielding to the event loop.\n        If it is asynchronous, it will be awaited after all synchronous\n        ``SchedulerPlugin.add_worker`` hooks have executed.\n\n        .. warning::\n\n            There are no guarantees about the execution order between individual\n            ``SchedulerPlugin.add_worker`` hooks and the ordering may be subject\n            to change without deprecation cycle.\n        \"\"\"\n\n    def remove_worker(\n        self, scheduler: Scheduler, worker: str, *, stimulus_id: str, **kwargs: Any\n    ) -> None | Awaitable[None]:\n        \"\"\"Run when a worker leaves the cluster\n\n        If this method is synchronous, it is immediately and synchronously executed\n        without ``Scheduler.remove_worker`` ever yielding to the event loop.\n        If it is asynchronous, it will be awaited after all synchronous\n        ``SchedulerPlugin.remove_worker`` hooks have executed.\n\n        .. warning::\n\n            There are no guarantees about the execution order between individual\n            ``SchedulerPlugin.remove_worker`` hooks and the ordering may be subject\n            to change without deprecation cycle.\n        \"\"\"\n\n    def add_client(self, scheduler: Scheduler, client: str) -> None:\n        \"\"\"Run when a new client connects\"\"\"\n\n    def remove_client(self, scheduler: Scheduler, client: str) -> None:\n        \"\"\"Run when a client disconnects\"\"\"\n\n    def valid_workers_downscaling(\n        self, scheduler: Scheduler, workers: list[scheduler_module.WorkerState]\n    ) -> list[scheduler_module.WorkerState]:\n        \"\"\"Determine which workers can be removed from the cluster\n\n        This method is called when the scheduler is about to downscale the cluster\n        by removing workers. The method should return a set of worker states that\n        can be removed from the cluster.\n\n        Parameters\n        ----------\n        workers : list\n            The list of worker states that are candidates for removal.\n        stimulus_id : str\n            ID of stimulus causing the downscaling.\n\n        Returns\n        -------\n        list\n            The list of worker states that can be removed from the cluster.\n        \"\"\"\n        return workers\n\n    def log_event(self, topic: str, msg: Any) -> None:\n        \"\"\"Run when an event is logged\"\"\"\n\n\nclass WorkerPlugin:\n    \"\"\"Interface to extend the Worker\n\n    A worker plugin enables custom code to run at different stages of the Workers'\n    lifecycle.\n\n    A plugin enables custom code to run at each of step of a Workers's life. Whenever such\n    an event happens, the corresponding method on this class will be called. Note that the\n    user code always runs within the Worker's main thread.\n\n    To implement a plugin:\n\n    1. inherit from this class\n    2. override some of its methods\n    3. register the plugin using :meth:`Client.register_plugin<distributed.Client.register_plugin>`.\n\n    The ``idempotent`` attribute is used to control whether or not the plugin should\n    be ignored upon registration if a worker plugin with the same name already exists.\n    If ``True``, the plugin is ignored, otherwise the existing plugin is replaced.\n    Defaults to ``False``.\n\n    Examples\n    --------\n    >>> class ErrorLogger(WorkerPlugin):\n    ...     def __init__(self, logger):\n    ...         self.logger = logger\n    ...\n    ...     def setup(self, worker):\n    ...         self.worker = worker\n    ...\n    ...     def transition(self, key, start, finish, *args, **kwargs):\n    ...         if finish == 'error':\n    ...             ts = self.worker.tasks[key]\n    ...             exc_info = (type(ts.exception), ts.exception, ts.traceback)\n    ...             self.logger.error(\n    ...                 \"Error during computation of '%s'.\", key,\n    ...                 exc_info=exc_info\n    ...             )\n\n    >>> import logging\n    >>> plugin = ErrorLogger(logging)\n    >>> client.register_plugin(plugin)  # doctest: +SKIP\n    \"\"\"\n\n    idempotent: bool = False\n\n    def setup(self, worker: Worker) -> None | Awaitable[None]:\n        \"\"\"\n        Run when the plugin is attached to a worker. This happens when the plugin is registered\n        and attached to existing workers, or when a worker is created after the plugin has been\n        registered.\n        \"\"\"\n\n    def teardown(self, worker: Worker) -> None | Awaitable[None]:\n        \"\"\"Run when the worker to which the plugin is attached is closed, or\n        when the plugin is removed.\"\"\"\n\n    def transition(\n        self,\n        key: Key,\n        start: WorkerTaskStateState,\n        finish: WorkerTaskStateState,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"\n        Throughout the lifecycle of a task (see :doc:`Worker State\n        <worker-state>`), Workers are instructed by the scheduler to compute\n        certain tasks, resulting in transitions in the state of each task. The\n        Worker owning the task is then notified of this state transition.\n\n        Whenever a task changes its state, this method will be called.\n\n        .. warning::\n\n            This is an advanced feature and the transition mechanism and details\n            of task states are subject to change without deprecation cycle.\n\n        Parameters\n        ----------\n        key :\n        start :\n            Start state of the transition.\n            One of waiting, ready, executing, long-running, memory, error.\n        finish :\n            Final state of the transition.\n        kwargs :\n            More options passed when transitioning\n        \"\"\"\n\n\nclass NannyPlugin:\n    \"\"\"Interface to extend the Nanny\n\n    A worker plugin enables custom code to run at different stages of the Workers'\n    lifecycle. A nanny plugin does the same thing, but benefits from being able\n    to run code before the worker is started, or to restart the worker if\n    necessary.\n\n    To implement a plugin:\n\n    1. inherit from this class\n    2. override some of its methods\n    3. register the plugin using :meth:`Client.register_plugin<distributed.Client.register_plugin>`.\n\n    The ``idempotent`` attribute is used to control whether or not the plugin should\n    be ignored upon registration if a nanny plugin with the same name already exists.\n    If ``True``, the plugin is ignored, otherwise the existing plugin is replaced.\n    Defaults to ``False``.\n\n    The ``restart`` attribute is used to control whether or not a running ``Worker``\n    needs to be restarted when registering the plugin.\n\n    See Also\n    --------\n    WorkerPlugin\n    SchedulerPlugin\n    \"\"\"\n\n    idempotent: bool = False\n    restart: bool = False\n\n    def setup(self, nanny):\n        \"\"\"\n        Run when the plugin is attached to a nanny. This happens when the plugin is registered\n        and attached to existing nannies, or when a nanny is created after the plugin has been\n        registered.\n        \"\"\"\n\n    def teardown(self, nanny):\n        \"\"\"Run when the nanny to which the plugin is attached to is closed\"\"\"\n\n\ndef _get_plugin_name(plugin: SchedulerPlugin | WorkerPlugin | NannyPlugin) -> str:\n    \"\"\"Return plugin name.\n\n    If plugin has no name attribute a random name is used.\n\n    \"\"\"\n    if hasattr(plugin, \"name\"):\n        return plugin.name\n    else:\n        return funcname(type(plugin)) + \"-\" + str(uuid.uuid4())\n\n\nclass SchedulerUploadFile(SchedulerPlugin):\n    name = \"upload_file\"\n\n    def __init__(self, filepath: str, load: bool = True):\n        \"\"\"\n        Initialize the plugin by reading in the data from the given file.\n        \"\"\"\n        self.filename = os.path.basename(filepath)\n        self.load = load\n        with open(filepath, \"rb\") as f:\n            self.data = f.read()\n\n    async def start(self, scheduler: Scheduler) -> None:\n        await scheduler.upload_file(self.filename, self.data, load=self.load)\n\n\nclass InstallPlugin(SchedulerPlugin):\n    \"\"\"Scheduler plugin to install software on the cluster\n\n    This accepts an function that installs software on the scheduler and\n    all workers. You can also optionally ask for the worker to restart\n    after performing this installation.\n\n    .. note::\n\n       This will increase the time it takes to start up\n       each worker. If possible, we recommend including the\n       software in the worker environment or image. This is\n       primarily intended for experimentation and debugging.\n\n    Parameters\n    ----------\n    install_fn\n        Callable used to install the software; must be idempotent.\n    restart_workers\n        Whether or not to restart the worker after installing the packages\n        Only functions if the worker has an attached nanny process\n\n    See Also\n    --------\n    CondaInstall\n    PipInstall\n    \"\"\"\n\n    idempotent = True\n    _lock: ClassVar[asyncio.Lock | None] = None\n\n    _install_fn: Callable[[], None]\n    name: str\n    restart_workers: bool\n    _scheduler: Scheduler\n\n    def __init__(\n        self,\n        install_fn: Callable[[], None],\n        restart_workers: bool,\n    ):\n        self._install_fn = install_fn\n        self.restart_workers = restart_workers\n        self.name = f\"{self.__class__.__name__}-{uuid.uuid4()}\"\n\n    async def start(self, scheduler: Scheduler) -> None:\n        from distributed.core import clean_exception\n        from distributed.protocol.serialize import Serialized, deserialize\n\n        self._scheduler = scheduler\n\n        if InstallPlugin._lock is None:\n            InstallPlugin._lock = asyncio.Lock()\n\n        async with InstallPlugin._lock:\n            self._install_fn()\n\n            if self.restart_workers:\n                nanny_plugin = _InstallNannyPlugin(self._install_fn, self.name)\n                responses = await scheduler.register_nanny_plugin(\n                    comm=None,\n                    plugin=dumps(nanny_plugin),\n                    name=self.name,\n                    idempotent=True,\n                )\n            else:\n                worker_plugin = _InstallWorkerPlugin(self._install_fn, self.name)\n                responses = await scheduler.register_worker_plugin(\n                    comm=None,\n                    plugin=dumps(worker_plugin),\n                    name=self.name,\n                    idempotent=True,\n                )\n            for response in responses.values():\n                if response[\"status\"] == \"error\":\n                    response = {  # type: ignore[unreachable]\n                        k: deserialize(v.header, v.frames)\n                        for k, v in response.items()\n                        if isinstance(v, Serialized)\n                    }\n                    _, exc, tb = clean_exception(**response)\n                    raise exc.with_traceback(tb)\n\n    async def close(self) -> None:\n        assert InstallPlugin._lock is not None\n\n        async with InstallPlugin._lock:\n            if self.restart_workers:\n                await self._scheduler.unregister_nanny_plugin(comm=None, name=self.name)\n            else:\n                await self._scheduler.unregister_worker_plugin(\n                    comm=None, name=self.name\n                )\n\n\nclass _InstallNannyPlugin(NannyPlugin):\n    \"\"\"Nanny plugin to install software on the nanny and then restart the workers\n\n    This accepts an installer which will installs software on all nannies.\n\n    .. note::\n\n       This will increase the time it takes to start up\n       the cluster. If possible, we recommend including the\n       software in the cluster environment or image. This is\n       primarily intended for experimentation and debugging.\n\n    Parameters\n    ----------\n    install_fn\n        Function that installs software\n    name:\n        Name of the plugin\n\n    See Also\n    --------\n    InstallPlugin\n    \"\"\"\n\n    _install_fn: Callable[[], None]\n    name: str\n    restart = True\n\n    def __init__(self, _install_fn: Callable[[], None], name: str):\n        self._install_fn = _install_fn\n        self.name = name\n\n    async def setup(self, nanny):\n        from distributed.semaphore import Semaphore\n\n        async with await Semaphore(\n            max_leases=1,\n            name=socket.gethostname(),\n            scheduler_rpc=nanny.scheduler,\n            loop=nanny.loop,\n        ):\n            self._install_fn()\n\n\nclass _InstallWorkerPlugin(WorkerPlugin):\n    \"\"\"Worker plugin to install software on the worker\n\n    This accepts an installer which will installs software on all workers.\n\n    .. note::\n\n       This will increase the time it takes to start up\n       the cluster. If possible, we recommend including the\n       software in the cluster environment or image. This is\n       primarily intended for experimentation and debugging.\n\n    Parameters\n    ----------\n    install_fn\n        Callable that installs software\n    name:\n        Name of the plugin\n\n    See Also\n    --------\n    InstallPlugin\n    \"\"\"\n\n    _install_fn: Callable[[], None]\n    name: str\n\n    def __init__(self, install_fn: Callable[[], None], name: str):\n        self._install_fn = install_fn\n        self.name = name\n\n    async def setup(self, worker):\n        from distributed.semaphore import Semaphore\n\n        async with await Semaphore(\n            max_leases=1,\n            name=socket.gethostname(),\n            scheduler_rpc=worker.scheduler,\n            loop=worker.loop,\n        ):\n            self._install_fn()\n\n\nclass CondaInstall(InstallPlugin):\n    \"\"\"A plugin to conda install a set of packages\n\n    This accepts a set of packages to install on the scheduler and all\n    workers as well as options to use when installing.\n\n    You can also optionally ask for the workers to restart after\n    performing this installation.\n\n    .. note::\n\n       This will increase the time it takes to start up\n       the cluster. If possible, we recommend including the\n       libraries in the cluster environment or image. This is\n       primarily intended for experimentation and debugging.\n\n    Parameters\n    ----------\n    packages\n        A list of packages (with optional versions) to install using conda\n    conda_options\n        Additional options to pass to conda\n    restart_workers\n        Whether or not to restart the worker after installing the packages\n        Only functions if the workers have an attached nanny process\n\n    Examples\n    --------\n    >>> from dask.distributed import CondaInstall\n    >>> plugin = CondaInstall(packages=[\"scikit-learn\"], conda_options=[\"--update-deps\"])\n\n    >>> client.register_plugin(plugin)\n\n    See Also\n    --------\n    InstallPlugin\n    PipInstall\n    \"\"\"\n\n    def __init__(\n        self,\n        packages: list[str],\n        conda_options: list[str] | None = None,\n        restart_workers: bool = False,\n    ):\n        installer = _CondaInstaller(packages, conda_options)\n        super().__init__(installer, restart_workers=restart_workers)\n\n\nclass _CondaInstaller:\n    INSTALLER = \"conda\"\n\n    packages: list[str]\n    conda_options: list[str]\n\n    def __init__(self, packages: list[str], conda_options: list[str] | None = None):\n        self.packages = packages\n        self.conda_options = conda_options or []\n\n    def __call__(self) -> None:\n        logger.info(\n            \"%s installing the following packages: %s\",\n            self.INSTALLER,\n            self.packages,\n        )\n        try:\n            from conda.cli.python_api import Commands, run_command\n        except ModuleNotFoundError as e:  # pragma: nocover\n            msg = (\n                \"conda install failed because conda could not be found. \"\n                \"Please make sure that conda is installed.\"\n            )\n            logger.error(msg)\n            raise RuntimeError(msg) from e\n        try:\n            _, stderr, returncode = run_command(\n                Commands.INSTALL, self.conda_options + self.packages\n            )\n        except Exception as e:\n            msg = \"conda install failed\"\n            logger.error(msg)\n            raise RuntimeError(msg) from e\n\n        if returncode != 0:\n            msg = f\"conda install failed with '{stderr.decode().strip()}'\"\n            logger.error(msg)\n            raise RuntimeError(msg)\n\n\nclass PipInstall(InstallPlugin):\n    \"\"\"A plugin to pip install a set of packages\n\n    This accepts a set of packages to install on all workers as well as\n    options to use when installing.\n    You can also optionally ask for the worker to restart itself after\n    performing this installation.\n\n    .. note::\n\n       This will increase the time it takes to start up\n       each worker. If possible, we recommend including the\n       libraries in the worker environment or image. This is\n       primarily intended for experimentation and debugging.\n\n    Parameters\n    ----------\n    packages\n        A list of packages to install using pip.\n        Packages should follow the structure defined for\n        `requirement files <https://pip.pypa.io/en/stable/reference/requirements-file-format/#structure>`_.\n        Packages also may include\n        `environment variables <https://pip.pypa.io/en/stable/reference/requirements-file-format/#using-environment-variables>`_.\n    pip_options\n        Additional options to pass to pip\n    restart_workers\n        Whether or not to restart the worker after installing the packages;\n        only functions if the worker has an attached nanny process.\n\n    Examples\n    --------\n    >>> from dask.distributed import PipInstall\n    >>> plugin = PipInstall(packages=[\"scikit-learn\"], pip_options=[\"--upgrade\"])\n    >>> client.register_plugin(plugin)\n\n    Install package from a private repository using a ``TOKEN`` environment variable.\n\n    >>> from dask.distributed import PipInstall\n    >>> plugin = PipInstall(packages=[\"private_package@git+https://${TOKEN}@github.com/dask/private_package.git])\n    >>> client.register_plugin(plugin)\n\n    See Also\n    --------\n    InstallPlugin\n    CondaInstall\n    \"\"\"\n\n    def __init__(\n        self,\n        packages: list[str],\n        pip_options: list[str] | None = None,\n        restart_workers: bool = False,\n    ):\n        installer = _PipInstaller(packages, pip_options)\n        super().__init__(installer, restart_workers=restart_workers)\n\n\nclass _PipInstaller:\n    INSTALLER = \"pip\"\n\n    packages: list[str]\n    pip_options: list[str]\n\n    def __init__(self, packages: list[str], pip_options: list[str] | None = None):\n        self.packages = packages\n        self.pip_options = pip_options or []\n\n    def __call__(self) -> None:\n        logger.info(\n            \"%s installing the following packages: %s\",\n            self.INSTALLER,\n            self.packages,\n        )\n        # Use a requirements file under the hood to support\n        # environment variables\n        # See https://pip.pypa.io/en/stable/reference/requirements-file-format/#using-environment-variables\n        with tempfile.NamedTemporaryFile(mode=\"w+\") as f:\n            f.write(\"\\n\".join(self.packages))\n            f.flush()\n            proc = subprocess.Popen(\n                [\n                    sys.executable,\n                    \"-m\",\n                    \"pip\",\n                    \"install\",\n                    *self.pip_options,\n                    \"-r\",\n                    f.name,\n                ],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n            )\n            _, stderr = proc.communicate()\n            returncode = proc.wait()\n            if returncode != 0:\n                msg = f\"pip install failed with '{stderr.decode().strip()}'\"\n                logger.error(msg)\n                raise RuntimeError(msg)\n\n\n# Adapted from https://github.com/dask/distributed/issues/3560#issuecomment-596138522\nclass UploadFile(WorkerPlugin):\n    \"\"\"A WorkerPlugin to upload a local file to workers.\n\n    Parameters\n    ----------\n    filepath: str\n        A path to the file (.py, egg, or zip) to upload\n\n    Examples\n    --------\n    >>> from distributed.diagnostics.plugin import UploadFile\n\n    >>> client.register_plugin(UploadFile(\"/path/to/file.py\"))  # doctest: +SKIP\n    \"\"\"\n\n    name = \"upload_file\"\n\n    def __init__(self, filepath: str, load: bool = True):\n        \"\"\"\n        Initialize the plugin by reading in the data from the given file.\n        \"\"\"\n        self.filename = os.path.basename(filepath)\n        self.load = load\n        with open(filepath, \"rb\") as f:\n            self.data = f.read()\n\n    async def setup(self, worker):\n        response = await worker.upload_file(\n            filename=self.filename, data=self.data, load=self.load\n        )\n        assert len(self.data) == response[\"nbytes\"]\n\n\nclass ForwardLoggingPlugin(WorkerPlugin):\n    \"\"\"\n    A ``WorkerPlugin`` to forward python logging records from worker to client.\n    See :meth:`Client.forward_logging` for full documentation and usage. Needs\n    to be used in coordination with :meth:`Client.subscribe_topic`, the details\n    of which :meth:`Client.forward_logging` handles for you.\n\n    Parameters\n    ----------\n    logger_name : str\n        The name of the logger to begin forwarding.\n\n    level : str | int\n        Optionally restrict forwarding to ``LogRecord``s of this level or\n        higher, even if the forwarded logger's own level is lower.\n\n    topic : str\n        The name of the topic to which to the worker should log the forwarded log\n        records.\n    \"\"\"\n\n    def __init__(self, logger_name, level, topic):\n        self.logger_name = logger_name\n        self.level = level\n        self.topic = topic\n        self.handler = None\n\n    def setup(self, worker):\n        self.handler = _ForwardingLogHandler(worker, self.topic, level=self.level)\n        logger = logging.getLogger(self.logger_name)\n        logger.addHandler(self.handler)\n\n    def teardown(self, worker):\n        if self.handler is not None:\n            logger = logging.getLogger(self.logger_name)\n            logger.removeHandler(self.handler)\n\n\nclass _ForwardingLogHandler(logging.Handler):\n    \"\"\"\n    Handler class that gets installed inside workers by\n    :class:`ForwardLoggingPlugin`. Not intended to be instantiated by the user\n    directly.\n\n    In each affected worker, ``ForwardLoggingPlugin`` adds an instance of this\n    handler to one or more loggers (possibly the root logger). Tasks running on\n    the worker may then use the affected logger as normal, with the side effect\n    that any ``LogRecord``s handled by the logger (or by a logger below it in\n    the hierarchy) will be published to the dask client as a\n    ``topic`` event.\n    \"\"\"\n\n    def __init__(self, worker, topic, level=logging.NOTSET):\n        super().__init__(level)\n        self.worker = worker\n        self.topic = topic\n\n    def prepare_record_attributes(self, record):\n        # Adapted from the CPython standard library's\n        # logging.handlers.SocketHandler.makePickle; see its source at:\n        # https://github.com/python/cpython/blob/main/Lib/logging/handlers.py\n        ei = record.exc_info\n        if ei:\n            # just to get traceback text into record.exc_text ...\n            _ = self.format(record)\n        # If msg or args are objects, they may not be available on the receiving\n        # end. So we convert the msg % args to a string, save it as msg and zap\n        # the args.\n        d = dict(record.__dict__)\n        d[\"msg\"] = record.getMessage()\n        d[\"args\"] = None\n        d[\"exc_info\"] = None\n        # delete 'message' if present: redundant with 'msg'\n        d.pop(\"message\", None)\n        return d\n\n    def emit(self, record):\n        attributes = self.prepare_record_attributes(record)\n        self.worker.log_event(self.topic, attributes)\n\n\nclass Environ(NannyPlugin):\n    restart = True\n\n    def __init__(self, environ: dict | None = None):\n        environ = environ or {}\n        self.environ = {k: str(v) for k, v in environ.items()}\n\n    async def setup(self, nanny):\n        nanny.env.update(self.environ)\n\n\nUPLOAD_DIRECTORY_MODES = [\"all\", \"scheduler\", \"workers\"]\n\n\nclass UploadDirectory(SchedulerPlugin):\n    \"\"\"Scheduler to upload a local directory to the cluster.\n\n    Parameters\n    ----------\n    path:\n        Path to the directory to upload\n    scheduler:\n        Whether to upload the directory to the scheduler\n\n    Examples\n    --------\n    >>> from distributed.diagnostics.plugin import UploadDirectory\n    >>> client.register_plugin(UploadDirectory(\"/path/to/directory\"))  # doctest: +SKIP\n    \"\"\"\n\n    @_deprecated_kwarg(\"restart\", \"restart_workers\")\n    def __init__(\n        self,\n        path,\n        restart_workers=False,\n        update_path=False,\n        skip_words=(\".git\", \".github\", \".pytest_cache\", \"tests\", \"docs\"),\n        skip=(lambda fn: os.path.splitext(fn)[1] == \".pyc\",),\n        mode=\"workers\",\n    ):\n        path = os.path.expanduser(path)\n        self.path = os.path.split(path)[-1]\n        self.restart_workers = restart_workers\n        self.update_path = update_path\n\n        if mode not in UPLOAD_DIRECTORY_MODES:\n            raise ValueError(\n                f\"{mode=} not supported, expected one of {UPLOAD_DIRECTORY_MODES}\"\n            )\n        self.mode = mode\n\n        self.name = \"upload-directory-\" + os.path.split(path)[-1]\n\n        with tmpfile(extension=\"zip\") as fn:\n            with zipfile.ZipFile(fn, \"w\", zipfile.ZIP_DEFLATED) as z:\n                for root, dirs, files in os.walk(path):\n                    for file in files:\n                        filename = os.path.join(root, file)\n                        if any(predicate(filename) for predicate in skip):\n                            continue\n                        dirs = filename.split(os.sep)\n                        if any(word in dirs for word in skip_words):\n                            continue\n\n                        archive_name = os.path.relpath(\n                            os.path.join(root, file), os.path.join(path, \"..\")\n                        )\n                        z.write(filename, archive_name)\n\n            with open(fn, mode=\"rb\") as f:\n                self.data = f.read()\n\n    async def start(self, scheduler):\n        from distributed.core import clean_exception\n        from distributed.protocol.serialize import Serialized, deserialize\n\n        if self.mode in (\"all\", \"scheduler\"):\n            _extract_data(\n                scheduler.local_directory, self.path, self.data, self.update_graph\n            )\n\n        if self.mode in (\"all\", \"workers\"):\n            nanny_plugin = _UploadDirectoryNannyPlugin(\n                self.path, self.data, self.restart_workers, self.update_path, self.name\n            )\n            responses = await scheduler.register_nanny_plugin(\n                comm=None,\n                plugin=dumps(nanny_plugin),\n                name=self.name,\n                idempotent=False,\n            )\n\n            for response in responses.values():\n                if response[\"status\"] == \"error\":\n                    response = {\n                        k: deserialize(v.header, v.frames)\n                        for k, v in response.items()\n                        if isinstance(v, Serialized)\n                    }\n                    _, exc, tb = clean_exception(**response)\n                    raise exc.with_traceback(tb)\n\n\nclass _UploadDirectoryNannyPlugin(NannyPlugin):\n    def __init__(self, path, data, restart, update_path, name):\n        self.path = path\n        self.data = data\n        self.name = name\n        self.restart = restart\n        self.update_path = update_path\n\n    def setup(self, nanny):\n        _extract_data(nanny.local_directory, self.path, self.data, self.update_path)\n\n\ndef _extract_data(base_path, path, data, update_path):\n    with tmpfile(extension=\"zip\") as fn:\n        with open(fn, mode=\"wb\") as f:\n            f.write(data)\n\n        import zipfile\n\n        with zipfile.ZipFile(fn) as z:\n            z.extractall(path=base_path)\n\n        if update_path:\n            path = os.path.join(base_path, path)\n            if path not in sys.path:\n                sys.path.insert(0, path)\n\n\nclass forward_stream:\n    def __init__(self, stream, worker):\n        self._worker = worker\n        self._original_methods = {}\n        self._stream = getattr(sys, stream)\n        if stream == \"stdout\":\n            self._file = 1\n        elif stream == \"stderr\":\n            self._file = 2\n        else:\n            raise ValueError(\n                f\"Expected stream to be 'stdout' or 'stderr'; got '{stream}'\"\n            )\n\n        self._file = 1 if stream == \"stdout\" else 2\n        self._buffer = []\n\n    def _write(self, write_fn, data):\n        self._forward(data)\n        write_fn(data)\n\n    def _forward(self, data):\n        self._buffer.append(data)\n        # Mimic line buffering\n        if \"\\n\" in data or \"\\r\" in data:\n            self._send()\n\n    def _send(self):\n        msg = {\"args\": self._buffer, \"file\": self._file, \"sep\": \"\", \"end\": \"\"}\n        self._worker.log_event(\"print\", msg)\n        self._buffer = []\n\n    def _flush(self, flush_fn):\n        self._send()\n        flush_fn()\n\n    def _close(self, close_fn):\n        self._send()\n        close_fn()\n\n    def _intercept(self, method_name, interceptor):\n        original_method = getattr(self._stream, method_name)\n        self._original_methods[method_name] = original_method\n        setattr(\n            self._stream, method_name, functools.partial(interceptor, original_method)\n        )\n\n    def __enter__(self):\n        self._intercept(\"write\", self._write)\n        self._intercept(\"flush\", self._flush)\n        self._intercept(\"close\", self._close)\n        return self._stream\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._stream.flush()\n        for attr, original in self._original_methods.items():\n            setattr(self._stream, attr, original)\n        self._original_methods = {}\n\n\nclass ForwardOutput(WorkerPlugin):\n    \"\"\"A Worker Plugin that forwards ``stdout`` and ``stderr`` from workers to clients\n\n    This plugin forwards all output sent to ``stdout`` and ``stderr` on all workers\n    to all clients where it is written to the respective streams. Analogous to the\n    terminal, this plugin uses line buffering. To ensure that an output is written\n    without a newline, make sure to flush the stream.\n\n    .. warning::\n\n        Using this plugin will forward **all** output in ``stdout`` and ``stderr`` from\n        every worker to every client. If the output is very chatty, this will add\n        significant strain on the scheduler. Proceed with caution!\n\n    Examples\n    --------\n    >>> from dask.distributed import ForwardOutput\n    >>> plugin = ForwardOutput()\n\n    >>> client.register_plugin(plugin)\n    \"\"\"\n\n    def setup(self, worker):\n        self._exit_stack = contextlib.ExitStack()\n        self._exit_stack.enter_context(forward_stream(\"stdout\", worker=worker))\n        self._exit_stack.enter_context(forward_stream(\"stderr\", worker=worker))\n\n    def teardown(self, worker):\n        self._exit_stack.close()\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:18:01"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport asyncio\n\nimport pytest\n\nfrom distributed import Nanny\nfrom distributed.chaos import KillWorker\nfrom distributed.utils_test import WINDOWS, gen_cluster\n\n\n@pytest.mark.parametrize(\n    \"mode\",\n    [\n        \"sys.exit\",\n        \"graceful\",\n        pytest.param(\"segfault\", marks=pytest.mark.skipif(WINDOWS, reason=\"unknown\")),\n    ],\n)\n@gen_cluster(nthreads=[(\"127.0.0.1\", 1)], client=True, Worker=Nanny)\nasync def test_KillWorker(c, s, w, mode):\n    plugin = KillWorker(delay=\"1ms\", mode=mode)\n\n    await c.register_plugin(plugin)\n\n    while s.workers:\n        await asyncio.sleep(0.001)\n\n    await w.close()\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: distributed\nTest File Path: distributed\\test_cluster_dump\\test_cluster_dump.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"Utilities for generating and analyzing cluster dumps\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom collections import defaultdict\nfrom collections.abc import Awaitable, Callable, Collection, Mapping\nfrom pathlib import Path\nfrom typing import IO, Any, Literal\n\nimport msgpack\n\nfrom dask.typing import Key\n\nfrom distributed._stories import scheduler_story as _scheduler_story\nfrom distributed._stories import worker_story as _worker_story\n\nDEFAULT_CLUSTER_DUMP_FORMAT: Literal[\"msgpack\" | \"yaml\"] = \"msgpack\"\nDEFAULT_CLUSTER_DUMP_EXCLUDE: Collection[str] = (\"run_spec\",)\n\n\ndef _tuple_to_list(node):\n    if isinstance(node, (list, tuple)):\n        return [_tuple_to_list(el) for el in node]\n    elif isinstance(node, dict):\n        return {k: _tuple_to_list(v) for k, v in node.items()}\n    else:\n        return node\n\n\nasync def write_state(\n    get_state: Callable[[], Awaitable[Any]],\n    url: str,\n    format: Literal[\"msgpack\", \"yaml\"] = DEFAULT_CLUSTER_DUMP_FORMAT,\n    **storage_options: dict[str, Any],\n) -> None:\n    \"Await a cluster dump, then serialize and write it to a path\"\n    if format == \"msgpack\":\n        mode = \"wb\"\n        suffix = \".msgpack.gz\"\n        if not url.endswith(suffix):\n            url += suffix\n        writer = msgpack.pack\n    elif format == \"yaml\":\n        import yaml\n\n        mode = \"w\"\n        suffix = \".yaml\"\n        if not url.endswith(suffix):\n            url += suffix\n\n        def writer(state: dict, f: IO) -> None:\n            # YAML adds unnecessary `!!python/tuple` tags; convert tuples to lists to avoid them.\n            # Unnecessary for msgpack, since tuples and lists are encoded the same.\n            yaml.dump(_tuple_to_list(state), f)\n\n    else:\n        raise ValueError(\n            f\"Unsupported format {format!r}. Possible values are 'msgpack' or 'yaml'.\"\n        )\n\n    # Eagerly open the file to catch any errors before doing the full dump\n    # NOTE: `compression=\"infer\"` will automatically use gzip via the `.gz` suffix\n    # This module is the only place where fsspec is used and it is a relatively\n    # heavy import. Do lazy import to reduce import time\n    import fsspec\n\n    with fsspec.open(url, mode, compression=\"infer\", **storage_options) as f:\n        state = await get_state()\n        # Write from a thread so we don't block the event loop quite as badly\n        # (the writer will still hold the GIL a lot though).\n        await asyncio.to_thread(writer, state, f)\n\n\ndef load_cluster_dump(url: str, **kwargs: Any) -> dict:\n    \"\"\"Loads a cluster dump from a disk artefact\n\n    Parameters\n    ----------\n    url : str\n        Name of the disk artefact. This should have either a\n        ``.msgpack.gz`` or ``yaml`` suffix, depending on the dump format.\n    **kwargs :\n        Extra arguments passed to :func:`fsspec.open`.\n\n    Returns\n    -------\n    state : dict\n        The cluster state at the time of the dump.\n    \"\"\"\n    if url.endswith(\".msgpack.gz\"):\n        mode = \"rb\"\n        reader = msgpack.unpack\n    elif url.endswith(\".yaml\"):\n        import yaml\n\n        mode = \"r\"\n        reader = yaml.safe_load\n    else:\n        raise ValueError(f\"url ({url}) must have a .msgpack.gz or .yaml suffix\")\n\n    kwargs.setdefault(\"compression\", \"infer\")\n    # This module is the only place where fsspec is used and it is a relatively\n    # heavy import. Do lazy import to reduce import time\n    import fsspec\n\n    with fsspec.open(url, mode, **kwargs) as f:\n        return reader(f)\n\n\nclass DumpArtefact(Mapping):\n    \"\"\"\n    Utility class for inspecting the state of a cluster dump\n\n    .. code-block:: python\n\n        dump = DumpArtefact.from_url(\"dump.msgpack.gz\")\n        memory_tasks = dump.scheduler_tasks(\"memory\")\n        executing_tasks = dump.worker_tasks(\"executing\")\n    \"\"\"\n\n    def __init__(self, state: dict):\n        self.dump = state\n\n    @classmethod\n    def from_url(cls, url: str, **kwargs: Any) -> DumpArtefact:\n        \"\"\"Loads a cluster dump from a disk artefact\n\n        Parameters\n        ----------\n        url : str\n            Name of the disk artefact. This should have either a\n            ``.msgpack.gz`` or ``yaml`` suffix, depending on the dump format.\n        **kwargs :\n            Extra arguments passed to :func:`fsspec.open`.\n\n        Returns\n        -------\n        state : dict\n            The cluster state at the time of the dump.\n        \"\"\"\n        return DumpArtefact(load_cluster_dump(url, **kwargs))\n\n    def __getitem__(self, key):\n        return self.dump[key]\n\n    def __iter__(self):\n        return iter(self.dump)\n\n    def __len__(self):\n        return len(self.dump)\n\n    def _extract_tasks(self, state: str | None, context: dict[str, dict]) -> list[dict]:\n        if state:\n            return [v for v in context.values() if v[\"state\"] == state]\n        else:\n            return list(context.values())\n\n    def scheduler_tasks_in_state(self, state: str | None = None) -> list:\n        \"\"\"\n        Parameters\n        ----------\n        state : optional, str\n            If provided, only tasks in the given state are returned.\n            Otherwise, all tasks are returned.\n\n        Returns\n        -------\n        tasks : list\n            The list of scheduler tasks in ``state``.\n        \"\"\"\n        return self._extract_tasks(state, self.dump[\"scheduler\"][\"tasks\"])\n\n    def worker_tasks_in_state(self, state: str | None = None) -> list:\n        \"\"\"\n        Parameters\n        ----------\n        state : optional, str\n            If provided, only tasks in the given state are returned.\n            Otherwise, all tasks are returned.\n\n        Returns\n        -------\n        tasks : list\n            The list of worker tasks in ``state``\n        \"\"\"\n        tasks = []\n\n        for worker_dump in self.dump[\"workers\"].values():\n            if isinstance(worker_dump, dict) and \"tasks\" in worker_dump:\n                tasks.extend(self._extract_tasks(state, worker_dump[\"tasks\"]))\n\n        return tasks\n\n    def scheduler_story(self, *key_or_stimulus_id: Key | str) -> dict:\n        \"\"\"\n        Returns\n        -------\n        stories : dict\n            A list of stories for the keys/stimulus ID's in ``*key_or_stimulus_id``.\n        \"\"\"\n        stories = defaultdict(list)\n\n        log = self.dump[\"scheduler\"][\"transition_log\"]\n        keys = set(key_or_stimulus_id)\n\n        for story in _scheduler_story(keys, log):\n            stories[story[0]].append(tuple(story))\n\n        return dict(stories)\n\n    def worker_story(self, *key_or_stimulus_id: str) -> dict:\n        \"\"\"\n        Returns\n        -------\n        stories : dict\n            A dict of stories for the keys/stimulus ID's in ``*key_or_stimulus_id`.`\n        \"\"\"\n        keys = set(key_or_stimulus_id)\n        stories = defaultdict(list)\n\n        for worker_dump in self.dump[\"workers\"].values():\n            if isinstance(worker_dump, dict) and \"log\" in worker_dump:\n                for story in _worker_story(keys, worker_dump[\"log\"]):\n                    stories[story[0]].append(tuple(story))\n\n        return dict(stories)\n\n    def missing_workers(self) -> list:\n        \"\"\"\n        Returns\n        -------\n        missing : list\n            A list of workers connected to the scheduler, but which\n            did not respond to requests for a state dump.\n        \"\"\"\n        scheduler_workers = self.dump[\"scheduler\"][\"workers\"]\n        responsive_workers = self.dump[\"workers\"]\n        return [\n            w\n            for w in scheduler_workers\n            if w not in responsive_workers\n            or not isinstance(responsive_workers[w], dict)\n        ]\n\n    def _compact_state(self, state: dict, expand_keys: set[str]) -> dict[str, dict]:\n        \"\"\"Compacts ``state`` keys into a general key,\n        unless the key is in ``expand_keys``\"\"\"\n        assert \"general\" not in state\n        result = {}\n        general = {}\n\n        for k, v in state.items():\n            if k in expand_keys:\n                result[k] = v\n            else:\n                general[k] = v\n\n        result[\"general\"] = general\n        return result\n\n    def to_yamls(\n        self,\n        root_dir: str | Path | None = None,\n        worker_expand_keys: Collection[str] = (\"config\", \"log\", \"logs\", \"tasks\"),\n        scheduler_expand_keys: Collection[str] = (\n            \"events\",\n            \"extensions\",\n            \"log\",\n            \"task_groups\",\n            \"tasks\",\n            \"transition_log\",\n            \"workers\",\n        ),\n    ) -> None:\n        \"\"\"\n        Splits the Dump Artefact into a tree of yaml files with\n        ``root_dir`` as it's base.\n\n        The root level of the tree contains a directory for the scheduler\n        and directories for each individual worker.\n        Each directory contains yaml files describing the state of the scheduler\n        or worker when the artefact was created.\n\n        In general, keys associated with the state are compacted into a ``general.yaml``\n        file, unless they are in ``scheduler_expand_keys`` and ``worker_expand_keys``.\n\n        Parameters\n        ----------\n        root_dir : str or Path\n            The root directory into which the tree is written.\n            Defaults to the current working directory if ``None``.\n        worker_expand_keys : iterable of str\n            An iterable of artefact worker keys that will be expanded\n            into separate yaml files.\n            Keys that are not in this iterable are compacted into a\n            `general.yaml` file.\n        scheduler_expand_keys : iterable of str\n            An iterable of artefact scheduler keys that will be expanded\n            into separate yaml files.\n            Keys that are not in this iterable are compacted into a\n            ``general.yaml`` file.\n        \"\"\"\n        import yaml\n\n        root_dir = Path(root_dir) if root_dir else Path.cwd()\n        dumper = yaml.CSafeDumper\n        scheduler_expand_keys = set(scheduler_expand_keys)\n        worker_expand_keys = set(worker_expand_keys)\n\n        workers = self.dump[\"workers\"]\n        for info in workers.values():\n            try:\n                worker_id = info[\"id\"]\n            except KeyError:\n                continue\n\n            worker_state = self._compact_state(info, worker_expand_keys)\n\n            log_dir = root_dir / worker_id\n            log_dir.mkdir(parents=True, exist_ok=True)\n\n            for name, _logs in worker_state.items():\n                filename = str(log_dir / f\"{name}.yaml\")\n                with open(filename, \"w\") as fd:\n                    yaml.dump(_logs, fd, Dumper=dumper)\n\n        context = \"scheduler\"\n        scheduler_state = self._compact_state(self.dump[context], scheduler_expand_keys)\n\n        log_dir = root_dir / context\n        log_dir.mkdir(parents=True, exist_ok=True)\n        # Compact smaller keys into a general dict\n\n        for name, _logs in scheduler_state.items():\n            filename = str(log_dir / f\"{name}.yaml\")\n\n            with open(filename, \"w\") as fd:\n                yaml.dump(_logs, fd, Dumper=dumper)\n\n### Source File Dependency Files Content\n### Dependency File: _stories.py\nfrom __future__ import annotations\n\nfrom collections.abc import Collection, Iterable\nfrom typing import TYPE_CHECKING\n\nfrom dask.typing import Key\n\nif TYPE_CHECKING:\n    # Circular import\n    from distributed.scheduler import Transition\n\n\ndef scheduler_story(\n    keys_or_stimuli: set[Key | str], transition_log: Iterable[Transition]\n) -> list[Transition]:\n    \"\"\"Creates a story from the scheduler transition log given a set of keys\n    describing tasks or stimuli.\n\n    Parameters\n    ----------\n    keys_or_stimuli : set[str]\n        Task keys or stimulus_id's\n    log : iterable\n        The scheduler transition log\n\n    Returns\n    -------\n    story : list[tuple]\n    \"\"\"\n    return [\n        t\n        for t in transition_log\n        if t[0] in keys_or_stimuli or keys_or_stimuli.intersection(t[3])\n    ]\n\n\ndef worker_story(keys_or_stimuli: Collection[Key | str], log: Iterable[tuple]) -> list:\n    \"\"\"Creates a story from the worker log given a set of keys\n    describing tasks or stimuli.\n\n    Parameters\n    ----------\n    keys_or_stimuli : set[str]\n        Task keys or stimulus_id's\n    log : iterable\n        The worker log\n\n    Returns\n    -------\n    story : list[str]\n    \"\"\"\n    return [\n        msg\n        for msg in log\n        if any(key in msg for key in keys_or_stimuli)\n        or any(\n            key in c\n            for key in keys_or_stimuli\n            for c in msg\n            if isinstance(c, (tuple, list, set))\n        )\n    ]\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:18:01"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport asyncio\nfrom pathlib import Path\n\nimport fsspec\nimport msgpack\nimport pytest\nimport yaml\n\nimport distributed\nfrom distributed.cluster_dump import DumpArtefact, _tuple_to_list, write_state\nfrom distributed.utils_test import assert_story, gen_cluster, gen_test, inc\n\n\n@pytest.mark.parametrize(\n    \"input, expected\",\n    [\n        ([1, 2, 3], [1, 2, 3]),\n        ((1, 2, 3), [1, 2, 3]),\n        ({\"x\": (1, (2,))}, {\"x\": [1, [2]]}),\n        (\"foo\", \"foo\"),\n    ],\n)\ndef test_tuple_to_list(input, expected):\n    assert _tuple_to_list(input) == expected\n\n\nasync def get_state():\n    return {\"foo\": \"bar\", \"list\": [\"a\"], \"tuple\": (1, \"two\", 3)}\n\n\n@gen_test()\nasync def test_write_state_msgpack(tmp_path):\n    path = str(tmp_path / \"bar\")\n    await write_state(get_state, path, \"msgpack\")\n\n    with fsspec.open(f\"{path}.msgpack.gz\", \"rb\", compression=\"gzip\") as f:\n        readback = msgpack.load(f)\n        assert readback == _tuple_to_list(await get_state())\n\n\n@gen_test()\nasync def test_write_state_yaml(tmp_path):\n    path = str(tmp_path / \"bar\")\n    await write_state(get_state, path, \"yaml\")\n\n    with open(f\"{path}.yaml\") as f:\n        readback = yaml.safe_load(f)\n        assert readback == _tuple_to_list(await get_state())\n        f.seek(0)\n        assert \"!!python/tuple\" not in f.read()\n\n\ndef blocked_inc(x, event):\n    event.wait()\n    return x + 1\n\n\n@gen_cluster(client=True)\nasync def test_cluster_dump_state(c, s, a, b, tmp_path):\n    filename = tmp_path / \"dump\"\n    futs = c.map(inc, range(2))\n    fut_keys = {f.key for f in futs}\n    await c.gather(futs)\n\n    event = distributed.Event()\n    blocked_fut = c.submit(blocked_inc, 1, event)\n    await asyncio.sleep(0.05)\n    await c.dump_cluster_state(filename, format=\"msgpack\")\n\n    scheduler_tasks = list(s.tasks.values())\n    worker_tasks = [t for w in (a, b) for t in w.state.tasks.values()]\n\n    smem_tasks = [t for t in scheduler_tasks if t.state == \"memory\"]\n    wmem_tasks = [t for t in worker_tasks if t.state == \"memory\"]\n\n    assert len(smem_tasks) == 2\n    assert len(wmem_tasks) == 2\n\n    sproc_tasks = [t for t in scheduler_tasks if t.state == \"processing\"]\n    wproc_tasks = [t for t in worker_tasks if t.state == \"executing\"]\n\n    assert len(sproc_tasks) == 1\n    assert len(wproc_tasks) == 1\n\n    await c.gather(event.set(), blocked_fut)\n\n    dump = DumpArtefact.from_url(f\"{filename}.msgpack.gz\")\n\n    smem_keys = {t[\"key\"] for t in dump.scheduler_tasks_in_state(\"memory\")}\n    wmem_keys = {t[\"key\"] for t in dump.worker_tasks_in_state(\"memory\")}\n\n    assert smem_keys == fut_keys\n    assert smem_keys == {t.key for t in smem_tasks}\n    assert wmem_keys == fut_keys\n    assert wmem_keys == {t.key for t in wmem_tasks}\n\n    sproc_keys = {t[\"key\"] for t in dump.scheduler_tasks_in_state(\"processing\")}\n    wproc_keys = {t[\"key\"] for t in dump.worker_tasks_in_state(\"executing\")}\n\n    assert sproc_keys == {t.key for t in sproc_tasks}\n    assert wproc_keys == {t.key for t in wproc_tasks}\n\n    sall_keys = {t[\"key\"] for t in dump.scheduler_tasks_in_state()}\n    wall_keys = {t[\"key\"] for t in dump.worker_tasks_in_state()}\n\n    assert fut_keys | {blocked_fut.key} == sall_keys\n    assert fut_keys | {blocked_fut.key} == wall_keys\n\n    # Mapping API works\n    assert \"transition_log\" in dump[\"scheduler\"]\n    assert \"log\" in dump[\"workers\"][a.address]\n    assert len(dump) == 3\n\n\n@gen_cluster(client=True)\nasync def test_cluster_dump_story(c, s, a, b, tmp_path):\n    filename = tmp_path / \"dump\"\n    f1 = c.submit(inc, 0, key=\"f1\")\n    f2 = c.submit(inc, 1, key=\"f2\")\n    await c.gather([f1, f2])\n    await c.dump_cluster_state(filename, format=\"msgpack\")\n\n    dump = DumpArtefact.from_url(f\"{filename}.msgpack.gz\")\n\n    story = dump.scheduler_story(\"f1\", \"f2\")\n    assert story.keys() == {\"f1\", \"f2\"}\n\n    for k, task_story in story.items():\n        assert_story(task_story, s.story(k))\n\n    story = dump.worker_story(\"f1\", \"f2\")\n    assert story.keys() == {\"f1\", \"f2\"}\n    for k, task_story in story.items():\n        assert_story(task_story, a.state.story(k) + b.state.story(k))\n\n\n@gen_cluster(client=True)\nasync def test_cluster_dump_to_yamls(c, s, a, b, tmp_path):\n    futs = c.map(inc, range(2))\n    await c.gather(futs)\n\n    event = distributed.Event()\n    blocked_fut = c.submit(blocked_inc, 1, event)\n    filename = tmp_path / \"dump\"\n    await asyncio.sleep(0.05)\n    await c.dump_cluster_state(filename, format=\"msgpack\")\n    await event.set()\n    await blocked_fut\n\n    dump = DumpArtefact.from_url(f\"{filename}.msgpack.gz\")\n    yaml_path = Path(tmp_path / \"dump\")\n    dump.to_yamls(yaml_path)\n\n    scheduler_files = {\n        \"events.yaml\",\n        \"extensions.yaml\",\n        \"general.yaml\",\n        \"task_groups.yaml\",\n        \"tasks.yaml\",\n        \"transition_log.yaml\",\n        \"workers.yaml\",\n    }\n\n    scheduler_yaml_path = yaml_path / \"scheduler\"\n    expected = {scheduler_yaml_path / f for f in scheduler_files}\n    assert expected == set(scheduler_yaml_path.iterdir())\n\n    worker_files = {\n        \"config.yaml\",\n        \"general.yaml\",\n        \"log.yaml\",\n        \"logs.yaml\",\n        \"tasks.yaml\",\n    }\n\n    for worker in (a, b):\n        worker_yaml_path = yaml_path / worker.id\n        expected = {worker_yaml_path / f for f in worker_files}\n        assert expected == set(worker_yaml_path.iterdir())\n\n    # Internal dictionary state compaction\n    # has not been destructive of the original dictionary\n    assert \"id\" in dump[\"scheduler\"]\n    assert \"address\" in dump[\"scheduler\"]\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: distributed\nTest File Path: distributed\\test_collections\\test_collections.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport heapq\nimport itertools\nimport weakref\nfrom collections import OrderedDict, UserDict\nfrom collections.abc import Callable, Hashable, Iterable, Iterator, Mapping, MutableSet\nfrom typing import Any, TypeVar, cast\n\nT = TypeVar(\"T\", bound=Hashable)\nK = TypeVar(\"K\", bound=Hashable)\nV = TypeVar(\"V\")\n\n\nclass LRU(UserDict[K, V]):\n    \"\"\"Limited size mapping, evicting the least recently looked-up key when full\"\"\"\n\n    def __init__(self, maxsize: float) -> None:\n        super().__init__()\n        self.data = OrderedDict()\n        self.maxsize = maxsize\n\n    def __getitem__(self, key: K) -> V:\n        value = super().__getitem__(key)\n        cast(OrderedDict, self.data).move_to_end(key)\n        return value\n\n    def __setitem__(self, key: K, value: V) -> None:\n        if len(self) >= self.maxsize:\n            cast(OrderedDict, self.data).popitem(last=False)\n        super().__setitem__(key, value)\n\n\nclass HeapSet(MutableSet[T]):\n    \"\"\"A set-like where the `pop` method returns the smallest item, as sorted by an\n    arbitrary key function. Ties are broken by oldest first.\n\n    Values must be compatible with :mod:`weakref`.\n\n    Parameters\n    ----------\n    key: Callable\n        A function that takes a single element of the collection as a parameter and\n        returns a sorting key. The key does not need to be hashable and does not need to\n        support :mod:`weakref`.\n\n    Note\n    ----\n    The key returned for each element should not to change over time. If it does, the\n    position in the heap won't change, even if the element is re-added, and it *may* not\n    change even if it's discarded and then re-added later.\n    \"\"\"\n\n    __slots__ = (\"key\", \"_data\", \"_heap\", \"_inc\", \"_sorted\")\n    key: Callable[[T], Any]\n    _data: set[T]\n    _heap: list[tuple[Any, int, weakref.ref[T]]]\n    _inc: int\n    _sorted: bool\n\n    def __init__(self, *, key: Callable[[T], Any]):\n        self.key = key\n        self._data = set()\n        self._inc = 0\n        self._heap = []\n        self._sorted = True\n\n    def __repr__(self) -> str:\n        return f\"<{type(self).__name__}: {len(self)} items>\"\n\n    def __reduce__(self) -> tuple[Callable, tuple]:\n        heap = [(k, i, v) for k, i, vref in self._heap if (v := vref()) in self._data]\n        return HeapSet._unpickle, (self.key, self._inc, heap)\n\n    @staticmethod\n    def _unpickle(\n        key: Callable[[T], Any], inc: int, heap: list[tuple[Any, int, T]]\n    ) -> HeapSet[T]:\n        self = object.__new__(HeapSet)\n        self.key = key\n        self._data = {v for _, _, v in heap}\n        self._inc = inc\n        self._heap = [(k, i, weakref.ref(v)) for k, i, v in heap]\n        heapq.heapify(self._heap)\n        self._sorted = not heap\n        return self\n\n    def __contains__(self, value: object) -> bool:\n        return value in self._data\n\n    def __len__(self) -> int:\n        return len(self._data)\n\n    def add(self, value: T) -> None:\n        if value in self._data:\n            return\n        k = self.key(value)\n        vref = weakref.ref(value)\n        heapq.heappush(self._heap, (k, self._inc, vref))\n        self._sorted = False\n        self._data.add(value)\n        self._inc += 1\n\n    def discard(self, value: T) -> None:\n        self._data.discard(value)\n        if not self._data:\n            self.clear()\n\n    def peek(self) -> T:\n        \"\"\"Return the smallest element without removing it\"\"\"\n        if not self._data:\n            raise KeyError(\"peek into empty set\")\n        while True:\n            value = self._heap[0][2]()\n            if value in self._data:\n                return value\n            heapq.heappop(self._heap)\n            self._sorted = False\n\n    def peekn(self, n: int) -> Iterator[T]:\n        \"\"\"Iterate over the n smallest elements without removing them.\n        This is O(1) for n == 1; O(n*logn) otherwise.\n        \"\"\"\n        if n <= 0 or not self:\n            return  # empty iterator\n        if n == 1:\n            yield self.peek()\n        else:\n            # NOTE: we could pop N items off the queue, then push them back.\n            # But copying the list N times is probably slower than just sorting it\n            # with fast C code.\n            # If we had a `heappop` that sliced the list instead of popping from it,\n            # we could implement an optimized version for small `n`s.\n            yield from itertools.islice(self.sorted(), n)\n\n    def pop(self) -> T:\n        if not self._data:\n            raise KeyError(\"pop from an empty set\")\n        while True:\n            _, _, vref = heapq.heappop(self._heap)\n            self._sorted = False\n            value = vref()\n            if value in self._data:\n                self._data.discard(value)\n                if not self._data:\n                    self.clear()\n                return value\n\n    def peekright(self) -> T:\n        \"\"\"Return one of the largest elements (not necessarily the largest!) without\n        removing it. It's guaranteed that ``self.peekright() >= self.peek()``.\n        \"\"\"\n        if not self._data:\n            raise KeyError(\"peek into empty set\")\n        while True:\n            value = self._heap[-1][2]()\n            if value in self._data:\n                return value\n            del self._heap[-1]\n\n    def popright(self) -> T:\n        \"\"\"Remove and return one of the largest elements (not necessarily the largest!)\n        It's guaranteed that ``self.popright() >= self.peek()``.\n        \"\"\"\n        if not self._data:\n            raise KeyError(\"pop from an empty set\")\n        while True:\n            _, _, vref = self._heap.pop()\n            value = vref()\n            if value in self._data:\n                self._data.discard(value)\n                if not self._data:\n                    self.clear()\n                return value\n\n    def __iter__(self) -> Iterator[T]:\n        \"\"\"Iterate over all elements. This is a O(n) operation which returns the\n        elements in pseudo-random order.\n        \"\"\"\n        return iter(self._data)\n\n    def sorted(self) -> Iterator[T]:\n        \"\"\"Iterate over all elements. This is a O(n*logn) operation which returns the\n        elements in order, from smallest to largest according to the key and insertion\n        order.\n        \"\"\"\n        if not self._sorted:\n            self._heap.sort()  # A sorted list maintains the heap invariant\n            self._sorted = True\n        seen = set()\n        for _, _, vref in self._heap:\n            value = vref()\n            if value in self._data and value not in seen:\n                yield value\n                seen.add(value)\n\n    def clear(self) -> None:\n        self._data.clear()\n        self._heap.clear()\n        self._sorted = True\n\n\ndef sum_mappings(ds: Iterable[Mapping[K, V] | Iterable[tuple[K, V]]], /) -> dict[K, V]:\n    \"\"\"Sum the values of the given mappings, key by key\"\"\"\n    out: dict[K, V] = {}\n    for d in ds:\n        if isinstance(d, Mapping):\n            d = d.items()\n        for k, v in d:\n            try:\n                out[k] += v  # type: ignore\n            except KeyError:\n                out[k] = v\n    return out\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:18:01"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport heapq\nimport operator\nimport pickle\nimport random\nfrom collections.abc import Mapping\n\nimport pytest\n\nfrom distributed.collections import LRU, HeapSet, sum_mappings\n\n\ndef test_lru():\n    l = LRU(maxsize=3)\n    l[\"a\"] = 1\n    l[\"b\"] = 2\n    l[\"c\"] = 3\n    assert list(l.keys()) == [\"a\", \"b\", \"c\"]\n\n    # Use \"a\" and ensure it becomes the most recently used item\n    l[\"a\"]\n    assert list(l.keys()) == [\"b\", \"c\", \"a\"]\n\n    # Ensure maxsize is respected\n    l[\"d\"] = 4\n    assert len(l) == 3\n    assert list(l.keys()) == [\"c\", \"a\", \"d\"]\n\n\nclass C:\n    def __init__(self, k, i):\n        self.k = k\n        self.i = i\n\n    def __hash__(self):\n        return hash(self.k)\n\n    def __eq__(self, other):\n        return isinstance(other, C) and other.k == self.k\n\n    def __repr__(self):\n        return f\"C({self.k}, {self.i})\"\n\n\ndef test_heapset():\n    heap = HeapSet(key=operator.attrgetter(\"i\"))\n\n    cx = C(\"x\", 2)\n    cy = C(\"y\", 1)\n    cz = C(\"z\", 3)\n    cw = C(\"w\", 4)\n    heap.add(cx)\n    heap.add(cy)\n    heap.add(cz)\n    heap.add(cw)\n    heap.add(C(\"x\", 0))  # Ignored; x already in heap\n    assert len(heap) == 4\n    assert repr(heap) == \"<HeapSet: 4 items>\"\n\n    assert cx in heap\n    assert cy in heap\n    assert cz in heap\n    assert cw in heap\n\n    heap_sorted = heap.sorted()\n    # iteration does not empty heap\n    assert len(heap) == 4\n    assert next(heap_sorted) is cy\n    assert next(heap_sorted) is cx\n    assert next(heap_sorted) is cz\n    assert next(heap_sorted) is cw\n    with pytest.raises(StopIteration):\n        next(heap_sorted)\n\n    assert set(heap) == {cx, cy, cz, cw}\n\n    assert heap.peek() is cy\n    assert heap.pop() is cy\n    assert cx in heap\n    assert cy not in heap\n    assert cz in heap\n    assert cw in heap\n\n    assert heap.peek() is cx\n    assert heap.pop() is cx\n    assert heap.pop() is cz\n    assert heap.pop() is cw\n    assert not heap\n    with pytest.raises(KeyError):\n        heap.pop()\n    with pytest.raises(KeyError):\n        heap.peek()\n\n    # Test out-of-order discard\n    heap.add(cx)\n    heap.add(cy)\n    heap.add(cz)\n    heap.add(cw)\n    assert heap.peek() is cy\n\n    heap.remove(cy)\n    assert cy not in heap\n    with pytest.raises(KeyError):\n        heap.remove(cy)\n\n    heap.discard(cw)\n    assert cw not in heap\n    heap.discard(cw)\n\n    assert len(heap) == 2\n    assert list(heap.sorted()) == [cx, cz]\n    # cy is at the top of heap._heap, but is skipped\n    assert heap.peek() is cx\n    assert heap.pop() is cx\n    assert heap.peek() is cz\n    assert heap.pop() is cz\n    # heap._heap is not empty\n    assert not heap\n    with pytest.raises(KeyError):\n        heap.peek()\n    with pytest.raises(KeyError):\n        heap.pop()\n    assert list(heap.sorted()) == []\n\n    # Test clear()\n    heap.add(cx)\n    heap.clear()\n    assert not heap\n    heap.add(cx)\n    assert cx in heap\n    # Test discard last element\n    heap.discard(cx)\n    assert not heap\n    heap.add(cx)\n    assert cx in heap\n\n    # Test peekn()\n    heap.add(cy)\n    heap.add(cw)\n    heap.add(cz)\n    heap.add(cx)\n    assert list(heap.peekn(3)) == [cy, cx, cz]\n    heap.remove(cz)\n    assert list(heap.peekn(10)) == [cy, cx, cw]\n    assert list(heap.peekn(0)) == []\n    assert list(heap.peekn(-1)) == []\n    heap.remove(cy)\n    assert list(heap.peekn(1)) == [cx]\n    heap.remove(cw)\n    assert list(heap.peekn(1)) == [cx]\n    heap.remove(cx)\n    assert list(heap.peekn(-1)) == []\n    assert list(heap.peekn(0)) == []\n    assert list(heap.peekn(1)) == []\n    assert list(heap.peekn(2)) == []\n\n    # Test resilience to failure in key()\n    heap.add(cx)\n    bad_key = C(\"bad_key\", 0)\n    del bad_key.i\n    with pytest.raises(AttributeError):\n        heap.add(bad_key)\n    assert len(heap) == 1\n    assert set(heap) == {cx}\n\n    # Test resilience to failure in weakref.ref()\n    class D:\n        __slots__ = (\"i\",)\n\n        def __init__(self, i):\n            self.i = i\n\n    with pytest.raises(TypeError):\n        heap.add(D(\"bad_weakref\", 2))\n    assert len(heap) == 1\n    assert set(heap) == {cx}\n\n    # Test resilience to key() returning non-sortable output\n    with pytest.raises(TypeError):\n        heap.add(C(\"unsortable_key\", None))\n    assert len(heap) == 1\n    assert set(heap) == {cx}\n\n\ndef assert_heap_sorted(heap: HeapSet) -> None:\n    assert heap._sorted\n    assert heap._heap == sorted(heap._heap)\n\n\ndef test_heapset_sorted_flag_left():\n    heap = HeapSet(key=operator.attrgetter(\"i\"))\n    assert heap._sorted\n    c1 = C(\"1\", 1)\n    c2 = C(\"2\", 2)\n    c3 = C(\"3\", 3)\n    c4 = C(\"4\", 4)\n\n    heap.add(c4)\n    assert not heap._sorted\n    heap.add(c3)\n    heap.add(c2)\n    heap.add(c1)\n\n    list(heap.sorted())\n    assert_heap_sorted(heap)\n\n    # `peek` maintains sort if first element is not discarded\n    assert heap.peek() is c1\n    assert_heap_sorted(heap)\n\n    # `pop` always de-sorts\n    assert heap.pop() is c1\n    assert not heap._sorted\n\n    list(heap.sorted())\n\n    # discard first element\n    heap.discard(c2)\n    assert heap.peek() is c3\n    assert not heap._sorted\n\n    # popping the last element resets the sorted flag\n    assert heap.pop() is c3\n    assert heap.pop() is c4\n    assert not heap\n    assert_heap_sorted(heap)\n\n    # discarding`` the last element resets the sorted flag\n    heap.add(c1)\n    heap.add(c2)\n    assert not heap._sorted\n    heap.discard(c1)\n    assert not heap._sorted\n    heap.discard(c2)\n    assert not heap\n    assert_heap_sorted(heap)\n\n\ndef test_heapset_sorted_flag_right():\n    \"Verify right operations don't affect sortedness\"\n    heap = HeapSet(key=operator.attrgetter(\"i\"))\n    c1 = C(\"1\", 1)\n    c2 = C(\"2\", 2)\n    c3 = C(\"3\", 3)\n\n    heap.add(c2)\n    heap.add(c3)\n    heap.add(c1)\n\n    assert not heap._sorted\n    list(heap.sorted())\n    assert_heap_sorted(heap)\n\n    assert heap.peekright() is c3\n    assert_heap_sorted(heap)\n    assert heap.popright() is c3\n    assert_heap_sorted(heap)\n    assert heap.popright() is c2\n    assert_heap_sorted(heap)\n\n    heap.add(c2)\n    assert not heap._sorted\n    assert heap.popright() is c2\n    assert not heap._sorted\n    assert heap.popright() is c1\n    assert not heap\n    assert_heap_sorted(heap)\n\n\n@pytest.mark.parametrize(\"peek\", [False, True])\ndef test_heapset_popright(peek):\n    heap = HeapSet(key=operator.attrgetter(\"i\"))\n    with pytest.raises(KeyError):\n        heap.peekright()\n    with pytest.raises(KeyError):\n        heap.popright()\n\n    # The heap contains broken weakrefs\n    for i in range(200):\n        c = C(f\"y{i}\", random.random())\n        heap.add(c)\n        if random.random() > 0.7:\n            heap.remove(c)\n\n    c0 = heap.peek()\n    while len(heap) > 1:\n        # These two code paths determine which of the two methods deals with the\n        # removal of broken weakrefs\n        if peek:\n            c1 = heap.peekright()\n            assert c1.i >= c0.i\n            assert heap.popright() is c1\n        else:\n            c1 = heap.popright()\n            assert c1.i >= c0.i\n\n        # Test that the heap hasn't been corrupted\n        h2 = heap._heap[:]\n        heapq.heapify(h2)\n        assert h2 == heap._heap\n\n    assert heap.peekright() is c0\n    assert heap.popright() is c0\n    assert not heap\n\n\ndef test_heapset_pickle():\n    \"\"\"Test pickle roundtrip for a HeapSet.\n\n    Note\n    ----\n    To make this test work with plain pickle and not need cloudpickle, we had to avoid\n    lambdas and local classes in our test. Here we're testing that HeapSet doesn't add\n    lambdas etc. of its own.\n    \"\"\"\n    heap = HeapSet(key=operator.attrgetter(\"i\"))\n\n    # The heap contains broken weakrefs\n    for i in range(200):\n        c = C(f\"y{i}\", random.random())\n        heap.add(c)\n        if random.random() > 0.7:\n            heap.remove(c)\n\n    list(heap.sorted())  # trigger sort\n    assert heap._sorted\n    heap2 = pickle.loads(pickle.dumps(heap))\n    assert len(heap) == len(heap2)\n    assert not heap2._sorted  # re-heapification may have broken the sort\n    # Test that the heap has been re-heapified upon unpickle\n    assert len(heap2._heap) < len(heap._heap)\n    while heap:\n        assert heap.pop() == heap2.pop()\n\n\ndef test_heapset_sort_duplicate():\n    \"\"\"See https://github.com/dask/distributed/issues/6951\"\"\"\n    heap = HeapSet(key=operator.attrgetter(\"i\"))\n    c1 = C(\"x\", 1)\n    c2 = C(\"2\", 2)\n\n    heap.add(c1)\n    heap.add(c2)\n    heap.discard(c1)\n    heap.add(c1)\n\n    assert list(heap.sorted()) == [c1, c2]\n\n\nclass ReadOnlyMapping(Mapping):\n    def __init__(self, d: Mapping):\n        self.d = d\n\n    def __getitem__(self, item):\n        return self.d[item]\n\n    def __iter__(self):\n        return iter(self.d)\n\n    def __len__(self):\n        return len(self.d)\n\n\ndef test_sum_mappings():\n    a = {\"x\": 1, \"y\": 1.2, \"z\": [3, 4]}\n    b = ReadOnlyMapping({\"w\": 7, \"y\": 3.4, \"z\": [5, 6]})\n    c = iter([(\"y\", 0.2), (\"y\", -0.5)])\n    actual = sum_mappings(iter([a, b, c]))\n    assert isinstance(actual, dict)\n    assert actual == {\"x\": 1, \"y\": 4.3, \"z\": [3, 4, 5, 6], \"w\": 7}\n    assert isinstance(actual[\"x\"], int)  # Not 1.0\n    assert list(actual) == [\"x\", \"y\", \"z\", \"w\"]\n\n    d = {\"x0\": 1, \"x1\": 2, \"y0\": 4}\n    actual = sum_mappings([((k[0], v) for k, v in d.items())])\n    assert actual == {\"x\": 3, \"y\": 4}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: distributed\nTest File Path: distributed\\test_counter\\test_counter.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom collections.abc import Hashable, Iterable, Sequence\n\ntry:\n    from crick import TDigest\nexcept ImportError:\n    pass\nelse:\n\n    class Digest:\n        intervals: Sequence[float]\n        components: list[TDigest]\n        __slots__ = (\"intervals\", \"components\")\n\n        def __init__(self, intervals: Sequence[float] = (5, 60, 3600)):\n            self.intervals = intervals\n            self.components = [TDigest() for _ in intervals]\n\n        def add(self, item: float) -> None:\n            self.components[0].add(item)\n\n        def update(self, seq: Iterable[float]) -> None:\n            self.components[0].update(seq)\n\n        def shift(self) -> None:\n            for i in range(len(self.intervals) - 1):\n                frac = 0.2 * self.intervals[0] / self.intervals[i]\n                part = self.components[i].scale(frac)\n                rest = self.components[i].scale(1 - frac)\n\n                self.components[i + 1].merge(part)\n                self.components[i] = rest\n\n        def size(self) -> float:\n            return sum(d.size() for d in self.components)\n\n\nclass Counter:\n    intervals: Sequence[float]\n    components: list[defaultdict[Hashable, float]]\n    __slots__ = (\"intervals\", \"components\")\n\n    def __init__(self, intervals: Sequence[float] = (5, 60, 3600)):\n        self.intervals = intervals\n        self.components = [defaultdict(int) for _ in intervals]\n\n    def add(self, item: Hashable) -> None:\n        self.components[0][item] += 1\n\n    def shift(self) -> None:\n        for i in range(len(self.intervals) - 1):\n            frac = 0.2 * self.intervals[0] / self.intervals[i]\n            part = {k: v * frac for k, v in self.components[i].items()}\n            rest = {k: v * (1 - frac) for k, v in self.components[i].items()}\n\n            for k, v in part.items():\n                self.components[i + 1][k] += v\n            d: defaultdict[Hashable, float] = defaultdict(int)\n            d.update(rest)\n            self.components[i] = d\n\n    def size(self) -> float:\n        return sum(sum(d.values()) for d in self.components)\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:18:01"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport pytest\n\nfrom distributed.counter import Counter\n\ntry:\n    from distributed.counter import Digest\nexcept ImportError:\n    Digest = None  # type: ignore\n\n\n@pytest.mark.parametrize(\n    \"CD,size\",\n    [\n        (Counter, lambda d: sum(d.values())),\n        pytest.param(\n            Digest,\n            lambda x: x.size(),\n            marks=pytest.mark.skipif(not Digest, reason=\"no crick library\"),\n        ),\n    ],\n)\ndef test_digest(CD, size):\n    c = CD()\n    c.add(1)\n    c.add(2)\n    assert size(c.components[0]) == 2\n\n    c.shift()\n    assert 0 < size(c.components[0]) < 2\n    assert 0 < size(c.components[1]) < 1\n    assert sum(size(d) for d in c.components) == 2\n\n    for i in range(len(c.components) - 1):\n        assert size(c.components[i]) >= size(c.components[i + 1])\n\n    c.add(3)\n\n    assert sum(size(d) for d in c.components) == c.size()\n\n\ndef test_counter():\n    c = Counter()\n    c.add(1)\n\n    for _ in range(5):\n        c.shift()\n        assert abs(sum(cc[1] for cc in c.components) - 1) < 1e-13\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: distributed\nTest File Path: distributed\\test_preload\\test_preload.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport filecmp\nimport inspect\nimport logging\nimport os\nimport shutil\nimport sys\nfrom collections.abc import Iterable, Sequence\nfrom importlib import import_module\nfrom types import ModuleType\nfrom typing import TYPE_CHECKING, cast\n\nimport click\n\nfrom dask.utils import tmpfile\n\nfrom distributed.core import Server\nfrom distributed.utils import import_file\n\nif TYPE_CHECKING:\n    # This has to be inside this guard to avoid a circular import\n    from distributed.client import Client\n\nlogger = logging.getLogger(__name__)\n\n\ndef validate_preload_argv(ctx, param, value):\n    \"\"\"Click option callback providing validation of preload subcommand arguments.\"\"\"\n    if not value and not ctx.params.get(\"preload\", None):\n        # No preload argv provided and no preload modules specified.\n        return value\n\n    if value and not ctx.params.get(\"preload\", None):\n        # Report a usage error matching standard click error conventions.\n        unexpected_args = [v for v in value if v.startswith(\"-\")]\n        for a in unexpected_args:\n            raise click.NoSuchOption(a)\n        raise click.UsageError(\n            \"Got unexpected extra argument%s: (%s)\"\n            % (\"s\" if len(value) > 1 else \"\", \" \".join(value))\n        )\n\n    preload_modules = {\n        name: _import_module(name)\n        for name in ctx.params.get(\"preload\")\n        if not is_webaddress(name)\n    }\n\n    preload_commands = [\n        getattr(m, \"dask_setup\", None)\n        for m in preload_modules.values()\n        if isinstance(getattr(m, \"dask_setup\", None), click.Command)\n    ]\n\n    if len(preload_commands) > 1:\n        raise click.UsageError(\n            \"Multiple --preload modules with click-configurable setup: %s\"\n            % list(preload_modules.keys())\n        )\n\n    if value and not preload_commands:\n        raise click.UsageError(\n            \"Unknown argument specified: %r Was click-configurable --preload target provided?\"\n        )\n    if not preload_commands:\n        return value\n    else:\n        preload_command = preload_commands[0]\n\n    ctx = click.Context(preload_command, allow_extra_args=False)\n    preload_command.parse_args(ctx, list(value))\n\n    return value\n\n\ndef is_webaddress(s: str) -> bool:\n    return s.startswith((\"http://\", \"https://\"))\n\n\ndef _import_module(name: str, file_dir: str | None = None) -> ModuleType:\n    \"\"\"Imports module and extract preload interface functions.\n\n    Import modules specified by name and extract 'dask_setup'\n    and 'dask_teardown' if present.\n\n    Parameters\n    ----------\n    name : str\n        Module name, file path, or text of module or script\n    file_dir : string\n        Path of a directory where files should be copied\n\n    Returns\n    -------\n    Nest dict of names to extracted module interface components if present\n    in imported module.\n    \"\"\"\n    if name.endswith(\".py\"):\n        # name is a file path\n        if file_dir is not None:\n            basename = os.path.basename(name)\n            copy_dst = os.path.join(file_dir, basename)\n            if os.path.exists(copy_dst):\n                if not filecmp.cmp(name, copy_dst):\n                    logger.error(\"File name collision: %s\", basename)\n            shutil.copy(name, copy_dst)\n            module = import_file(copy_dst)[0]\n        else:\n            module = import_file(name)[0]\n\n    elif \" \" not in name:\n        # name is a module name\n        if name not in sys.modules:\n            import_module(name)\n        module = sys.modules[name]\n\n    else:\n        # not a name, actually the text of the script\n        with tmpfile(extension=\".py\") as fn:\n            with open(fn, mode=\"w\") as f:\n                f.write(name)\n            return _import_module(fn, file_dir=file_dir)\n\n    logger.info(\"Import preload module: %s\", name)\n    return module\n\n\ndef _download_module(url: str) -> ModuleType:\n    logger.info(\"Downloading preload at %s\", url)\n    assert is_webaddress(url)\n    # This is the only place where urrllib3 is used and it is a relatively heavy\n    # import. Do lazy import to reduce import time\n    import urllib3\n\n    with urllib3.PoolManager() as http:\n        response = http.request(\n            method=\"GET\",\n            url=url,\n            retries=urllib3.util.Retry(\n                status_forcelist=[429, 504, 503, 502],\n                backoff_factor=0.2,\n            ),\n        )\n\n        source = response.data\n\n    compiled = compile(source, url, \"exec\")\n    module = ModuleType(url)\n    exec(compiled, module.__dict__)\n    return module\n\n\nclass Preload:\n    \"\"\"\n    Manage state for setup/teardown of a preload module\n\n    Parameters\n    ----------\n    dask_server: dask.distributed.Server\n        The Worker or Scheduler\n    name: str\n        module name, file name, or web address to load\n    argv: [str]\n        List of string arguments passed to click-configurable `dask_setup`.\n    file_dir: str\n        Path of a directory where files should be copied\n    \"\"\"\n\n    dask_object: Server | Client\n    name: str\n    argv: list[str]\n    file_dir: str | None\n    module: ModuleType\n\n    def __init__(\n        self,\n        dask_object: Server | Client,\n        name: str,\n        argv: Iterable[str],\n        file_dir: str | None,\n    ):\n        self.dask_object = dask_object\n        self.name = name\n        self.argv = list(argv)\n        self.file_dir = file_dir\n\n        logger.info(\"Creating preload: %s\", self.name)\n\n        if is_webaddress(name):\n            self.module = _download_module(name)\n        else:\n            self.module = _import_module(name, file_dir)\n\n    async def start(self):\n        \"\"\"Run when the server finishes its start method\"\"\"\n        dask_setup = getattr(self.module, \"dask_setup\", None)\n\n        if dask_setup:\n            logger.info(\"Run preload setup: %s\", self.name)\n            if isinstance(dask_setup, click.Command):\n                context = dask_setup.make_context(\n                    \"dask_setup\", self.argv, allow_extra_args=False\n                )\n                result = dask_setup.callback(\n                    self.dask_object, *context.args, **context.params\n                )\n                if inspect.isawaitable(result):\n                    await result\n            else:\n                future = dask_setup(self.dask_object)\n                if inspect.isawaitable(future):\n                    await future\n\n    async def teardown(self):\n        \"\"\"Run when the server starts its close method\"\"\"\n        dask_teardown = getattr(self.module, \"dask_teardown\", None)\n        if dask_teardown:\n            logger.info(\"Run preload teardown: %s\", self.name)\n            future = dask_teardown(self.dask_object)\n            if inspect.isawaitable(future):\n                await future\n\n\nclass PreloadManager(Sequence[Preload]):\n    _preloads: list[Preload]\n\n    def __init__(self, preloads: list[Preload]):\n        self._preloads = preloads\n\n    async def start(self) -> None:\n        for preload in self._preloads:\n            try:\n                await preload.start()\n            except Exception:\n                logger.exception(\"Failed to start preload: %s\", preload.name)\n\n    async def teardown(self) -> None:\n        for preload in reversed(self._preloads):\n            try:\n                await preload.teardown()\n            except Exception:\n                logger.exception(\"Failed to tear down preload: %s\", preload.name)\n\n    def __getitem__(self, index):\n        return self._preloads[index]\n\n    def __len__(self) -> int:\n        return len(self._preloads)\n\n\ndef process_preloads(\n    dask_server: Server | Client,\n    preload: str | list[str],\n    preload_argv: list[str] | list[list[str]],\n    *,\n    file_dir: str | None = None,\n) -> PreloadManager:\n    if isinstance(preload, str):\n        preload = [preload]\n    if preload_argv and isinstance(preload_argv[0], str):\n        preload_argv = [cast(\"list[str]\", preload_argv)] * len(preload)\n    elif not preload_argv:\n        preload_argv = [cast(\"list[str]\", [])] * len(preload)\n    if len(preload) != len(preload_argv):\n        raise ValueError(\n            \"preload and preload_argv have mismatched lengths \"\n            f\"{len(preload)} != {len(preload_argv)}\"\n        )\n\n    return PreloadManager(\n        [\n            Preload(dask_server, p, argv, file_dir)\n            for p, argv in zip(preload, preload_argv)\n        ]\n    )\n\n### Source File Dependency Files Content\n### Dependency File: core.py\nfrom __future__ import annotations\n\nimport asyncio\nimport functools\nimport inspect\nimport logging\nimport math\nimport os\nimport sys\nimport tempfile\nimport threading\nimport traceback\nimport types\nimport uuid\nimport warnings\nimport weakref\nfrom collections import defaultdict, deque\nfrom collections.abc import (\n    Awaitable,\n    Callable,\n    Container,\n    Coroutine,\n    Generator,\n    Hashable,\n)\nfrom enum import Enum\nfrom functools import wraps\nfrom typing import TYPE_CHECKING, Any, ClassVar, Literal, TypedDict, TypeVar, final\n\nimport tblib\nfrom tlz import merge\nfrom tornado.ioloop import IOLoop\n\nimport dask\nfrom dask.utils import parse_timedelta\n\nfrom distributed import profile, protocol\nfrom distributed._async_taskgroup import AsyncTaskGroup, AsyncTaskGroupClosedError\nfrom distributed.comm import (\n    Comm,\n    CommClosedError,\n    connect,\n    get_address_host_port,\n    listen,\n    normalize_address,\n    unparse_host_port,\n)\nfrom distributed.comm.core import Listener\nfrom distributed.compatibility import PeriodicCallback\nfrom distributed.counter import Counter\nfrom distributed.diskutils import WorkDir, WorkSpace\nfrom distributed.metrics import context_meter, time\nfrom distributed.system_monitor import SystemMonitor\nfrom distributed.utils import (\n    NoOpAwaitable,\n    get_traceback,\n    has_keyword,\n    import_file,\n    iscoroutinefunction,\n    offload,\n    recursive_to_dict,\n    truncate_exception,\n    wait_for,\n    warn_on_duration,\n)\n\nif TYPE_CHECKING:\n    from typing_extensions import ParamSpec, Self\n\n    from distributed.counter import Digest\n\n    P = ParamSpec(\"P\")\n    R = TypeVar(\"R\")\n    T = TypeVar(\"T\")\n    Coro = Coroutine[Any, Any, T]\n\n\nclass Status(Enum):\n    \"\"\"\n    This Enum contains the various states a cluster, worker, scheduler and nanny can be\n    in. Some of the status can only be observed in one of cluster, nanny, scheduler or\n    worker but we put them in the same Enum as they are compared with each\n    other.\n    \"\"\"\n\n    undefined = \"undefined\"\n    created = \"created\"\n    init = \"init\"\n    starting = \"starting\"\n    running = \"running\"\n    paused = \"paused\"\n    stopping = \"stopping\"\n    stopped = \"stopped\"\n    closing = \"closing\"\n    closing_gracefully = \"closing_gracefully\"\n    closed = \"closed\"\n    failed = \"failed\"\n    dont_reply = \"dont_reply\"\n\n\nStatus.lookup = {s.name: s for s in Status}  # type: ignore\n\n\nclass RPCClosed(IOError):\n    pass\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef raise_later(exc):\n    def _raise(*args, **kwargs):\n        raise exc\n\n    return _raise\n\n\ntick_maximum_delay = parse_timedelta(\n    dask.config.get(\"distributed.admin.tick.limit\"), default=\"ms\"\n)\n\nLOG_PDB = dask.config.get(\"distributed.admin.pdb-on-err\")\n\n\n@functools.cache\ndef _expects_comm(func: Callable) -> bool:\n    sig = inspect.signature(func)\n    params = list(sig.parameters)\n    if params and params[0] == \"comm\":\n        return True\n    if params and params[0] == \"stream\":\n        warnings.warn(\n            \"Calling the first argument of a RPC handler `stream` is \"\n            \"deprecated. Defining this argument is optional. Either remove the \"\n            f\"argument or rename it to `comm` in {func}.\",\n            FutureWarning,\n        )\n        return True\n    return False\n\n\nclass Server:\n    \"\"\"Dask Distributed Server\n\n    Superclass for endpoints in a distributed cluster, such as Worker\n    and Scheduler objects.\n\n    **Handlers**\n\n    Servers define operations with a ``handlers`` dict mapping operation names\n    to functions.  The first argument of a handler function will be a ``Comm``\n    for the communication established with the client.  Other arguments\n    will receive inputs from the keys of the incoming message which will\n    always be a dictionary.\n\n    >>> def pingpong(comm):\n    ...     return b'pong'\n\n    >>> def add(comm, x, y):\n    ...     return x + y\n\n    >>> handlers = {'ping': pingpong, 'add': add}\n    >>> server = Server(handlers)  # doctest: +SKIP\n    >>> server.listen('tcp://0.0.0.0:8000')  # doctest: +SKIP\n\n    **Message Format**\n\n    The server expects messages to be dictionaries with a special key, `'op'`\n    that corresponds to the name of the operation, and other key-value pairs as\n    required by the function.\n\n    So in the example above the following would be good messages.\n\n    *  ``{'op': 'ping'}``\n    *  ``{'op': 'add', 'x': 10, 'y': 20}``\n\n    \"\"\"\n\n    _is_finalizing: staticmethod[[], bool] = staticmethod(sys.is_finalizing)\n\n    default_ip: ClassVar[str] = \"\"\n    default_port: ClassVar[int] = 0\n\n    id: str\n    blocked_handlers: list[str]\n    handlers: dict[str, Callable]\n    stream_handlers: dict[str, Callable]\n    listeners: list[Listener]\n    counters: defaultdict[str, Counter]\n    deserialize: bool\n\n    local_directory: str\n\n    monitor: SystemMonitor\n    io_loop: IOLoop\n    thread_id: int\n\n    periodic_callbacks: dict[str, PeriodicCallback]\n    digests: defaultdict[Hashable, Digest] | None\n    digests_total: defaultdict[Hashable, float]\n    digests_total_since_heartbeat: defaultdict[Hashable, float]\n    digests_max: defaultdict[Hashable, float]\n\n    _last_tick: float\n    _tick_counter: int\n    _last_tick_counter: int\n    _tick_interval: float\n    _tick_interval_observed: float\n\n    _status: Status\n\n    _address: str | None\n    _listen_address: str | None\n    _host: str | None\n    _port: int | None\n\n    _comms: dict[Comm, str | None]\n\n    _ongoing_background_tasks: AsyncTaskGroup\n    _event_finished: asyncio.Event\n\n    _original_local_dir: str\n    _updated_sys_path: bool\n    _workspace: WorkSpace\n    _workdir: None | WorkDir\n\n    _startup_lock: asyncio.Lock\n    __startup_exc: Exception | None\n\n    def __init__(\n        self,\n        handlers,\n        blocked_handlers=None,\n        stream_handlers=None,\n        connection_limit=512,\n        deserialize=True,\n        serializers=None,\n        deserializers=None,\n        connection_args=None,\n        timeout=None,\n        io_loop=None,\n        local_directory=None,\n        needs_workdir=True,\n    ):\n        if local_directory is None:\n            local_directory = (\n                dask.config.get(\"temporary-directory\") or tempfile.gettempdir()\n            )\n\n        if \"dask-scratch-space\" not in str(local_directory):\n            local_directory = os.path.join(local_directory, \"dask-scratch-space\")\n\n        self._original_local_dir = local_directory\n\n        with warn_on_duration(\n            \"1s\",\n            \"Creating scratch directories is taking a surprisingly long time. ({duration:.2f}s) \"\n            \"This is often due to running workers on a network file system. \"\n            \"Consider specifying a local-directory to point workers to write \"\n            \"scratch data to a local disk.\",\n        ):\n            self._workspace = WorkSpace(local_directory)\n\n            if not needs_workdir:  # eg. Nanny will not need a WorkDir\n                self._workdir = None\n                self.local_directory = self._workspace.base_dir\n            else:\n                name = type(self).__name__.lower()\n                self._workdir = self._workspace.new_work_dir(prefix=f\"{name}-\")\n                self.local_directory = self._workdir.dir_path\n\n        self._updated_sys_path = False\n        if self.local_directory not in sys.path:\n            sys.path.insert(0, self.local_directory)\n            self._updated_sys_path = True\n\n        if io_loop is not None:\n            warnings.warn(\n                \"The io_loop kwarg to Server is ignored and will be deprecated\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        self._status = Status.init\n        self.handlers = {\n            \"identity\": self.identity,\n            \"echo\": self.echo,\n            \"connection_stream\": self.handle_stream,\n            \"dump_state\": self._to_dict,\n        }\n        self.handlers.update(handlers)\n        if blocked_handlers is None:\n            blocked_handlers = dask.config.get(\n                \"distributed.%s.blocked-handlers\" % type(self).__name__.lower(), []\n            )\n        self.blocked_handlers = blocked_handlers\n        self.stream_handlers = {}\n        self.stream_handlers.update(stream_handlers or {})\n\n        self.id = type(self).__name__ + \"-\" + str(uuid.uuid4())\n        self._address = None\n        self._listen_address = None\n        self._port = None\n        self._host = None\n        self._comms = {}\n        self.deserialize = deserialize\n        self.monitor = SystemMonitor()\n        self._ongoing_background_tasks = AsyncTaskGroup()\n        self._event_finished = asyncio.Event()\n\n        self.listeners = []\n        self.io_loop = self.loop = IOLoop.current()\n\n        if not hasattr(self.io_loop, \"profile\"):\n            if (\n                dask.config.get(\"distributed.worker.profile.enabled\")\n                and sys.version_info.minor != 11\n            ):\n                ref = weakref.ref(self.io_loop)\n\n                def stop() -> bool:\n                    loop = ref()\n                    return loop is None or loop.asyncio_loop.is_closed()\n\n                self.io_loop.profile = profile.watch(\n                    omit=(\"profile.py\", \"selectors.py\"),\n                    interval=dask.config.get(\"distributed.worker.profile.interval\"),\n                    cycle=dask.config.get(\"distributed.worker.profile.cycle\"),\n                    stop=stop,\n                )\n            else:\n                self.io_loop.profile = deque()\n\n        self.periodic_callbacks = {}\n\n        # Statistics counters for various events\n        try:\n            from distributed.counter import Digest\n\n            self.digests = defaultdict(Digest)\n        except ImportError:\n            self.digests = None\n\n        # Also log cumulative totals (reset at server restart)\n        # and local maximums (reset by prometheus poll)\n        # Don't cast int metrics to float\n        self.digests_total = defaultdict(int)\n        self.digests_total_since_heartbeat = defaultdict(int)\n        self.digests_max = defaultdict(int)\n\n        self.counters = defaultdict(Counter)\n        pc = PeriodicCallback(self._shift_counters, 5000)\n        self.periodic_callbacks[\"shift_counters\"] = pc\n\n        pc = PeriodicCallback(\n            self.monitor.update,\n            parse_timedelta(\n                dask.config.get(\"distributed.admin.system-monitor.interval\")\n            )\n            * 1000,\n        )\n        self.periodic_callbacks[\"monitor\"] = pc\n\n        self._last_tick = time()\n        self._tick_counter = 0\n        self._last_tick_counter = 0\n        self._last_tick_cycle = time()\n        self._tick_interval = parse_timedelta(\n            dask.config.get(\"distributed.admin.tick.interval\"), default=\"ms\"\n        )\n        self._tick_interval_observed = self._tick_interval\n        self.periodic_callbacks[\"tick\"] = PeriodicCallback(\n            self._measure_tick, self._tick_interval * 1000\n        )\n        self.periodic_callbacks[\"ticks\"] = PeriodicCallback(\n            self._cycle_ticks,\n            parse_timedelta(dask.config.get(\"distributed.admin.tick.cycle\")) * 1000,\n        )\n\n        self.thread_id = 0\n\n        def set_thread_ident():\n            self.thread_id = threading.get_ident()\n\n        self.io_loop.add_callback(set_thread_ident)\n        self._startup_lock = asyncio.Lock()\n        self.__startup_exc = None\n\n        self.rpc = ConnectionPool(\n            limit=connection_limit,\n            deserialize=deserialize,\n            serializers=serializers,\n            deserializers=deserializers,\n            connection_args=connection_args,\n            timeout=timeout,\n            server=self,\n        )\n\n        self.__stopped = False\n\n    async def upload_file(\n        self, filename: str, data: str | bytes, load: bool = True\n    ) -> dict[str, Any]:\n        out_filename = os.path.join(self.local_directory, filename)\n\n        def func(data):\n            if isinstance(data, str):\n                data = data.encode()\n            with open(out_filename, \"wb\") as f:\n                f.write(data)\n                f.flush()\n                os.fsync(f.fileno())\n            return data\n\n        if len(data) < 10000:\n            data = func(data)\n        else:\n            data = await offload(func, data)\n\n        if load:\n            try:\n                import_file(out_filename)\n            except Exception as e:\n                logger.exception(e)\n                raise e\n\n        return {\"status\": \"OK\", \"nbytes\": len(data)}\n\n    def _shift_counters(self):\n        # Copy counters before iterating to avoid concurrent modification\n        for counter in list(self.counters.values()):\n            counter.shift()\n        if self.digests is not None:\n            # Copy digests before iterating to avoid concurrent modification\n            for digest in list(self.digests.values()):\n                digest.shift()\n\n    @property\n    def status(self) -> Status:\n        try:\n            return self._status\n        except AttributeError:\n            return Status.undefined\n\n    @status.setter\n    def status(self, value: Status) -> None:\n        if not isinstance(value, Status):\n            raise TypeError(f\"Expected Status; got {value!r}\")\n        self._status = value\n\n    @property\n    def incoming_comms_open(self) -> int:\n        \"\"\"The number of total incoming connections listening to remote RPCs\"\"\"\n        return len(self._comms)\n\n    @property\n    def incoming_comms_active(self) -> int:\n        \"\"\"The number of connections currently handling a remote RPC\"\"\"\n        return len([c for c, op in self._comms.items() if op is not None])\n\n    @property\n    def outgoing_comms_open(self) -> int:\n        \"\"\"The number of connections currently open and waiting for a remote RPC\"\"\"\n        return self.rpc.open\n\n    @property\n    def outgoing_comms_active(self) -> int:\n        \"\"\"The number of outgoing connections that are currently used to\n        execute a RPC\"\"\"\n        return self.rpc.active\n\n    def get_connection_counters(self) -> dict[str, int]:\n        \"\"\"A dict with various connection counters\n\n        See also\n        --------\n        Server.incoming_comms_open\n        Server.incoming_comms_active\n        Server.outgoing_comms_open\n        Server.outgoing_comms_active\n        \"\"\"\n        return {\n            attr: getattr(self, attr)\n            for attr in [\n                \"incoming_comms_open\",\n                \"incoming_comms_active\",\n                \"outgoing_comms_open\",\n                \"outgoing_comms_active\",\n            ]\n        }\n\n    async def finished(self):\n        \"\"\"Wait until the server has finished\"\"\"\n        await self._event_finished.wait()\n\n    def __await__(self):\n        return self.start().__await__()\n\n    async def start_unsafe(self):\n        \"\"\"Attempt to start the server. This is not idempotent and not protected against concurrent startup attempts.\n\n        This is intended to be overwritten or called by subclasses. For a safe\n        startup, please use ``Server.start`` instead.\n\n        If ``death_timeout`` is configured, we will require this coroutine to\n        finish before this timeout is reached. If the timeout is reached we will\n        close the instance and raise an ``asyncio.TimeoutError``\n        \"\"\"\n        await self.rpc.start()\n        return self\n\n    @final\n    async def start(self):\n        async with self._startup_lock:\n            if self.status == Status.failed:\n                assert self.__startup_exc is not None\n                raise self.__startup_exc\n            elif self.status != Status.init:\n                return self\n            timeout = getattr(self, \"death_timeout\", None)\n\n            async def _close_on_failure(exc: Exception) -> None:\n                await self.close(reason=f\"failure-to-start-{str(type(exc))}\")\n                self.status = Status.failed\n                self.__startup_exc = exc\n\n            try:\n                await wait_for(self.start_unsafe(), timeout=timeout)\n            except asyncio.TimeoutError as exc:\n                await _close_on_failure(exc)\n                raise asyncio.TimeoutError(\n                    f\"{type(self).__name__} start timed out after {timeout}s.\"\n                ) from exc\n            except Exception as exc:\n                await _close_on_failure(exc)\n                raise RuntimeError(f\"{type(self).__name__} failed to start.\") from exc\n            if self.status == Status.init:\n                self.status = Status.running\n        return self\n\n    async def __aenter__(self):\n        await self\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        await self.close()\n\n    def start_periodic_callbacks(self):\n        \"\"\"Start Periodic Callbacks consistently\n\n        This starts all PeriodicCallbacks stored in self.periodic_callbacks if\n        they are not yet running. It does this safely by checking that it is using the\n        correct event loop.\n        \"\"\"\n        if self.io_loop.asyncio_loop is not asyncio.get_running_loop():\n            raise RuntimeError(f\"{self!r} is bound to a different event loop\")\n\n        self._last_tick = time()\n        for pc in self.periodic_callbacks.values():\n            if not pc.is_running():\n                pc.start()\n\n    def _stop_listeners(self) -> asyncio.Future:\n        listeners_to_stop: set[Awaitable] = set()\n\n        for listener in self.listeners:\n            future = listener.stop()\n            if inspect.isawaitable(future):\n                warnings.warn(\n                    f\"{type(listener)} is using an asynchronous `stop` method. \"\n                    \"Support for asynchronous `Listener.stop` has been deprecated and \"\n                    \"will be removed in a future version\",\n                    DeprecationWarning,\n                )\n                listeners_to_stop.add(future)\n            elif hasattr(listener, \"abort_handshaking_comms\"):\n                listener.abort_handshaking_comms()\n\n        return asyncio.gather(*listeners_to_stop)\n\n    def stop(self) -> None:\n        if self.__stopped:\n            return\n        self.__stopped = True\n        self.monitor.close()\n        if not (stop_listeners := self._stop_listeners()).done():\n            self._ongoing_background_tasks.call_soon(\n                asyncio.wait_for(stop_listeners, timeout=None)  # type: ignore[arg-type]\n            )\n        if self._workdir is not None:\n            self._workdir.release()\n\n    @property\n    def listener(self) -> Listener | None:\n        if self.listeners:\n            return self.listeners[0]\n        else:\n            return None\n\n    def _measure_tick(self):\n        now = time()\n        tick_duration = now - self._last_tick\n        self._last_tick = now\n        self._tick_counter += 1\n        # This metric is exposed in Prometheus and is reset there during\n        # collection\n        if tick_duration > tick_maximum_delay:\n            logger.info(\n                \"Event loop was unresponsive in %s for %.2fs.  \"\n                \"This is often caused by long-running GIL-holding \"\n                \"functions or moving large chunks of data. \"\n                \"This can cause timeouts and instability.\",\n                type(self).__name__,\n                tick_duration,\n            )\n        self.digest_metric(\"tick-duration\", tick_duration)\n\n    def _cycle_ticks(self):\n        if not self._tick_counter:\n            return\n        now = time()\n        last_tick_cycle, self._last_tick_cycle = self._last_tick_cycle, now\n        count = self._tick_counter - self._last_tick_counter\n        self._last_tick_counter = self._tick_counter\n        self._tick_interval_observed = (now - last_tick_cycle) / (count or 1)\n\n    @property\n    def address(self) -> str:\n        \"\"\"\n        The address this Server can be contacted on.\n        If the server is not up, yet, this raises a ValueError.\n        \"\"\"\n        if not self._address:\n            if self.listener is None:\n                raise ValueError(\"cannot get address of non-running Server\")\n            self._address = self.listener.contact_address\n            assert self._address\n        return self._address\n\n    @property\n    def address_safe(self) -> str:\n        \"\"\"\n        The address this Server can be contacted on.\n        If the server is not up, yet, this returns a ``\"not-running\"``.\n        \"\"\"\n        try:\n            return self.address\n        except ValueError:\n            return \"not-running\"\n\n    @property\n    def listen_address(self):\n        \"\"\"\n        The address this Server is listening on.  This may be a wildcard\n        address such as `tcp://0.0.0.0:1234`.\n        \"\"\"\n        if not self._listen_address:\n            if self.listener is None:\n                raise ValueError(\"cannot get listen address of non-running Server\")\n            self._listen_address = self.listener.listen_address\n        return self._listen_address\n\n    @property\n    def host(self):\n        \"\"\"\n        The host this Server is running on.\n\n        This will raise ValueError if the Server is listening on a\n        non-IP based protocol.\n        \"\"\"\n        if not self._host:\n            self._host, self._port = get_address_host_port(self.address)\n        return self._host\n\n    @property\n    def port(self):\n        \"\"\"\n        The port number this Server is listening on.\n\n        This will raise ValueError if the Server is listening on a\n        non-IP based protocol.\n        \"\"\"\n        if not self._port:\n            self._host, self._port = get_address_host_port(self.address)\n        return self._port\n\n    def identity(self) -> dict[str, str]:\n        return {\"type\": type(self).__name__, \"id\": self.id}\n\n    def _to_dict(self, *, exclude: Container[str] = ()) -> dict[str, Any]:\n        \"\"\"Dictionary representation for debugging purposes.\n        Not type stable and not intended for roundtrips.\n\n        See also\n        --------\n        Server.identity\n        Client.dump_cluster_state\n        distributed.utils.recursive_to_dict\n        \"\"\"\n        info: dict[str, Any] = self.identity()\n        extra = {\n            \"address\": self.address,\n            \"status\": self.status.name,\n            \"thread_id\": self.thread_id,\n        }\n        info.update(extra)\n        info = {k: v for k, v in info.items() if k not in exclude}\n        return recursive_to_dict(info, exclude=exclude)\n\n    def echo(self, data=None):\n        return data\n\n    async def listen(self, port_or_addr=None, allow_offload=True, **kwargs):\n        if port_or_addr is None:\n            port_or_addr = self.default_port\n        if isinstance(port_or_addr, int):\n            addr = unparse_host_port(self.default_ip, port_or_addr)\n        elif isinstance(port_or_addr, tuple):\n            addr = unparse_host_port(*port_or_addr)\n        else:\n            addr = port_or_addr\n            assert isinstance(addr, str)\n        listener = await listen(\n            addr,\n            self.handle_comm,\n            deserialize=self.deserialize,\n            allow_offload=allow_offload,\n            **kwargs,\n        )\n        self.listeners.append(listener)\n\n    def handle_comm(self, comm: Comm) -> NoOpAwaitable:\n        \"\"\"Start a background task that dispatches new communications to coroutine-handlers\"\"\"\n        try:\n            self._ongoing_background_tasks.call_soon(self._handle_comm, comm)\n        except AsyncTaskGroupClosedError:\n            comm.abort()\n        return NoOpAwaitable()\n\n    async def _handle_comm(self, comm: Comm) -> None:\n        \"\"\"Dispatch new communications to coroutine-handlers\n\n        Handlers is a dictionary mapping operation names to functions or\n        coroutines.\n\n            {'get_data': get_data,\n             'ping': pingpong}\n\n        Coroutines should expect a single Comm object.\n        \"\"\"\n        if self.__stopped:\n            comm.abort()\n            return\n        address = comm.peer_address\n        op = None\n\n        logger.debug(\"Connection from %r to %s\", address, type(self).__name__)\n        self._comms[comm] = op\n\n        await self\n        try:\n            while not self.__stopped:\n                try:\n                    msg = await comm.read()\n                    logger.debug(\"Message from %r: %s\", address, msg)\n                except OSError as e:\n                    if not self._is_finalizing():\n                        logger.debug(\n                            \"Lost connection to %r while reading message: %s.\"\n                            \" Last operation: %s\",\n                            address,\n                            e,\n                            op,\n                        )\n                    break\n                except Exception as e:\n                    logger.exception(\"Exception while reading from %s\", address)\n                    if comm.closed():\n                        raise\n                    else:\n                        await comm.write(error_message(e, status=\"uncaught-error\"))\n                        continue\n                if not isinstance(msg, dict):\n                    raise TypeError(\n                        \"Bad message type.  Expected dict, got\\n  \" + str(msg)\n                    )\n\n                try:\n                    op = msg.pop(\"op\")\n                except KeyError as e:\n                    raise ValueError(\n                        \"Received unexpected message without 'op' key: \" + str(msg)\n                    ) from e\n                if self.counters is not None:\n                    self.counters[\"op\"].add(op)\n                self._comms[comm] = op\n                serializers = msg.pop(\"serializers\", None)\n                close_desired = msg.pop(\"close\", False)\n                reply = msg.pop(\"reply\", True)\n                if op == \"close\":\n                    if reply:\n                        await comm.write(\"OK\")\n                    break\n\n                result = None\n                try:\n                    if op in self.blocked_handlers:\n                        _msg = (\n                            \"The '{op}' handler has been explicitly disallowed \"\n                            \"in {obj}, possibly due to security concerns.\"\n                        )\n                        exc = ValueError(_msg.format(op=op, obj=type(self).__name__))\n                        handler = raise_later(exc)\n                    else:\n                        handler = self.handlers[op]\n                except KeyError:\n                    logger.warning(\n                        \"No handler %s found in %s\",\n                        op,\n                        type(self).__name__,\n                        exc_info=True,\n                    )\n                else:\n                    if serializers is not None and has_keyword(handler, \"serializers\"):\n                        msg[\"serializers\"] = serializers  # add back in\n\n                    logger.debug(\"Calling into handler %s\", handler.__name__)\n                    try:\n                        if _expects_comm(handler):\n                            result = handler(comm, **msg)\n                        else:\n                            result = handler(**msg)\n                        if inspect.iscoroutine(result):\n                            result = await result\n                        elif inspect.isawaitable(result):\n                            raise RuntimeError(\n                                f\"Comm handler returned unknown awaitable. Expected coroutine, instead got {type(result)}\"\n                            )\n                    except CommClosedError:\n                        if self.status == Status.running:\n                            logger.info(\"Lost connection to %r\", address, exc_info=True)\n                        break\n                    except Exception as e:\n                        logger.exception(\"Exception while handling op %s\", op)\n                        if comm.closed():\n                            raise\n                        else:\n                            result = error_message(e, status=\"uncaught-error\")\n\n                if reply and result != Status.dont_reply:\n                    try:\n                        await comm.write(result, serializers=serializers)\n                    except (OSError, TypeError) as e:\n                        logger.debug(\n                            \"Lost connection to %r while sending result for op %r: %s\",\n                            address,\n                            op,\n                            e,\n                        )\n                        break\n\n                self._comms[comm] = None\n                msg = result = None\n                if close_desired:\n                    await comm.close()\n                if comm.closed():\n                    break\n\n        finally:\n            del self._comms[comm]\n            if not self._is_finalizing() and not comm.closed():\n                try:\n                    comm.abort()\n                except Exception as e:\n                    logger.error(\n                        \"Failed while closing connection to %r: %s\", address, e\n                    )\n\n    async def handle_stream(\n        self, comm: Comm, extra: dict[str, Any] | None = None\n    ) -> None:\n        extra = extra or {}\n        logger.info(\"Starting established connection to %s\", comm.peer_address)\n\n        closed = False\n        try:\n            while not closed:\n                try:\n                    msgs = await comm.read()\n                # If another coroutine has closed the comm, stop handling the stream.\n                except CommClosedError:\n                    closed = True\n                    logger.info(\n                        \"Connection to %s has been closed.\",\n                        comm.peer_address,\n                    )\n                    break\n                if not isinstance(msgs, (tuple, list)):\n                    msgs = (msgs,)\n\n                for msg in msgs:\n                    if msg == \"OK\":\n                        break\n                    op = msg.pop(\"op\")\n                    if op:\n                        if op == \"close-stream\":\n                            closed = True\n                            logger.info(\n                                \"Received 'close-stream' from %s; closing.\",\n                                comm.peer_address,\n                            )\n                            break\n                        handler = self.stream_handlers[op]\n                        if iscoroutinefunction(handler):\n                            await handler(**merge(extra, msg))\n                        else:\n                            handler(**merge(extra, msg))\n                    else:\n                        logger.error(\"odd message %s\", msg)\n                await asyncio.sleep(0)\n        except Exception:\n            if LOG_PDB:\n                import pdb\n\n                pdb.set_trace()\n            raise\n        finally:\n            await comm.close()\n            assert comm.closed()\n\n    async def close(self, timeout: float | None = None, reason: str = \"\") -> None:\n        try:\n            for pc in self.periodic_callbacks.values():\n                pc.stop()\n\n            self.__stopped = True\n            self.monitor.close()\n            await self._stop_listeners()\n\n            # TODO: Deal with exceptions\n            await self._ongoing_background_tasks.stop()\n\n            await self.rpc.close()\n            await asyncio.gather(*[comm.close() for comm in list(self._comms)])\n\n            # Remove scratch directory from global sys.path\n            if self._updated_sys_path and sys.path[0] == self.local_directory:\n                sys.path.remove(self.local_directory)\n        finally:\n            self._event_finished.set()\n\n    def digest_metric(self, name: Hashable, value: float) -> None:\n        # Granular data (requires crick)\n        if self.digests is not None:\n            self.digests[name].add(value)\n        # Cumulative data (reset by server restart)\n        self.digests_total[name] += value\n        # Cumulative data sent to scheduler and reset on heartbeat\n        self.digests_total_since_heartbeat[name] += value\n        # Local maximums (reset by Prometheus poll)\n        self.digests_max[name] = max(self.digests_max[name], value)\n\n\ndef context_meter_to_server_digest(digest_tag: str) -> Callable:\n    \"\"\"Decorator for an async method of a Server subclass that calls\n    ``distributed.metrics.context_meter.meter`` and/or ``digest_metric``.\n    It routes the calls from ``context_meter.digest_metric(label, value, unit)`` to\n    ``Server.digest_metric((digest_tag, label, unit), value)``.\n    \"\"\"\n\n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        async def wrapper(self: Server, *args: Any, **kwargs: Any) -> Any:\n            def metrics_callback(label: Hashable, value: float, unit: str) -> None:\n                if not isinstance(label, tuple):\n                    label = (label,)\n                name = (digest_tag, *label, unit)\n                self.digest_metric(name, value)\n\n            with context_meter.add_callback(metrics_callback, allow_offload=True):\n                return await func(self, *args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\ndef pingpong(comm):\n    return b\"pong\"\n\n\nasync def send_recv(  # type: ignore[no-untyped-def]\n    comm: Comm,\n    *,\n    reply: bool = True,\n    serializers=None,\n    deserializers=None,\n    **kwargs,\n):\n    \"\"\"Send and recv with a Comm.\n\n    Keyword arguments turn into the message\n\n    response = await send_recv(comm, op='ping', reply=True)\n    \"\"\"\n    msg = kwargs\n    msg[\"reply\"] = reply\n    please_close = kwargs.get(\"close\", False)\n    force_close = False\n    if deserializers is None:\n        deserializers = serializers\n    if deserializers is not None:\n        msg[\"serializers\"] = deserializers\n\n    try:\n        await comm.write(msg, serializers=serializers, on_error=\"raise\")\n        if reply:\n            response = await comm.read(deserializers=deserializers)\n        else:\n            response = None\n    except (asyncio.TimeoutError, OSError):\n        # On communication errors, we should simply close the communication\n        # Note that OSError includes CommClosedError and socket timeouts\n        force_close = True\n        raise\n    except asyncio.CancelledError:\n        # Do not reuse the comm to prevent the next call of send_recv from receiving\n        # data from this call and/or accidentally putting multiple waiters on read().\n        # Note that this relies on all Comm implementations to allow a write() in the\n        # middle of a read().\n        please_close = True\n        raise\n    finally:\n        if force_close:\n            comm.abort()\n        elif please_close:\n            await comm.close()\n\n    if isinstance(response, dict) and response.get(\"status\") == \"uncaught-error\":\n        if comm.deserialize:\n            _, exc, tb = clean_exception(**response)\n            assert exc\n            raise exc.with_traceback(tb)\n        else:\n            raise Exception(response[\"exception_text\"])\n    return response\n\n\ndef addr_from_args(\n    addr: str | tuple[str, int | None] | None = None,\n    ip: str | None = None,\n    port: int | None = None,\n) -> str:\n    if addr is None:\n        assert ip is not None\n        return normalize_address(unparse_host_port(ip, port))\n\n    assert ip is None and port is None\n    if isinstance(addr, tuple):\n        return normalize_address(unparse_host_port(*addr))\n\n    return normalize_address(addr)\n\n\nclass rpc:\n    \"\"\"Conveniently interact with a remote server\n\n    >>> remote = rpc(address)  # doctest: +SKIP\n    >>> response = await remote.add(x=10, y=20)  # doctest: +SKIP\n\n    One rpc object can be reused for several interactions.\n    Additionally, this object creates and destroys many comms as necessary\n    and so is safe to use in multiple overlapping communications.\n\n    When done, close comms explicitly.\n\n    >>> remote.close_comms()  # doctest: +SKIP\n    \"\"\"\n\n    active: ClassVar[weakref.WeakSet[rpc]] = weakref.WeakSet()\n    comms = ()\n    address = None\n\n    def __init__(\n        self,\n        arg=None,\n        comm=None,\n        deserialize=True,\n        timeout=None,\n        connection_args=None,\n        serializers=None,\n        deserializers=None,\n    ):\n        self.comms = {}\n        self.address = coerce_to_address(arg)\n        self.timeout = timeout\n        self.status = Status.running\n        self.deserialize = deserialize\n        self.serializers = serializers\n        self.deserializers = deserializers if deserializers is not None else serializers\n        self.connection_args = connection_args or {}\n        self._created = weakref.WeakSet()\n        rpc.active.add(self)\n\n    async def live_comm(self):\n        \"\"\"Get an open communication\n\n        Some comms to the ip/port target may be in current use by other\n        coroutines.  We track this with the `comms` dict\n\n            :: {comm: True/False if open and ready for use}\n\n        This function produces an open communication, either by taking one\n        that we've already made or making a new one if they are all taken.\n        This also removes comms that have been closed.\n\n        When the caller is done with the stream they should set\n\n            self.comms[comm] = True\n\n        As is done in __getattr__ below.\n        \"\"\"\n        if self.status == Status.closed:\n            raise RPCClosed(\"RPC Closed\")\n        to_clear = set()\n        open = False\n        for comm, open in self.comms.items():\n            if comm.closed():\n                to_clear.add(comm)\n            if open:\n                break\n        for s in to_clear:\n            del self.comms[s]\n        if not open or comm.closed():\n            comm = await connect(\n                self.address,\n                self.timeout,\n                deserialize=self.deserialize,\n                **self.connection_args,\n            )\n            comm.name = \"rpc\"\n        self.comms[comm] = False  # mark as taken\n        return comm\n\n    def close_comms(self):\n        async def _close_comm(comm):\n            # Make sure we tell the peer to close\n            try:\n                if not comm.closed():\n                    await comm.write({\"op\": \"close\", \"reply\": False})\n                    await comm.close()\n            except OSError:\n                comm.abort()\n\n        tasks = []\n        for comm in list(self.comms):\n            if comm and not comm.closed():\n                task = asyncio.ensure_future(_close_comm(comm))\n                tasks.append(task)\n        for comm in list(self._created):\n            if comm and not comm.closed():\n                task = asyncio.ensure_future(_close_comm(comm))\n                tasks.append(task)\n\n        self.comms.clear()\n        return tasks\n\n    def __getattr__(self, key):\n        async def send_recv_from_rpc(**kwargs):\n            if self.serializers is not None and kwargs.get(\"serializers\") is None:\n                kwargs[\"serializers\"] = self.serializers\n            if self.deserializers is not None and kwargs.get(\"deserializers\") is None:\n                kwargs[\"deserializers\"] = self.deserializers\n            comm = None\n            try:\n                comm = await self.live_comm()\n                comm.name = \"rpc.\" + key\n                result = await send_recv(comm=comm, op=key, **kwargs)\n            except (RPCClosed, CommClosedError) as e:\n                if comm:\n                    raise type(e)(\n                        f\"Exception while trying to call remote method {key!r} using comm {comm!r}.\"\n                    ) from e\n                else:\n                    raise type(e)(\n                        f\"Exception while trying to call remote method {key!r} before comm was established.\"\n                    ) from e\n\n            self.comms[comm] = True  # mark as open\n            return result\n\n        return send_recv_from_rpc\n\n    async def close_rpc(self):\n        if self.status != Status.closed:\n            rpc.active.discard(self)\n        self.status = Status.closed\n        return await asyncio.gather(*self.close_comms())\n\n    def __enter__(self):\n        warnings.warn(\n            \"the rpc synchronous context manager is deprecated\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        asyncio.ensure_future(self.close_rpc())\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        await self.close_rpc()\n\n    def __del__(self):\n        if self.status != Status.closed:\n            rpc.active.discard(self)\n            self.status = Status.closed\n            still_open = [comm for comm in self.comms if not comm.closed()]\n            if still_open:\n                logger.warning(\n                    \"rpc object %s deleted with %d open comms\", self, len(still_open)\n                )\n                for comm in still_open:\n                    comm.abort()\n\n    def __repr__(self):\n        return \"<rpc to %r, %d comms>\" % (self.address, len(self.comms))\n\n\nclass PooledRPCCall:\n    \"\"\"The result of ConnectionPool()('host:port')\n\n    See Also:\n        ConnectionPool\n    \"\"\"\n\n    def __init__(self, addr, pool, serializers=None, deserializers=None):\n        self.addr = addr\n        self.pool = pool\n        self.serializers = serializers\n        self.deserializers = deserializers if deserializers is not None else serializers\n\n    @property\n    def address(self):\n        return self.addr\n\n    def __getattr__(self, key):\n        async def send_recv_from_rpc(**kwargs):\n            if self.serializers is not None and kwargs.get(\"serializers\") is None:\n                kwargs[\"serializers\"] = self.serializers\n            if self.deserializers is not None and kwargs.get(\"deserializers\") is None:\n                kwargs[\"deserializers\"] = self.deserializers\n            comm = await self.pool.connect(self.addr)\n            prev_name, comm.name = comm.name, \"ConnectionPool.\" + key\n            try:\n                return await send_recv(comm=comm, op=key, **kwargs)\n            finally:\n                self.pool.reuse(self.addr, comm)\n                comm.name = prev_name\n\n        return send_recv_from_rpc\n\n    async def close_rpc(self):\n        pass\n\n    # For compatibility with rpc()\n    def __enter__(self):\n        warnings.warn(\n            \"the rpc synchronous context manager is deprecated\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *args):\n        pass\n\n    def __repr__(self):\n        return f\"<pooled rpc to {self.addr!r}>\"\n\n\nclass ConnectionPool:\n    \"\"\"A maximum sized pool of Comm objects.\n\n    This provides a connect method that mirrors the normal distributed.connect\n    method, but provides connection sharing and tracks connection limits.\n\n    This object provides an ``rpc`` like interface::\n\n        >>> rpc = ConnectionPool(limit=512)\n        >>> scheduler = rpc('127.0.0.1:8786')\n        >>> workers = [rpc(address) for address in ...]\n\n        >>> info = await scheduler.identity()\n\n    It creates enough comms to satisfy concurrent connections to any\n    particular address::\n\n        >>> a, b = await asyncio.gather(scheduler.who_has(), scheduler.has_what())\n\n    It reuses existing comms so that we don't have to continuously reconnect.\n\n    It also maintains a comm limit to avoid \"too many open file handle\"\n    issues.  Whenever this maximum is reached we clear out all idling comms.\n    If that doesn't do the trick then we wait until one of the occupied comms\n    closes.\n\n    Parameters\n    ----------\n    limit: int\n        The number of open comms to maintain at once\n    deserialize: bool\n        Whether or not to deserialize data by default or pass it through\n    \"\"\"\n\n    _instances: ClassVar[weakref.WeakSet[ConnectionPool]] = weakref.WeakSet()\n\n    def __init__(\n        self,\n        limit: int = 512,\n        deserialize: bool = True,\n        serializers: list[str] | None = None,\n        allow_offload: bool = True,\n        deserializers: list[str] | None = None,\n        connection_args: dict[str, object] | None = None,\n        timeout: float | None = None,\n        server: object = None,\n    ) -> None:\n        self.limit = limit  # Max number of open comms\n        # Invariant: len(available) == open - active\n        self.available: defaultdict[str, set[Comm]] = defaultdict(set)\n        # Invariant: len(occupied) == active\n        self.occupied: defaultdict[str, set[Comm]] = defaultdict(set)\n        self.allow_offload = allow_offload\n        self.deserialize = deserialize\n        self.serializers = serializers\n        self.deserializers = deserializers if deserializers is not None else serializers\n        self.connection_args = connection_args or {}\n        self.timeout = timeout\n        self.server = weakref.ref(server) if server else None\n        self._created: weakref.WeakSet[Comm] = weakref.WeakSet()\n        self._instances.add(self)\n        # _n_connecting and _connecting have subtle different semantics. The set\n        # _connecting contains futures actively trying to establish a connection\n        # while the _n_connecting also accounts for connection attempts which\n        # are waiting due to the connection limit\n        self._connecting: defaultdict[str, set[Callable[[str], None]]] = defaultdict(\n            set\n        )\n        self._pending_count = 0\n        self._connecting_count = 0\n        self._connecting_close_timeout = 5\n        self.status = Status.init\n\n    def _validate(self) -> None:\n        \"\"\"\n        Validate important invariants of this class\n\n        Used only for testing / debugging\n        \"\"\"\n        assert self.semaphore._value == self.limit - self.open - self._n_connecting\n\n    @property\n    def active(self) -> int:\n        return sum(map(len, self.occupied.values()))\n\n    @property\n    def open(self) -> int:\n        return self.active + sum(map(len, self.available.values()))\n\n    def __repr__(self) -> str:\n        return \"<ConnectionPool: open=%d, active=%d, connecting=%d>\" % (\n            self.open,\n            self.active,\n            len(self._connecting),\n        )\n\n    def __call__(\n        self,\n        addr: str | tuple[str, int | None] | None = None,\n        ip: str | None = None,\n        port: int | None = None,\n    ) -> PooledRPCCall:\n        \"\"\"Cached rpc objects\"\"\"\n        addr = addr_from_args(addr=addr, ip=ip, port=port)\n        return PooledRPCCall(\n            addr, self, serializers=self.serializers, deserializers=self.deserializers\n        )\n\n    def __await__(self) -> Generator[Any, Any, Self]:\n        async def _() -> Self:\n            await self.start()\n            return self\n\n        return _().__await__()\n\n    async def __aenter__(self):\n        await self\n        return self\n\n    async def __aexit__(self, *args):\n        await self.close()\n        return\n\n    async def start(self) -> None:\n        # Invariant: semaphore._value == limit - open - _n_connecting\n        self.semaphore = asyncio.Semaphore(self.limit)\n        self.status = Status.running\n\n    @property\n    def _n_connecting(self) -> int:\n        return self._connecting_count\n\n    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:\n        self._pending_count += 1\n        try:\n            await self.semaphore.acquire()\n            try:\n                self._connecting_count += 1\n                comm = await connect(\n                    addr,\n                    timeout=timeout or self.timeout,\n                    deserialize=self.deserialize,\n                    **self.connection_args,\n                )\n                comm.name = \"ConnectionPool\"\n                comm._pool = weakref.ref(self)\n                comm.allow_offload = self.allow_offload\n                self._created.add(comm)\n\n                self.occupied[addr].add(comm)\n\n                return comm\n            except BaseException:\n                self.semaphore.release()\n                raise\n            finally:\n                self._connecting_count -= 1\n        finally:\n            self._pending_count -= 1\n\n    async def connect(self, addr: str, timeout: float | None = None) -> Comm:\n        \"\"\"\n        Get a Comm to the given address.  For internal use.\n        \"\"\"\n        if self.status != Status.running:\n            raise RuntimeError(\"ConnectionPool is closed\")\n        available = self.available[addr]\n        occupied = self.occupied[addr]\n        while available:\n            comm = available.pop()\n            if comm.closed():\n                self.semaphore.release()\n            else:\n                occupied.add(comm)\n                return comm\n\n        if self.semaphore.locked():\n            self.collect()\n\n        # on 3.11 this uses asyncio.timeout as a cancel scope to avoid having\n        # to track inner and outer CancelledError exceptions and correctly\n        # call .uncancel() when a CancelledError is caught.\n        if sys.version_info >= (3, 11):\n            reason: str | None = None\n            try:\n                async with asyncio.timeout(math.inf) as scope:\n\n                    def cancel_timeout_cb(reason_: str) -> None:\n                        nonlocal reason\n                        reason = reason_\n                        scope.reschedule(-1)\n\n                    self._connecting[addr].add(cancel_timeout_cb)\n                    try:\n                        return await self._connect(addr=addr, timeout=timeout)\n                    finally:\n                        connecting = self._connecting[addr]\n                        connecting.discard(cancel_timeout_cb)\n                        if not connecting:\n                            try:\n                                del self._connecting[addr]\n                            except KeyError:\n                                pass\n            except TimeoutError:\n                if reason is None:\n                    raise\n                raise CommClosedError(reason)\n        else:\n            # This construction is there to ensure that cancellation requests from\n            # the outside can be distinguished from cancellations of our own.\n            # Once the CommPool closes, we'll cancel the connect_attempt which will\n            # raise an OSError\n            # If the ``connect`` is cancelled from the outside, the Event.wait will\n            # be cancelled instead which we'll reraise as a CancelledError and allow\n            # it to propagate\n            connect_attempt = asyncio.create_task(self._connect(addr, timeout))\n            done = asyncio.Event()\n            reason = \"ConnectionPool closing.\"\n\n            def cancel_task_cb(reason_: str) -> None:\n                nonlocal reason\n                reason = reason_\n                connect_attempt.cancel()\n\n            connecting = self._connecting[addr]\n            connecting.add(cancel_task_cb)\n\n            def callback(task: asyncio.Task[Comm]) -> None:\n                done.set()\n                connecting = self._connecting[addr]\n                connecting.discard(cancel_task_cb)\n\n                if not connecting:\n                    try:\n                        del self._connecting[addr]\n                    except KeyError:  # pragma: no cover\n                        pass\n\n            connect_attempt.add_done_callback(callback)\n\n            try:\n                await done.wait()\n            except asyncio.CancelledError:\n                # This is an outside cancel attempt\n                connect_attempt.cancel()\n                await connect_attempt\n                raise\n            try:\n                return connect_attempt.result()\n            except asyncio.CancelledError:\n                if reason:\n                    raise CommClosedError(reason)\n                raise\n\n    def reuse(self, addr: str, comm: Comm) -> None:\n        \"\"\"\n        Reuse an open communication to the given address.  For internal use.\n        \"\"\"\n        # if the pool is asked to re-use a comm it does not know about, ignore\n        # this comm: just close it.\n        if comm not in self.occupied[addr]:\n            IOLoop.current().add_callback(comm.close)\n        else:\n            self.occupied[addr].remove(comm)\n            if comm.closed():\n                # Either the user passed the close=True parameter to send_recv, or\n                # the RPC call raised OSError or CancelledError\n                self.semaphore.release()\n            else:\n                self.available[addr].add(comm)\n                if self.semaphore.locked() and self._pending_count:\n                    self.collect()\n\n    def collect(self) -> None:\n        \"\"\"\n        Collect open but unused communications, to allow opening other ones.\n        \"\"\"\n        logger.info(\n            \"Collecting unused comms.  open: %d, active: %d, connecting: %d\",\n            self.open,\n            self.active,\n            len(self._connecting),\n        )\n        for comms in self.available.values():\n            for comm in comms:\n                IOLoop.current().add_callback(comm.close)\n                self.semaphore.release()\n            comms.clear()\n\n    def remove(self, addr: str, *, reason: str = \"Address removed.\") -> None:\n        \"\"\"\n        Remove all Comms to a given address.\n        \"\"\"\n        logger.debug(\"Removing comms to %s\", addr)\n        if addr in self.available:\n            comms = self.available.pop(addr)\n            for comm in comms:\n                IOLoop.current().add_callback(comm.close)\n                self.semaphore.release()\n        if addr in self.occupied:\n            comms = self.occupied.pop(addr)\n            for comm in comms:\n                IOLoop.current().add_callback(comm.close)\n                self.semaphore.release()\n\n        if addr in self._connecting:\n            cbs = self._connecting[addr]\n            for cb in cbs:\n                cb(reason)\n\n    async def close(self) -> None:\n        \"\"\"\n        Close all communications\n        \"\"\"\n        self.status = Status.closed\n        for cbs in self._connecting.values():\n            for cb in cbs:\n                cb(\"ConnectionPool closing.\")\n        for d in [self.available, self.occupied]:\n            comms = set()\n            while d:\n                comms.update(d.popitem()[1])\n\n            await asyncio.gather(\n                *(comm.close() for comm in comms), return_exceptions=True\n            )\n\n            for _ in comms:\n                self.semaphore.release()\n\n        start = time()\n        while self._connecting:\n            if time() - start > self._connecting_close_timeout:\n                logger.warning(\n                    \"Pending connections refuse to cancel. %d connections pending. Closing anyway.\",\n                    len(self._connecting),\n                )\n                break\n            await asyncio.sleep(0.01)\n\n\ndef coerce_to_address(o):\n    if isinstance(o, (list, tuple)):\n        o = unparse_host_port(*o)\n\n    return normalize_address(o)\n\n\ndef collect_causes(e: BaseException) -> list[BaseException]:\n    causes = []\n    while e.__cause__ is not None:\n        causes.append(e.__cause__)\n        e = e.__cause__\n    return causes\n\n\nclass ErrorMessage(TypedDict):\n    status: str\n    exception: protocol.Serialize\n    traceback: protocol.Serialize | None\n    exception_text: str\n    traceback_text: str\n\n\nclass OKMessage(TypedDict):\n    status: Literal[\"OK\"]\n\n\ndef error_message(e: BaseException, status: str = \"error\") -> ErrorMessage:\n    \"\"\"Produce message to send back given an exception has occurred\n\n    This does the following:\n\n    1.  Gets the traceback\n    2.  Truncates the exception and the traceback\n    3.  Serializes the exception and traceback or\n    4.  If they can't be serialized send string versions\n    5.  Format a message and return\n\n    See Also\n    --------\n    clean_exception : deserialize and unpack message into exception/traceback\n    \"\"\"\n    MAX_ERROR_LEN = dask.config.get(\"distributed.admin.max-error-length\")\n    tblib.pickling_support.install(e, *collect_causes(e))\n    tb = get_traceback()\n    tb_text = \"\".join(traceback.format_tb(tb))\n    e = truncate_exception(e, MAX_ERROR_LEN)\n    try:\n        e_bytes = protocol.pickle.dumps(e)\n        protocol.pickle.loads(e_bytes)\n    except Exception:\n        e_bytes = protocol.pickle.dumps(Exception(repr(e)))\n    e_serialized = protocol.to_serialize(e_bytes)\n\n    try:\n        tb_bytes = protocol.pickle.dumps(tb)\n        protocol.pickle.loads(tb_bytes)\n    except Exception:\n        tb_bytes = protocol.pickle.dumps(tb_text)\n\n    if len(tb_bytes) > MAX_ERROR_LEN:\n        tb_serialized = None\n    else:\n        tb_serialized = protocol.to_serialize(tb_bytes)\n\n    return {\n        \"status\": status,\n        \"exception\": e_serialized,\n        \"traceback\": tb_serialized,\n        \"exception_text\": repr(e),\n        \"traceback_text\": tb_text,\n    }\n\n\ndef clean_exception(\n    exception: BaseException | bytes | bytearray | str | None,\n    traceback: types.TracebackType | bytes | str | None = None,\n    **kwargs: Any,\n) -> tuple[\n    type[BaseException | None], BaseException | None, types.TracebackType | None\n]:\n    \"\"\"Reraise exception and traceback. Deserialize if necessary\n\n    See Also\n    --------\n    error_message : create and serialize errors into message\n    \"\"\"\n    if isinstance(exception, (bytes, bytearray)):\n        try:\n            exception = protocol.pickle.loads(exception)\n        except Exception:\n            exception = Exception(exception)\n    elif isinstance(exception, str):\n        exception = Exception(exception)\n\n    if isinstance(traceback, bytes):\n        try:\n            traceback = protocol.pickle.loads(traceback)\n        except (TypeError, AttributeError):\n            traceback = None\n    elif isinstance(traceback, str):\n        traceback = None  # happens if the traceback failed serializing\n\n    assert isinstance(exception, BaseException) or exception is None\n    assert isinstance(traceback, types.TracebackType) or traceback is None\n    return type(exception), exception, traceback\n\n\n### Dependency File: utils.py\nfrom __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport contextvars\nimport functools\nimport importlib\nimport inspect\nimport json\nimport logging\nimport multiprocessing\nimport os\nimport pkgutil\nimport re\nimport socket\nimport sys\nimport tempfile\nimport threading\nimport traceback\nimport warnings\nimport weakref\nimport xml.etree.ElementTree\nfrom asyncio import Event as LateLoopEvent\nfrom asyncio import TimeoutError\nfrom collections import deque\nfrom collections.abc import (\n    Awaitable,\n    Callable,\n    Collection,\n    Container,\n    Generator,\n    Iterator,\n    KeysView,\n    ValuesView,\n)\nfrom concurrent.futures import (  # noqa: F401\n    CancelledError,\n    Executor,\n    ThreadPoolExecutor,\n)\nfrom contextvars import ContextVar\nfrom datetime import timedelta\nfrom functools import wraps\nfrom hashlib import md5\nfrom importlib.util import cache_from_source\nfrom pickle import PickleBuffer\nfrom time import sleep\nfrom types import ModuleType\nfrom typing import TYPE_CHECKING\nfrom typing import Any as AnyType\nfrom typing import ClassVar, TypeVar, overload\n\nimport psutil\nimport tblib.pickling_support\nfrom tornado import escape\n\nfrom distributed.compatibility import asyncio_run\nfrom distributed.config import get_loop_factory\n\ntry:\n    import resource\nexcept ImportError:\n    resource = None  # type: ignore\n\nimport tlz as toolz\nfrom tornado import gen\nfrom tornado.ioloop import IOLoop\n\nimport dask\nfrom dask.utils import ensure_bytes as _ensure_bytes\nfrom dask.utils import key_split\nfrom dask.utils import parse_timedelta as _parse_timedelta\nfrom dask.widgets import get_template\n\nfrom distributed.compatibility import WINDOWS\nfrom distributed.metrics import context_meter, monotonic, time\n\ntry:\n    from dask.context import thread_state\nexcept ImportError:\n    thread_state = threading.local()\n\n# For some reason this is required in python >= 3.9\nif WINDOWS:\n    import multiprocessing.popen_spawn_win32\nelse:\n    import multiprocessing.popen_spawn_posix\n\nlogger = _logger = logging.getLogger(__name__)\n\n\nif TYPE_CHECKING:\n    # TODO: import from typing (requires Python >=3.10)\n    from typing_extensions import ParamSpec\n\n    P = ParamSpec(\"P\")\n    T = TypeVar(\"T\")\n\n_forkserver_preload_set = False\n\n\ndef get_mp_context():\n    \"\"\"Create a multiprocessing context\n\n    The context type is controlled by the\n    ``distributed.worker.multiprocessing-method`` configuration key.\n\n    Returns\n    -------\n    multiprocessing.BaseContext\n        The multiprocessing context\n\n    Notes\n    -----\n    Repeated calls with the same method will return the same object\n    (since multiprocessing.get_context returns singleton instances).\n    \"\"\"\n    global _forkserver_preload_set\n    method = dask.config.get(\"distributed.worker.multiprocessing-method\")\n    ctx = multiprocessing.get_context(method)\n    if method == \"forkserver\" and not _forkserver_preload_set:\n        # Makes the test suite much faster\n        preload = [\"distributed\"]\n\n        from distributed.versions import optional_packages, required_packages\n\n        for pkg, _ in required_packages + optional_packages:\n            try:\n                importlib.import_module(pkg)\n            except ImportError:\n                pass\n            else:\n                preload.append(pkg)\n        ctx.set_forkserver_preload(preload)\n        _forkserver_preload_set = True\n\n    return ctx\n\n\ndef has_arg(func, argname):\n    \"\"\"\n    Whether the function takes an argument with the given name.\n    \"\"\"\n    while True:\n        try:\n            argspec = inspect.getfullargspec(func)\n            if argname in set(argspec.args) | set(argspec.kwonlyargs):\n                return True\n        except TypeError:\n            break\n        try:\n            # For Tornado coroutines and other decorated functions\n            func = func.__wrapped__\n        except AttributeError:\n            break\n    return False\n\n\ndef get_fileno_limit():\n    \"\"\"\n    Get the maximum number of open files per process.\n    \"\"\"\n    if resource is not None:\n        return resource.getrlimit(resource.RLIMIT_NOFILE)[0]\n    else:\n        # Default ceiling for Windows when using the CRT, though it\n        # is settable using _setmaxstdio().\n        return 512\n\n\n@toolz.memoize\ndef _get_ip(host, port, family):\n    def hostname_fallback():\n        addr_info = socket.getaddrinfo(\n            socket.gethostname(), port, family, socket.SOCK_DGRAM, socket.IPPROTO_UDP\n        )[0]\n        return addr_info[4][0]\n\n    # By using a UDP socket, we don't actually try to connect but\n    # simply select the local address through which *host* is reachable.\n    sock = socket.socket(family, socket.SOCK_DGRAM)\n    try:\n        sock.connect((host, port))\n        ip = sock.getsockname()[0]\n        if ip == \"0.0.0.0\":\n            return hostname_fallback()\n        return ip\n    except OSError as e:\n        warnings.warn(\n            \"Couldn't detect a suitable IP address for \"\n            \"reaching %r, defaulting to hostname: %s\" % (host, e),\n            RuntimeWarning,\n        )\n        return hostname_fallback()\n    finally:\n        sock.close()\n\n\ndef get_ip(host=\"8.8.8.8\", port=80):\n    \"\"\"\n    Get the local IP address through which the *host* is reachable.\n\n    *host* defaults to a well-known Internet host (one of Google's public\n    DNS servers).\n    \"\"\"\n    return _get_ip(host, port, family=socket.AF_INET)\n\n\ndef get_ipv6(host=\"2001:4860:4860::8888\", port=80):\n    \"\"\"\n    The same as get_ip(), but for IPv6.\n    \"\"\"\n    return _get_ip(host, port, family=socket.AF_INET6)\n\n\ndef get_ip_interface(ifname):\n    \"\"\"\n    Get the local IPv4 address of a network interface.\n\n    KeyError is raised if the interface doesn't exist.\n    ValueError is raised if the interface does no have an IPv4 address\n    associated with it.\n    \"\"\"\n    net_if_addrs = psutil.net_if_addrs()\n\n    if ifname not in net_if_addrs:\n        allowed_ifnames = list(net_if_addrs.keys())\n        raise ValueError(\n            \"{!r} is not a valid network interface. \"\n            \"Valid network interfaces are: {}\".format(ifname, allowed_ifnames)\n        )\n\n    for info in net_if_addrs[ifname]:\n        if info.family == socket.AF_INET:\n            return info.address\n    raise ValueError(f\"interface {ifname!r} doesn't have an IPv4 address\")\n\n\nasync def All(args, quiet_exceptions=()):\n    \"\"\"Wait on many tasks at the same time\n\n    Err once any of the tasks err.\n\n    See https://github.com/tornadoweb/tornado/issues/1546\n\n    Parameters\n    ----------\n    args: futures to wait for\n    quiet_exceptions: tuple, Exception\n        Exception types to avoid logging if they fail\n    \"\"\"\n    tasks = gen.WaitIterator(*map(asyncio.ensure_future, args))\n    results = [None for _ in args]\n    while not tasks.done():\n        try:\n            result = await tasks.next()\n        except Exception:\n\n            @gen.coroutine\n            def quiet():\n                \"\"\"Watch unfinished tasks\n\n                Otherwise if they err they get logged in a way that is hard to\n                control.  They need some other task to watch them so that they\n                are not orphaned\n                \"\"\"\n                for task in list(tasks._unfinished):\n                    try:\n                        yield task\n                    except quiet_exceptions:\n                        pass\n\n            quiet()\n            raise\n        results[tasks.current_index] = result\n    return results\n\n\nasync def Any(args, quiet_exceptions=()):\n    \"\"\"Wait on many tasks at the same time and return when any is finished\n\n    Err once any of the tasks err.\n\n    Parameters\n    ----------\n    args: futures to wait for\n    quiet_exceptions: tuple, Exception\n        Exception types to avoid logging if they fail\n    \"\"\"\n    tasks = gen.WaitIterator(*map(asyncio.ensure_future, args))\n    results = [None for _ in args]\n    while not tasks.done():\n        try:\n            result = await tasks.next()\n        except Exception:\n\n            @gen.coroutine\n            def quiet():\n                \"\"\"Watch unfinished tasks\n\n                Otherwise if they err they get logged in a way that is hard to\n                control.  They need some other task to watch them so that they\n                are not orphaned\n                \"\"\"\n                for task in list(tasks._unfinished):\n                    try:\n                        yield task\n                    except quiet_exceptions:\n                        pass\n\n            quiet()\n            raise\n\n        results[tasks.current_index] = result\n        break\n    return results\n\n\nclass NoOpAwaitable:\n    \"\"\"An awaitable object that always returns None.\n\n    Useful to return from a method that can be called in both asynchronous and\n    synchronous contexts\"\"\"\n\n    def __await__(self):\n        async def f():\n            return None\n\n        return f().__await__()\n\n\nclass SyncMethodMixin:\n    \"\"\"\n    A mixin for adding an `asynchronous` attribute and `sync` method to a class.\n\n    Subclasses must define a `loop` attribute for an associated\n    `tornado.IOLoop`, and may also add a `_asynchronous` attribute indicating\n    whether the class should default to asynchronous behavior.\n    \"\"\"\n\n    @property\n    def asynchronous(self):\n        \"\"\"Are we running in the event loop?\"\"\"\n        try:\n            return in_async_call(\n                self.loop, default=getattr(self, \"_asynchronous\", False)\n            )\n        except RuntimeError:\n            return False\n\n    def sync(self, func, *args, asynchronous=None, callback_timeout=None, **kwargs):\n        \"\"\"Call `func` with `args` synchronously or asynchronously depending on\n        the calling context\"\"\"\n        callback_timeout = _parse_timedelta(callback_timeout)\n        if asynchronous is None:\n            asynchronous = self.asynchronous\n        if asynchronous:\n            future = func(*args, **kwargs)\n            if callback_timeout is not None:\n                future = wait_for(future, callback_timeout)\n            return future\n        else:\n            return sync(\n                self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\n            )\n\n\ndef in_async_call(loop, default=False):\n    \"\"\"Whether this call is currently within an async call\"\"\"\n    try:\n        return loop.asyncio_loop is asyncio.get_running_loop()\n    except RuntimeError:\n        # No *running* loop in thread. If the event loop isn't running, it\n        # _could_ be started later in this thread though. Return the default.\n        if not loop.asyncio_loop.is_running():\n            return default\n        return False\n\n\ndef sync(\n    loop: IOLoop,\n    func: Callable[..., Awaitable[T]],\n    *args: AnyType,\n    callback_timeout: str | float | timedelta | None = None,\n    **kwargs: AnyType,\n) -> T:\n    \"\"\"\n    Run coroutine in loop running in separate thread.\n    \"\"\"\n    timeout = _parse_timedelta(callback_timeout, \"s\")\n    if loop.asyncio_loop.is_closed():  # type: ignore[attr-defined]\n        raise RuntimeError(\"IOLoop is closed\")\n\n    e = threading.Event()\n    main_tid = threading.get_ident()\n\n    # set up non-locals\n    result: T\n    error: BaseException | None = None\n    future: asyncio.Future[T]\n\n    @gen.coroutine\n    def f() -> Generator[AnyType, AnyType, None]:\n        nonlocal result, error, future\n        try:\n            if main_tid == threading.get_ident():\n                raise RuntimeError(\"sync() called from thread of running loop\")\n            yield gen.moment\n            awaitable = func(*args, **kwargs)\n            if timeout is not None:\n                awaitable = wait_for(awaitable, timeout)\n            future = asyncio.ensure_future(awaitable)\n            result = yield future\n        except Exception as exception:\n            error = exception\n        finally:\n            e.set()\n\n    def cancel() -> None:\n        if future is not None:\n            future.cancel()\n\n    def wait(timeout: float | None) -> bool:\n        try:\n            return e.wait(timeout)\n        except KeyboardInterrupt:\n            loop.add_callback(cancel)\n            raise\n\n    loop.add_callback(f)\n    if timeout is not None:\n        if not wait(timeout):\n            raise TimeoutError(f\"timed out after {timeout} s.\")\n    else:\n        while not e.is_set():\n            wait(10)\n\n    if error is not None:\n        raise error\n    else:\n        return result\n\n\nclass _CollectErrorThread:\n    def __init__(self, target: Callable[[], None], daemon: bool, name: str):\n        self._exception: BaseException | None = None\n\n        def wrapper() -> None:\n            try:\n                target()\n            except BaseException as e:  # noqa: B036\n                self._exception = e\n\n        self._thread = thread = threading.Thread(\n            target=wrapper, daemon=daemon, name=name\n        )\n        thread.start()\n\n    def join(self, timeout: float | None = None) -> None:\n        thread = self._thread\n        thread.join(timeout=timeout)\n        if thread.is_alive():\n            raise TimeoutError(\"join timed out\")\n        if self._exception is not None:\n            try:\n                raise self._exception\n            finally:  # remove a reference cycle\n                del self._exception\n\n\nclass LoopRunner:\n    \"\"\"\n    A helper to start and stop an IO loop in a controlled way.\n    Several loop runners can associate safely to the same IO loop.\n\n    Parameters\n    ----------\n    loop: IOLoop (optional)\n        If given, this loop will be re-used, otherwise an appropriate one\n        will be looked up or created.\n    asynchronous: boolean (optional, default False)\n        If false (the default), the loop is meant to run in a separate\n        thread and will be started if necessary.\n        If true, the loop is meant to run in the thread this\n        object is instantiated from, and will not be started automatically.\n    \"\"\"\n\n    # All loops currently associated to loop runners\n    _all_loops: ClassVar[\n        weakref.WeakKeyDictionary[IOLoop, tuple[int, LoopRunner | None]]\n    ] = weakref.WeakKeyDictionary()\n    _lock = threading.Lock()\n    _loop_thread: _CollectErrorThread | None\n\n    def __init__(self, loop: IOLoop | None = None, asynchronous: bool = False):\n        if loop is None:\n            if asynchronous:\n                # raises RuntimeError if there's no running loop\n                try:\n                    asyncio.get_running_loop()\n                except RuntimeError as e:\n                    raise RuntimeError(\n                        \"Constructing LoopRunner(asynchronous=True) without a running loop is not supported\"\n                    ) from e\n                loop = IOLoop.current()\n        elif not loop.asyncio_loop.is_running():  # type: ignore[attr-defined]\n            # LoopRunner is not responsible for starting a foreign IOLoop\n            raise RuntimeError(\n                \"Constructing LoopRunner(loop=loop) without a running loop is not supported\"\n            )\n\n        self._loop = loop\n        self._asynchronous = asynchronous\n        self._loop_thread = None\n        self._started = False\n        self._stop_event = LateLoopEvent()\n\n    def start(self):\n        \"\"\"\n        Start the IO loop if required.  The loop is run in a dedicated\n        thread.\n\n        If the loop is already running, this method does nothing.\n        \"\"\"\n        with self._lock:\n            self._start_unlocked()\n\n    def _start_unlocked(self) -> None:\n        assert not self._started\n\n        if self._loop is not None:\n            try:\n                count, real_runner = self._all_loops[self._loop]\n            except KeyError:\n                assert self._loop.asyncio_loop.is_running()  # type: ignore[attr-defined]\n                self._started = True\n                return\n\n            self._all_loops[self._loop] = count + 1, real_runner\n            self._started = True\n            return\n\n        assert self._loop_thread is None\n\n        start_evt = threading.Event()\n        start_exc = None\n        loop = None\n\n        async def amain() -> None:\n            nonlocal loop\n            loop = IOLoop.current()\n            start_evt.set()\n            await self._stop_event.wait()\n\n        def run_loop() -> None:\n            nonlocal start_exc\n            try:\n                asyncio_run(amain(), loop_factory=get_loop_factory())\n            except BaseException as e:\n                if start_evt.is_set():\n                    raise\n                start_exc = e\n                start_evt.set()\n\n        self._loop_thread = _CollectErrorThread(\n            target=run_loop, daemon=True, name=\"IO loop\"\n        )\n        start_evt.wait(timeout=10)\n        if start_exc is not None:\n            raise start_exc\n        assert loop is not None\n        self._loop = loop\n        self._started = True\n        self._all_loops[loop] = (1, self)\n\n    def stop(self, timeout=10):\n        \"\"\"\n        Stop and close the loop if it was created by us.\n        Otherwise, just mark this object \"stopped\".\n        \"\"\"\n        with self._lock:\n            self._stop_unlocked(timeout)\n\n    def _stop_unlocked(self, timeout):\n        if not self._started:\n            return\n\n        self._started = False\n\n        try:\n            count, real_runner = self._all_loops[self._loop]\n        except KeyError:\n            return\n\n        if count > 1:\n            self._all_loops[self._loop] = count - 1, real_runner\n            return\n\n        assert count == 1\n        del self._all_loops[self._loop]\n        real_runner._real_stop(timeout)\n\n    def _real_stop(self, timeout):\n        assert self._loop_thread is not None\n        try:\n            self._loop.add_callback(self._stop_event.set)\n            self._loop_thread.join(timeout=timeout)\n        finally:\n            self._loop_thread = None\n\n    def is_started(self):\n        \"\"\"\n        Return True between start() and stop() calls, False otherwise.\n        \"\"\"\n        return self._started\n\n    def run_sync(self, func, *args, **kwargs):\n        \"\"\"\n        Convenience helper: start the loop if needed,\n        run sync(func, *args, **kwargs), then stop the loop again.\n        \"\"\"\n        if self._started:\n            return sync(self.loop, func, *args, **kwargs)\n        else:\n            self.start()\n            try:\n                return sync(self.loop, func, *args, **kwargs)\n            finally:\n                self.stop()\n\n    @property\n    def loop(self):\n        loop = self._loop\n        if loop is None or not loop.asyncio_loop.is_running():\n            raise RuntimeError(\n                \"Accessing the loop property while the loop is not running is not supported\"\n            )\n        return self._loop\n\n\n@contextlib.contextmanager\ndef set_thread_state(**kwargs):\n    old = {}\n    for k in kwargs:\n        try:\n            old[k] = getattr(thread_state, k)\n        except AttributeError:\n            pass\n    for k, v in kwargs.items():\n        setattr(thread_state, k, v)\n    try:\n        yield\n    finally:\n        for k in kwargs:\n            try:\n                v = old[k]\n            except KeyError:\n                delattr(thread_state, k)\n            else:\n                setattr(thread_state, k, v)\n\n\n@contextlib.contextmanager\ndef tmp_text(filename, text):\n    fn = os.path.join(tempfile.gettempdir(), filename)\n    with open(fn, \"w\") as f:\n        f.write(text)\n\n    try:\n        yield fn\n    finally:\n        if os.path.exists(fn):\n            os.remove(fn)\n\n\ndef is_kernel():\n    \"\"\"Determine if we're running within an IPython kernel\n\n    >>> is_kernel()\n    False\n    \"\"\"\n    # http://stackoverflow.com/questions/34091701/determine-if-were-in-an-ipython-notebook-session\n    if \"IPython\" not in sys.modules:  # IPython hasn't been imported\n        return False\n    from IPython import get_ipython\n\n    # check for `kernel` attribute on the IPython instance\n    return getattr(get_ipython(), \"kernel\", None) is not None\n\n\ndef key_split_group(x: object) -> str:\n    \"\"\"A more fine-grained version of key_split.\n\n    >>> key_split_group(('x-2', 1))\n    'x-2'\n    >>> key_split_group(\"('x-2', 1)\")\n    'x-2'\n    >>> key_split_group('ae05086432ca935f6eba409a8ecd4896')\n    'data'\n    >>> key_split_group('<module.submodule.myclass object at 0xdaf372')\n    'myclass'\n    >>> key_split_group('x')\n    'x'\n    >>> key_split_group('x-1')\n    'x'\n    \"\"\"\n    if isinstance(x, tuple):\n        return x[0]\n    elif isinstance(x, str):\n        if x[0] == \"(\":\n            return x.split(\",\", 1)[0].strip(\"()\\\"'\")\n        elif len(x) == 32 and re.match(r\"[a-f0-9]{32}\", x):\n            return \"data\"\n        elif x[0] == \"<\":\n            return x.strip(\"<>\").split()[0].split(\".\")[-1]\n        else:\n            return key_split(x)\n    elif isinstance(x, bytes):\n        return key_split_group(x.decode())\n    else:\n        return \"Other\"\n\n\n@overload\ndef log_errors(func: Callable[P, T], /) -> Callable[P, T]: ...\n\n\n@overload\ndef log_errors(*, pdb: bool = False, unroll_stack: int = 1) -> _LogErrors: ...\n\n\ndef log_errors(func=None, /, *, pdb=False, unroll_stack=0):\n    \"\"\"Log any errors and then reraise them.\n\n    This can be used:\n\n    - As a context manager::\n\n        with log_errors(...):\n            ...\n\n    - As a bare function decorator::\n\n        @log_errors\n        def func(...):\n            ...\n\n    - As a function decorator with parameters::\n\n        @log_errors(...)\n        def func(...):\n            ...\n\n    Parameters\n    ----------\n    pdb: bool, optional\n        Set to True to break into the debugger in case of exception\n    unroll_stack: int, optional\n        Number of levels of stack to unroll when determining the module's name for the\n        purpose of logging. Normally you should omit this. Set to 1 if you are writing a\n        helper function, context manager, or decorator.\n    \"\"\"\n    le = _LogErrors(pdb=pdb, unroll_stack=unroll_stack)\n    return le(func) if func else le\n\n\n_getmodulename_with_path_map: dict[str, str] = {}\n\n\ndef _getmodulename_with_path(fname: str) -> str:\n    \"\"\"Variant of inspect.getmodulename that returns the full module path\"\"\"\n    try:\n        return _getmodulename_with_path_map[fname]\n    except KeyError:\n        pass\n\n    for modname, mod in sys.modules.copy().items():\n        fname2 = getattr(mod, \"__file__\", None)\n        if fname2:\n            _getmodulename_with_path_map[fname2] = modname\n\n    try:\n        return _getmodulename_with_path_map[fname]\n    except KeyError:  # pragma: nocover\n        return os.path.splitext(os.path.basename(fname))[0]\n\n\nclass _LogErrors:\n    __slots__ = (\"pdb\", \"unroll_stack\")\n\n    pdb: bool\n    unroll_stack: int\n\n    def __init__(self, pdb: bool, unroll_stack: int):\n        self.pdb = pdb\n        self.unroll_stack = unroll_stack\n\n    def __call__(self, func: Callable[P, T], /) -> Callable[P, T]:\n        self.unroll_stack += 1\n\n        if inspect.iscoroutinefunction(func):\n\n            async def wrapper(*args, **kwargs):\n                with self:\n                    return await func(*args, **kwargs)\n\n        else:\n\n            def wrapper(*args, **kwargs):\n                with self:\n                    return func(*args, **kwargs)\n\n        return wraps(func)(wrapper)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, tb):\n        from distributed.comm import CommClosedError\n\n        if not exc_type or issubclass(exc_type, (CommClosedError, gen.Return)):\n            return\n\n        stack = traceback.extract_tb(tb)\n        frame = stack[min(self.unroll_stack, len(stack) - 1)]\n        modname = _getmodulename_with_path(frame.filename)\n\n        try:\n            logger = logging.getLogger(modname)\n            logger.exception(exc_value)\n        except Exception:  # Interpreter teardown\n            pass  # pragma: nocover\n\n        if self.pdb:\n            import pdb  # pragma: nocover\n\n            pdb.set_trace()  # pragma: nocover\n\n\ndef silence_logging(level, root=\"distributed\"):\n    \"\"\"\n    Change all StreamHandlers for the given logger to the given level\n    \"\"\"\n    warnings.warn(\n        \"silence_logging is deprecated, call silence_logging_cmgr\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if isinstance(level, str):\n        level = getattr(logging, level.upper())\n\n    old = None\n    logger = logging.getLogger(root)\n    for handler in logger.handlers:\n        if isinstance(handler, logging.StreamHandler):\n            old = handler.level\n            handler.setLevel(level)\n\n    return old\n\n\n@contextlib.contextmanager\ndef silence_logging_cmgr(\n    level: str | int, root: str = \"distributed\"\n) -> Generator[None]:\n    \"\"\"\n    Temporarily change all StreamHandlers for the given logger to the given level\n    \"\"\"\n    if isinstance(level, str):\n        level = getattr(logging, level.upper())\n\n    logger = logging.getLogger(root)\n    with contextlib.ExitStack() as stack:\n        for handler in logger.handlers:\n            if isinstance(handler, logging.StreamHandler):\n                old = handler.level\n                if old != level:\n                    handler.setLevel(level)\n                    stack.callback(handler.setLevel, old)\n        yield\n\n\n@toolz.memoize\ndef ensure_ip(hostname):\n    \"\"\"Ensure that address is an IP address\n\n    Examples\n    --------\n    >>> ensure_ip('localhost')\n    '127.0.0.1'\n    >>> ensure_ip('')  # Maps as localhost for binding e.g. 'tcp://:8811'\n    '127.0.0.1'\n    >>> ensure_ip('123.123.123.123')  # pass through IP addresses\n    '123.123.123.123'\n    \"\"\"\n    if not hostname:\n        hostname = \"localhost\"\n\n    # Prefer IPv4 over IPv6, for compatibility\n    families = [socket.AF_INET, socket.AF_INET6]\n    for fam in families:\n        try:\n            results = socket.getaddrinfo(\n                hostname, 1234, fam, socket.SOCK_STREAM  # dummy port number\n            )\n        except socket.gaierror as e:\n            exc = e\n        else:\n            return results[0][4][0]\n\n    raise exc\n\n\ntblib.pickling_support.install()\n\n\ndef get_traceback():\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    bad = [\n        os.path.join(\"distributed\", \"worker\"),\n        os.path.join(\"distributed\", \"scheduler\"),\n        os.path.join(\"tornado\", \"gen.py\"),\n        os.path.join(\"concurrent\", \"futures\"),\n        os.path.join(\"dask\", \"_task_spec\"),\n    ]\n    while exc_traceback and any(\n        b in exc_traceback.tb_frame.f_code.co_filename for b in bad\n    ):\n        exc_traceback = exc_traceback.tb_next\n    return exc_traceback\n\n\ndef truncate_exception(e, n=10000):\n    \"\"\"Truncate exception to be about a certain length\"\"\"\n    if len(str(e)) > n:\n        try:\n            return type(e)(\"Long error message\", str(e)[:n])\n        except Exception:\n            return Exception(\"Long error message\", type(e), str(e)[:n])\n    else:\n        return e\n\n\ndef seek_delimiter(file, delimiter, blocksize):\n    \"\"\"Seek current file to next byte after a delimiter bytestring\n\n    This seeks the file to the next byte following the delimiter.  It does\n    not return anything.  Use ``file.tell()`` to see location afterwards.\n\n    Parameters\n    ----------\n    file: a file\n    delimiter: bytes\n        a delimiter like ``b'\\n'`` or message sentinel\n    blocksize: int\n        Number of bytes to read from the file at once.\n    \"\"\"\n\n    if file.tell() == 0:\n        return\n\n    last = b\"\"\n    while True:\n        current = file.read(blocksize)\n        if not current:\n            return\n        full = last + current\n        try:\n            i = full.index(delimiter)\n            file.seek(file.tell() - (len(full) - i) + len(delimiter))\n            return\n        except ValueError:\n            pass\n        last = full[-len(delimiter) :]\n\n\ndef read_block(f, offset, length, delimiter=None):\n    \"\"\"Read a block of bytes from a file\n\n    Parameters\n    ----------\n    f: file\n        File-like object supporting seek, read, tell, etc..\n    offset: int\n        Byte offset to start read\n    length: int\n        Number of bytes to read\n    delimiter: bytes (optional)\n        Ensure reading starts and stops at delimiter bytestring\n\n    If using the ``delimiter=`` keyword argument we ensure that the read\n    starts and stops at delimiter boundaries that follow the locations\n    ``offset`` and ``offset + length``.  If ``offset`` is zero then we\n    start at zero.  The bytestring returned WILL include the\n    terminating delimiter string.\n\n    Examples\n    --------\n\n    >>> from io import BytesIO  # doctest: +SKIP\n    >>> f = BytesIO(b'Alice, 100\\\\nBob, 200\\\\nCharlie, 300')  # doctest: +SKIP\n    >>> read_block(f, 0, 13)  # doctest: +SKIP\n    b'Alice, 100\\\\nBo'\n\n    >>> read_block(f, 0, 13, delimiter=b'\\\\n')  # doctest: +SKIP\n    b'Alice, 100\\\\nBob, 200\\\\n'\n\n    >>> read_block(f, 10, 10, delimiter=b'\\\\n')  # doctest: +SKIP\n    b'Bob, 200\\\\nCharlie, 300'\n    \"\"\"\n    if delimiter:\n        f.seek(offset)\n        seek_delimiter(f, delimiter, 2**16)\n        start = f.tell()\n        length -= start - offset\n\n        f.seek(start + length)\n        seek_delimiter(f, delimiter, 2**16)\n        end = f.tell()\n\n        offset = start\n        length = end - start\n\n    f.seek(offset)\n    bytes = f.read(length)\n    return bytes\n\n\ndef ensure_bytes(s):\n    \"\"\"Attempt to turn `s` into bytes.\n\n    Parameters\n    ----------\n    s : Any\n        The object to be converted. Will correctly handled\n\n        * str\n        * bytes\n        * objects implementing the buffer protocol (memoryview, ndarray, etc.)\n\n    Returns\n    -------\n    b : bytes\n\n    Raises\n    ------\n    TypeError\n        When `s` cannot be converted\n\n    Examples\n    --------\n    >>> ensure_bytes('123')\n    b'123'\n    >>> ensure_bytes(b'123')\n    b'123'\n    \"\"\"\n    warnings.warn(\n        \"`distributed.utils.ensure_bytes` is deprecated. \"\n        \"Please switch to `dask.utils.ensure_bytes`. \"\n        \"This will be removed in `2022.6.0`.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return _ensure_bytes(s)\n\n\ndef ensure_memoryview(obj: bytes | bytearray | memoryview | PickleBuffer) -> memoryview:\n    \"\"\"Ensure `obj` is a 1-D contiguous `uint8` `memoryview`\"\"\"\n    if not isinstance(obj, memoryview):\n        obj = memoryview(obj)\n\n    if not obj.nbytes:\n        # Drop `obj` reference to permit freeing underlying data\n        return memoryview(bytearray())\n    elif not obj.contiguous:\n        # Copy to contiguous form of expected shape & type\n        return memoryview(bytearray(obj))\n    elif obj.ndim != 1 or obj.format != \"B\":\n        # Perform zero-copy reshape & cast\n        # Use `PickleBuffer.raw()` as `memoryview.cast()` fails with F-order\n        # xref: https://github.com/python/cpython/issues/91484\n        return PickleBuffer(obj).raw()\n    else:\n        # Return `memoryview` as it already meets requirements\n        return obj\n\n\ndef open_port(host: str = \"\") -> int:\n    \"\"\"Return a probably-open port\n\n    There is a chance that this port will be taken by the operating system soon\n    after returning from this function.\n    \"\"\"\n    # http://stackoverflow.com/questions/2838244/get-open-tcp-port-in-python\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((host, 0))\n        s.listen(1)\n        port = s.getsockname()[1]\n        return port\n\n\ndef import_file(path: str) -> list[ModuleType]:\n    \"\"\"Loads modules for a file (.py, .zip, .egg)\"\"\"\n    directory, filename = os.path.split(path)\n    name, ext = os.path.splitext(filename)\n    names_to_import: list[str] = []\n    tmp_python_path: str | None = None\n\n    if ext in (\".py\",):  # , '.pyc'):\n        if directory not in sys.path:\n            tmp_python_path = directory\n        names_to_import.append(name)\n    if ext == \".py\":  # Ensure that no pyc file will be reused\n        cache_file = cache_from_source(path)\n        with contextlib.suppress(OSError):\n            os.remove(cache_file)\n    if ext in (\".egg\", \".zip\", \".pyz\"):\n        if path not in sys.path:\n            sys.path.insert(0, path)\n        names = (mod_info.name for mod_info in pkgutil.iter_modules([path]))\n        names_to_import.extend(names)\n\n    loaded: list[ModuleType] = []\n    if not names_to_import:\n        logger.warning(\"Found nothing to import from %s\", filename)\n    else:\n        importlib.invalidate_caches()\n        if tmp_python_path is not None:\n            sys.path.insert(0, tmp_python_path)\n        try:\n            for name in names_to_import:\n                logger.info(\"Reload module %s from %s file\", name, ext)\n                loaded.append(importlib.reload(importlib.import_module(name)))\n        finally:\n            if tmp_python_path is not None:\n                sys.path.remove(tmp_python_path)\n    return loaded\n\n\ndef asciitable(columns, rows):\n    \"\"\"Formats an ascii table for given columns and rows.\n\n    Parameters\n    ----------\n    columns : list\n        The column names\n    rows : list of tuples\n        The rows in the table. Each tuple must be the same length as\n        ``columns``.\n    \"\"\"\n    rows = [tuple(str(i) for i in r) for r in rows]\n    columns = tuple(str(i) for i in columns)\n    widths = tuple(max(max(map(len, x)), len(c)) for x, c in zip(zip(*rows), columns))\n    row_template = (\"|\" + (\" %%-%ds |\" * len(columns))) % widths\n    header = row_template % tuple(columns)\n    bar = \"+%s+\" % \"+\".join(\"-\" * (w + 2) for w in widths)\n    data = \"\\n\".join(row_template % r for r in rows)\n    return \"\\n\".join([bar, header, bar, data, bar])\n\n\ndef nbytes(frame, _bytes_like=(bytes, bytearray)):\n    \"\"\"Number of bytes of a frame or memoryview\"\"\"\n    if isinstance(frame, _bytes_like):\n        return len(frame)\n    else:\n        try:\n            return frame.nbytes\n        except AttributeError:\n            return len(frame)\n\n\ndef json_load_robust(fn, load=json.load, timeout=None):\n    \"\"\"Reads a JSON file from disk that may be being written as we read\"\"\"\n    deadline = Deadline.after(timeout)\n    while not deadline.expires or deadline.remaining:\n        if os.path.exists(fn):\n            try:\n                with open(fn) as f:\n                    cfg = load(f)\n                if cfg:\n                    return cfg\n            except (ValueError, KeyError):  # race with writing process\n                pass\n        sleep(0.1)\n    else:\n        raise TimeoutError(f\"Could not load file after {timeout}s.\")\n\n\nclass DequeHandler(logging.Handler):\n    \"\"\"A logging.Handler that records records into a deque\"\"\"\n\n    _instances: ClassVar[weakref.WeakSet[DequeHandler]] = weakref.WeakSet()\n\n    def __init__(self, *args, n=10000, **kwargs):\n        self.deque = deque(maxlen=n)\n        super().__init__(*args, **kwargs)\n        self._instances.add(self)\n\n    def emit(self, record):\n        self.deque.append(record)\n\n    def clear(self):\n        \"\"\"\n        Clear internal storage.\n        \"\"\"\n        self.deque.clear()\n\n    @classmethod\n    def clear_all_instances(cls):\n        \"\"\"\n        Clear the internal storage of all live DequeHandlers.\n        \"\"\"\n        for inst in list(cls._instances):\n            inst.clear()\n\n\ndef reset_logger_locks():\n    \"\"\"Python 2's logger's locks don't survive a fork event\n\n    https://github.com/dask/distributed/issues/1491\n    \"\"\"\n    for name in logging.Logger.manager.loggerDict.keys():\n        for handler in logging.getLogger(name).handlers:\n            handler.createLock()\n\n\n@functools.lru_cache(1000)\ndef has_keyword(func, keyword):\n    return keyword in inspect.signature(func).parameters\n\n\n@functools.lru_cache(1000)\ndef command_has_keyword(cmd, k):\n    # Click is a relatively expensive import\n    # That hurts startup time a little\n    import click\n\n    if cmd is not None:\n        if isinstance(cmd, str):\n            try:\n                from importlib import import_module\n\n                cmd = import_module(cmd)\n            except ImportError:\n                raise ImportError(\"Module for command %s is not available\" % cmd)\n\n        if isinstance(cmd.main, click.core.Command):\n            cmd = cmd.main\n        if isinstance(cmd, click.core.Command):\n            cmd_params = {\n                p.human_readable_name\n                for p in cmd.params\n                if isinstance(p, click.core.Option)\n            }\n            return k in cmd_params\n\n    return False\n\n\n# from bokeh.palettes import viridis\n# palette = viridis(18)\npalette = [\n    \"#440154\",\n    \"#471669\",\n    \"#472A79\",\n    \"#433C84\",\n    \"#3C4D8A\",\n    \"#355D8C\",\n    \"#2E6C8E\",\n    \"#287A8E\",\n    \"#23898D\",\n    \"#1E978A\",\n    \"#20A585\",\n    \"#2EB27C\",\n    \"#45BF6F\",\n    \"#64CB5D\",\n    \"#88D547\",\n    \"#AFDC2E\",\n    \"#D7E219\",\n    \"#FDE724\",\n]\n\n\n@toolz.memoize\ndef color_of(x, palette=palette):\n    h = md5(str(x).encode(), usedforsecurity=False)\n    n = int(h.hexdigest()[:8], 16)\n    return palette[n % len(palette)]\n\n\ndef iscoroutinefunction(f):\n    return inspect.iscoroutinefunction(f) or gen.is_coroutine_function(f)\n\n\n@contextlib.contextmanager\ndef warn_on_duration(duration: str | float | timedelta, msg: str) -> Iterator[None]:\n    \"\"\"Generate a UserWarning if the operation in this context takes longer than\n    *duration* and print *msg*\n\n    The message may include a format string `{duration}` which will be formatted\n    to include the actual duration it took\n    \"\"\"\n    start = time()\n    yield\n    stop = time()\n    diff = stop - start\n    if diff > _parse_timedelta(duration):\n        warnings.warn(msg.format(duration=diff), stacklevel=2)\n\n\ndef format_dashboard_link(host, port):\n    template = dask.config.get(\"distributed.dashboard.link\")\n    if dask.config.get(\"distributed.scheduler.dashboard.tls.cert\"):\n        scheme = \"https\"\n    else:\n        scheme = \"http\"\n    return template.format(\n        **toolz.merge(os.environ, dict(scheme=scheme, host=host, port=port))\n    )\n\n\ndef parse_ports(port: int | str | Collection[int] | None) -> list[int] | list[None]:\n    \"\"\"Parse input port information into list of ports\n\n    Parameters\n    ----------\n    port : int, str, list[int], None\n        Input port or ports. Can be an integer like 8787, a string for a\n        single port like \"8787\", string for a sequential range of ports like\n        \"8000:8200\", a collection of ints, or None.\n\n    Returns\n    -------\n    ports : list\n        List of ports\n\n    Examples\n    --------\n    A single port can be specified using an integer:\n\n    >>> parse_ports(8787)\n    [8787]\n\n    or a string:\n\n    >>> parse_ports(\"8787\")\n    [8787]\n\n    A sequential range of ports can be specified by a string which indicates\n    the first and last ports which should be included in the sequence of ports:\n\n    >>> parse_ports(\"8787:8790\")\n    [8787, 8788, 8789, 8790]\n\n    An input of ``None`` is also valid and can be used to indicate that no port\n    has been specified:\n\n    >>> parse_ports(None)\n    [None]\n\n    \"\"\"\n    if isinstance(port, str) and \":\" in port:\n        port_start, port_stop = map(int, port.split(\":\"))\n        if port_stop <= port_start:\n            raise ValueError(\n                \"When specifying a range of ports like port_start:port_stop, \"\n                \"port_stop must be greater than port_start, but got \"\n                f\"{port_start=} and {port_stop=}\"\n            )\n        return list(range(port_start, port_stop + 1))\n\n    if isinstance(port, str):\n        return [int(port)]\n\n    if isinstance(port, int) or port is None:\n        return [port]  # type: ignore\n\n    if isinstance(port, Collection):\n        if not all(isinstance(p, int) for p in port):\n            raise TypeError(port)\n        return list(port)  # type: ignore\n\n    raise TypeError(port)\n\n\nis_coroutine_function = iscoroutinefunction\n\n\nclass Log(str):\n    \"\"\"A container for newline-delimited string of log entries\"\"\"\n\n    def _repr_html_(self):\n        return get_template(\"log.html.j2\").render(log=self)\n\n\nclass Logs(dict):\n    \"\"\"A container for a dict mapping names to strings of log entries\"\"\"\n\n    def _repr_html_(self):\n        return get_template(\"logs.html.j2\").render(logs=self)\n\n\ndef cli_keywords(\n    d: dict[str, AnyType],\n    cls: Callable | None = None,\n    cmd: str | ModuleType | None = None,\n) -> list[str]:\n    \"\"\"Convert a kwargs dictionary into a list of CLI keywords\n\n    Parameters\n    ----------\n    d : dict\n        The keywords to convert\n    cls : callable\n        The callable that consumes these terms to check them for validity\n    cmd : string or object\n        A string with the name of a module, or the module containing a\n        click-generated command with a \"main\" function, or the function itself.\n        It may be used to parse a module's custom arguments (that is, arguments that\n        are not part of Worker class), such as nworkers from dask worker CLI or\n        enable_nvlink from dask-cuda-worker CLI.\n\n    Examples\n    --------\n    >>> cli_keywords({\"x\": 123, \"save_file\": \"foo.txt\"})\n    ['--x', '123', '--save-file', 'foo.txt']\n\n    >>> from dask.distributed import Worker\n    >>> cli_keywords({\"x\": 123}, Worker)\n    Traceback (most recent call last):\n    ...\n    ValueError: Class distributed.worker.Worker does not support keyword x\n    \"\"\"\n    from dask.utils import typename\n\n    if cls or cmd:\n        for k in d:\n            if not has_keyword(cls, k) and not command_has_keyword(cmd, k):\n                if cls and cmd:\n                    raise ValueError(\n                        \"Neither class %s or module %s support keyword %s\"\n                        % (typename(cls), typename(cmd), k)\n                    )\n                elif cls:\n                    raise ValueError(\n                        f\"Class {typename(cls)} does not support keyword {k}\"\n                    )\n                else:\n                    raise ValueError(\n                        f\"Module {typename(cmd)} does not support keyword {k}\"\n                    )\n\n    def convert_value(v):\n        out = str(v)\n        if \" \" in out and \"'\" not in out and '\"' not in out:\n            out = '\"' + out + '\"'\n        return out\n\n    return sum(\n        ([\"--\" + k.replace(\"_\", \"-\"), convert_value(v)] for k, v in d.items()), []\n    )\n\n\ndef is_valid_xml(text):\n    return xml.etree.ElementTree.fromstring(text) is not None\n\n\n_offload_executor = ThreadPoolExecutor(max_workers=1, thread_name_prefix=\"Dask-Offload\")\n\n\ndef import_term(name: str) -> AnyType:\n    \"\"\"Return the fully qualified term\n\n    Examples\n    --------\n    >>> import_term(\"math.sin\") # doctest: +SKIP\n    <function math.sin(x, /)>\n    \"\"\"\n    try:\n        module_name, attr_name = name.rsplit(\".\", 1)\n    except ValueError:\n        return importlib.import_module(name)\n\n    module = importlib.import_module(module_name)\n    return getattr(module, attr_name)\n\n\nasync def run_in_executor_with_context(\n    executor: Executor | None,\n    func: Callable[P, T],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -> T:\n    \"\"\"Variant of :meth:`~asyncio.AbstractEventLoop.run_in_executor`, which\n    propagates contextvars.\n    Note that this limits the type of Executor to those that do not pickle objects.\n\n    See also\n    --------\n    asyncio.AbstractEventLoop.run_in_executor\n    offload\n    https://bugs.python.org/issue34014\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    context = contextvars.copy_context()\n    return await loop.run_in_executor(\n        executor, lambda: context.run(func, *args, **kwargs)\n    )\n\n\ndef offload(\n    func: Callable[P, T],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -> Awaitable[T]:\n    \"\"\"Run a synchronous function in a separate thread.\n    Unlike :meth:`asyncio.to_thread`, this propagates contextvars and offloads to an\n    ad-hoc thread pool with a single worker.\n\n    See also\n    --------\n    asyncio.to_thread\n    run_in_executor_with_context\n    https://bugs.python.org/issue34014\n    \"\"\"\n    with context_meter.meter(\"offload\"):\n        return run_in_executor_with_context(_offload_executor, func, *args, **kwargs)\n\n\nclass EmptyContext:\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n\n    async def __aenter__(self):\n        pass\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        pass\n\n\nempty_context = EmptyContext()\n\n\ndef clean_dashboard_address(addrs: AnyType, default_listen_ip: str = \"\") -> list[dict]:\n    \"\"\"\n    Examples\n    --------\n    >>> clean_dashboard_address(8787)\n    [{'address': '', 'port': 8787}]\n    >>> clean_dashboard_address(\":8787\")\n    [{'address': '', 'port': 8787}]\n    >>> clean_dashboard_address(\"8787\")\n    [{'address': '', 'port': 8787}]\n    >>> clean_dashboard_address(\"8787\")\n    [{'address': '', 'port': 8787}]\n    >>> clean_dashboard_address(\"foo:8787\")\n    [{'address': 'foo', 'port': 8787}]\n    >>> clean_dashboard_address([8787, 8887])\n    [{'address': '', 'port': 8787}, {'address': '', 'port': 8887}]\n    >>> clean_dashboard_address(\":8787,:8887\")\n    [{'address': '', 'port': 8787}, {'address': '', 'port': 8887}]\n    \"\"\"\n\n    if default_listen_ip == \"0.0.0.0\":\n        default_listen_ip = \"\"  # for IPV6\n\n    if isinstance(addrs, str):\n        addrs = addrs.split(\",\")\n    if not isinstance(addrs, list):\n        addrs = [addrs]\n\n    addresses = []\n    for addr in addrs:\n        try:\n            addr = int(addr)\n        except (TypeError, ValueError):\n            pass\n\n        if isinstance(addr, str):\n            addr = addr.split(\":\")\n\n        if isinstance(addr, (tuple, list)):\n            if len(addr) == 2:\n                host, port = (addr[0], int(addr[1]))\n            elif len(addr) == 1:\n                [host], port = addr, 0\n            else:\n                raise ValueError(addr)\n        elif isinstance(addr, int):\n            host = default_listen_ip\n            port = addr\n\n        addresses.append({\"address\": host, \"port\": port})\n    return addresses\n\n\n_deprecations = {\n    \"no_default\": \"dask.typing.no_default\",\n}\n\n\ndef __getattr__(name):\n    if name in _deprecations:\n        use_instead = _deprecations[name]\n\n        warnings.warn(\n            f\"{name} is deprecated and will be removed in a future release. \"\n            f\"Please use {use_instead} instead.\",\n            category=FutureWarning,\n            stacklevel=2,\n        )\n        return import_term(use_instead)\n    else:\n        raise AttributeError(f\"module {__name__} has no attribute {name}\")\n\n\n# Used internally by recursive_to_dict to stop infinite recursion. If an object has\n# already been encountered, a string representation will be returned instead. This is\n# necessary since we have multiple cyclic referencing data structures.\n_recursive_to_dict_seen: ContextVar[set[int]] = ContextVar(\"_recursive_to_dict_seen\")\n_to_dict_no_nest_flag = False\n\n\ndef recursive_to_dict(\n    obj: AnyType, *, exclude: Container[str] = (), members: bool = False\n) -> AnyType:\n    \"\"\"Recursively convert arbitrary Python objects to a JSON-serializable\n    representation. This is intended for debugging purposes only.\n\n    The following objects are supported:\n\n    list, tuple, set, frozenset, deque, dict, dict_keys, dict_values\n        Descended into these objects recursively. Python-specific collections are\n        converted to JSON-friendly variants.\n    Classes that define ``_to_dict(self, *, exclude: Container[str] = ())``:\n        Call the method and dump its output\n    Classes that define ``_to_dict_no_nest(self, *, exclude: Container[str] = ())``:\n        Like above, but prevents nested calls (see below)\n    Other Python objects\n        Dump the output of ``repr()``\n    Objects already encountered before, regardless of type\n        Dump the output of ``repr()``. This breaks circular references and shortens the\n        output.\n\n    Parameters\n    ----------\n    exclude:\n        A list of attribute names to be excluded from the dump.\n        This will be forwarded to the objects ``_to_dict`` methods and these methods\n        are required to accept this parameter.\n    members:\n        If True, convert the top-level Python object to a dict of its public members\n\n    **``_to_dict_no_nest`` vs. ``_to_dict``**\n\n    The presence of the ``_to_dict_no_nest`` method signals ``recursive_to_dict`` to\n    have a mutually exclusive full dict representation with other objects that also have\n    the ``_to_dict_no_nest``, regardless of their class. Only the outermost object in a\n    nested structure has the method invoked; all others are\n    dumped as their string repr instead, even if they were not encountered before.\n\n    Example:\n\n    .. code-block:: python\n\n        >>> class Person:\n        ...     def __init__(self, name):\n        ...         self.name = name\n        ...         self.children = []\n        ...         self.pets = []\n        ...\n        ...     def _to_dict_no_nest(self, exclude=()):\n        ...         return recursive_to_dict(self.__dict__, exclude=exclude)\n        ...\n        ...     def __repr__(self):\n        ...         return self.name\n\n        >>> class Pet:\n        ...     def __init__(self, name):\n        ...         self.name = name\n        ...         self.owners = []\n        ...\n        ...     def _to_dict_no_nest(self, exclude=()):\n        ...         return recursive_to_dict(self.__dict__, exclude=exclude)\n        ...\n        ...     def __repr__(self):\n        ...         return self.name\n\n        >>> alice = Person(\"Alice\")\n        >>> bob = Person(\"Bob\")\n        >>> charlie = Pet(\"Charlie\")\n        >>> alice.children.append(bob)\n        >>> alice.pets.append(charlie)\n        >>> bob.pets.append(charlie)\n        >>> charlie.owners[:] = [alice, bob]\n        >>> recursive_to_dict({\"people\": [alice, bob], \"pets\": [charlie]})\n        {\n            \"people\": [\n                {\"name\": \"Alice\", \"children\": [\"Bob\"], \"pets\": [\"Charlie\"]},\n                {\"name\": \"Bob\", \"children\": [], \"pets\": [\"Charlie\"]},\n            ],\n            \"pets\": [\n                {\"name\": \"Charlie\", \"owners\": [\"Alice\", \"Bob\"]},\n            ],\n        }\n\n    If we changed the methods to ``_to_dict``, the output would instead be:\n\n    .. code-block:: python\n\n        {\n            \"people\": [\n                {\n                    \"name\": \"Alice\",\n                    \"children\": [\n                        {\n                            \"name\": \"Bob\",\n                            \"children\": [],\n                            \"pets\": [{\"name\": \"Charlie\", \"owners\": [\"Alice\", \"Bob\"]}],\n                        },\n                    ],\n                    pets: [\"Charlie\"],\n                ],\n                \"Bob\",\n            ],\n            \"pets\": [\"Charlie\"],\n        }\n\n    Also notice that, if in the future someone will swap the creation of the\n    ``children`` and ``pets`` attributes inside ``Person.__init__``, the output with\n    ``_to_dict`` will change completely whereas the one with ``_to_dict_no_nest`` won't!\n    \"\"\"\n    if isinstance(obj, (int, float, bool, str)) or obj is None:\n        return obj\n    if isinstance(obj, (type, bytes)):\n        return repr(obj)\n\n    if members:\n        obj = {\n            k: v\n            for k, v in inspect.getmembers(obj)\n            if not k.startswith(\"_\") and k not in exclude and not callable(v)\n        }\n\n    # Prevent infinite recursion\n    try:\n        seen = _recursive_to_dict_seen.get()\n    except LookupError:\n        seen = set()\n    seen = seen.copy()\n    tok = _recursive_to_dict_seen.set(seen)\n    try:\n        if id(obj) in seen:\n            return repr(obj)\n\n        if hasattr(obj, \"_to_dict_no_nest\"):\n            global _to_dict_no_nest_flag\n            if _to_dict_no_nest_flag:\n                return repr(obj)\n\n            seen.add(id(obj))\n            _to_dict_no_nest_flag = True\n            try:\n                return obj._to_dict_no_nest(exclude=exclude)\n            finally:\n                _to_dict_no_nest_flag = False\n\n        seen.add(id(obj))\n\n        if hasattr(obj, \"_to_dict\"):\n            return obj._to_dict(exclude=exclude)\n        if isinstance(obj, (list, tuple, set, frozenset, deque, KeysView, ValuesView)):\n            return [recursive_to_dict(el, exclude=exclude) for el in obj]\n        if isinstance(obj, dict):\n            res = {}\n            for k, v in obj.items():\n                k = recursive_to_dict(k, exclude=exclude)\n                v = recursive_to_dict(v, exclude=exclude)\n                try:\n                    res[k] = v\n                except TypeError:\n                    res[str(k)] = v\n            return res\n\n        return repr(obj)\n    finally:\n        tok.var.reset(tok)\n\n\nclass Deadline:\n    \"\"\"Utility class tracking a deadline and the progress toward it\"\"\"\n\n    #: Expiry time of the deadline in seconds since the start of the monotonic timer\n    #: or None if the deadline never expires\n    expires_at_mono: float | None\n    #: Seconds since the epoch when the deadline was created\n    started_at_mono: float\n    #: Seconds since the start of the monotonic timer when the deadline was created\n    started_at: float\n\n    __slots__ = tuple(__annotations__)\n\n    def __init__(self, expires_at: float | None = None):\n        self.started_at = time()\n        self.started_at_mono = monotonic()\n        if expires_at is not None:\n            self.expires_at_mono = expires_at - self.started_at + self.started_at_mono\n        else:\n            self.expires_at_mono = None\n\n    @classmethod\n    def after(cls, duration: float | None = None) -> Deadline:\n        \"\"\"Create a new ``Deadline`` that expires in ``duration`` seconds\n        or never if ``duration`` is None\n        \"\"\"\n        inst = cls()\n        if duration is not None:\n            inst.expires_at_mono = inst.started_at_mono + duration\n        return inst\n\n    @property\n    def expires_at(self) -> float | None:\n        \"\"\"Expiry time of the deadline in seconds since the unix epoch\n        or None if the deadline never expires.\n\n        Note that this can change over time if the wall clock is adjusted by the OS.\n        \"\"\"\n        if (exp := self.expires_at_mono) is None:\n            return None\n        return exp - monotonic() + time()\n\n    @property\n    def duration(self) -> float | None:\n        \"\"\"Seconds between the creation and expiration time of the deadline\n        if the deadline expires, None otherwise\n        \"\"\"\n        if (exp := self.expires_at_mono) is None:\n            return None\n        return exp - self.started_at_mono\n\n    @property\n    def expires(self) -> bool:\n        \"\"\"Whether the deadline ever expires\"\"\"\n        return self.expires_at_mono is not None\n\n    @property\n    def elapsed(self) -> float:\n        \"\"\"Seconds that elapsed since the deadline was created\"\"\"\n        return monotonic() - self.started_at_mono\n\n    @property\n    def remaining(self) -> float | None:\n        \"\"\"Seconds remaining until the deadline expires if an expiry time is set,\n        None otherwise\n        \"\"\"\n        if (exp := self.expires_at_mono) is None:\n            return None\n        else:\n            return max(0, exp - monotonic())\n\n    @property\n    def expired(self) -> bool:\n        \"\"\"Whether the deadline has already expired\"\"\"\n        return self.remaining == 0\n\n\nclass RateLimiterFilter(logging.Filter):\n    \"\"\"A Logging filter that ensures a matching message is emitted at most every\n    `rate` seconds\"\"\"\n\n    pattern: re.Pattern\n    rate: float\n    _last_seen: float\n\n    def __init__(self, pattern: str, *, name: str = \"\", rate: str | float = \"10s\"):\n        super().__init__(name)\n        self.pattern = re.compile(pattern)\n        self.rate = _parse_timedelta(rate)\n        self._last_seen = -self.rate\n\n    def filter(self, record: logging.LogRecord) -> bool:\n        if self.pattern.match(record.msg):\n            now = monotonic()\n            if now - self._last_seen < self.rate:\n                return False\n            self._last_seen = now\n        return True\n\n    @classmethod\n    def reset_timer(cls, logger: logging.Logger | str) -> None:\n        \"\"\"Reset the timer on all RateLimiterFilters on a logger.\n        Useful in unit testing.\n        \"\"\"\n        if isinstance(logger, str):\n            logger = logging.getLogger(logger)\n        for filter in logger.filters:\n            if isinstance(filter, cls):\n                filter._last_seen = -filter.rate\n\n\nif sys.version_info >= (3, 11):\n\n    async def wait_for(fut: Awaitable[T], timeout: float) -> T:\n        async with asyncio.timeout(timeout):\n            return await fut\n\nelse:\n\n    async def wait_for(fut: Awaitable[T], timeout: float) -> T:\n        return await asyncio.wait_for(fut, timeout)\n\n\nclass TupleComparable:\n    \"\"\"Wrap object so that we can compare tuple, int or None\n\n    When comparing two objects of different types Python fails\n\n    >>> (1, 2) < 1\n    Traceback (most recent call last):\n        ...\n    TypeError: '<' not supported between instances of 'tuple' and 'int'\n\n    This class replaces None with 0, and wraps ints with tuples\n\n    >>> TupleComparable((1, 2)) < TupleComparable(1)\n    False\n    \"\"\"\n\n    __slots__ = (\"obj\",)\n\n    def __init__(self, obj):\n        if obj is None:\n            self.obj = (0,)\n        elif isinstance(obj, tuple):\n            self.obj = obj\n        elif isinstance(obj, (int, float)):\n            self.obj = (obj,)\n        else:\n            raise ValueError(f\"Object must be tuple, int, float or None, got {obj}\")\n\n    def __eq__(self, other):\n        return self.obj == other.obj\n\n    def __lt__(self, other):\n        return self.obj < other.obj\n\n\n@functools.cache\ndef url_escape(url, *args, **kwargs):\n    \"\"\"\n    Escape a URL path segment. Cache results for better performance.\n    \"\"\"\n    return escape.url_escape(url, *args, **kwargs)\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:18:01"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport os\nimport re\nimport shutil\nimport sys\nimport tempfile\nfrom textwrap import dedent\nfrom unittest import mock\n\nimport pytest\n\nimport dask\n\nfrom distributed import Client, Nanny, Scheduler, Worker\nfrom distributed.preloading import Preload\nfrom distributed.utils import open_port\nfrom distributed.utils_test import captured_logger, cluster, gen_cluster, gen_test\n\nPRELOAD_TEXT = \"\"\"\n_worker_info = {}\n\ndef dask_setup(worker):\n    _worker_info['address'] = worker.address\n\ndef get_worker_address():\n    return _worker_info['address']\n\"\"\"\n\n\ndef test_worker_preload_file(loop):\n    def check_worker():\n        import worker_info\n\n        return worker_info.get_worker_address()\n\n    tmpdir = tempfile.mkdtemp()\n    try:\n        path = os.path.join(tmpdir, \"worker_info.py\")\n        with open(path, \"w\") as f:\n            f.write(PRELOAD_TEXT)\n\n        with (\n            cluster(worker_kwargs={\"preload\": [path]}) as (s, workers),\n            Client(s[\"address\"], loop=loop) as c,\n        ):\n            assert c.run(check_worker) == {\n                worker[\"address\"]: worker[\"address\"] for worker in workers\n            }\n    finally:\n        shutil.rmtree(tmpdir)\n\n\n@gen_test()\nasync def test_worker_preload_text():\n    text = \"\"\"\ndef dask_setup(worker):\n    worker.foo = 'setup'\n\"\"\"\n    async with Scheduler(dashboard_address=\":0\", preload=text) as s:\n        assert s.foo == \"setup\"\n        async with Worker(s.address, preload=[text]) as w:\n            assert w.foo == \"setup\"\n\n\n@gen_test()\nasync def test_preload_manager_sequence():\n    text = \"\"\"\ndef dask_setup(worker):\n    worker.foo = 'setup'\n\"\"\"\n    async with Scheduler(dashboard_address=\":0\", preload=text) as s:\n        assert len(s.preloads) == 1\n        assert isinstance(s.preloads[0], Preload)\n        # Make sure list comprehensions return the correct # of items\n        assert len([x for x in s.preloads]) == len(s.preloads)\n\n\n@gen_cluster(nthreads=[])\nasync def test_worker_preload_config(s):\n    text = \"\"\"\ndef dask_setup(worker):\n    worker.foo = 'setup'\n\ndef dask_teardown(worker):\n    worker.foo = 'teardown'\n\"\"\"\n    with dask.config.set(\n        {\"distributed.worker.preload\": [text], \"distributed.nanny.preload\": [text]}\n    ):\n        async with Nanny(s.address) as w:\n            assert w.foo == \"setup\"\n            async with Client(s.address, asynchronous=True) as c:\n                d = await c.run(lambda dask_worker: dask_worker.foo)\n                assert d == {w.worker_address: \"setup\"}\n        assert w.foo == \"teardown\"\n\n\ndef test_worker_preload_module(loop):\n    def check_worker():\n        import worker_info\n\n        return worker_info.get_worker_address()\n\n    tmpdir = tempfile.mkdtemp()\n    sys.path.insert(0, tmpdir)\n    try:\n        path = os.path.join(tmpdir, \"worker_info.py\")\n        with open(path, \"w\") as f:\n            f.write(PRELOAD_TEXT)\n\n        with (\n            cluster(worker_kwargs={\"preload\": [\"worker_info\"]}) as (\n                s,\n                workers,\n            ),\n            Client(s[\"address\"], loop=loop) as c,\n        ):\n            assert c.run(check_worker) == {\n                worker[\"address\"]: worker[\"address\"] for worker in workers\n            }\n    finally:\n        sys.path.remove(tmpdir)\n        shutil.rmtree(tmpdir)\n\n\n@gen_cluster(nthreads=[])\nasync def test_worker_preload_click(s):\n    text = \"\"\"\nimport click\n\n@click.command()\ndef dask_setup(worker):\n    worker.foo = 'setup'\n\"\"\"\n\n    async with Worker(s.address, preload=text) as w:\n        assert w.foo == \"setup\"\n\n\n@gen_cluster(nthreads=[])\nasync def test_worker_preload_click_async(s, tmp_path):\n    # Ensure we allow for click commands wrapping coroutines\n    # https://github.com/dask/distributed/issues/4169\n    text = \"\"\"\nimport click\n\n@click.command()\nasync def dask_setup(worker):\n    worker.foo = 'setup'\n\"\"\"\n    async with Worker(s.address, preload=text) as w:\n        assert w.foo == \"setup\"\n\n\n@gen_test()\nasync def test_preload_import_time():\n    text = \"\"\"\nfrom distributed.comm.registry import backends\nfrom distributed.comm.tcp import TCPBackend\n\nbackends[\"foo\"] = TCPBackend()\n\"\"\".strip()\n    try:\n        async with Scheduler(dashboard_address=\":0\", preload=text, protocol=\"foo\") as s:\n            async with Nanny(s.address, preload=text, protocol=\"foo\") as n:\n                async with Client(s.address, asynchronous=True) as c:\n                    await c.wait_for_workers(1)\n    finally:\n        from distributed.comm.registry import backends\n\n        del backends[\"foo\"]\n\n\n@gen_test()\nasync def test_web_preload():\n    with (\n        mock.patch(\n            \"urllib3.PoolManager.request\",\n            **{\n                \"return_value.data\": b\"def dask_setup(dask_server):\"\n                b\"\\n    dask_server.foo = 1\"\n                b\"\\n\"\n            },\n        ) as request,\n        captured_logger(\"distributed.preloading\") as log,\n    ):\n        async with Scheduler(\n            host=\"localhost\",\n            preload=[\"http://example.com/preload\"],\n            dashboard_address=\":0\",\n        ) as s:\n            assert s.foo == 1\n        assert (\n            re.match(\n                r\"(?s).*Downloading preload at http://example.com/preload\\n\"\n                r\".*Run preload setup: http://example.com/preload\\n\"\n                r\".*\",\n                log.getvalue(),\n            )\n            is not None\n        )\n    assert request.mock_calls == [\n        mock.call(method=\"GET\", url=\"http://example.com/preload\", retries=mock.ANY)\n    ]\n\n\n@gen_cluster(nthreads=[])\nasync def test_scheduler_startup(s):\n    text = f\"\"\"\nimport dask\ndask.config.set(scheduler_address=\"{s.address}\")\n\"\"\"\n    async with Worker(preload=text) as w:\n        assert w.scheduler.address == s.address\n\n\n@gen_cluster(nthreads=[])\nasync def test_scheduler_startup_nanny(s):\n    text = f\"\"\"\nimport dask\ndask.config.set(scheduler_address=\"{s.address}\")\n\"\"\"\n    async with Nanny(preload_nanny=text) as w:\n        assert w.scheduler.address == s.address\n\n\n@gen_test()\nasync def test_web_preload_worker():\n    port = open_port()\n    data = dedent(\n        f\"\"\"\\\n        import dask\n        dask.config.set(scheduler_address=\"tcp://127.0.0.1:{port}\")\n        \"\"\"\n    ).encode()\n    with mock.patch(\n        \"urllib3.PoolManager.request\",\n        **{\"return_value.data\": data},\n    ) as request:\n        async with Scheduler(port=port, host=\"localhost\", dashboard_address=\":0\") as s:\n            async with Nanny(preload_nanny=[\"http://example.com/preload\"]) as nanny:\n                assert nanny.scheduler_addr == s.address\n    assert request.mock_calls == [\n        mock.call(method=\"GET\", url=\"http://example.com/preload\", retries=mock.ANY)\n    ]\n\n\n# This test is blocked on https://github.com/dask/distributed/issues/5819\n@pytest.mark.xfail(\n    reason=\"The preload argument to the client isn't supported yet\", strict=True\n)\n@gen_cluster(nthreads=[])\nasync def test_client_preload_text(s):\n    text = dedent(\n        \"\"\"\\\n        def dask_setup(client):\n            client.foo = \"setup\"\n\n\n        def dask_teardown(client):\n            client.foo = \"teardown\"\n        \"\"\"\n    )\n    async with Client(address=s.address, asynchronous=True, preload=text) as c:\n        assert c.foo == \"setup\"\n    assert c.foo == \"teardown\"\n\n\n@gen_cluster(nthreads=[])\nasync def test_client_preload_config(s):\n    text = dedent(\n        \"\"\"\\\n        def dask_setup(client):\n            client.foo = \"setup\"\n\n\n        def dask_teardown(client):\n            client.foo = \"teardown\"\n        \"\"\"\n    )\n    with dask.config.set({\"distributed.client.preload\": [text]}):\n        async with Client(address=s.address, asynchronous=True) as c:\n            assert c.foo == \"setup\"\n        assert c.foo == \"teardown\"\n\n\n# This test is blocked on https://github.com/dask/distributed/issues/5819\n@pytest.mark.xfail(\n    reason=\"The preload argument to the client isn't supported yet\", strict=True\n)\n@gen_cluster(nthreads=[])\nasync def test_client_preload_click(s):\n    text = dedent(\n        \"\"\"\\\n        import click\n\n        @click.command()\n        @click.argument(\"value\")\n        def dask_setup(client, value):\n            client.foo = value\n        \"\"\"\n    )\n    value = \"setup\"\n    async with Client(\n        address=s.address, asynchronous=True, preload=text, preload_argv=[[value]]\n    ) as c:\n        assert c.foo == value\n\n\n@gen_test()\nasync def test_failure_doesnt_crash_scheduler():\n    text = \"\"\"\ndef dask_setup(worker):\n    raise Exception(123)\n\ndef dask_teardown(worker):\n    raise Exception(456)\n\"\"\"\n\n    with captured_logger(\"distributed.preloading\") as logger:\n        async with Scheduler(dashboard_address=\":0\", preload=text):\n            pass\n\n    logs = logger.getvalue()\n    assert \"123\" in logs\n    assert \"456\" in logs\n\n\n@gen_cluster(client=False, nthreads=[])\nasync def test_failure_doesnt_crash_worker(s):\n    text = \"\"\"\ndef dask_setup(worker):\n    raise Exception(123)\n\ndef dask_teardown(worker):\n    raise Exception(456)\n\"\"\"\n\n    with captured_logger(\"distributed.preloading\") as logger:\n        async with Worker(s.address, preload=[text]):\n            pass\n\n    logs = logger.getvalue()\n    assert \"123\" in logs\n    assert \"456\" in logs\n\n\n@gen_cluster(client=False, nthreads=[])\nasync def test_failure_doesnt_crash_nanny(s):\n    text = \"\"\"\ndef dask_setup(worker):\n    raise Exception(123)\n\ndef dask_teardown(worker):\n    raise Exception(456)\n\"\"\"\n\n    with captured_logger(\"distributed.preloading\") as logger:\n        async with Nanny(s.address, preload_nanny=[text]):\n            pass\n\n    logs = logger.getvalue()\n    assert \"123\" in logs\n    assert \"456\" in logs\n\n\n@gen_cluster(client=False)\nasync def test_failure_doesnt_crash_client(s, a, b):\n    text = \"\"\"\ndef dask_setup(worker):\n    raise Exception(123)\n\ndef dask_teardown(worker):\n    raise Exception(456)\n\"\"\"\n\n    with dask.config.set({\"distributed.client.preload\": [text]}):\n        with captured_logger(\"distributed.preloading\") as logger:\n            async with Client(s.address, asynchronous=True):\n                pass\n\n    logs = logger.getvalue()\n    assert \"123\" in logs\n    assert \"456\" in logs\n\n\n@gen_cluster(nthreads=[])\nasync def test_client_preload_config_click(s):\n    text = dedent(\n        \"\"\"\\\n        import click\n\n        @click.command()\n        @click.argument(\"value\")\n        def dask_setup(client, value):\n            client.foo = value\n        \"\"\"\n    )\n    value = \"setup\"\n    with dask.config.set(\n        {\n            \"distributed.client.preload\": [text],\n            \"distributed.client.preload-argv\": [[value]],\n        }\n    ):\n        async with Client(address=s.address, asynchronous=True) as c:\n            assert c.foo == value\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: distributed\nTest File Path: distributed\\test_profile\\test_profile.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\" This module contains utility functions to construct and manipulate counting\ndata structures for frames.\n\nWhen performing statistical profiling we obtain many call stacks.  We aggregate\nthese call stacks into data structures that maintain counts of how many times\neach function in that call stack has been called.  Because these stacks will\noverlap this aggregation counting structure forms a tree, such as is commonly\nvisualized by profiling tools.\n\nWe represent this tree as a nested dictionary with the following form:\n\n    {\n     'identifier': 'root',\n     'description': 'A long description of the line of code being run.',\n     'count': 10  # the number of times we have seen this line\n     'children': {  # callers of this line. Recursive dicts\n         'ident-b': {'description': ...\n                   'identifier': 'ident-a',\n                   'count': ...\n                   'children': {...}},\n         'ident-b': {'description': ...\n                   'identifier': 'ident-b',\n                   'count': ...\n                   'children': {...}}}\n    }\n\"\"\"\n\nfrom __future__ import annotations\n\nimport bisect\nimport dis\nimport linecache\nimport sys\nimport threading\nfrom collections import defaultdict, deque\nfrom collections.abc import Callable, Collection\nfrom time import sleep\nfrom types import FrameType\nfrom typing import Any\n\nimport tlz as toolz\n\nimport dask.config\nfrom dask.typing import NoDefault, no_default\nfrom dask.utils import format_time, parse_timedelta\n\nfrom distributed.metrics import time\nfrom distributed.utils import color_of\n\n#: This lock can be acquired to ensure that no instance of watch() is concurrently holding references to frames\nlock = threading.Lock()\n\n\ndef identifier(frame: FrameType | None) -> str:\n    \"\"\"A string identifier from a frame\n\n    Strings are cheaper to use as indexes into dicts than tuples or dicts\n    \"\"\"\n    if frame is None:\n        return \"None\"\n    else:\n        co = frame.f_code\n        try:\n            return \";\".join(\n                (\n                    co.co_name,\n                    co.co_filename,\n                    str(co.co_firstlineno),\n                )\n            )\n        except AttributeError:\n            if co.__module__:\n                return f\"{co.__module__}.{co.__qualname__}:{_f_lineno(frame)}\"  # type: ignore[attr-defined]\n            else:\n                return f\"{co.__qualname__}:{_f_lineno(frame)}\"  # type: ignore[attr-defined]\n\n\ndef _f_lineno(frame: FrameType) -> int:\n    \"\"\"Work around some frames lacking an f_lineno\n    See: https://bugs.python.org/issue47085\n    \"\"\"\n    f_lineno = frame.f_lineno\n    if f_lineno is not None:\n        return f_lineno\n\n    f_lasti = frame.f_lasti\n    code = frame.f_code\n    try:\n        prev_line = code.co_firstlineno\n\n        for start, next_line in dis.findlinestarts(code):\n            if f_lasti < start:\n                return prev_line\n            if next_line:\n                prev_line = next_line\n\n        return prev_line\n    except Exception:\n        return -1\n\n\ndef repr_frame(frame: FrameType) -> str:\n    \"\"\"Render a frame as a line for inclusion into a text traceback\"\"\"\n    co = frame.f_code\n    f_lineno = _f_lineno(frame)\n    text = f'  File \"{co.co_filename}\", line {f_lineno}, in {co.co_name}'\n    line = linecache.getline(co.co_filename, f_lineno, frame.f_globals).lstrip()\n    return text + \"\\n\\t\" + line\n\n\ndef info_frame(frame: FrameType) -> dict[str, Any]:\n    co = frame.f_code\n    f_lineno = _f_lineno(frame)\n    try:\n        name = co.co_name\n        filename = co.co_filename\n        line = linecache.getline(co.co_filename, f_lineno, frame.f_globals).lstrip()\n    except (AttributeError, IndexError):\n        line = \"\"\n        name = co.__qualname__  # type: ignore[attr-defined]\n        filename = \"<built-in>\"\n    return {\n        \"filename\": filename,\n        \"name\": name,\n        \"line_number\": f_lineno,\n        \"line\": line,\n    }\n\n\ndef process(\n    frame: FrameType,\n    child: object | None,\n    state: dict[str, Any],\n    *,\n    stop: str | None = None,\n    omit: Collection[str] = (),\n    depth: int | None = None,\n) -> dict[str, Any] | None:\n    \"\"\"Add counts from a frame stack onto existing state\n\n    This recursively adds counts to the existing state dictionary and creates\n    new entries for new functions.\n\n    Parameters\n    ----------\n    frame:\n        The frame to process onto the state\n    child:\n        For internal use only\n    state:\n        The profile state to accumulate this frame onto, see ``create``\n    stop:\n        Filename suffix that should stop processing if we encounter it\n    omit:\n        Filenames that we should omit from processing\n    depth:\n        For internal use only, how deep we are in the call stack\n        Used to prevent stack overflow\n\n    Examples\n    --------\n    >>> import sys, threading\n    >>> ident = threading.get_ident()  # replace with your thread of interest\n    >>> frame = sys._current_frames()[ident]\n    >>> state = create()\n    >>> process(frame, None, state)\n    >>> state\n    {'count': 1,\n     'identifier': 'root',\n     'description': 'root',\n     'children': {'...'}}\n\n    See also\n    --------\n    create\n    merge\n    \"\"\"\n    if depth is None:\n        # Cut off rather conservatively since the output of the profiling\n        # sometimes need to be recursed into as well, e.g. for serialization\n        # which can cause recursion errors later on since this can generate\n        # deeply nested dictionaries\n        depth = min(100, sys.getrecursionlimit() // 4)\n\n    if any(frame.f_code.co_filename.endswith(o) for o in omit):\n        return None\n\n    prev = frame.f_back\n    if (\n        depth > 0\n        and prev is not None\n        and (stop is None or not prev.f_code.co_filename.endswith(stop))\n    ):\n        new_state = process(prev, frame, state, stop=stop, depth=depth - 1)\n        if new_state is None:\n            return None\n        state = new_state\n\n    ident = identifier(frame)\n\n    try:\n        d = state[\"children\"][ident]\n    except KeyError:\n        d = {\n            \"count\": 0,\n            \"description\": info_frame(frame),\n            \"children\": {},\n            \"identifier\": ident,\n        }\n        state[\"children\"][ident] = d\n\n    state[\"count\"] += 1\n\n    if child is not None:\n        return d\n    else:\n        d[\"count\"] += 1\n        return None\n\n\ndef merge(*args: dict[str, Any]) -> dict[str, Any]:\n    \"\"\"Merge multiple frame states together\"\"\"\n    if not args:\n        return create()\n    s = {arg[\"identifier\"] for arg in args}\n    if len(s) != 1:  # pragma: no cover\n        raise ValueError(f\"Expected identifiers, got {s}\")\n    children = defaultdict(list)\n    for arg in args:\n        for child in arg[\"children\"]:\n            children[child].append(arg[\"children\"][child])\n\n    try:\n        children_dict = {k: merge(*v) for k, v in children.items()}\n    except RecursionError:  # pragma: no cover\n        children_dict = {}\n    count = sum(arg[\"count\"] for arg in args)\n    return {\n        \"description\": args[0][\"description\"],\n        \"children\": children_dict,\n        \"count\": count,\n        \"identifier\": args[0][\"identifier\"],\n    }\n\n\ndef create() -> dict[str, Any]:\n    return {\n        \"count\": 0,\n        \"children\": {},\n        \"identifier\": \"root\",\n        \"description\": {\"filename\": \"\", \"name\": \"\", \"line_number\": 0, \"line\": \"\"},\n    }\n\n\ndef call_stack(frame: FrameType) -> list[str]:\n    \"\"\"Create a call text stack from a frame\n\n    Returns\n    -------\n    list of strings\n    \"\"\"\n    L = []\n    cur_frame: FrameType | None = frame\n    while cur_frame:\n        L.append(repr_frame(cur_frame))\n        cur_frame = cur_frame.f_back\n    return L[::-1]\n\n\ndef plot_data(state, profile_interval=0.010):\n    \"\"\"Convert a profile state into data useful by Bokeh\n\n    See Also\n    --------\n    plot_figure\n    distributed.bokeh.components.ProfilePlot\n    \"\"\"\n    starts = []\n    stops = []\n    heights = []\n    widths = []\n    colors = []\n    states = []\n    times = []\n\n    filenames = []\n    lines = []\n    line_numbers = []\n    names = []\n\n    def traverse(state, start, stop, height):\n        if not state[\"count\"]:\n            return\n        starts.append(start)\n        stops.append(stop)\n        heights.append(height)\n        width = stop - start\n        widths.append(width)\n        states.append(state)\n        times.append(format_time(state[\"count\"] * profile_interval))\n\n        desc = state[\"description\"]\n        filenames.append(desc[\"filename\"])\n        lines.append(desc[\"line\"])\n        line_numbers.append(desc[\"line_number\"])\n        names.append(desc[\"name\"])\n\n        try:\n            fn = desc[\"filename\"]\n        except IndexError:  # pragma: no cover\n            colors.append(\"gray\")\n        else:\n            if fn == \"<low-level>\":  # pragma: no cover\n                colors.append(\"lightgray\")\n            else:\n                colors.append(color_of(fn))\n\n        delta = (stop - start) / state[\"count\"]\n\n        x = start\n\n        for _, child in state[\"children\"].items():\n            width = child[\"count\"] * delta\n            traverse(child, x, x + width, height + 1)\n            x += width\n\n    traverse(state, 0, 1, 0)\n    percentages = [f\"{100 * w:.1f}%\" for w in widths]\n    return {\n        \"left\": starts,\n        \"right\": stops,\n        \"bottom\": heights,\n        \"width\": widths,\n        \"top\": [x + 1 for x in heights],\n        \"color\": colors,\n        \"states\": states,\n        \"filename\": filenames,\n        \"line\": lines,\n        \"line_number\": line_numbers,\n        \"name\": names,\n        \"time\": times,\n        \"percentage\": percentages,\n    }\n\n\ndef _watch(\n    thread_id: int,\n    log: deque[tuple[float, dict[str, Any]]],  # [(timestamp, output of create()), ...]\n    interval: float,\n    cycle: float,\n    omit: Collection[str],\n    stop: Callable[[], bool],\n) -> None:\n    recent = create()\n    last = time()\n\n    while not stop():\n        if time() > last + cycle:\n            recent = create()\n            with lock:\n                log.append((time(), recent))\n                last = time()\n                try:\n                    frame = sys._current_frames()[thread_id]\n                except KeyError:\n                    return\n\n                process(frame, None, recent, omit=omit)\n                del frame\n        sleep(interval)\n\n\ndef watch(\n    thread_id: int | None = None,\n    interval: str = \"20ms\",\n    cycle: str = \"2s\",\n    maxlen: int | None | NoDefault = no_default,\n    omit: Collection[str] = (),\n    stop: Callable[[], bool] = lambda: False,\n) -> deque[tuple[float, dict[str, Any]]]:\n    \"\"\"Gather profile information on a particular thread\n\n    This starts a new thread to watch a particular thread and returns a deque\n    that holds periodic profile information.\n\n    Parameters\n    ----------\n    thread_id : int, optional\n        Defaults to current thread\n    interval : str\n        Time per sample\n    cycle : str\n        Time per refreshing to a new profile state\n    maxlen : int\n        Passed onto deque, maximum number of periods\n    omit : collection of str\n        Don't include entries whose filename includes any of these substrings\n    stop : callable\n        Function to call to see if we should stop. It must\n        accept no arguments and return a bool (True to stop,\n        False to continue).\n\n    Returns\n    -------\n    deque of tuples:\n\n    - timestamp\n    - dict[str, Any] (output of ``create()``)\n    \"\"\"\n    if maxlen is no_default:\n        maxlen = dask.config.get(\"distributed.admin.low-level-log-length\")\n        assert isinstance(maxlen, int) or maxlen is None\n    log: deque[tuple[float, dict[str, Any]]] = deque(maxlen=maxlen)\n\n    thread = threading.Thread(\n        target=_watch,\n        name=\"Profile\",\n        kwargs={\n            \"thread_id\": thread_id or threading.get_ident(),\n            \"interval\": parse_timedelta(interval),\n            \"cycle\": parse_timedelta(cycle),\n            \"log\": log,\n            \"omit\": omit,\n            \"stop\": stop,\n        },\n    )\n    thread.daemon = True\n    thread.start()\n\n    return log\n\n\ndef get_profile(history, recent=None, start=None, stop=None, key=None):\n    \"\"\"Collect profile information from a sequence of profile states\n\n    Parameters\n    ----------\n    history : Sequence[Tuple[time, Dict]]\n        A list or deque of profile states\n    recent : dict\n        The most recent accumulating state\n    start : time\n    stop : time\n    \"\"\"\n    if start is None:\n        istart = 0\n    else:\n        istart = bisect.bisect_left(history, (start,))\n\n    if stop is None:\n        istop = None\n    else:\n        istop = bisect.bisect_right(history, (stop,)) + 1\n        if istop >= len(history):\n            istop = None  # include end\n\n    if istart == 0 and istop is None:\n        history = list(history)\n    else:\n        iistop = len(history) if istop is None else istop\n        history = [history[i] for i in range(istart, iistop)]\n\n    prof = merge(*toolz.pluck(1, history))\n\n    if not history:\n        return create()\n\n    if recent:\n        prof = merge(prof, recent)\n\n    return prof\n\n\ndef plot_figure(data, **kwargs):\n    \"\"\"Plot profile data using Bokeh\n\n    This takes the output from the function ``plot_data`` and produces a Bokeh\n    figure\n\n    See Also\n    --------\n    plot_data\n    \"\"\"\n    from bokeh.models import HoverTool\n    from bokeh.plotting import ColumnDataSource, figure\n\n    if \"states\" in data:\n        data = toolz.dissoc(data, \"states\")\n\n    source = ColumnDataSource(data=data)\n\n    fig = figure(tools=\"tap,box_zoom,xwheel_zoom,reset\", **kwargs)\n    r = fig.quad(\n        \"left\",\n        \"right\",\n        \"top\",\n        \"bottom\",\n        color=\"color\",\n        line_color=\"black\",\n        line_width=2,\n        source=source,\n    )\n\n    r.selection_glyph = None\n    r.nonselection_glyph = None\n\n    hover = HoverTool(\n        point_policy=\"follow_mouse\",\n        tooltips=\"\"\"\n            <div>\n                <span style=\"font-size: 14px; font-weight: bold;\">Name:</span>&nbsp;\n                <span style=\"font-size: 10px; font-family: Monaco, monospace;\">@name</span>\n            </div>\n            <div>\n                <span style=\"font-size: 14px; font-weight: bold;\">Filename:</span>&nbsp;\n                <span style=\"font-size: 10px; font-family: Monaco, monospace;\">@filename</span>\n            </div>\n            <div>\n                <span style=\"font-size: 14px; font-weight: bold;\">Line number:</span>&nbsp;\n                <span style=\"font-size: 10px; font-family: Monaco, monospace;\">@line_number</span>\n            </div>\n            <div>\n                <span style=\"font-size: 14px; font-weight: bold;\">Line:</span>&nbsp;\n                <span style=\"font-size: 10px; font-family: Monaco, monospace;\">@line</span>\n            </div>\n            <div>\n                <span style=\"font-size: 14px; font-weight: bold;\">Time:</span>&nbsp;\n                <span style=\"font-size: 10px; font-family: Monaco, monospace;\">@time</span>\n            </div>\n            <div>\n                <span style=\"font-size: 14px; font-weight: bold;\">Percentage:</span>&nbsp;\n                <span style=\"font-size: 10px; font-family: Monaco, monospace;\">@percentage</span>\n            </div>\n            \"\"\",\n    )\n    fig.add_tools(hover)\n\n    fig.xaxis.visible = False\n    fig.yaxis.visible = False\n    fig.grid.visible = False\n\n    return fig, source\n\n\ndef _remove_py_stack(frames):\n    for entry in frames:\n        if entry.is_python:\n            break\n        yield entry\n\n\ndef llprocess(  # type: ignore[no-untyped-def]\n    frames,\n    child: object | None,\n    state: dict[str, Any] | None,\n) -> dict[str, Any] | None:\n    \"\"\"Add counts from low level profile information onto existing state\n\n    This uses the ``stacktrace`` module to collect low level stack trace\n    information and place it onto the given state.\n\n    It is configured with the ``distributed.worker.profile.low-level`` config\n    entry.\n\n    See Also\n    --------\n    process\n    ll_get_stack\n    \"\"\"\n    if not frames:\n        return None\n    frame = frames.pop()\n    if frames:\n        state = llprocess(frames, frame, state)\n    assert state\n\n    addr = hex(frame.addr - frame.offset)\n    ident = \";\".join(map(str, (frame.name, \"<low-level>\", addr)))\n    try:\n        d = state[\"children\"][ident]\n    except KeyError:\n        d = {\n            \"count\": 0,\n            \"description\": {\n                \"filename\": \"<low-level>\",\n                \"name\": frame.name,\n                \"line_number\": 0,\n                \"line\": str(frame),\n            },\n            \"children\": {},\n            \"identifier\": ident,\n        }\n        state[\"children\"][ident] = d\n\n    state[\"count\"] += 1\n\n    if child is not None:\n        return d\n    else:\n        d[\"count\"] += 1\n        return None\n\n\ndef ll_get_stack(tid):\n    \"\"\"Collect low level stack information from thread id\"\"\"\n    from stacktrace import get_thread_stack\n\n    frames = get_thread_stack(tid, show_python=False)\n    llframes = list(_remove_py_stack(frames))[::-1]\n    return llframes\n\n### Source File Dependency Files Content\n### Dependency File: metrics.py\nfrom __future__ import annotations\n\nimport asyncio\nimport collections\nimport sys\nimport threading\nimport time as timemod\nfrom collections.abc import Callable, Hashable, Iterator\nfrom contextlib import contextmanager\nfrom contextvars import ContextVar\nfrom dataclasses import dataclass\nfrom functools import wraps\nfrom math import nan\nfrom typing import Literal\n\nimport psutil\n\nfrom distributed.compatibility import WINDOWS\n\n_empty_namedtuple = collections.namedtuple(\"_empty_namedtuple\", ())\n\n\ndef _psutil_caller(method_name, default=_empty_namedtuple):\n    \"\"\"\n    Return a function calling the given psutil *method_name*,\n    or returning *default* if psutil fails.\n    \"\"\"\n    meth = getattr(psutil, method_name)\n\n    @wraps(meth)\n    def wrapper():  # pragma: no cover\n        try:\n            return meth()\n        except RuntimeError:\n            # This can happen on some systems (e.g. no physical disk in worker)\n            return default()\n\n    return wrapper\n\n\ndisk_io_counters = _psutil_caller(\"disk_io_counters\")\n\nnet_io_counters = _psutil_caller(\"net_io_counters\")\n\n\nclass _WindowsTime:\n    \"\"\"Combine time.time() or time.monotonic() with time.perf_counter() to get an\n    absolute clock with fine resolution.\n    \"\"\"\n\n    base_timer: Callable[[], float]\n    delta: float\n    previous: float | None\n    next_resync: float\n    resync_every: float\n\n    def __init__(\n        self, base: Callable[[], float], is_monotonic: bool, resync_every: float = 600.0\n    ):\n        self.base_timer = base\n        self.previous = float(\"-inf\") if is_monotonic else None\n        self.next_resync = float(\"-inf\")\n        self.resync_every = resync_every\n\n    def time(self) -> float:\n        cur = timemod.perf_counter()\n        if cur > self.next_resync:\n            self.resync()\n            self.next_resync = cur + self.resync_every\n        cur += self.delta\n        if self.previous is not None:\n            # Monotonic timer\n            if cur <= self.previous:\n                cur = self.previous + 1e-9\n            self.previous = cur\n        return cur\n\n    def resync(self) -> None:\n        _time = self.base_timer\n        _perf_counter = timemod.perf_counter\n        min_samples = 5\n        while True:\n            times = [(_time(), _perf_counter()) for _ in range(min_samples * 2)]\n            abs_times = collections.Counter(t[0] for t in times)\n            first, nfirst = abs_times.most_common()[0]\n            if nfirst < min_samples:\n                # System too noisy? Start again\n                continue\n\n            perf_times = [t[1] for t in times if t[0] == first][:-1]\n            assert len(perf_times) >= min_samples - 1, perf_times\n            self.delta = first - sum(perf_times) / len(perf_times)\n            break\n\n\n# A high-resolution wall clock timer measuring the seconds since Unix epoch\nif WINDOWS and sys.version_info < (3, 13):\n    time = _WindowsTime(timemod.time, is_monotonic=False).time\n    monotonic = _WindowsTime(timemod.monotonic, is_monotonic=True).time\nelse:\n    # Under modern Unixes, time.time() and time.monotonic() should be good enough\n    time = timemod.time\n    monotonic = timemod.monotonic\n\nprocess_time = timemod.process_time\n\n# Get a per-thread CPU timer function if possible, otherwise\n# use a per-process CPU timer function.\ntry:\n    # thread_time is not supported on all platforms\n    thread_time = timemod.thread_time\nexcept (AttributeError, OSError):  # pragma: no cover\n    thread_time = process_time\n\n\n@dataclass\nclass MeterOutput:\n    start: float\n    stop: float\n    delta: float\n    __slots__ = tuple(__annotations__)\n\n\n@contextmanager\ndef meter(\n    func: Callable[[], float] = timemod.perf_counter,\n    floor: float | Literal[False] = 0.0,\n) -> Iterator[MeterOutput]:\n    \"\"\"Convenience context manager which calls func() before and after the wrapped\n    code and calculates the delta.\n\n    Parameters\n    ----------\n    label: str\n        label to pass to the callback\n    func: callable\n        function to call before and after, which must return a number.\n        Besides time, it could return e.g. cumulative network traffic or disk usage.\n        Default: :func:`timemod.perf_counter`\n    floor: float or False, optional\n        Floor the delta to the given value (default: 0). This is useful for strictly\n        cumulative functions that can occasionally glitch and go backwards.\n        Set to False to disable.\n\n    Yields\n    ------\n    :class:`MeterOutput` where the ``start`` attribute is populated straight away, while\n    ``stop`` and ``delta`` are nan until context exit.\n    \"\"\"\n    out = MeterOutput(func(), nan, nan)\n    try:\n        yield out\n    finally:\n        out.stop = func()\n        out.delta = out.stop - out.start\n        if floor is not False:\n            out.delta = max(floor, out.delta)\n\n\nclass ContextMeter:\n    \"\"\"Context-based general purpose meter.\n\n    Usage\n    -----\n    1. In high level code, call :meth:`add_callback` to install a hook that defines an\n       activity\n    2. In low level code, typically many stack levels below, log quantitative events\n       (e.g. elapsed time, transferred bytes, etc.) so that they will be attributed to\n       the high-level code calling it, either with :meth:`meter`,\n       :meth:`meter_function`, or :meth:`digest_metric`.\n\n    Examples\n    --------\n    In the code that e.g. sends a Python object from A to B over the network:\n    >>> from distributed.metrics import context_meter\n    >>> with context_meter.add_callback(partial(print, \"A->B comms:\")):\n    ...     await send_over_the_network(obj)\n\n    In the serialization utilities, called many stack levels below:\n    >>> with context_meter.meter(\"dumps\"):\n    ...     pik = pickle.dumps(obj)\n    >>> with context_meter.meter(\"compress\"):\n    ...     pik = lz4.compress(pik)\n\n    And finally, elsewhere, deep into the TCP stack:\n    >>> with context_meter.meter(\"network-write\"):\n    ...     await comm.write(frames)\n\n    When you call the top-level code, you'll get::\n      A->B comms: dumps 0.012 seconds\n      A->B comms: compress 0.034 seconds\n      A->B comms: network-write 0.567 seconds\n    \"\"\"\n\n    _callbacks: ContextVar[dict[Hashable, Callable[[Hashable, float, str], None]]]\n\n    def __init__(self):\n        self._callbacks = ContextVar(\n            f\"MetricHook<{id(self)}>._callbacks\", default={}  # noqa: B039\n        )\n\n    def __reduce__(self):\n        assert self is context_meter, \"Found copy of singleton\"\n        return self._unpickle_singleton, ()\n\n    @staticmethod\n    def _unpickle_singleton():\n        return context_meter\n\n    @contextmanager\n    def add_callback(\n        self,\n        callback: Callable[[Hashable, float, str], None],\n        *,\n        key: Hashable | None = None,\n        allow_offload: bool = False,\n    ) -> Iterator[None]:\n        \"\"\"Add a callback when entering the context and remove it when exiting it.\n        The callback must accept the same parameters as :meth:`digest_metric`.\n\n        Parameters\n        ----------\n        callback: Callable\n            ``f(label, value, unit)`` to be executed\n        key: Hashable, optional\n            Unique key for the callback. If two nested calls to ``add_callback`` use the\n            same key, suppress the outermost callback.\n        allow_offload: bool, optional\n            If set to True, this context must be executed inside a running asyncio\n            event loop. If a call to :meth:`digest_metric` is performed from a different\n            thread, e.g. from inside :func:`distributed.utils.offload`, ensure that\n            the callback is executed in the event loop's thread instead.\n        \"\"\"\n        if allow_offload:\n            loop = asyncio.get_running_loop()\n            tid = threading.get_ident()\n\n            def safe_cb(label: Hashable, value: float, unit: str, /) -> None:\n                if threading.get_ident() == tid:\n                    callback(label, value, unit)\n                else:  # We're inside offload()\n                    loop.call_soon_threadsafe(callback, label, value, unit)\n\n        else:\n            safe_cb = callback\n\n        if key is None:\n            key = object()\n        cbs = self._callbacks.get()\n        cbs = cbs.copy()\n        cbs[key] = safe_cb\n        tok = self._callbacks.set(cbs)\n        try:\n            yield\n        finally:\n            tok.var.reset(tok)\n\n    @contextmanager\n    def clear_callbacks(self) -> Iterator[None]:\n        \"\"\"Do not trigger any callbacks set outside of this context\"\"\"\n        tok = self._callbacks.set({})\n        try:\n            yield\n        finally:\n            tok.var.reset(tok)\n\n    def digest_metric(self, label: Hashable, value: float, unit: str) -> None:\n        \"\"\"Invoke the currently set context callbacks for an arbitrary quantitative\n        metric.\n        \"\"\"\n        cbs = self._callbacks.get()\n        for cb in cbs.values():\n            cb(label, value, unit)\n\n    @contextmanager\n    def meter(\n        self,\n        label: Hashable,\n        unit: str = \"seconds\",\n        func: Callable[[], float] = timemod.perf_counter,\n        floor: float | Literal[False] = 0.0,\n    ) -> Iterator[MeterOutput]:\n        \"\"\"Convenience context manager or decorator which calls func() before and after\n        the wrapped code, calculates the delta, and finally calls :meth:`digest_metric`.\n\n        If unit=='seconds', it also subtracts any other calls to :meth:`meter` or\n        :meth:`digest_metric` with the same unit performed within the context, so that\n        the total is strictly additive.\n\n        Parameters\n        ----------\n        label: Hashable\n            label to pass to the callback\n        unit: str, optional\n            unit to pass to the callback. Default: seconds\n        func: callable\n            see :func:`meter`\n        floor: bool, optional\n            see :func:`meter`\n\n        Yields\n        ------\n        :class:`MeterOutput` where the ``start`` attribute is populated straight away,\n        while ``stop`` and ``delta`` are nan until context exit. In case of multiple\n        nested calls to :meth:`meter`, then delta (for seconds only) is reduced by the\n        inner metrics, to a minimum of ``floor``.\n        \"\"\"\n        if unit != \"seconds\":\n            try:\n                with meter(func, floor=floor) as m:\n                    yield m\n            finally:\n                self.digest_metric(label, m.delta, unit)\n            return\n\n        # If unit==\"seconds\", subtract time metered from the sub-contexts\n        offsets = []\n\n        def callback(label2: Hashable, value2: float, unit2: str) -> None:\n            if unit2 == unit:\n                # This must be threadsafe to support callbacks invoked from\n                # distributed.utils.offload; '+=' on a float would not be threadsafe!\n                offsets.append(value2)\n\n        try:\n            with self.add_callback(callback), meter(func, floor=False) as m:\n                yield m\n        finally:\n            delta = m.delta - sum(offsets)\n            if floor is not False:\n                delta = max(floor, delta)\n            m.delta = delta\n            self.digest_metric(label, delta, unit)\n\n\ncontext_meter = ContextMeter()\n\n\nclass DelayedMetricsLedger:\n    \"\"\"Add-on to :class:`ContextMeter` that helps in the case where:\n\n    - The code to be metered is not easily expressed as a self-contained code block\n      e.g. you want to measure latency in the asyncio event loop before and after\n      running a task\n    - You want to alter the metrics depending on how the code ends; e.g. you want to\n      post them differently in case of failure.\n\n    Examples\n    --------\n    >>> ledger = DelayedMetricsLedger()  # Metering starts here\n    >>> async def wrapper():\n    ...     with ledger.record():\n    ...         return await metered_function()\n    >>> task = asyncio.create_task(wrapper())\n    >>> # (later, elsewhere)\n    >>> try:\n    ...     await task\n    ...     coarse_time = False\n    ... except Exception:\n    ...     coarse_time = \"failed\"\n    ...     raise\n    ... finally:\n    ...     # Metering stops here\n    ...     for label, value, unit in ledger.finalize(coarse_time):\n    ...         # actually log metrics\n    \"\"\"\n\n    func: Callable[[], float]\n    start: float\n    metrics: list[tuple[Hashable, float, str]]  # (label, value, unit)\n\n    def __init__(self, func: Callable[[], float] = timemod.perf_counter):\n        self.func = func\n        self.start = func()\n        self.metrics = []\n\n    def _callback(self, label: Hashable, value: float, unit: str) -> None:\n        self.metrics.append((label, value, unit))\n\n    @contextmanager\n    def record(self, *, key: Hashable | None = None) -> Iterator[None]:\n        \"\"\"Ingest metrics logged with :meth:`ContextMeter.digest_metric` or\n        :meth:`ContextMeter.meter` and temporarily store them in :ivar:`metrics`.\n\n        Parameters\n        ----------\n        key: Hashable, optional\n            See :meth:`ContextMeter.add_callback`\n        \"\"\"\n        with context_meter.add_callback(self._callback, key=key):\n            yield\n\n    def finalize(\n        self,\n        coarse_time: str | Literal[False] = False,\n        floor: float | Literal[False] = 0.0,\n    ) -> Iterator[tuple[Hashable, float, str]]:\n        \"\"\"The metered code is terminated, and we now know how to log it.\n\n        Parameters\n        ----------\n        coarse_time: str | False, optional\n            False\n                Yield all acquired metrics, plus an extra time metric, labelled \"other\",\n                which is the time between creating the DelayedMetricsLedger and\n                calling this method, minus any time logged in the metrics.\n            label\n                Yield all acquired non-time metrics.\n                Yield a single metric, labelled <coarse_time>, which is the time\n                between creating the DelayedMetricsLedger and calling this method.\n        floor: float | False, optional\n            Floor either the \"other\" or the <coarse_time> metric to this value\n             (default: 0). Set to False to disable.\n        \"\"\"\n        stop = self.func()\n        delta = stop - self.start\n\n        for label, value, unit in self.metrics:\n            if unit != \"seconds\" or not coarse_time:\n                yield label, value, unit\n            if unit == \"seconds\" and not coarse_time:\n                delta -= value\n\n        if floor is not False:\n            delta = max(floor, delta)\n        yield coarse_time or \"other\", delta, \"seconds\"\n\n\n### Dependency File: utils.py\nfrom __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport contextvars\nimport functools\nimport importlib\nimport inspect\nimport json\nimport logging\nimport multiprocessing\nimport os\nimport pkgutil\nimport re\nimport socket\nimport sys\nimport tempfile\nimport threading\nimport traceback\nimport warnings\nimport weakref\nimport xml.etree.ElementTree\nfrom asyncio import Event as LateLoopEvent\nfrom asyncio import TimeoutError\nfrom collections import deque\nfrom collections.abc import (\n    Awaitable,\n    Callable,\n    Collection,\n    Container,\n    Generator,\n    Iterator,\n    KeysView,\n    ValuesView,\n)\nfrom concurrent.futures import (  # noqa: F401\n    CancelledError,\n    Executor,\n    ThreadPoolExecutor,\n)\nfrom contextvars import ContextVar\nfrom datetime import timedelta\nfrom functools import wraps\nfrom hashlib import md5\nfrom importlib.util import cache_from_source\nfrom pickle import PickleBuffer\nfrom time import sleep\nfrom types import ModuleType\nfrom typing import TYPE_CHECKING\nfrom typing import Any as AnyType\nfrom typing import ClassVar, TypeVar, overload\n\nimport psutil\nimport tblib.pickling_support\nfrom tornado import escape\n\nfrom distributed.compatibility import asyncio_run\nfrom distributed.config import get_loop_factory\n\ntry:\n    import resource\nexcept ImportError:\n    resource = None  # type: ignore\n\nimport tlz as toolz\nfrom tornado import gen\nfrom tornado.ioloop import IOLoop\n\nimport dask\nfrom dask.utils import ensure_bytes as _ensure_bytes\nfrom dask.utils import key_split\nfrom dask.utils import parse_timedelta as _parse_timedelta\nfrom dask.widgets import get_template\n\nfrom distributed.compatibility import WINDOWS\nfrom distributed.metrics import context_meter, monotonic, time\n\ntry:\n    from dask.context import thread_state\nexcept ImportError:\n    thread_state = threading.local()\n\n# For some reason this is required in python >= 3.9\nif WINDOWS:\n    import multiprocessing.popen_spawn_win32\nelse:\n    import multiprocessing.popen_spawn_posix\n\nlogger = _logger = logging.getLogger(__name__)\n\n\nif TYPE_CHECKING:\n    # TODO: import from typing (requires Python >=3.10)\n    from typing_extensions import ParamSpec\n\n    P = ParamSpec(\"P\")\n    T = TypeVar(\"T\")\n\n_forkserver_preload_set = False\n\n\ndef get_mp_context():\n    \"\"\"Create a multiprocessing context\n\n    The context type is controlled by the\n    ``distributed.worker.multiprocessing-method`` configuration key.\n\n    Returns\n    -------\n    multiprocessing.BaseContext\n        The multiprocessing context\n\n    Notes\n    -----\n    Repeated calls with the same method will return the same object\n    (since multiprocessing.get_context returns singleton instances).\n    \"\"\"\n    global _forkserver_preload_set\n    method = dask.config.get(\"distributed.worker.multiprocessing-method\")\n    ctx = multiprocessing.get_context(method)\n    if method == \"forkserver\" and not _forkserver_preload_set:\n        # Makes the test suite much faster\n        preload = [\"distributed\"]\n\n        from distributed.versions import optional_packages, required_packages\n\n        for pkg, _ in required_packages + optional_packages:\n            try:\n                importlib.import_module(pkg)\n            except ImportError:\n                pass\n            else:\n                preload.append(pkg)\n        ctx.set_forkserver_preload(preload)\n        _forkserver_preload_set = True\n\n    return ctx\n\n\ndef has_arg(func, argname):\n    \"\"\"\n    Whether the function takes an argument with the given name.\n    \"\"\"\n    while True:\n        try:\n            argspec = inspect.getfullargspec(func)\n            if argname in set(argspec.args) | set(argspec.kwonlyargs):\n                return True\n        except TypeError:\n            break\n        try:\n            # For Tornado coroutines and other decorated functions\n            func = func.__wrapped__\n        except AttributeError:\n            break\n    return False\n\n\ndef get_fileno_limit():\n    \"\"\"\n    Get the maximum number of open files per process.\n    \"\"\"\n    if resource is not None:\n        return resource.getrlimit(resource.RLIMIT_NOFILE)[0]\n    else:\n        # Default ceiling for Windows when using the CRT, though it\n        # is settable using _setmaxstdio().\n        return 512\n\n\n@toolz.memoize\ndef _get_ip(host, port, family):\n    def hostname_fallback():\n        addr_info = socket.getaddrinfo(\n            socket.gethostname(), port, family, socket.SOCK_DGRAM, socket.IPPROTO_UDP\n        )[0]\n        return addr_info[4][0]\n\n    # By using a UDP socket, we don't actually try to connect but\n    # simply select the local address through which *host* is reachable.\n    sock = socket.socket(family, socket.SOCK_DGRAM)\n    try:\n        sock.connect((host, port))\n        ip = sock.getsockname()[0]\n        if ip == \"0.0.0.0\":\n            return hostname_fallback()\n        return ip\n    except OSError as e:\n        warnings.warn(\n            \"Couldn't detect a suitable IP address for \"\n            \"reaching %r, defaulting to hostname: %s\" % (host, e),\n            RuntimeWarning,\n        )\n        return hostname_fallback()\n    finally:\n        sock.close()\n\n\ndef get_ip(host=\"8.8.8.8\", port=80):\n    \"\"\"\n    Get the local IP address through which the *host* is reachable.\n\n    *host* defaults to a well-known Internet host (one of Google's public\n    DNS servers).\n    \"\"\"\n    return _get_ip(host, port, family=socket.AF_INET)\n\n\ndef get_ipv6(host=\"2001:4860:4860::8888\", port=80):\n    \"\"\"\n    The same as get_ip(), but for IPv6.\n    \"\"\"\n    return _get_ip(host, port, family=socket.AF_INET6)\n\n\ndef get_ip_interface(ifname):\n    \"\"\"\n    Get the local IPv4 address of a network interface.\n\n    KeyError is raised if the interface doesn't exist.\n    ValueError is raised if the interface does no have an IPv4 address\n    associated with it.\n    \"\"\"\n    net_if_addrs = psutil.net_if_addrs()\n\n    if ifname not in net_if_addrs:\n        allowed_ifnames = list(net_if_addrs.keys())\n        raise ValueError(\n            \"{!r} is not a valid network interface. \"\n            \"Valid network interfaces are: {}\".format(ifname, allowed_ifnames)\n        )\n\n    for info in net_if_addrs[ifname]:\n        if info.family == socket.AF_INET:\n            return info.address\n    raise ValueError(f\"interface {ifname!r} doesn't have an IPv4 address\")\n\n\nasync def All(args, quiet_exceptions=()):\n    \"\"\"Wait on many tasks at the same time\n\n    Err once any of the tasks err.\n\n    See https://github.com/tornadoweb/tornado/issues/1546\n\n    Parameters\n    ----------\n    args: futures to wait for\n    quiet_exceptions: tuple, Exception\n        Exception types to avoid logging if they fail\n    \"\"\"\n    tasks = gen.WaitIterator(*map(asyncio.ensure_future, args))\n    results = [None for _ in args]\n    while not tasks.done():\n        try:\n            result = await tasks.next()\n        except Exception:\n\n            @gen.coroutine\n            def quiet():\n                \"\"\"Watch unfinished tasks\n\n                Otherwise if they err they get logged in a way that is hard to\n                control.  They need some other task to watch them so that they\n                are not orphaned\n                \"\"\"\n                for task in list(tasks._unfinished):\n                    try:\n                        yield task\n                    except quiet_exceptions:\n                        pass\n\n            quiet()\n            raise\n        results[tasks.current_index] = result\n    return results\n\n\nasync def Any(args, quiet_exceptions=()):\n    \"\"\"Wait on many tasks at the same time and return when any is finished\n\n    Err once any of the tasks err.\n\n    Parameters\n    ----------\n    args: futures to wait for\n    quiet_exceptions: tuple, Exception\n        Exception types to avoid logging if they fail\n    \"\"\"\n    tasks = gen.WaitIterator(*map(asyncio.ensure_future, args))\n    results = [None for _ in args]\n    while not tasks.done():\n        try:\n            result = await tasks.next()\n        except Exception:\n\n            @gen.coroutine\n            def quiet():\n                \"\"\"Watch unfinished tasks\n\n                Otherwise if they err they get logged in a way that is hard to\n                control.  They need some other task to watch them so that they\n                are not orphaned\n                \"\"\"\n                for task in list(tasks._unfinished):\n                    try:\n                        yield task\n                    except quiet_exceptions:\n                        pass\n\n            quiet()\n            raise\n\n        results[tasks.current_index] = result\n        break\n    return results\n\n\nclass NoOpAwaitable:\n    \"\"\"An awaitable object that always returns None.\n\n    Useful to return from a method that can be called in both asynchronous and\n    synchronous contexts\"\"\"\n\n    def __await__(self):\n        async def f():\n            return None\n\n        return f().__await__()\n\n\nclass SyncMethodMixin:\n    \"\"\"\n    A mixin for adding an `asynchronous` attribute and `sync` method to a class.\n\n    Subclasses must define a `loop` attribute for an associated\n    `tornado.IOLoop`, and may also add a `_asynchronous` attribute indicating\n    whether the class should default to asynchronous behavior.\n    \"\"\"\n\n    @property\n    def asynchronous(self):\n        \"\"\"Are we running in the event loop?\"\"\"\n        try:\n            return in_async_call(\n                self.loop, default=getattr(self, \"_asynchronous\", False)\n            )\n        except RuntimeError:\n            return False\n\n    def sync(self, func, *args, asynchronous=None, callback_timeout=None, **kwargs):\n        \"\"\"Call `func` with `args` synchronously or asynchronously depending on\n        the calling context\"\"\"\n        callback_timeout = _parse_timedelta(callback_timeout)\n        if asynchronous is None:\n            asynchronous = self.asynchronous\n        if asynchronous:\n            future = func(*args, **kwargs)\n            if callback_timeout is not None:\n                future = wait_for(future, callback_timeout)\n            return future\n        else:\n            return sync(\n                self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\n            )\n\n\ndef in_async_call(loop, default=False):\n    \"\"\"Whether this call is currently within an async call\"\"\"\n    try:\n        return loop.asyncio_loop is asyncio.get_running_loop()\n    except RuntimeError:\n        # No *running* loop in thread. If the event loop isn't running, it\n        # _could_ be started later in this thread though. Return the default.\n        if not loop.asyncio_loop.is_running():\n            return default\n        return False\n\n\ndef sync(\n    loop: IOLoop,\n    func: Callable[..., Awaitable[T]],\n    *args: AnyType,\n    callback_timeout: str | float | timedelta | None = None,\n    **kwargs: AnyType,\n) -> T:\n    \"\"\"\n    Run coroutine in loop running in separate thread.\n    \"\"\"\n    timeout = _parse_timedelta(callback_timeout, \"s\")\n    if loop.asyncio_loop.is_closed():  # type: ignore[attr-defined]\n        raise RuntimeError(\"IOLoop is closed\")\n\n    e = threading.Event()\n    main_tid = threading.get_ident()\n\n    # set up non-locals\n    result: T\n    error: BaseException | None = None\n    future: asyncio.Future[T]\n\n    @gen.coroutine\n    def f() -> Generator[AnyType, AnyType, None]:\n        nonlocal result, error, future\n        try:\n            if main_tid == threading.get_ident():\n                raise RuntimeError(\"sync() called from thread of running loop\")\n            yield gen.moment\n            awaitable = func(*args, **kwargs)\n            if timeout is not None:\n                awaitable = wait_for(awaitable, timeout)\n            future = asyncio.ensure_future(awaitable)\n            result = yield future\n        except Exception as exception:\n            error = exception\n        finally:\n            e.set()\n\n    def cancel() -> None:\n        if future is not None:\n            future.cancel()\n\n    def wait(timeout: float | None) -> bool:\n        try:\n            return e.wait(timeout)\n        except KeyboardInterrupt:\n            loop.add_callback(cancel)\n            raise\n\n    loop.add_callback(f)\n    if timeout is not None:\n        if not wait(timeout):\n            raise TimeoutError(f\"timed out after {timeout} s.\")\n    else:\n        while not e.is_set():\n            wait(10)\n\n    if error is not None:\n        raise error\n    else:\n        return result\n\n\nclass _CollectErrorThread:\n    def __init__(self, target: Callable[[], None], daemon: bool, name: str):\n        self._exception: BaseException | None = None\n\n        def wrapper() -> None:\n            try:\n                target()\n            except BaseException as e:  # noqa: B036\n                self._exception = e\n\n        self._thread = thread = threading.Thread(\n            target=wrapper, daemon=daemon, name=name\n        )\n        thread.start()\n\n    def join(self, timeout: float | None = None) -> None:\n        thread = self._thread\n        thread.join(timeout=timeout)\n        if thread.is_alive():\n            raise TimeoutError(\"join timed out\")\n        if self._exception is not None:\n            try:\n                raise self._exception\n            finally:  # remove a reference cycle\n                del self._exception\n\n\nclass LoopRunner:\n    \"\"\"\n    A helper to start and stop an IO loop in a controlled way.\n    Several loop runners can associate safely to the same IO loop.\n\n    Parameters\n    ----------\n    loop: IOLoop (optional)\n        If given, this loop will be re-used, otherwise an appropriate one\n        will be looked up or created.\n    asynchronous: boolean (optional, default False)\n        If false (the default), the loop is meant to run in a separate\n        thread and will be started if necessary.\n        If true, the loop is meant to run in the thread this\n        object is instantiated from, and will not be started automatically.\n    \"\"\"\n\n    # All loops currently associated to loop runners\n    _all_loops: ClassVar[\n        weakref.WeakKeyDictionary[IOLoop, tuple[int, LoopRunner | None]]\n    ] = weakref.WeakKeyDictionary()\n    _lock = threading.Lock()\n    _loop_thread: _CollectErrorThread | None\n\n    def __init__(self, loop: IOLoop | None = None, asynchronous: bool = False):\n        if loop is None:\n            if asynchronous:\n                # raises RuntimeError if there's no running loop\n                try:\n                    asyncio.get_running_loop()\n                except RuntimeError as e:\n                    raise RuntimeError(\n                        \"Constructing LoopRunner(asynchronous=True) without a running loop is not supported\"\n                    ) from e\n                loop = IOLoop.current()\n        elif not loop.asyncio_loop.is_running():  # type: ignore[attr-defined]\n            # LoopRunner is not responsible for starting a foreign IOLoop\n            raise RuntimeError(\n                \"Constructing LoopRunner(loop=loop) without a running loop is not supported\"\n            )\n\n        self._loop = loop\n        self._asynchronous = asynchronous\n        self._loop_thread = None\n        self._started = False\n        self._stop_event = LateLoopEvent()\n\n    def start(self):\n        \"\"\"\n        Start the IO loop if required.  The loop is run in a dedicated\n        thread.\n\n        If the loop is already running, this method does nothing.\n        \"\"\"\n        with self._lock:\n            self._start_unlocked()\n\n    def _start_unlocked(self) -> None:\n        assert not self._started\n\n        if self._loop is not None:\n            try:\n                count, real_runner = self._all_loops[self._loop]\n            except KeyError:\n                assert self._loop.asyncio_loop.is_running()  # type: ignore[attr-defined]\n                self._started = True\n                return\n\n            self._all_loops[self._loop] = count + 1, real_runner\n            self._started = True\n            return\n\n        assert self._loop_thread is None\n\n        start_evt = threading.Event()\n        start_exc = None\n        loop = None\n\n        async def amain() -> None:\n            nonlocal loop\n            loop = IOLoop.current()\n            start_evt.set()\n            await self._stop_event.wait()\n\n        def run_loop() -> None:\n            nonlocal start_exc\n            try:\n                asyncio_run(amain(), loop_factory=get_loop_factory())\n            except BaseException as e:\n                if start_evt.is_set():\n                    raise\n                start_exc = e\n                start_evt.set()\n\n        self._loop_thread = _CollectErrorThread(\n            target=run_loop, daemon=True, name=\"IO loop\"\n        )\n        start_evt.wait(timeout=10)\n        if start_exc is not None:\n            raise start_exc\n        assert loop is not None\n        self._loop = loop\n        self._started = True\n        self._all_loops[loop] = (1, self)\n\n    def stop(self, timeout=10):\n        \"\"\"\n        Stop and close the loop if it was created by us.\n        Otherwise, just mark this object \"stopped\".\n        \"\"\"\n        with self._lock:\n            self._stop_unlocked(timeout)\n\n    def _stop_unlocked(self, timeout):\n        if not self._started:\n            return\n\n        self._started = False\n\n        try:\n            count, real_runner = self._all_loops[self._loop]\n        except KeyError:\n            return\n\n        if count > 1:\n            self._all_loops[self._loop] = count - 1, real_runner\n            return\n\n        assert count == 1\n        del self._all_loops[self._loop]\n        real_runner._real_stop(timeout)\n\n    def _real_stop(self, timeout):\n        assert self._loop_thread is not None\n        try:\n            self._loop.add_callback(self._stop_event.set)\n            self._loop_thread.join(timeout=timeout)\n        finally:\n            self._loop_thread = None\n\n    def is_started(self):\n        \"\"\"\n        Return True between start() and stop() calls, False otherwise.\n        \"\"\"\n        return self._started\n\n    def run_sync(self, func, *args, **kwargs):\n        \"\"\"\n        Convenience helper: start the loop if needed,\n        run sync(func, *args, **kwargs), then stop the loop again.\n        \"\"\"\n        if self._started:\n            return sync(self.loop, func, *args, **kwargs)\n        else:\n            self.start()\n            try:\n                return sync(self.loop, func, *args, **kwargs)\n            finally:\n                self.stop()\n\n    @property\n    def loop(self):\n        loop = self._loop\n        if loop is None or not loop.asyncio_loop.is_running():\n            raise RuntimeError(\n                \"Accessing the loop property while the loop is not running is not supported\"\n            )\n        return self._loop\n\n\n@contextlib.contextmanager\ndef set_thread_state(**kwargs):\n    old = {}\n    for k in kwargs:\n        try:\n            old[k] = getattr(thread_state, k)\n        except AttributeError:\n            pass\n    for k, v in kwargs.items():\n        setattr(thread_state, k, v)\n    try:\n        yield\n    finally:\n        for k in kwargs:\n            try:\n                v = old[k]\n            except KeyError:\n                delattr(thread_state, k)\n            else:\n                setattr(thread_state, k, v)\n\n\n@contextlib.contextmanager\ndef tmp_text(filename, text):\n    fn = os.path.join(tempfile.gettempdir(), filename)\n    with open(fn, \"w\") as f:\n        f.write(text)\n\n    try:\n        yield fn\n    finally:\n        if os.path.exists(fn):\n            os.remove(fn)\n\n\ndef is_kernel():\n    \"\"\"Determine if we're running within an IPython kernel\n\n    >>> is_kernel()\n    False\n    \"\"\"\n    # http://stackoverflow.com/questions/34091701/determine-if-were-in-an-ipython-notebook-session\n    if \"IPython\" not in sys.modules:  # IPython hasn't been imported\n        return False\n    from IPython import get_ipython\n\n    # check for `kernel` attribute on the IPython instance\n    return getattr(get_ipython(), \"kernel\", None) is not None\n\n\ndef key_split_group(x: object) -> str:\n    \"\"\"A more fine-grained version of key_split.\n\n    >>> key_split_group(('x-2', 1))\n    'x-2'\n    >>> key_split_group(\"('x-2', 1)\")\n    'x-2'\n    >>> key_split_group('ae05086432ca935f6eba409a8ecd4896')\n    'data'\n    >>> key_split_group('<module.submodule.myclass object at 0xdaf372')\n    'myclass'\n    >>> key_split_group('x')\n    'x'\n    >>> key_split_group('x-1')\n    'x'\n    \"\"\"\n    if isinstance(x, tuple):\n        return x[0]\n    elif isinstance(x, str):\n        if x[0] == \"(\":\n            return x.split(\",\", 1)[0].strip(\"()\\\"'\")\n        elif len(x) == 32 and re.match(r\"[a-f0-9]{32}\", x):\n            return \"data\"\n        elif x[0] == \"<\":\n            return x.strip(\"<>\").split()[0].split(\".\")[-1]\n        else:\n            return key_split(x)\n    elif isinstance(x, bytes):\n        return key_split_group(x.decode())\n    else:\n        return \"Other\"\n\n\n@overload\ndef log_errors(func: Callable[P, T], /) -> Callable[P, T]: ...\n\n\n@overload\ndef log_errors(*, pdb: bool = False, unroll_stack: int = 1) -> _LogErrors: ...\n\n\ndef log_errors(func=None, /, *, pdb=False, unroll_stack=0):\n    \"\"\"Log any errors and then reraise them.\n\n    This can be used:\n\n    - As a context manager::\n\n        with log_errors(...):\n            ...\n\n    - As a bare function decorator::\n\n        @log_errors\n        def func(...):\n            ...\n\n    - As a function decorator with parameters::\n\n        @log_errors(...)\n        def func(...):\n            ...\n\n    Parameters\n    ----------\n    pdb: bool, optional\n        Set to True to break into the debugger in case of exception\n    unroll_stack: int, optional\n        Number of levels of stack to unroll when determining the module's name for the\n        purpose of logging. Normally you should omit this. Set to 1 if you are writing a\n        helper function, context manager, or decorator.\n    \"\"\"\n    le = _LogErrors(pdb=pdb, unroll_stack=unroll_stack)\n    return le(func) if func else le\n\n\n_getmodulename_with_path_map: dict[str, str] = {}\n\n\ndef _getmodulename_with_path(fname: str) -> str:\n    \"\"\"Variant of inspect.getmodulename that returns the full module path\"\"\"\n    try:\n        return _getmodulename_with_path_map[fname]\n    except KeyError:\n        pass\n\n    for modname, mod in sys.modules.copy().items():\n        fname2 = getattr(mod, \"__file__\", None)\n        if fname2:\n            _getmodulename_with_path_map[fname2] = modname\n\n    try:\n        return _getmodulename_with_path_map[fname]\n    except KeyError:  # pragma: nocover\n        return os.path.splitext(os.path.basename(fname))[0]\n\n\nclass _LogErrors:\n    __slots__ = (\"pdb\", \"unroll_stack\")\n\n    pdb: bool\n    unroll_stack: int\n\n    def __init__(self, pdb: bool, unroll_stack: int):\n        self.pdb = pdb\n        self.unroll_stack = unroll_stack\n\n    def __call__(self, func: Callable[P, T], /) -> Callable[P, T]:\n        self.unroll_stack += 1\n\n        if inspect.iscoroutinefunction(func):\n\n            async def wrapper(*args, **kwargs):\n                with self:\n                    return await func(*args, **kwargs)\n\n        else:\n\n            def wrapper(*args, **kwargs):\n                with self:\n                    return func(*args, **kwargs)\n\n        return wraps(func)(wrapper)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, tb):\n        from distributed.comm import CommClosedError\n\n        if not exc_type or issubclass(exc_type, (CommClosedError, gen.Return)):\n            return\n\n        stack = traceback.extract_tb(tb)\n        frame = stack[min(self.unroll_stack, len(stack) - 1)]\n        modname = _getmodulename_with_path(frame.filename)\n\n        try:\n            logger = logging.getLogger(modname)\n            logger.exception(exc_value)\n        except Exception:  # Interpreter teardown\n            pass  # pragma: nocover\n\n        if self.pdb:\n            import pdb  # pragma: nocover\n\n            pdb.set_trace()  # pragma: nocover\n\n\ndef silence_logging(level, root=\"distributed\"):\n    \"\"\"\n    Change all StreamHandlers for the given logger to the given level\n    \"\"\"\n    warnings.warn(\n        \"silence_logging is deprecated, call silence_logging_cmgr\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if isinstance(level, str):\n        level = getattr(logging, level.upper())\n\n    old = None\n    logger = logging.getLogger(root)\n    for handler in logger.handlers:\n        if isinstance(handler, logging.StreamHandler):\n            old = handler.level\n            handler.setLevel(level)\n\n    return old\n\n\n@contextlib.contextmanager\ndef silence_logging_cmgr(\n    level: str | int, root: str = \"distributed\"\n) -> Generator[None]:\n    \"\"\"\n    Temporarily change all StreamHandlers for the given logger to the given level\n    \"\"\"\n    if isinstance(level, str):\n        level = getattr(logging, level.upper())\n\n    logger = logging.getLogger(root)\n    with contextlib.ExitStack() as stack:\n        for handler in logger.handlers:\n            if isinstance(handler, logging.StreamHandler):\n                old = handler.level\n                if old != level:\n                    handler.setLevel(level)\n                    stack.callback(handler.setLevel, old)\n        yield\n\n\n@toolz.memoize\ndef ensure_ip(hostname):\n    \"\"\"Ensure that address is an IP address\n\n    Examples\n    --------\n    >>> ensure_ip('localhost')\n    '127.0.0.1'\n    >>> ensure_ip('')  # Maps as localhost for binding e.g. 'tcp://:8811'\n    '127.0.0.1'\n    >>> ensure_ip('123.123.123.123')  # pass through IP addresses\n    '123.123.123.123'\n    \"\"\"\n    if not hostname:\n        hostname = \"localhost\"\n\n    # Prefer IPv4 over IPv6, for compatibility\n    families = [socket.AF_INET, socket.AF_INET6]\n    for fam in families:\n        try:\n            results = socket.getaddrinfo(\n                hostname, 1234, fam, socket.SOCK_STREAM  # dummy port number\n            )\n        except socket.gaierror as e:\n            exc = e\n        else:\n            return results[0][4][0]\n\n    raise exc\n\n\ntblib.pickling_support.install()\n\n\ndef get_traceback():\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    bad = [\n        os.path.join(\"distributed\", \"worker\"),\n        os.path.join(\"distributed\", \"scheduler\"),\n        os.path.join(\"tornado\", \"gen.py\"),\n        os.path.join(\"concurrent\", \"futures\"),\n        os.path.join(\"dask\", \"_task_spec\"),\n    ]\n    while exc_traceback and any(\n        b in exc_traceback.tb_frame.f_code.co_filename for b in bad\n    ):\n        exc_traceback = exc_traceback.tb_next\n    return exc_traceback\n\n\ndef truncate_exception(e, n=10000):\n    \"\"\"Truncate exception to be about a certain length\"\"\"\n    if len(str(e)) > n:\n        try:\n            return type(e)(\"Long error message\", str(e)[:n])\n        except Exception:\n            return Exception(\"Long error message\", type(e), str(e)[:n])\n    else:\n        return e\n\n\ndef seek_delimiter(file, delimiter, blocksize):\n    \"\"\"Seek current file to next byte after a delimiter bytestring\n\n    This seeks the file to the next byte following the delimiter.  It does\n    not return anything.  Use ``file.tell()`` to see location afterwards.\n\n    Parameters\n    ----------\n    file: a file\n    delimiter: bytes\n        a delimiter like ``b'\\n'`` or message sentinel\n    blocksize: int\n        Number of bytes to read from the file at once.\n    \"\"\"\n\n    if file.tell() == 0:\n        return\n\n    last = b\"\"\n    while True:\n        current = file.read(blocksize)\n        if not current:\n            return\n        full = last + current\n        try:\n            i = full.index(delimiter)\n            file.seek(file.tell() - (len(full) - i) + len(delimiter))\n            return\n        except ValueError:\n            pass\n        last = full[-len(delimiter) :]\n\n\ndef read_block(f, offset, length, delimiter=None):\n    \"\"\"Read a block of bytes from a file\n\n    Parameters\n    ----------\n    f: file\n        File-like object supporting seek, read, tell, etc..\n    offset: int\n        Byte offset to start read\n    length: int\n        Number of bytes to read\n    delimiter: bytes (optional)\n        Ensure reading starts and stops at delimiter bytestring\n\n    If using the ``delimiter=`` keyword argument we ensure that the read\n    starts and stops at delimiter boundaries that follow the locations\n    ``offset`` and ``offset + length``.  If ``offset`` is zero then we\n    start at zero.  The bytestring returned WILL include the\n    terminating delimiter string.\n\n    Examples\n    --------\n\n    >>> from io import BytesIO  # doctest: +SKIP\n    >>> f = BytesIO(b'Alice, 100\\\\nBob, 200\\\\nCharlie, 300')  # doctest: +SKIP\n    >>> read_block(f, 0, 13)  # doctest: +SKIP\n    b'Alice, 100\\\\nBo'\n\n    >>> read_block(f, 0, 13, delimiter=b'\\\\n')  # doctest: +SKIP\n    b'Alice, 100\\\\nBob, 200\\\\n'\n\n    >>> read_block(f, 10, 10, delimiter=b'\\\\n')  # doctest: +SKIP\n    b'Bob, 200\\\\nCharlie, 300'\n    \"\"\"\n    if delimiter:\n        f.seek(offset)\n        seek_delimiter(f, delimiter, 2**16)\n        start = f.tell()\n        length -= start - offset\n\n        f.seek(start + length)\n        seek_delimiter(f, delimiter, 2**16)\n        end = f.tell()\n\n        offset = start\n        length = end - start\n\n    f.seek(offset)\n    bytes = f.read(length)\n    return bytes\n\n\ndef ensure_bytes(s):\n    \"\"\"Attempt to turn `s` into bytes.\n\n    Parameters\n    ----------\n    s : Any\n        The object to be converted. Will correctly handled\n\n        * str\n        * bytes\n        * objects implementing the buffer protocol (memoryview, ndarray, etc.)\n\n    Returns\n    -------\n    b : bytes\n\n    Raises\n    ------\n    TypeError\n        When `s` cannot be converted\n\n    Examples\n    --------\n    >>> ensure_bytes('123')\n    b'123'\n    >>> ensure_bytes(b'123')\n    b'123'\n    \"\"\"\n    warnings.warn(\n        \"`distributed.utils.ensure_bytes` is deprecated. \"\n        \"Please switch to `dask.utils.ensure_bytes`. \"\n        \"This will be removed in `2022.6.0`.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return _ensure_bytes(s)\n\n\ndef ensure_memoryview(obj: bytes | bytearray | memoryview | PickleBuffer) -> memoryview:\n    \"\"\"Ensure `obj` is a 1-D contiguous `uint8` `memoryview`\"\"\"\n    if not isinstance(obj, memoryview):\n        obj = memoryview(obj)\n\n    if not obj.nbytes:\n        # Drop `obj` reference to permit freeing underlying data\n        return memoryview(bytearray())\n    elif not obj.contiguous:\n        # Copy to contiguous form of expected shape & type\n        return memoryview(bytearray(obj))\n    elif obj.ndim != 1 or obj.format != \"B\":\n        # Perform zero-copy reshape & cast\n        # Use `PickleBuffer.raw()` as `memoryview.cast()` fails with F-order\n        # xref: https://github.com/python/cpython/issues/91484\n        return PickleBuffer(obj).raw()\n    else:\n        # Return `memoryview` as it already meets requirements\n        return obj\n\n\ndef open_port(host: str = \"\") -> int:\n    \"\"\"Return a probably-open port\n\n    There is a chance that this port will be taken by the operating system soon\n    after returning from this function.\n    \"\"\"\n    # http://stackoverflow.com/questions/2838244/get-open-tcp-port-in-python\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((host, 0))\n        s.listen(1)\n        port = s.getsockname()[1]\n        return port\n\n\ndef import_file(path: str) -> list[ModuleType]:\n    \"\"\"Loads modules for a file (.py, .zip, .egg)\"\"\"\n    directory, filename = os.path.split(path)\n    name, ext = os.path.splitext(filename)\n    names_to_import: list[str] = []\n    tmp_python_path: str | None = None\n\n    if ext in (\".py\",):  # , '.pyc'):\n        if directory not in sys.path:\n            tmp_python_path = directory\n        names_to_import.append(name)\n    if ext == \".py\":  # Ensure that no pyc file will be reused\n        cache_file = cache_from_source(path)\n        with contextlib.suppress(OSError):\n            os.remove(cache_file)\n    if ext in (\".egg\", \".zip\", \".pyz\"):\n        if path not in sys.path:\n            sys.path.insert(0, path)\n        names = (mod_info.name for mod_info in pkgutil.iter_modules([path]))\n        names_to_import.extend(names)\n\n    loaded: list[ModuleType] = []\n    if not names_to_import:\n        logger.warning(\"Found nothing to import from %s\", filename)\n    else:\n        importlib.invalidate_caches()\n        if tmp_python_path is not None:\n            sys.path.insert(0, tmp_python_path)\n        try:\n            for name in names_to_import:\n                logger.info(\"Reload module %s from %s file\", name, ext)\n                loaded.append(importlib.reload(importlib.import_module(name)))\n        finally:\n            if tmp_python_path is not None:\n                sys.path.remove(tmp_python_path)\n    return loaded\n\n\ndef asciitable(columns, rows):\n    \"\"\"Formats an ascii table for given columns and rows.\n\n    Parameters\n    ----------\n    columns : list\n        The column names\n    rows : list of tuples\n        The rows in the table. Each tuple must be the same length as\n        ``columns``.\n    \"\"\"\n    rows = [tuple(str(i) for i in r) for r in rows]\n    columns = tuple(str(i) for i in columns)\n    widths = tuple(max(max(map(len, x)), len(c)) for x, c in zip(zip(*rows), columns))\n    row_template = (\"|\" + (\" %%-%ds |\" * len(columns))) % widths\n    header = row_template % tuple(columns)\n    bar = \"+%s+\" % \"+\".join(\"-\" * (w + 2) for w in widths)\n    data = \"\\n\".join(row_template % r for r in rows)\n    return \"\\n\".join([bar, header, bar, data, bar])\n\n\ndef nbytes(frame, _bytes_like=(bytes, bytearray)):\n    \"\"\"Number of bytes of a frame or memoryview\"\"\"\n    if isinstance(frame, _bytes_like):\n        return len(frame)\n    else:\n        try:\n            return frame.nbytes\n        except AttributeError:\n            return len(frame)\n\n\ndef json_load_robust(fn, load=json.load, timeout=None):\n    \"\"\"Reads a JSON file from disk that may be being written as we read\"\"\"\n    deadline = Deadline.after(timeout)\n    while not deadline.expires or deadline.remaining:\n        if os.path.exists(fn):\n            try:\n                with open(fn) as f:\n                    cfg = load(f)\n                if cfg:\n                    return cfg\n            except (ValueError, KeyError):  # race with writing process\n                pass\n        sleep(0.1)\n    else:\n        raise TimeoutError(f\"Could not load file after {timeout}s.\")\n\n\nclass DequeHandler(logging.Handler):\n    \"\"\"A logging.Handler that records records into a deque\"\"\"\n\n    _instances: ClassVar[weakref.WeakSet[DequeHandler]] = weakref.WeakSet()\n\n    def __init__(self, *args, n=10000, **kwargs):\n        self.deque = deque(maxlen=n)\n        super().__init__(*args, **kwargs)\n        self._instances.add(self)\n\n    def emit(self, record):\n        self.deque.append(record)\n\n    def clear(self):\n        \"\"\"\n        Clear internal storage.\n        \"\"\"\n        self.deque.clear()\n\n    @classmethod\n    def clear_all_instances(cls):\n        \"\"\"\n        Clear the internal storage of all live DequeHandlers.\n        \"\"\"\n        for inst in list(cls._instances):\n            inst.clear()\n\n\ndef reset_logger_locks():\n    \"\"\"Python 2's logger's locks don't survive a fork event\n\n    https://github.com/dask/distributed/issues/1491\n    \"\"\"\n    for name in logging.Logger.manager.loggerDict.keys():\n        for handler in logging.getLogger(name).handlers:\n            handler.createLock()\n\n\n@functools.lru_cache(1000)\ndef has_keyword(func, keyword):\n    return keyword in inspect.signature(func).parameters\n\n\n@functools.lru_cache(1000)\ndef command_has_keyword(cmd, k):\n    # Click is a relatively expensive import\n    # That hurts startup time a little\n    import click\n\n    if cmd is not None:\n        if isinstance(cmd, str):\n            try:\n                from importlib import import_module\n\n                cmd = import_module(cmd)\n            except ImportError:\n                raise ImportError(\"Module for command %s is not available\" % cmd)\n\n        if isinstance(cmd.main, click.core.Command):\n            cmd = cmd.main\n        if isinstance(cmd, click.core.Command):\n            cmd_params = {\n                p.human_readable_name\n                for p in cmd.params\n                if isinstance(p, click.core.Option)\n            }\n            return k in cmd_params\n\n    return False\n\n\n# from bokeh.palettes import viridis\n# palette = viridis(18)\npalette = [\n    \"#440154\",\n    \"#471669\",\n    \"#472A79\",\n    \"#433C84\",\n    \"#3C4D8A\",\n    \"#355D8C\",\n    \"#2E6C8E\",\n    \"#287A8E\",\n    \"#23898D\",\n    \"#1E978A\",\n    \"#20A585\",\n    \"#2EB27C\",\n    \"#45BF6F\",\n    \"#64CB5D\",\n    \"#88D547\",\n    \"#AFDC2E\",\n    \"#D7E219\",\n    \"#FDE724\",\n]\n\n\n@toolz.memoize\ndef color_of(x, palette=palette):\n    h = md5(str(x).encode(), usedforsecurity=False)\n    n = int(h.hexdigest()[:8], 16)\n    return palette[n % len(palette)]\n\n\ndef iscoroutinefunction(f):\n    return inspect.iscoroutinefunction(f) or gen.is_coroutine_function(f)\n\n\n@contextlib.contextmanager\ndef warn_on_duration(duration: str | float | timedelta, msg: str) -> Iterator[None]:\n    \"\"\"Generate a UserWarning if the operation in this context takes longer than\n    *duration* and print *msg*\n\n    The message may include a format string `{duration}` which will be formatted\n    to include the actual duration it took\n    \"\"\"\n    start = time()\n    yield\n    stop = time()\n    diff = stop - start\n    if diff > _parse_timedelta(duration):\n        warnings.warn(msg.format(duration=diff), stacklevel=2)\n\n\ndef format_dashboard_link(host, port):\n    template = dask.config.get(\"distributed.dashboard.link\")\n    if dask.config.get(\"distributed.scheduler.dashboard.tls.cert\"):\n        scheme = \"https\"\n    else:\n        scheme = \"http\"\n    return template.format(\n        **toolz.merge(os.environ, dict(scheme=scheme, host=host, port=port))\n    )\n\n\ndef parse_ports(port: int | str | Collection[int] | None) -> list[int] | list[None]:\n    \"\"\"Parse input port information into list of ports\n\n    Parameters\n    ----------\n    port : int, str, list[int], None\n        Input port or ports. Can be an integer like 8787, a string for a\n        single port like \"8787\", string for a sequential range of ports like\n        \"8000:8200\", a collection of ints, or None.\n\n    Returns\n    -------\n    ports : list\n        List of ports\n\n    Examples\n    --------\n    A single port can be specified using an integer:\n\n    >>> parse_ports(8787)\n    [8787]\n\n    or a string:\n\n    >>> parse_ports(\"8787\")\n    [8787]\n\n    A sequential range of ports can be specified by a string which indicates\n    the first and last ports which should be included in the sequence of ports:\n\n    >>> parse_ports(\"8787:8790\")\n    [8787, 8788, 8789, 8790]\n\n    An input of ``None`` is also valid and can be used to indicate that no port\n    has been specified:\n\n    >>> parse_ports(None)\n    [None]\n\n    \"\"\"\n    if isinstance(port, str) and \":\" in port:\n        port_start, port_stop = map(int, port.split(\":\"))\n        if port_stop <= port_start:\n            raise ValueError(\n                \"When specifying a range of ports like port_start:port_stop, \"\n                \"port_stop must be greater than port_start, but got \"\n                f\"{port_start=} and {port_stop=}\"\n            )\n        return list(range(port_start, port_stop + 1))\n\n    if isinstance(port, str):\n        return [int(port)]\n\n    if isinstance(port, int) or port is None:\n        return [port]  # type: ignore\n\n    if isinstance(port, Collection):\n        if not all(isinstance(p, int) for p in port):\n            raise TypeError(port)\n        return list(port)  # type: ignore\n\n    raise TypeError(port)\n\n\nis_coroutine_function = iscoroutinefunction\n\n\nclass Log(str):\n    \"\"\"A container for newline-delimited string of log entries\"\"\"\n\n    def _repr_html_(self):\n        return get_template(\"log.html.j2\").render(log=self)\n\n\nclass Logs(dict):\n    \"\"\"A container for a dict mapping names to strings of log entries\"\"\"\n\n    def _repr_html_(self):\n        return get_template(\"logs.html.j2\").render(logs=self)\n\n\ndef cli_keywords(\n    d: dict[str, AnyType],\n    cls: Callable | None = None,\n    cmd: str | ModuleType | None = None,\n) -> list[str]:\n    \"\"\"Convert a kwargs dictionary into a list of CLI keywords\n\n    Parameters\n    ----------\n    d : dict\n        The keywords to convert\n    cls : callable\n        The callable that consumes these terms to check them for validity\n    cmd : string or object\n        A string with the name of a module, or the module containing a\n        click-generated command with a \"main\" function, or the function itself.\n        It may be used to parse a module's custom arguments (that is, arguments that\n        are not part of Worker class), such as nworkers from dask worker CLI or\n        enable_nvlink from dask-cuda-worker CLI.\n\n    Examples\n    --------\n    >>> cli_keywords({\"x\": 123, \"save_file\": \"foo.txt\"})\n    ['--x', '123', '--save-file', 'foo.txt']\n\n    >>> from dask.distributed import Worker\n    >>> cli_keywords({\"x\": 123}, Worker)\n    Traceback (most recent call last):\n    ...\n    ValueError: Class distributed.worker.Worker does not support keyword x\n    \"\"\"\n    from dask.utils import typename\n\n    if cls or cmd:\n        for k in d:\n            if not has_keyword(cls, k) and not command_has_keyword(cmd, k):\n                if cls and cmd:\n                    raise ValueError(\n                        \"Neither class %s or module %s support keyword %s\"\n                        % (typename(cls), typename(cmd), k)\n                    )\n                elif cls:\n                    raise ValueError(\n                        f\"Class {typename(cls)} does not support keyword {k}\"\n                    )\n                else:\n                    raise ValueError(\n                        f\"Module {typename(cmd)} does not support keyword {k}\"\n                    )\n\n    def convert_value(v):\n        out = str(v)\n        if \" \" in out and \"'\" not in out and '\"' not in out:\n            out = '\"' + out + '\"'\n        return out\n\n    return sum(\n        ([\"--\" + k.replace(\"_\", \"-\"), convert_value(v)] for k, v in d.items()), []\n    )\n\n\ndef is_valid_xml(text):\n    return xml.etree.ElementTree.fromstring(text) is not None\n\n\n_offload_executor = ThreadPoolExecutor(max_workers=1, thread_name_prefix=\"Dask-Offload\")\n\n\ndef import_term(name: str) -> AnyType:\n    \"\"\"Return the fully qualified term\n\n    Examples\n    --------\n    >>> import_term(\"math.sin\") # doctest: +SKIP\n    <function math.sin(x, /)>\n    \"\"\"\n    try:\n        module_name, attr_name = name.rsplit(\".\", 1)\n    except ValueError:\n        return importlib.import_module(name)\n\n    module = importlib.import_module(module_name)\n    return getattr(module, attr_name)\n\n\nasync def run_in_executor_with_context(\n    executor: Executor | None,\n    func: Callable[P, T],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -> T:\n    \"\"\"Variant of :meth:`~asyncio.AbstractEventLoop.run_in_executor`, which\n    propagates contextvars.\n    Note that this limits the type of Executor to those that do not pickle objects.\n\n    See also\n    --------\n    asyncio.AbstractEventLoop.run_in_executor\n    offload\n    https://bugs.python.org/issue34014\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    context = contextvars.copy_context()\n    return await loop.run_in_executor(\n        executor, lambda: context.run(func, *args, **kwargs)\n    )\n\n\ndef offload(\n    func: Callable[P, T],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -> Awaitable[T]:\n    \"\"\"Run a synchronous function in a separate thread.\n    Unlike :meth:`asyncio.to_thread`, this propagates contextvars and offloads to an\n    ad-hoc thread pool with a single worker.\n\n    See also\n    --------\n    asyncio.to_thread\n    run_in_executor_with_context\n    https://bugs.python.org/issue34014\n    \"\"\"\n    with context_meter.meter(\"offload\"):\n        return run_in_executor_with_context(_offload_executor, func, *args, **kwargs)\n\n\nclass EmptyContext:\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n\n    async def __aenter__(self):\n        pass\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        pass\n\n\nempty_context = EmptyContext()\n\n\ndef clean_dashboard_address(addrs: AnyType, default_listen_ip: str = \"\") -> list[dict]:\n    \"\"\"\n    Examples\n    --------\n    >>> clean_dashboard_address(8787)\n    [{'address': '', 'port': 8787}]\n    >>> clean_dashboard_address(\":8787\")\n    [{'address': '', 'port': 8787}]\n    >>> clean_dashboard_address(\"8787\")\n    [{'address': '', 'port': 8787}]\n    >>> clean_dashboard_address(\"8787\")\n    [{'address': '', 'port': 8787}]\n    >>> clean_dashboard_address(\"foo:8787\")\n    [{'address': 'foo', 'port': 8787}]\n    >>> clean_dashboard_address([8787, 8887])\n    [{'address': '', 'port': 8787}, {'address': '', 'port': 8887}]\n    >>> clean_dashboard_address(\":8787,:8887\")\n    [{'address': '', 'port': 8787}, {'address': '', 'port': 8887}]\n    \"\"\"\n\n    if default_listen_ip == \"0.0.0.0\":\n        default_listen_ip = \"\"  # for IPV6\n\n    if isinstance(addrs, str):\n        addrs = addrs.split(\",\")\n    if not isinstance(addrs, list):\n        addrs = [addrs]\n\n    addresses = []\n    for addr in addrs:\n        try:\n            addr = int(addr)\n        except (TypeError, ValueError):\n            pass\n\n        if isinstance(addr, str):\n            addr = addr.split(\":\")\n\n        if isinstance(addr, (tuple, list)):\n            if len(addr) == 2:\n                host, port = (addr[0], int(addr[1]))\n            elif len(addr) == 1:\n                [host], port = addr, 0\n            else:\n                raise ValueError(addr)\n        elif isinstance(addr, int):\n            host = default_listen_ip\n            port = addr\n\n        addresses.append({\"address\": host, \"port\": port})\n    return addresses\n\n\n_deprecations = {\n    \"no_default\": \"dask.typing.no_default\",\n}\n\n\ndef __getattr__(name):\n    if name in _deprecations:\n        use_instead = _deprecations[name]\n\n        warnings.warn(\n            f\"{name} is deprecated and will be removed in a future release. \"\n            f\"Please use {use_instead} instead.\",\n            category=FutureWarning,\n            stacklevel=2,\n        )\n        return import_term(use_instead)\n    else:\n        raise AttributeError(f\"module {__name__} has no attribute {name}\")\n\n\n# Used internally by recursive_to_dict to stop infinite recursion. If an object has\n# already been encountered, a string representation will be returned instead. This is\n# necessary since we have multiple cyclic referencing data structures.\n_recursive_to_dict_seen: ContextVar[set[int]] = ContextVar(\"_recursive_to_dict_seen\")\n_to_dict_no_nest_flag = False\n\n\ndef recursive_to_dict(\n    obj: AnyType, *, exclude: Container[str] = (), members: bool = False\n) -> AnyType:\n    \"\"\"Recursively convert arbitrary Python objects to a JSON-serializable\n    representation. This is intended for debugging purposes only.\n\n    The following objects are supported:\n\n    list, tuple, set, frozenset, deque, dict, dict_keys, dict_values\n        Descended into these objects recursively. Python-specific collections are\n        converted to JSON-friendly variants.\n    Classes that define ``_to_dict(self, *, exclude: Container[str] = ())``:\n        Call the method and dump its output\n    Classes that define ``_to_dict_no_nest(self, *, exclude: Container[str] = ())``:\n        Like above, but prevents nested calls (see below)\n    Other Python objects\n        Dump the output of ``repr()``\n    Objects already encountered before, regardless of type\n        Dump the output of ``repr()``. This breaks circular references and shortens the\n        output.\n\n    Parameters\n    ----------\n    exclude:\n        A list of attribute names to be excluded from the dump.\n        This will be forwarded to the objects ``_to_dict`` methods and these methods\n        are required to accept this parameter.\n    members:\n        If True, convert the top-level Python object to a dict of its public members\n\n    **``_to_dict_no_nest`` vs. ``_to_dict``**\n\n    The presence of the ``_to_dict_no_nest`` method signals ``recursive_to_dict`` to\n    have a mutually exclusive full dict representation with other objects that also have\n    the ``_to_dict_no_nest``, regardless of their class. Only the outermost object in a\n    nested structure has the method invoked; all others are\n    dumped as their string repr instead, even if they were not encountered before.\n\n    Example:\n\n    .. code-block:: python\n\n        >>> class Person:\n        ...     def __init__(self, name):\n        ...         self.name = name\n        ...         self.children = []\n        ...         self.pets = []\n        ...\n        ...     def _to_dict_no_nest(self, exclude=()):\n        ...         return recursive_to_dict(self.__dict__, exclude=exclude)\n        ...\n        ...     def __repr__(self):\n        ...         return self.name\n\n        >>> class Pet:\n        ...     def __init__(self, name):\n        ...         self.name = name\n        ...         self.owners = []\n        ...\n        ...     def _to_dict_no_nest(self, exclude=()):\n        ...         return recursive_to_dict(self.__dict__, exclude=exclude)\n        ...\n        ...     def __repr__(self):\n        ...         return self.name\n\n        >>> alice = Person(\"Alice\")\n        >>> bob = Person(\"Bob\")\n        >>> charlie = Pet(\"Charlie\")\n        >>> alice.children.append(bob)\n        >>> alice.pets.append(charlie)\n        >>> bob.pets.append(charlie)\n        >>> charlie.owners[:] = [alice, bob]\n        >>> recursive_to_dict({\"people\": [alice, bob], \"pets\": [charlie]})\n        {\n            \"people\": [\n                {\"name\": \"Alice\", \"children\": [\"Bob\"], \"pets\": [\"Charlie\"]},\n                {\"name\": \"Bob\", \"children\": [], \"pets\": [\"Charlie\"]},\n            ],\n            \"pets\": [\n                {\"name\": \"Charlie\", \"owners\": [\"Alice\", \"Bob\"]},\n            ],\n        }\n\n    If we changed the methods to ``_to_dict``, the output would instead be:\n\n    .. code-block:: python\n\n        {\n            \"people\": [\n                {\n                    \"name\": \"Alice\",\n                    \"children\": [\n                        {\n                            \"name\": \"Bob\",\n                            \"children\": [],\n                            \"pets\": [{\"name\": \"Charlie\", \"owners\": [\"Alice\", \"Bob\"]}],\n                        },\n                    ],\n                    pets: [\"Charlie\"],\n                ],\n                \"Bob\",\n            ],\n            \"pets\": [\"Charlie\"],\n        }\n\n    Also notice that, if in the future someone will swap the creation of the\n    ``children`` and ``pets`` attributes inside ``Person.__init__``, the output with\n    ``_to_dict`` will change completely whereas the one with ``_to_dict_no_nest`` won't!\n    \"\"\"\n    if isinstance(obj, (int, float, bool, str)) or obj is None:\n        return obj\n    if isinstance(obj, (type, bytes)):\n        return repr(obj)\n\n    if members:\n        obj = {\n            k: v\n            for k, v in inspect.getmembers(obj)\n            if not k.startswith(\"_\") and k not in exclude and not callable(v)\n        }\n\n    # Prevent infinite recursion\n    try:\n        seen = _recursive_to_dict_seen.get()\n    except LookupError:\n        seen = set()\n    seen = seen.copy()\n    tok = _recursive_to_dict_seen.set(seen)\n    try:\n        if id(obj) in seen:\n            return repr(obj)\n\n        if hasattr(obj, \"_to_dict_no_nest\"):\n            global _to_dict_no_nest_flag\n            if _to_dict_no_nest_flag:\n                return repr(obj)\n\n            seen.add(id(obj))\n            _to_dict_no_nest_flag = True\n            try:\n                return obj._to_dict_no_nest(exclude=exclude)\n            finally:\n                _to_dict_no_nest_flag = False\n\n        seen.add(id(obj))\n\n        if hasattr(obj, \"_to_dict\"):\n            return obj._to_dict(exclude=exclude)\n        if isinstance(obj, (list, tuple, set, frozenset, deque, KeysView, ValuesView)):\n            return [recursive_to_dict(el, exclude=exclude) for el in obj]\n        if isinstance(obj, dict):\n            res = {}\n            for k, v in obj.items():\n                k = recursive_to_dict(k, exclude=exclude)\n                v = recursive_to_dict(v, exclude=exclude)\n                try:\n                    res[k] = v\n                except TypeError:\n                    res[str(k)] = v\n            return res\n\n        return repr(obj)\n    finally:\n        tok.var.reset(tok)\n\n\nclass Deadline:\n    \"\"\"Utility class tracking a deadline and the progress toward it\"\"\"\n\n    #: Expiry time of the deadline in seconds since the start of the monotonic timer\n    #: or None if the deadline never expires\n    expires_at_mono: float | None\n    #: Seconds since the epoch when the deadline was created\n    started_at_mono: float\n    #: Seconds since the start of the monotonic timer when the deadline was created\n    started_at: float\n\n    __slots__ = tuple(__annotations__)\n\n    def __init__(self, expires_at: float | None = None):\n        self.started_at = time()\n        self.started_at_mono = monotonic()\n        if expires_at is not None:\n            self.expires_at_mono = expires_at - self.started_at + self.started_at_mono\n        else:\n            self.expires_at_mono = None\n\n    @classmethod\n    def after(cls, duration: float | None = None) -> Deadline:\n        \"\"\"Create a new ``Deadline`` that expires in ``duration`` seconds\n        or never if ``duration`` is None\n        \"\"\"\n        inst = cls()\n        if duration is not None:\n            inst.expires_at_mono = inst.started_at_mono + duration\n        return inst\n\n    @property\n    def expires_at(self) -> float | None:\n        \"\"\"Expiry time of the deadline in seconds since the unix epoch\n        or None if the deadline never expires.\n\n        Note that this can change over time if the wall clock is adjusted by the OS.\n        \"\"\"\n        if (exp := self.expires_at_mono) is None:\n            return None\n        return exp - monotonic() + time()\n\n    @property\n    def duration(self) -> float | None:\n        \"\"\"Seconds between the creation and expiration time of the deadline\n        if the deadline expires, None otherwise\n        \"\"\"\n        if (exp := self.expires_at_mono) is None:\n            return None\n        return exp - self.started_at_mono\n\n    @property\n    def expires(self) -> bool:\n        \"\"\"Whether the deadline ever expires\"\"\"\n        return self.expires_at_mono is not None\n\n    @property\n    def elapsed(self) -> float:\n        \"\"\"Seconds that elapsed since the deadline was created\"\"\"\n        return monotonic() - self.started_at_mono\n\n    @property\n    def remaining(self) -> float | None:\n        \"\"\"Seconds remaining until the deadline expires if an expiry time is set,\n        None otherwise\n        \"\"\"\n        if (exp := self.expires_at_mono) is None:\n            return None\n        else:\n            return max(0, exp - monotonic())\n\n    @property\n    def expired(self) -> bool:\n        \"\"\"Whether the deadline has already expired\"\"\"\n        return self.remaining == 0\n\n\nclass RateLimiterFilter(logging.Filter):\n    \"\"\"A Logging filter that ensures a matching message is emitted at most every\n    `rate` seconds\"\"\"\n\n    pattern: re.Pattern\n    rate: float\n    _last_seen: float\n\n    def __init__(self, pattern: str, *, name: str = \"\", rate: str | float = \"10s\"):\n        super().__init__(name)\n        self.pattern = re.compile(pattern)\n        self.rate = _parse_timedelta(rate)\n        self._last_seen = -self.rate\n\n    def filter(self, record: logging.LogRecord) -> bool:\n        if self.pattern.match(record.msg):\n            now = monotonic()\n            if now - self._last_seen < self.rate:\n                return False\n            self._last_seen = now\n        return True\n\n    @classmethod\n    def reset_timer(cls, logger: logging.Logger | str) -> None:\n        \"\"\"Reset the timer on all RateLimiterFilters on a logger.\n        Useful in unit testing.\n        \"\"\"\n        if isinstance(logger, str):\n            logger = logging.getLogger(logger)\n        for filter in logger.filters:\n            if isinstance(filter, cls):\n                filter._last_seen = -filter.rate\n\n\nif sys.version_info >= (3, 11):\n\n    async def wait_for(fut: Awaitable[T], timeout: float) -> T:\n        async with asyncio.timeout(timeout):\n            return await fut\n\nelse:\n\n    async def wait_for(fut: Awaitable[T], timeout: float) -> T:\n        return await asyncio.wait_for(fut, timeout)\n\n\nclass TupleComparable:\n    \"\"\"Wrap object so that we can compare tuple, int or None\n\n    When comparing two objects of different types Python fails\n\n    >>> (1, 2) < 1\n    Traceback (most recent call last):\n        ...\n    TypeError: '<' not supported between instances of 'tuple' and 'int'\n\n    This class replaces None with 0, and wraps ints with tuples\n\n    >>> TupleComparable((1, 2)) < TupleComparable(1)\n    False\n    \"\"\"\n\n    __slots__ = (\"obj\",)\n\n    def __init__(self, obj):\n        if obj is None:\n            self.obj = (0,)\n        elif isinstance(obj, tuple):\n            self.obj = obj\n        elif isinstance(obj, (int, float)):\n            self.obj = (obj,)\n        else:\n            raise ValueError(f\"Object must be tuple, int, float or None, got {obj}\")\n\n    def __eq__(self, other):\n        return self.obj == other.obj\n\n    def __lt__(self, other):\n        return self.obj < other.obj\n\n\n@functools.cache\ndef url_escape(url, *args, **kwargs):\n    \"\"\"\n    Escape a URL path segment. Cache results for better performance.\n    \"\"\"\n    return escape.url_escape(url, *args, **kwargs)\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:18:01"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport dataclasses\nimport sys\nimport threading\nfrom collections.abc import Iterator, Sequence\nfrom time import sleep\n\nimport pytest\nfrom tlz import first\n\nfrom distributed.compatibility import WINDOWS\nfrom distributed.metrics import time\nfrom distributed.profile import (\n    call_stack,\n    create,\n    identifier,\n    info_frame,\n    ll_get_stack,\n    llprocess,\n    lock,\n    merge,\n    plot_data,\n    process,\n    watch,\n)\n\n\ndef test_basic():\n    def test_g():\n        sleep(0.01)\n\n    def test_h():\n        sleep(0.02)\n\n    def test_f():\n        for _ in range(100):\n            test_g()\n            test_h()\n\n    thread = threading.Thread(target=test_f)\n    thread.daemon = True\n    thread.start()\n\n    state = create()\n\n    for _ in range(100):\n        sleep(0.02)\n        frame = sys._current_frames()[thread.ident]\n        process(frame, None, state)\n\n    assert state[\"count\"] == 100\n    d = state\n    while len(d[\"children\"]) == 1:\n        d = first(d[\"children\"].values())\n\n    assert d[\"count\"] == 100\n    assert \"test_f\" in str(d[\"description\"])\n    g = [c for c in d[\"children\"].values() if \"test_g\" in str(c[\"description\"])][0]\n    h = [c for c in d[\"children\"].values() if \"test_h\" in str(c[\"description\"])][0]\n\n    assert g[\"count\"] < h[\"count\"]\n    assert 95 < g[\"count\"] + h[\"count\"] <= 100\n\n    pd = plot_data(state)\n    assert len(set(map(len, pd.values()))) == 1  # all same length\n    assert len(set(pd[\"color\"])) > 1  # different colors\n\n\n@pytest.mark.skipif(\n    WINDOWS, reason=\"no low-level profiler support for Windows available\"\n)\ndef test_basic_low_level():\n    pytest.importorskip(\"stacktrace\")\n\n    state = create()\n\n    for _ in range(100):\n        sleep(0.02)\n        frame = sys._current_frames()[threading.get_ident()]\n        llframes = {threading.get_ident(): ll_get_stack(threading.get_ident())}\n        for f in llframes.values():\n            if f is not None:\n                llprocess(f, None, state)\n\n    assert state[\"count\"] == 100\n    children = state.get(\"children\")\n    assert children\n    expected = \"<low-level>\"\n    for k, v in zip(children.keys(), children.values()):\n        desc = v.get(\"description\")\n        assert desc\n        filename = desc.get(\"filename\")\n        assert expected in k and filename == expected\n\n\ndef test_merge():\n    a1 = {\n        \"count\": 5,\n        \"identifier\": \"root\",\n        \"description\": \"a\",\n        \"children\": {\n            \"b\": {\n                \"count\": 3,\n                \"description\": \"b-func\",\n                \"identifier\": \"b\",\n                \"children\": {},\n            },\n            \"c\": {\n                \"count\": 2,\n                \"description\": \"c-func\",\n                \"identifier\": \"c\",\n                \"children\": {},\n            },\n        },\n    }\n\n    a2 = {\n        \"count\": 4,\n        \"description\": \"a\",\n        \"identifier\": \"root\",\n        \"children\": {\n            \"d\": {\n                \"count\": 2,\n                \"description\": \"d-func\",\n                \"children\": {},\n                \"identifier\": \"d\",\n            },\n            \"c\": {\n                \"count\": 2,\n                \"description\": \"c-func\",\n                \"children\": {},\n                \"identifier\": \"c\",\n            },\n        },\n    }\n\n    expected = {\n        \"count\": 9,\n        \"identifier\": \"root\",\n        \"description\": \"a\",\n        \"children\": {\n            \"b\": {\n                \"count\": 3,\n                \"description\": \"b-func\",\n                \"identifier\": \"b\",\n                \"children\": {},\n            },\n            \"d\": {\n                \"count\": 2,\n                \"description\": \"d-func\",\n                \"identifier\": \"d\",\n                \"children\": {},\n            },\n            \"c\": {\n                \"count\": 4,\n                \"description\": \"c-func\",\n                \"identifier\": \"c\",\n                \"children\": {},\n            },\n        },\n    }\n\n    assert merge(a1, a2) == expected\n\n\ndef test_merge_empty():\n    assert merge() == create()\n    assert merge(create()) == create()\n    assert merge(create(), create()) == create()\n\n\ndef test_call_stack():\n    frame = sys._current_frames()[threading.get_ident()]\n    L = call_stack(frame)\n    assert isinstance(L, list)\n    assert all(isinstance(s, str) for s in L)\n    assert \"test_call_stack\" in str(L[-1])\n\n\ndef test_identifier():\n    frame = sys._current_frames()[threading.get_ident()]\n    assert identifier(frame) == identifier(frame)\n    assert identifier(None) == identifier(None)\n\n\ndef test_watch():\n    stop_called = threading.Event()\n    watch_thread = None\n    start = time()\n\n    def stop():\n        if not stop_called.is_set():  # Run setup code\n            nonlocal watch_thread\n            nonlocal start\n            watch_thread = threading.current_thread()\n            start = time()\n            stop_called.set()\n        return time() > start + 0.500\n\n    try:\n        log = watch(interval=\"10ms\", cycle=\"50ms\", stop=stop)\n\n        stop_called.wait(2)\n        sleep(0.5)\n        assert 1 < len(log) < 10\n    finally:\n        stop_called.wait()\n        watch_thread.join()\n\n\ndef test_watch_requires_lock_to_run():\n    start = time()\n\n    stop_profiling_called = threading.Event()\n    profiling_thread = None\n\n    def stop_profiling():\n        if not stop_profiling_called.is_set():  # Run setup code\n            nonlocal profiling_thread\n            nonlocal start\n            profiling_thread = threading.current_thread()\n            start = time()\n            stop_profiling_called.set()\n        return time() > start + 0.500\n\n    release_lock = threading.Event()\n\n    def block_lock():\n        with lock:\n            release_lock.wait()\n\n    start_threads = threading.active_count()\n\n    # Block the lock over the entire duration of watch\n    blocking_thread = threading.Thread(target=block_lock, name=\"Block Lock\")\n    blocking_thread.daemon = True\n\n    try:\n        blocking_thread.start()\n\n        log = watch(interval=\"10ms\", cycle=\"50ms\", stop=stop_profiling)\n\n        start = time()  # wait until thread starts up\n        while threading.active_count() < start_threads + 2:\n            assert time() < start + 2\n            sleep(0.01)\n\n        sleep(0.5)\n        assert len(log) == 0\n        release_lock.set()\n    finally:\n        release_lock.set()\n        stop_profiling_called.wait()\n        blocking_thread.join()\n        profiling_thread.join()\n\n\n@dataclasses.dataclass(frozen=True)\nclass FakeCode:\n    co_filename: str\n    co_name: str\n    co_firstlineno: int\n    co_lnotab: bytes\n    co_lines_seq: Sequence[tuple[int, int, int | None]]\n    co_code: bytes\n\n    def co_lines(self) -> Iterator[tuple[int, int, int | None]]:\n        yield from self.co_lines_seq\n\n\nFAKE_CODE = FakeCode(\n    co_filename=\"<stdin>\",\n    co_name=\"example\",\n    co_firstlineno=1,\n    # https://github.com/python/cpython/blob/b68431fadb3150134ac6ccbf501cdfeaf4c75678/Objects/lnotab_notes.txt#L84\n    # generated from:\n    # def example():\n    #     for i in range(1):\n    #         if i >= 0:\n    #             pass\n    # example.__code__.co_lnotab\n    co_lnotab=b\"\\x00\\x01\\x0c\\x01\\x08\\x01\\x04\\xfe\",\n    # generated with list(example.__code__.co_lines())\n    co_lines_seq=[\n        (0, 12, 2),\n        (12, 20, 3),\n        (20, 22, 4),\n        (22, 24, None),\n        (24, 28, 2),\n    ],\n    # used in dis.findlinestarts as bytecode_len = len(code.co_code)\n    # https://github.com/python/cpython/blob/6f345d363308e3e6ecf0ad518ea0fcc30afde2a8/Lib/dis.py#L457\n    co_code=bytes(28),\n)\n\n\n@dataclasses.dataclass(frozen=True)\nclass FakeFrame:\n    f_lasti: int\n    f_code: FakeCode\n    f_lineno: int | None = None\n    f_back: FakeFrame | None = None\n    f_globals: dict[str, object] = dataclasses.field(default_factory=dict)\n\n\n@pytest.mark.parametrize(\n    \"f_lasti,f_lineno\",\n    [\n        (-1, 1),\n        (0, 2),\n        (1, 2),\n        (11, 2),\n        (12, 3),\n        (21, 4),\n        (22, 4),\n        (23, 4),\n        (24, 2),\n        (25, 2),\n        (26, 2),\n        (27, 2),\n        (100, 2),\n    ],\n)\ndef test_info_frame_f_lineno(f_lasti: int, f_lineno: int) -> None:\n    assert info_frame(FakeFrame(f_lasti=f_lasti, f_code=FAKE_CODE)) == {  # type: ignore\n        \"filename\": \"<stdin>\",\n        \"name\": \"example\",\n        \"line_number\": f_lineno,\n        \"line\": \"\",\n    }\n\n\n@pytest.mark.parametrize(\n    \"f_lasti,f_lineno\",\n    [\n        (-1, 1),\n        (0, 2),\n        (1, 2),\n        (11, 2),\n        (12, 3),\n        (21, 4),\n        (22, 4),\n        (23, 4),\n        (24, 2),\n        (25, 2),\n        (26, 2),\n        (27, 2),\n        (100, 2),\n    ],\n)\ndef test_call_stack_f_lineno(f_lasti: int, f_lineno: int) -> None:\n    assert call_stack(FakeFrame(f_lasti=f_lasti, f_code=FAKE_CODE)) == [  # type: ignore\n        f'  File \"<stdin>\", line {f_lineno}, in example\\n\\t'\n    ]\n\n\ndef test_stack_overflow():\n    old = sys.getrecursionlimit()\n    sys.setrecursionlimit(300)\n    try:\n        state = create()\n        frame = None\n\n        def f(i):\n            if i == 0:\n                nonlocal frame\n                frame = sys._current_frames()[threading.get_ident()]\n                return\n            else:\n                return f(i - 1)\n\n        f(sys.getrecursionlimit() - 100)\n        process(frame, None, state)\n        assert state[\"children\"]\n        assert state[\"count\"]\n        assert merge(state, state, state)\n\n    finally:\n        sys.setrecursionlimit(old)\n\n\nclass MockFrame:\n    def __init__(self, f_back=None, f_code=None):\n        self.f_back = self\n        l = []\n        self.f_code = l.append\n        self.f_lineno = 1\n\n\ndef test_builtin():\n    # https://github.com/dask/distributed/issues/8163\n    assert identifier(MockFrame()) == \"list.append:1\"\n\n    assert info_frame(MockFrame()) == {\n        \"filename\": \"<built-in>\",\n        \"name\": \"list.append\",\n        \"line_number\": 1,\n        \"line\": \"\",\n    }\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: distributed\nTest File Path: distributed\\test_system\\test_system.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport logging\nimport sys\n\nimport psutil\n\n__all__ = (\"memory_limit\", \"MEMORY_LIMIT\")\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef memory_limit() -> int:\n    \"\"\"Get the memory limit (in bytes) for this system.\n\n    Takes the minimum value from the following locations:\n\n    - Total system host memory\n    - Cgroups limit (if set)\n    - RSS rlimit (if set)\n    \"\"\"\n    limit = psutil.virtual_memory().total\n\n    # Check cgroups if available\n    # Note: can't use LINUX and WINDOWS constants as they upset mypy\n    if sys.platform == \"linux\":\n        path_used = None\n        for path in [\n            \"/sys/fs/cgroup/memory/memory.limit_in_bytes\",  # cgroups v1 hard limit\n            \"/sys/fs/cgroup/memory/memory.soft_limit_in_bytes\",  # cgroups v1 soft limit\n            \"/sys/fs/cgroup/memory.max\",  # cgroups v2 hard limit\n            \"/sys/fs/cgroup/memory.high\",  # cgroups v2 soft limit\n            \"/sys/fs/cgroup/memory.low\",  # cgroups v2 softest limit\n        ]:\n            try:\n                with open(path) as f:\n                    cgroups_limit = int(f.read())\n                if cgroups_limit > 0:\n                    path_used = path\n                    limit = min(limit, cgroups_limit)\n            except Exception:\n                pass\n        if path_used:\n            logger.debug(\n                \"Setting system memory limit based on cgroup value defined in %s\",\n                path_used,\n            )\n\n    # Check rlimit if available\n    if sys.platform != \"win32\":\n        try:\n            import resource\n\n            hard_limit = resource.getrlimit(resource.RLIMIT_RSS)[1]\n            if 0 < hard_limit < limit:\n                logger.debug(\n                    \"Limiting system memory based on RLIMIT_RSS to %s\", hard_limit\n                )\n                limit = hard_limit\n        except (ImportError, OSError):\n            pass\n\n    return limit\n\n\nMEMORY_LIMIT = memory_limit()\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-03 20:18:01"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport builtins\nimport io\nimport sys\n\nimport psutil\nimport pytest\n\nfrom distributed.compatibility import MACOS\nfrom distributed.system import memory_limit\n\n\ndef test_memory_limit():\n    limit = memory_limit()\n    assert isinstance(limit, int)\n    assert limit <= psutil.virtual_memory().total\n    assert limit >= 1\n\n\ndef test_hard_memory_limit_cgroups(monkeypatch):\n    builtin_open = builtins.open\n\n    def myopen(path, *args, **kwargs):\n        if path == \"/sys/fs/cgroup/memory/memory.limit_in_bytes\":\n            # Absurdly low, unlikely to match real value\n            return io.StringIO(\"20\")\n        return builtin_open(path, *args, **kwargs)\n\n    monkeypatch.setattr(builtins, \"open\", myopen)\n    monkeypatch.setattr(sys, \"platform\", \"linux\")\n\n    limit = memory_limit()\n    assert limit == 20\n\n\ndef test_soft_memory_limit_cgroups(monkeypatch):\n    builtin_open = builtins.open\n\n    def myopen(path, *args, **kwargs):\n        if path == \"/sys/fs/cgroup/memory/memory.limit_in_bytes\":\n            # Absurdly low, unlikely to match real value\n            return io.StringIO(\"20\")\n        if path == \"/sys/fs/cgroup/memory/memory.soft_limit_in_bytes\":\n            # Should take precedence\n            return io.StringIO(\"10\")\n        return builtin_open(path, *args, **kwargs)\n\n    monkeypatch.setattr(builtins, \"open\", myopen)\n    monkeypatch.setattr(sys, \"platform\", \"linux\")\n\n    limit = memory_limit()\n    assert limit == 10\n\n\ndef test_hard_memory_limit_cgroups2(monkeypatch):\n    builtin_open = builtins.open\n\n    def myopen(path, *args, **kwargs):\n        if path == \"/sys/fs/cgroup/memory.max\":\n            # Absurdly low, unlikely to match real value\n            return io.StringIO(\"20\")\n        return builtin_open(path, *args, **kwargs)\n\n    monkeypatch.setattr(builtins, \"open\", myopen)\n    monkeypatch.setattr(sys, \"platform\", \"linux\")\n\n    limit = memory_limit()\n    assert limit == 20\n\n\ndef test_soft_memory_limit_cgroups2(monkeypatch):\n    builtin_open = builtins.open\n\n    def myopen(path, *args, **kwargs):\n        if path == \"/sys/fs/cgroup/memory.max\":\n            # Absurdly low, unlikely to match real value\n            return io.StringIO(\"20\")\n        if path == \"/sys/fs/cgroup/memory.high\":\n            # should take precedence\n            return io.StringIO(\"10\")\n        return builtin_open(path, *args, **kwargs)\n\n    monkeypatch.setattr(builtins, \"open\", myopen)\n    monkeypatch.setattr(sys, \"platform\", \"linux\")\n\n    limit = memory_limit()\n    assert limit == 10\n\n\ndef test_softest_memory_limit_cgroups2(monkeypatch):\n    builtin_open = builtins.open\n\n    def myopen(path, *args, **kwargs):\n        if path == \"/sys/fs/cgroup/memory.max\":\n            # Absurdly low, unlikely to match real value\n            return io.StringIO(\"20\")\n        if path == \"/sys/fs/cgroup/memory.low\":\n            # should take precedence\n            return io.StringIO(\"10\")\n        return builtin_open(path, *args, **kwargs)\n\n    monkeypatch.setattr(builtins, \"open\", myopen)\n    monkeypatch.setattr(sys, \"platform\", \"linux\")\n\n    limit = memory_limit()\n    assert limit == 10\n\n\n@pytest.mark.xfail(\n    MACOS,\n    reason=\"Mac OS raises 'ValueError: current limit exceeds maximum limit' \"\n    \"when calling setrlimit with as little as 7 GiB (getrlimit returns 2**63)\",\n)\ndef test_rlimit():\n    resource = pytest.importorskip(\"resource\")\n\n    # decrease memory limit by one byte\n    new_limit = memory_limit() - 1\n    resource.setrlimit(resource.RLIMIT_RSS, (new_limit, new_limit))\n    assert memory_limit() == new_limit\n\n```"}]}
