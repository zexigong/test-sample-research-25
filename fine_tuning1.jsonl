{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_cache\\test_cache.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport sys\nfrom numbers import Number\nfrom timeit import default_timer\n\nfrom dask.callbacks import Callback\n\noverhead = sys.getsizeof(1.23) * 4 + sys.getsizeof(()) * 4\n\n\nclass Cache(Callback):\n    \"\"\"Use cache for computation\n\n    Examples\n    --------\n\n    >>> cache = Cache(1e9)  # doctest: +SKIP\n\n    The cache can be used locally as a context manager around ``compute`` or\n    ``get`` calls:\n\n    >>> with cache:  # doctest: +SKIP\n    ...     result = x.compute()\n\n    You can also register a cache globally, so that it works for all\n    computations:\n\n    >>> cache.register()    # doctest: +SKIP\n    >>> cache.unregister()  # doctest: +SKIP\n    \"\"\"\n\n    def __init__(self, cache, *args, **kwargs):\n        try:\n            import cachey\n        except ImportError as ex:\n            raise ImportError(\n                f'Cache requires cachey, \"{str(ex)}\" problem importing'\n            ) from ex\n        self._nbytes = cachey.nbytes\n        if isinstance(cache, Number):\n            cache = cachey.Cache(cache, *args, **kwargs)\n        else:\n            assert not args and not kwargs\n        self.cache = cache\n        self.starttimes = dict()\n\n    def _start(self, dsk):\n        self.durations = dict()\n        overlap = set(dsk) & set(self.cache.data)\n        for key in overlap:\n            dsk[key] = self.cache.data[key]\n\n    def _pretask(self, key, dsk, state):\n        self.starttimes[key] = default_timer()\n\n    def _posttask(self, key, value, dsk, state, id):\n        duration = default_timer() - self.starttimes[key]\n        deps = state[\"dependencies\"][key]\n        if deps:\n            duration += max(self.durations.get(k, 0) for k in deps)\n        self.durations[key] = duration\n        nb = self._nbytes(value) + overhead + sys.getsizeof(key) * 4\n        self.cache.put(key, value, cost=duration / nb / 1e9, nbytes=nb)\n\n    def _finish(self, dsk, state, errored):\n        self.starttimes.clear()\n        self.durations.clear()\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:31:08"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nfrom operator import add\nfrom time import sleep\n\nimport pytest\n\nfrom dask.cache import Cache\nfrom dask.callbacks import Callback\nfrom dask.local import get_sync\nfrom dask.threaded import get\n\ncachey = pytest.importorskip(\"cachey\")\n\n\nflag = []\n\n\ndef inc(x):\n    flag.append(x)\n    return x + 1\n\n\ndef test_cache():\n    c = cachey.Cache(10000)\n    cc = Cache(c)\n\n    with cc:\n        assert get({\"x\": (inc, 1)}, \"x\") == 2\n\n    assert flag == [1]\n    assert c.data[\"x\"] == 2\n\n    assert not cc.starttimes\n    assert not cc.durations\n\n    while flag:\n        flag.pop()\n    dsk = {\"x\": (inc, 1), \"y\": (inc, 2), \"z\": (add, \"x\", \"y\")}\n    with cc:\n        assert get(dsk, \"z\") == 5\n\n    assert flag == [2]  # no x present\n\n    assert not Callback.active\n\n\ndef test_cache_with_number():\n    c = Cache(10000, limit=1)\n    assert isinstance(c.cache, cachey.Cache)\n    assert c.cache.available_bytes == 10000\n    assert c.cache.limit == 1\n\n\ndef test_cache_correctness():\n    # https://github.com/dask/dask/issues/3631\n    c = Cache(10000)\n    pytest.importorskip(\"numpy\")\n    da = pytest.importorskip(\"dask.array\")\n    from numpy import ones, zeros\n\n    z = da.from_array(zeros(1), chunks=10)\n    o = da.from_array(ones(1), chunks=10)\n    with c:\n        assert (z.compute() == 0).all()\n        assert (o.compute() == 1).all()\n\n\ndef f(duration, size, *args):\n    sleep(duration)\n    return [0] * size\n\n\ndef test_prefer_cheap_dependent():\n    dsk = {\"x\": (f, 0.01, 10), \"y\": (f, 0.000001, 1, \"x\")}\n    c = Cache(10000)\n    with c:\n        get_sync(dsk, \"y\")\n\n    assert c.cache.scorer.cost[\"x\"] < c.cache.scorer.cost[\"y\"]\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_callbacks\\test_callbacks.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections.abc import Callable\nfrom contextlib import contextmanager\nfrom typing import ClassVar\n\n__all__ = [\"Callback\", \"add_callbacks\"]\n\n\nclass Callback:\n    \"\"\"Base class for using the callback mechanism\n\n    Create a callback with functions of the following signatures:\n\n    >>> def start(dsk):\n    ...     pass\n    >>> def start_state(dsk, state):\n    ...     pass\n    >>> def pretask(key, dsk, state):\n    ...     pass\n    >>> def posttask(key, result, dsk, state, worker_id):\n    ...     pass\n    >>> def finish(dsk, state, failed):\n    ...     pass\n\n    You may then construct a callback object with any number of them\n\n    >>> cb = Callback(pretask=pretask, finish=finish)\n\n    And use it either as a context manager over a compute/get call\n\n    >>> with cb:            # doctest: +SKIP\n    ...     x.compute()\n\n    Or globally with the ``register`` method\n\n    >>> cb.register()\n    >>> cb.unregister()\n\n    Alternatively subclass the ``Callback`` class with your own methods.\n\n    >>> class PrintKeys(Callback):\n    ...     def _pretask(self, key, dask, state):\n    ...         print(\"Computing: {0}!\".format(repr(key)))\n\n    >>> with PrintKeys():   # doctest: +SKIP\n    ...     x.compute()\n    \"\"\"\n\n    active: ClassVar[set[tuple[Callable | None, ...]]] = set()\n\n    def __init__(\n        self, start=None, start_state=None, pretask=None, posttask=None, finish=None\n    ):\n        if start:\n            self._start = start\n        if start_state:\n            self._start_state = start_state\n        if pretask:\n            self._pretask = pretask\n        if posttask:\n            self._posttask = posttask\n        if finish:\n            self._finish = finish\n\n    @property\n    def _callback(self) -> tuple[Callable | None, ...]:\n        fields = [\"_start\", \"_start_state\", \"_pretask\", \"_posttask\", \"_finish\"]\n        return tuple(getattr(self, i, None) for i in fields)\n\n    def __enter__(self):\n        self._cm = add_callbacks(self)\n        self._cm.__enter__()\n        return self\n\n    def __exit__(self, *args):\n        self._cm.__exit__(*args)\n\n    def register(self) -> None:\n        Callback.active.add(self._callback)\n\n    def unregister(self) -> None:\n        Callback.active.remove(self._callback)\n\n\ndef unpack_callbacks(cbs):\n    \"\"\"Take an iterable of callbacks, return a list of each callback.\"\"\"\n    if cbs:\n        return [[i for i in f if i] for f in zip(*cbs)]\n    else:\n        return [(), (), (), (), ()]\n\n\n@contextmanager\ndef local_callbacks(callbacks=None):\n    \"\"\"Allows callbacks to work with nested schedulers.\n\n    Callbacks will only be used by the first started scheduler they encounter.\n    This means that only the outermost scheduler will use global callbacks.\"\"\"\n    global_callbacks = callbacks is None\n    if global_callbacks:\n        callbacks, Callback.active = Callback.active, set()\n    try:\n        yield callbacks or ()\n    finally:\n        if global_callbacks:\n            Callback.active = callbacks\n\n\ndef normalize_callback(cb):\n    \"\"\"Normalizes a callback to a tuple\"\"\"\n    if isinstance(cb, Callback):\n        return cb._callback\n    elif isinstance(cb, tuple):\n        return cb\n    else:\n        raise TypeError(\"Callbacks must be either `Callback` or `tuple`\")\n\n\nclass add_callbacks:\n    \"\"\"Context manager for callbacks.\n\n    Takes several callbacks and applies them only in the enclosed context.\n    Callbacks can either be represented as a ``Callback`` object, or as a tuple\n    of length 4.\n\n    Examples\n    --------\n    >>> def pretask(key, dsk, state):\n    ...     print(\"Now running {0}\").format(key)\n    >>> callbacks = (None, pretask, None, None)\n    >>> with add_callbacks(callbacks):    # doctest: +SKIP\n    ...     res.compute()\n    \"\"\"\n\n    def __init__(self, *callbacks):\n        self.callbacks = [normalize_callback(c) for c in callbacks]\n        Callback.active.update(self.callbacks)\n\n    def __enter__(self):\n        return\n\n    def __exit__(self, type, value, traceback):\n        for c in self.callbacks:\n            Callback.active.discard(c)\n\n\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections.abc import Callable\nfrom contextlib import contextmanager\nfrom typing import ClassVar\n\n__all__ = [\"Callback\", \"add_callbacks\"]\n\n\nclass Callback:\n    \"\"\"Base class for using the callback mechanism\n\n    Create a callback with functions of the following signatures:\n\n    >>> def start(dsk):\n    ...     pass\n    >>> def start_state(dsk, state):\n    ...     pass\n    >>> def pretask(key, dsk, state):\n    ...     pass\n    >>> def posttask(key, result, dsk, state, worker_id):\n    ...     pass\n    >>> def finish(dsk, state, failed):\n    ...     pass\n\n    You may then construct a callback object with any number of them\n\n    >>> cb = Callback(pretask=pretask, finish=finish)\n\n    And use it either as a context manager over a compute/get call\n\n    >>> with cb:            # doctest: +SKIP\n    ...     x.compute()\n\n    Or globally with the ``register`` method\n\n    >>> cb.register()\n    >>> cb.unregister()\n\n    Alternatively subclass the ``Callback`` class with your own methods.\n\n    >>> class PrintKeys(Callback):\n    ...     def _pretask(self, key, dask, state):\n    ...         print(\"Computing: {0}!\".format(repr(key)))\n\n    >>> with PrintKeys():   # doctest: +SKIP\n    ...     x.compute()\n    \"\"\"\n\n    active: ClassVar[set[tuple[Callable | None, ...]]] = set()\n\n    def __init__(\n        self, start=None, start_state=None, pretask=None, posttask=None, finish=None\n    ):\n        if start:\n            self._start = start\n        if start_state:\n            self._start_state = start_state\n        if pretask:\n            self._pretask = pretask\n        if posttask:\n            self._posttask = posttask\n        if finish:\n            self._finish = finish\n\n    @property\n    def _callback(self) -> tuple[Callable | None, ...]:\n        fields = [\"_start\", \"_start_state\", \"_pretask\", \"_posttask\", \"_finish\"]\n        return tuple(getattr(self, i, None) for i in fields)\n\n    def __enter__(self):\n        self._cm = add_callbacks(self)\n        self._cm.__enter__()\n        return self\n\n    def __exit__(self, *args):\n        self._cm.__exit__(*args)\n\n    def register(self) -> None:\n        Callback.active.add(self._callback)\n\n    def unregister(self) -> None:\n        Callback.active.remove(self._callback)\n\n\ndef unpack_callbacks(cbs):\n    \"\"\"Take an iterable of callbacks, return a list of each callback.\"\"\"\n    if cbs:\n        return [[i for i in f if i] for f in zip(*cbs)]\n    else:\n        return [(), (), (), (), ()]\n\n\n@contextmanager\ndef local_callbacks(callbacks=None):\n    \"\"\"Allows callbacks to work with nested schedulers.\n\n    Callbacks will only be used by the first started scheduler they encounter.\n    This means that only the outermost scheduler will use global callbacks.\"\"\"\n    global_callbacks = callbacks is None\n    if global_callbacks:\n        callbacks, Callback.active = Callback.active, set()\n    try:\n        yield callbacks or ()\n    finally:\n        if global_callbacks:\n            Callback.active = callbacks\n\n\ndef normalize_callback(cb):\n    \"\"\"Normalizes a callback to a tuple\"\"\"\n    if isinstance(cb, Callback):\n        return cb._callback\n    elif isinstance(cb, tuple):\n        return cb\n    else:\n        raise TypeError(\"Callbacks must be either `Callback` or `tuple`\")\n\n\nclass add_callbacks:\n    \"\"\"Context manager for callbacks.\n\n    Takes several callbacks and applies them only in the enclosed context.\n    Callbacks can either be represented as a ``Callback`` object, or as a tuple\n    of length 4.\n\n    Examples\n    --------\n    >>> def pretask(key, dsk, state):\n    ...     print(\"Now running {0}\").format(key)\n    >>> callbacks = (None, pretask, None, None)\n    >>> with add_callbacks(callbacks):    # doctest: +SKIP\n    ...     res.compute()\n    \"\"\"\n\n    def __init__(self, *callbacks):\n        self.callbacks = [normalize_callback(c) for c in callbacks]\n        Callback.active.update(self.callbacks)\n\n    def __enter__(self):\n        return\n\n    def __exit__(self, type, value, traceback):\n        for c in self.callbacks:\n            Callback.active.discard(c)\n\n\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections.abc import Callable\nfrom contextlib import contextmanager\nfrom typing import ClassVar\n\n__all__ = [\"Callback\", \"add_callbacks\"]\n\n\nclass Callback:\n    \"\"\"Base class for using the callback mechanism\n\n    Create a callback with functions of the following signatures:\n\n    >>> def start(dsk):\n    ...     pass\n    >>> def start_state(dsk, state):\n    ...     pass\n    >>> def pretask(key, dsk, state):\n    ...     pass\n    >>> def posttask(key, result, dsk, state, worker_id):\n    ...     pass\n    >>> def finish(dsk, state, failed):\n    ...     pass\n\n    You may then construct a callback object with any number of them\n\n    >>> cb = Callback(pretask=pretask, finish=finish)\n\n    And use it either as a context manager over a compute/get call\n\n    >>> with cb:            # doctest: +SKIP\n    ...     x.compute()\n\n    Or globally with the ``register`` method\n\n    >>> cb.register()\n    >>> cb.unregister()\n\n    Alternatively subclass the ``Callback`` class with your own methods.\n\n    >>> class PrintKeys(Callback):\n    ...     def _pretask(self, key, dask, state):\n    ...         print(\"Computing: {0}!\".format(repr(key)))\n\n    >>> with PrintKeys():   # doctest: +SKIP\n    ...     x.compute()\n    \"\"\"\n\n    active: ClassVar[set[tuple[Callable | None, ...]]] = set()\n\n    def __init__(\n        self, start=None, start_state=None, pretask=None, posttask=None, finish=None\n    ):\n        if start:\n            self._start = start\n        if start_state:\n            self._start_state = start_state\n        if pretask:\n            self._pretask = pretask\n        if posttask:\n            self._posttask = posttask\n        if finish:\n            self._finish = finish\n\n    @property\n    def _callback(self) -> tuple[Callable | None, ...]:\n        fields = [\"_start\", \"_start_state\", \"_pretask\", \"_posttask\", \"_finish\"]\n        return tuple(getattr(self, i, None) for i in fields)\n\n    def __enter__(self):\n        self._cm = add_callbacks(self)\n        self._cm.__enter__()\n        return self\n\n    def __exit__(self, *args):\n        self._cm.__exit__(*args)\n\n    def register(self) -> None:\n        Callback.active.add(self._callback)\n\n    def unregister(self) -> None:\n        Callback.active.remove(self._callback)\n\n\ndef unpack_callbacks(cbs):\n    \"\"\"Take an iterable of callbacks, return a list of each callback.\"\"\"\n    if cbs:\n        return [[i for i in f if i] for f in zip(*cbs)]\n    else:\n        return [(), (), (), (), ()]\n\n\n@contextmanager\ndef local_callbacks(callbacks=None):\n    \"\"\"Allows callbacks to work with nested schedulers.\n\n    Callbacks will only be used by the first started scheduler they encounter.\n    This means that only the outermost scheduler will use global callbacks.\"\"\"\n    global_callbacks = callbacks is None\n    if global_callbacks:\n        callbacks, Callback.active = Callback.active, set()\n    try:\n        yield callbacks or ()\n    finally:\n        if global_callbacks:\n            Callback.active = callbacks\n\n\ndef normalize_callback(cb):\n    \"\"\"Normalizes a callback to a tuple\"\"\"\n    if isinstance(cb, Callback):\n        return cb._callback\n    elif isinstance(cb, tuple):\n        return cb\n    else:\n        raise TypeError(\"Callbacks must be either `Callback` or `tuple`\")\n\n\nclass add_callbacks:\n    \"\"\"Context manager for callbacks.\n\n    Takes several callbacks and applies them only in the enclosed context.\n    Callbacks can either be represented as a ``Callback`` object, or as a tuple\n    of length 4.\n\n    Examples\n    --------\n    >>> def pretask(key, dsk, state):\n    ...     print(\"Now running {0}\").format(key)\n    >>> callbacks = (None, pretask, None, None)\n    >>> with add_callbacks(callbacks):    # doctest: +SKIP\n    ...     res.compute()\n    \"\"\"\n\n    def __init__(self, *callbacks):\n        self.callbacks = [normalize_callback(c) for c in callbacks]\n        Callback.active.update(self.callbacks)\n\n    def __enter__(self):\n        return\n\n    def __exit__(self, type, value, traceback):\n        for c in self.callbacks:\n            Callback.active.discard(c)\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:31:08"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport pytest\n\nfrom dask.callbacks import Callback\nfrom dask.local import get_sync\nfrom dask.threaded import get as get_threaded\nfrom dask.utils_test import add\n\n\ndef test_start_callback():\n    flag = [False]\n\n    class MyCallback(Callback):\n        def _start(self, dsk):\n            flag[0] = True\n\n    with MyCallback():\n        get_sync({\"x\": 1}, \"x\")\n\n    assert flag[0] is True\n\n\ndef test_start_state_callback():\n    flag = [False]\n\n    class MyCallback(Callback):\n        def _start_state(self, dsk, state):\n            flag[0] = True\n            assert dsk[\"x\"]() == 1\n            assert len(state[\"cache\"]) == 1\n\n    with MyCallback():\n        get_sync({\"x\": 1}, \"x\")\n\n    assert flag[0] is True\n\n\ndef test_finish_always_called():\n    flag = [False]\n\n    class MyCallback(Callback):\n        def _finish(self, dsk, state, errored):\n            flag[0] = True\n            assert errored\n\n    dsk = {\"x\": (lambda: 1 / 0,)}\n\n    # `raise_on_exception=True`\n    try:\n        with MyCallback():\n            get_sync(dsk, \"x\")\n    except Exception as e:\n        assert isinstance(e, ZeroDivisionError)\n    assert flag[0]\n\n    # `raise_on_exception=False`\n    flag[0] = False\n    try:\n        with MyCallback():\n            get_threaded(dsk, \"x\")\n    except Exception as e:\n        assert isinstance(e, ZeroDivisionError)\n    assert flag[0]\n\n    # KeyboardInterrupt\n    def raise_keyboard():\n        raise KeyboardInterrupt()\n\n    dsk = {\"x\": (raise_keyboard,)}\n    flag[0] = False\n    with pytest.raises(KeyboardInterrupt):\n        with MyCallback():\n            get_sync(dsk, \"x\")\n    assert flag[0]\n\n\ndef test_nested_schedulers():\n    class MyCallback(Callback):\n        def _start(self, dsk):\n            self.dsk = dsk\n\n        def _pretask(self, key, dsk, state):\n            assert key in self.dsk\n\n    inner_callback = MyCallback()\n    inner_dsk = {\"x\": (add, 1, 2), \"y\": (add, \"x\", 3)}\n\n    def nested_call(x):\n        assert not Callback.active\n        with inner_callback:\n            return get_threaded(inner_dsk, \"y\") + x\n\n    outer_callback = MyCallback()\n    outer_dsk = {\"a\": (nested_call, 1), \"b\": (add, \"a\", 2)}\n\n    with outer_callback:\n        get_threaded(outer_dsk, \"b\")\n\n    assert not Callback.active\n    assert outer_callback.dsk.keys() == outer_dsk.keys()\n    assert inner_callback.dsk.keys() == inner_dsk.keys()\n    assert not Callback.active\n\n\ndef test_add_remove_mutates_not_replaces():\n    assert not Callback.active\n\n    with Callback():\n        assert Callback.active\n\n    assert not Callback.active\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_core\\test_core.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom collections.abc import Collection, Iterable, Mapping, MutableMapping\nfrom typing import Any, Literal, TypeVar, cast, overload\n\nimport toolz\n\nfrom dask._task_spec import DependenciesMapping, convert_legacy_graph, execute_graph\nfrom dask.typing import Graph, Key, NoDefault, no_default\n\n\ndef ishashable(x):\n    \"\"\"Is x hashable?\n\n    Examples\n    --------\n\n    >>> ishashable(1)\n    True\n    >>> ishashable([1])\n    False\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    try:\n        hash(x)\n        return True\n    except TypeError:\n        return False\n\n\ndef istask(x):\n    \"\"\"Is x a runnable task?\n\n    A task is a tuple with a callable first argument\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> istask((inc, 1))\n    True\n    >>> istask(1)\n    False\n    \"\"\"\n    from dask._task_spec import DataNode, GraphNode\n\n    if isinstance(x, GraphNode):\n        if isinstance(x, DataNode):\n            return False\n        return True\n    return type(x) is tuple and x and callable(x[0])\n\n\ndef preorder_traversal(task):\n    \"\"\"A generator to preorder-traverse a task.\"\"\"\n\n    for item in task:\n        if istask(item):\n            yield from preorder_traversal(item)\n        elif isinstance(item, list):\n            yield list\n            yield from preorder_traversal(item)\n        else:\n            yield item\n\n\ndef lists_to_tuples(res, keys):\n    if isinstance(keys, list):\n        return tuple(lists_to_tuples(r, k) for r, k in zip(res, keys))\n    return res\n\n\ndef _pack_result(result: Mapping, keys: list | Key) -> Any:\n    if isinstance(keys, list):\n        return tuple(_pack_result(result, k) for k in keys)\n    return result[keys]\n\n\ndef get(dsk: Mapping, out: list | Key, cache: MutableMapping | None = None) -> Any:\n    \"\"\"Get value from Dask\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': 1, 'y': (inc, 'x')}\n\n    >>> get(d, 'x')\n    1\n    >>> get(d, 'y')\n    2\n    \"\"\"\n    for k in flatten(out):\n        if k not in dsk:\n            raise KeyError(f\"{k} is not a key in the graph\")\n    if cache is None:\n        cache = {}\n\n    dsk2 = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    result = execute_graph(dsk2, cache, keys=set(flatten([out])))\n    return _pack_result(result, out)\n\n\ndef keys_in_tasks(keys: Collection[Key], tasks: Iterable[Any], as_list: bool = False):\n    \"\"\"Returns the keys in `keys` that are also in `tasks`\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> keys_in_tasks(dsk, ['x', 'y', 'j'])  # doctest: +SKIP\n    {'x', 'y'}\n    \"\"\"\n    from dask._task_spec import GraphNode\n\n    ret: list[Key] = []\n    while tasks:\n        work = []\n        for w in tasks:\n            typ = type(w)\n            if typ is tuple and w and callable(w[0]):  # istask(w)\n                work.extend(w[1:])\n            elif typ is list:\n                work.extend(w)\n            elif typ is dict:\n                work.extend(w.values())\n            elif isinstance(w, GraphNode):\n                work.extend(w.dependencies)\n            else:\n                try:\n                    if w in keys:\n                        ret.append(w)\n                except TypeError:  # not hashable\n                    pass\n        tasks = work\n    return ret if as_list else set(ret)\n\n\ndef iskey(key: object) -> bool:\n    \"\"\"Return True if the given object is a potential dask key; False otherwise.\n\n    The definition of a key in a Dask graph is any str, bytes, int, float, or tuple\n    thereof.\n\n    See Also\n    --------\n    ishashable\n    validate_key\n    dask.typing.Key\n    \"\"\"\n    typ = type(key)\n    if typ is tuple:\n        return all(iskey(i) for i in cast(tuple, key))\n    return typ in {bytes, int, float, str}\n\n\ndef validate_key(key: object) -> None:\n    \"\"\"Validate the format of a dask key.\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    if iskey(key):\n        return\n    typ = type(key)\n\n    if typ is tuple:\n        index = None\n        try:\n            for index, part in enumerate(cast(tuple, key)):  # noqa: B007\n                validate_key(part)\n        except TypeError as e:\n            raise TypeError(\n                f\"Composite key contains unexpected key type at {index=} ({key=!r})\"\n            ) from e\n    raise TypeError(f\"Unexpected key type {typ} ({key=!r})\")\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = ...,\n    task: Key | NoDefault = ...,\n    as_list: Literal[False] = ...,\n) -> set[Key]: ...\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None,\n    task: Key | NoDefault,\n    as_list: Literal[True],\n) -> list[Key]: ...\n\n\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = None,\n    task: Key | NoDefault = no_default,\n    as_list: bool = False,\n) -> set[Key] | list[Key]:\n    \"\"\"Get the immediate tasks on which this task depends\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> get_dependencies(dsk, 'x')\n    set()\n\n    >>> get_dependencies(dsk, 'y')\n    {'x'}\n\n    >>> get_dependencies(dsk, 'z')  # doctest: +SKIP\n    {'x', 'y'}\n\n    >>> get_dependencies(dsk, 'w')  # Only direct dependencies\n    {'z'}\n\n    >>> get_dependencies(dsk, 'a')  # Ignore non-keys\n    {'x'}\n\n    >>> get_dependencies(dsk, task=(inc, 'x'))  # provide tasks directly\n    {'x'}\n    \"\"\"\n    if key is not None:\n        arg = dsk[key]\n    elif task is not no_default:\n        arg = task\n    else:\n        raise ValueError(\"Provide either key or task\")\n\n    return keys_in_tasks(dsk, [arg], as_list=as_list)\n\n\ndef get_deps(dsk: Graph) -> tuple[dict[Key, set[Key]], dict[Key, set[Key]]]:\n    \"\"\"Get dependencies and dependents from dask dask graph\n\n    >>> inc = lambda x: x + 1\n    >>> dsk = {'a': 1, 'b': (inc, 'a'), 'c': (inc, 'b')}\n    >>> dependencies, dependents = get_deps(dsk)\n    >>> dependencies\n    {'a': set(), 'b': {'a'}, 'c': {'b'}}\n    >>> dependents  # doctest: +SKIP\n    {'a': {'b'}, 'b': {'c'}, 'c': set()}\n    \"\"\"\n    dependencies = {k: get_dependencies(dsk, task=v) for k, v in dsk.items()}\n    dependents = reverse_dict(dependencies)\n    return dependencies, dependents\n\n\ndef flatten(seq, container=list):\n    \"\"\"\n\n    >>> list(flatten([1]))\n    [1]\n\n    >>> list(flatten([[1, 2], [1, 2]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten([[[1], [2]], [[1], [2]]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten(((1, 2), (1, 2)))) # Don't flatten tuples\n    [(1, 2), (1, 2)]\n\n    >>> list(flatten((1, 2, [3, 4]))) # support heterogeneous\n    [1, 2, 3, 4]\n    \"\"\"\n    if isinstance(seq, str):\n        yield seq\n    else:\n        for item in seq:\n            if isinstance(item, container):\n                yield from flatten(item, container=container)\n            else:\n                yield item\n\n\nT_ = TypeVar(\"T_\")\n\n\ndef reverse_dict(d: Mapping[T_, Iterable[T_]]) -> dict[T_, set[T_]]:\n    \"\"\"\n\n    >>> a, b, c = 'abc'\n    >>> d = {a: [b, c], b: [c]}\n    >>> reverse_dict(d)  # doctest: +SKIP\n    {'a': set([]), 'b': set(['a']}, 'c': set(['a', 'b'])}\n    \"\"\"\n    result: defaultdict[T_, set[T_]] = defaultdict(set)\n    _add = set.add\n    for k, vals in d.items():\n        result[k]\n        for val in vals:\n            _add(result[val], k)\n    return dict(result)\n\n\ndef subs(task, key, val):\n    \"\"\"Perform a substitution on a task\n\n    Examples\n    --------\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> subs((inc, 'x'), 'x', 1)  # doctest: +ELLIPSIS\n    (<function inc at ...>, 1)\n    \"\"\"\n    type_task = type(task)\n    if not (type_task is tuple and task and callable(task[0])):  # istask(task):\n        try:\n            if type_task is type(key) and task == key:\n                return val\n        except Exception:\n            pass\n        if type_task is list:\n            return [subs(x, key, val) for x in task]\n        return task\n    newargs = []\n    hash_key = {key}\n    for arg in task[1:]:\n        type_arg = type(arg)\n        if type_arg is tuple and arg and callable(arg[0]):  # istask(task):\n            arg = subs(arg, key, val)\n        elif type_arg is list:\n            arg = [subs(x, key, val) for x in arg]\n        else:\n            try:\n                if arg in hash_key:  # Hash and equality match\n                    arg = val\n            except TypeError:  # not hashable\n                pass\n        newargs.append(arg)\n    return task[:1] + tuple(newargs)\n\n\ndef _toposort(dsk, keys=None, returncycle=False, dependencies=None):\n\n    # Stack-based depth-first search traversal.  This is based on Tarjan's\n    # method for topological sorting (see wikipedia for pseudocode)\n    if keys is None:\n        keys = dsk\n    elif not isinstance(keys, list):\n        keys = [keys]\n    if not returncycle:\n        ordered = []\n\n    # Nodes whose descendents have been completely explored.\n    # These nodes are guaranteed to not be part of a cycle.\n    completed = set()\n\n    # All nodes that have been visited in the current traversal.  Because\n    # we are doing depth-first search, going \"deeper\" should never result\n    # in visiting a node that has already been seen.  The `seen` and\n    # `completed` sets are mutually exclusive; it is okay to visit a node\n    # that has already been added to `completed`.\n    seen = set()\n\n    if dependencies is None:\n\n        dependencies = DependenciesMapping(dsk)\n\n    for key in keys:\n        if key in completed:\n            continue\n        nodes = [key]\n        while nodes:\n            # Keep current node on the stack until all descendants are visited\n            cur = nodes[-1]\n            if cur in completed:\n                # Already fully traversed descendants of cur\n                nodes.pop()\n                continue\n            seen.add(cur)\n\n            # Add direct descendants of cur to nodes stack\n            next_nodes = []\n            for nxt in dependencies[cur]:\n                if nxt not in completed:\n                    if nxt in seen:\n                        # Cycle detected!\n                        # Let's report only the nodes that directly participate in the cycle.\n                        # We use `priorities` below to greedily construct a short cycle.\n                        # Shorter cycles may exist.\n                        priorities = {}\n                        prev = nodes[-1]\n                        # Give priority to nodes that were seen earlier.\n                        while nodes[-1] != nxt:\n                            priorities[nodes.pop()] = -len(priorities)\n                        priorities[nxt] = -len(priorities)\n                        # We're going to get the cycle by walking backwards along dependents,\n                        # so calculate dependents only for the nodes in play.\n                        inplay = set(priorities)\n                        dependents = reverse_dict(\n                            {k: inplay.intersection(dependencies[k]) for k in inplay}\n                        )\n                        # Begin with the node that was seen twice and the node `prev` from\n                        # which we detected the cycle.\n                        cycle = [nodes.pop()]\n                        cycle.append(prev)\n                        while prev != cycle[0]:\n                            # Greedily take a step that takes us closest to completing the cycle.\n                            # This may not give us the shortest cycle, but we get *a* short cycle.\n                            deps = dependents[cycle[-1]]\n                            prev = min(deps, key=priorities.__getitem__)\n                            cycle.append(prev)\n                        cycle.reverse()\n\n                        if returncycle:\n                            return cycle\n                        else:\n                            cycle = \"->\".join(str(x) for x in cycle)\n                            raise RuntimeError(\"Cycle detected in Dask: %s\" % cycle)\n                    next_nodes.append(nxt)\n\n            if next_nodes:\n                nodes.extend(next_nodes)\n            else:\n                # cur has no more descendants to explore, so we're done with it\n                if not returncycle:\n                    ordered.append(cur)\n                completed.add(cur)\n                seen.remove(cur)\n                nodes.pop()\n    if returncycle:\n        return []\n    return ordered\n\n\ndef toposort(dsk, dependencies=None):\n    \"\"\"Return a list of keys of dask sorted in topological order.\"\"\"\n    return _toposort(dsk, dependencies=dependencies)\n\n\ndef getcycle(d, keys):\n    \"\"\"Return a list of nodes that form a cycle if Dask is not a DAG.\n\n    Returns an empty list if no cycle is found.\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': (inc, 'z'), 'y': (inc, 'x'), 'z': (inc, 'y')}\n    >>> getcycle(d, 'x')\n    ['x', 'z', 'y', 'x']\n\n    See Also\n    --------\n    isdag\n    \"\"\"\n    return _toposort(d, keys=keys, returncycle=True)\n\n\ndef isdag(d, keys):\n    \"\"\"Does Dask form a directed acyclic graph when calculating keys?\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> inc = lambda x: x + 1\n    >>> isdag({'x': 0, 'y': (inc, 'x')}, 'y')\n    True\n    >>> isdag({'x': (inc, 'y'), 'y': (inc, 'x')}, 'y')\n    False\n\n    See Also\n    --------\n    getcycle\n    \"\"\"\n    return not getcycle(d, keys)\n\n\nclass literal:\n    \"\"\"A small serializable object to wrap literal values without copying\"\"\"\n\n    __slots__ = (\"data\",)\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return \"literal<type=%s>\" % type(self.data).__name__\n\n    def __reduce__(self):\n        return (literal, (self.data,))\n\n    def __call__(self):\n        return self.data\n\n\ndef quote(x):\n    \"\"\"Ensure that this value remains this value in a dask graph\n\n    Some values in dask graph take on special meaning. Sometimes we want to\n    ensure that our data is not interpreted but remains literal.\n\n    >>> add = lambda x, y: x + y\n    >>> quote((add, 1, 2))\n    (literal<type=tuple>,)\n    \"\"\"\n    if istask(x) or type(x) is list or type(x) is dict:\n        return (literal(x),)\n    return x\n\n\ndef reshapelist(shape, seq):\n    \"\"\"Reshape iterator to nested shape\n\n    >>> reshapelist((2, 3), range(6))\n    [[0, 1, 2], [3, 4, 5]]\n    \"\"\"\n    if len(shape) == 1:\n        return list(seq)\n    else:\n        n = int(len(seq) / shape[0])\n        return [reshapelist(shape[1:], part) for part in toolz.partition(n, seq)]\n\n\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom collections.abc import Collection, Iterable, Mapping, MutableMapping\nfrom typing import Any, Literal, TypeVar, cast, overload\n\nimport toolz\n\nfrom dask._task_spec import DependenciesMapping, convert_legacy_graph, execute_graph\nfrom dask.typing import Graph, Key, NoDefault, no_default\n\n\ndef ishashable(x):\n    \"\"\"Is x hashable?\n\n    Examples\n    --------\n\n    >>> ishashable(1)\n    True\n    >>> ishashable([1])\n    False\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    try:\n        hash(x)\n        return True\n    except TypeError:\n        return False\n\n\ndef istask(x):\n    \"\"\"Is x a runnable task?\n\n    A task is a tuple with a callable first argument\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> istask((inc, 1))\n    True\n    >>> istask(1)\n    False\n    \"\"\"\n    from dask._task_spec import DataNode, GraphNode\n\n    if isinstance(x, GraphNode):\n        if isinstance(x, DataNode):\n            return False\n        return True\n    return type(x) is tuple and x and callable(x[0])\n\n\ndef preorder_traversal(task):\n    \"\"\"A generator to preorder-traverse a task.\"\"\"\n\n    for item in task:\n        if istask(item):\n            yield from preorder_traversal(item)\n        elif isinstance(item, list):\n            yield list\n            yield from preorder_traversal(item)\n        else:\n            yield item\n\n\ndef lists_to_tuples(res, keys):\n    if isinstance(keys, list):\n        return tuple(lists_to_tuples(r, k) for r, k in zip(res, keys))\n    return res\n\n\ndef _pack_result(result: Mapping, keys: list | Key) -> Any:\n    if isinstance(keys, list):\n        return tuple(_pack_result(result, k) for k in keys)\n    return result[keys]\n\n\ndef get(dsk: Mapping, out: list | Key, cache: MutableMapping | None = None) -> Any:\n    \"\"\"Get value from Dask\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': 1, 'y': (inc, 'x')}\n\n    >>> get(d, 'x')\n    1\n    >>> get(d, 'y')\n    2\n    \"\"\"\n    for k in flatten(out):\n        if k not in dsk:\n            raise KeyError(f\"{k} is not a key in the graph\")\n    if cache is None:\n        cache = {}\n\n    dsk2 = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    result = execute_graph(dsk2, cache, keys=set(flatten([out])))\n    return _pack_result(result, out)\n\n\ndef keys_in_tasks(keys: Collection[Key], tasks: Iterable[Any], as_list: bool = False):\n    \"\"\"Returns the keys in `keys` that are also in `tasks`\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> keys_in_tasks(dsk, ['x', 'y', 'j'])  # doctest: +SKIP\n    {'x', 'y'}\n    \"\"\"\n    from dask._task_spec import GraphNode\n\n    ret: list[Key] = []\n    while tasks:\n        work = []\n        for w in tasks:\n            typ = type(w)\n            if typ is tuple and w and callable(w[0]):  # istask(w)\n                work.extend(w[1:])\n            elif typ is list:\n                work.extend(w)\n            elif typ is dict:\n                work.extend(w.values())\n            elif isinstance(w, GraphNode):\n                work.extend(w.dependencies)\n            else:\n                try:\n                    if w in keys:\n                        ret.append(w)\n                except TypeError:  # not hashable\n                    pass\n        tasks = work\n    return ret if as_list else set(ret)\n\n\ndef iskey(key: object) -> bool:\n    \"\"\"Return True if the given object is a potential dask key; False otherwise.\n\n    The definition of a key in a Dask graph is any str, bytes, int, float, or tuple\n    thereof.\n\n    See Also\n    --------\n    ishashable\n    validate_key\n    dask.typing.Key\n    \"\"\"\n    typ = type(key)\n    if typ is tuple:\n        return all(iskey(i) for i in cast(tuple, key))\n    return typ in {bytes, int, float, str}\n\n\ndef validate_key(key: object) -> None:\n    \"\"\"Validate the format of a dask key.\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    if iskey(key):\n        return\n    typ = type(key)\n\n    if typ is tuple:\n        index = None\n        try:\n            for index, part in enumerate(cast(tuple, key)):  # noqa: B007\n                validate_key(part)\n        except TypeError as e:\n            raise TypeError(\n                f\"Composite key contains unexpected key type at {index=} ({key=!r})\"\n            ) from e\n    raise TypeError(f\"Unexpected key type {typ} ({key=!r})\")\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = ...,\n    task: Key | NoDefault = ...,\n    as_list: Literal[False] = ...,\n) -> set[Key]: ...\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None,\n    task: Key | NoDefault,\n    as_list: Literal[True],\n) -> list[Key]: ...\n\n\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = None,\n    task: Key | NoDefault = no_default,\n    as_list: bool = False,\n) -> set[Key] | list[Key]:\n    \"\"\"Get the immediate tasks on which this task depends\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> get_dependencies(dsk, 'x')\n    set()\n\n    >>> get_dependencies(dsk, 'y')\n    {'x'}\n\n    >>> get_dependencies(dsk, 'z')  # doctest: +SKIP\n    {'x', 'y'}\n\n    >>> get_dependencies(dsk, 'w')  # Only direct dependencies\n    {'z'}\n\n    >>> get_dependencies(dsk, 'a')  # Ignore non-keys\n    {'x'}\n\n    >>> get_dependencies(dsk, task=(inc, 'x'))  # provide tasks directly\n    {'x'}\n    \"\"\"\n    if key is not None:\n        arg = dsk[key]\n    elif task is not no_default:\n        arg = task\n    else:\n        raise ValueError(\"Provide either key or task\")\n\n    return keys_in_tasks(dsk, [arg], as_list=as_list)\n\n\ndef get_deps(dsk: Graph) -> tuple[dict[Key, set[Key]], dict[Key, set[Key]]]:\n    \"\"\"Get dependencies and dependents from dask dask graph\n\n    >>> inc = lambda x: x + 1\n    >>> dsk = {'a': 1, 'b': (inc, 'a'), 'c': (inc, 'b')}\n    >>> dependencies, dependents = get_deps(dsk)\n    >>> dependencies\n    {'a': set(), 'b': {'a'}, 'c': {'b'}}\n    >>> dependents  # doctest: +SKIP\n    {'a': {'b'}, 'b': {'c'}, 'c': set()}\n    \"\"\"\n    dependencies = {k: get_dependencies(dsk, task=v) for k, v in dsk.items()}\n    dependents = reverse_dict(dependencies)\n    return dependencies, dependents\n\n\ndef flatten(seq, container=list):\n    \"\"\"\n\n    >>> list(flatten([1]))\n    [1]\n\n    >>> list(flatten([[1, 2], [1, 2]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten([[[1], [2]], [[1], [2]]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten(((1, 2), (1, 2)))) # Don't flatten tuples\n    [(1, 2), (1, 2)]\n\n    >>> list(flatten((1, 2, [3, 4]))) # support heterogeneous\n    [1, 2, 3, 4]\n    \"\"\"\n    if isinstance(seq, str):\n        yield seq\n    else:\n        for item in seq:\n            if isinstance(item, container):\n                yield from flatten(item, container=container)\n            else:\n                yield item\n\n\nT_ = TypeVar(\"T_\")\n\n\ndef reverse_dict(d: Mapping[T_, Iterable[T_]]) -> dict[T_, set[T_]]:\n    \"\"\"\n\n    >>> a, b, c = 'abc'\n    >>> d = {a: [b, c], b: [c]}\n    >>> reverse_dict(d)  # doctest: +SKIP\n    {'a': set([]), 'b': set(['a']}, 'c': set(['a', 'b'])}\n    \"\"\"\n    result: defaultdict[T_, set[T_]] = defaultdict(set)\n    _add = set.add\n    for k, vals in d.items():\n        result[k]\n        for val in vals:\n            _add(result[val], k)\n    return dict(result)\n\n\ndef subs(task, key, val):\n    \"\"\"Perform a substitution on a task\n\n    Examples\n    --------\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> subs((inc, 'x'), 'x', 1)  # doctest: +ELLIPSIS\n    (<function inc at ...>, 1)\n    \"\"\"\n    type_task = type(task)\n    if not (type_task is tuple and task and callable(task[0])):  # istask(task):\n        try:\n            if type_task is type(key) and task == key:\n                return val\n        except Exception:\n            pass\n        if type_task is list:\n            return [subs(x, key, val) for x in task]\n        return task\n    newargs = []\n    hash_key = {key}\n    for arg in task[1:]:\n        type_arg = type(arg)\n        if type_arg is tuple and arg and callable(arg[0]):  # istask(task):\n            arg = subs(arg, key, val)\n        elif type_arg is list:\n            arg = [subs(x, key, val) for x in arg]\n        else:\n            try:\n                if arg in hash_key:  # Hash and equality match\n                    arg = val\n            except TypeError:  # not hashable\n                pass\n        newargs.append(arg)\n    return task[:1] + tuple(newargs)\n\n\ndef _toposort(dsk, keys=None, returncycle=False, dependencies=None):\n\n    # Stack-based depth-first search traversal.  This is based on Tarjan's\n    # method for topological sorting (see wikipedia for pseudocode)\n    if keys is None:\n        keys = dsk\n    elif not isinstance(keys, list):\n        keys = [keys]\n    if not returncycle:\n        ordered = []\n\n    # Nodes whose descendents have been completely explored.\n    # These nodes are guaranteed to not be part of a cycle.\n    completed = set()\n\n    # All nodes that have been visited in the current traversal.  Because\n    # we are doing depth-first search, going \"deeper\" should never result\n    # in visiting a node that has already been seen.  The `seen` and\n    # `completed` sets are mutually exclusive; it is okay to visit a node\n    # that has already been added to `completed`.\n    seen = set()\n\n    if dependencies is None:\n\n        dependencies = DependenciesMapping(dsk)\n\n    for key in keys:\n        if key in completed:\n            continue\n        nodes = [key]\n        while nodes:\n            # Keep current node on the stack until all descendants are visited\n            cur = nodes[-1]\n            if cur in completed:\n                # Already fully traversed descendants of cur\n                nodes.pop()\n                continue\n            seen.add(cur)\n\n            # Add direct descendants of cur to nodes stack\n            next_nodes = []\n            for nxt in dependencies[cur]:\n                if nxt not in completed:\n                    if nxt in seen:\n                        # Cycle detected!\n                        # Let's report only the nodes that directly participate in the cycle.\n                        # We use `priorities` below to greedily construct a short cycle.\n                        # Shorter cycles may exist.\n                        priorities = {}\n                        prev = nodes[-1]\n                        # Give priority to nodes that were seen earlier.\n                        while nodes[-1] != nxt:\n                            priorities[nodes.pop()] = -len(priorities)\n                        priorities[nxt] = -len(priorities)\n                        # We're going to get the cycle by walking backwards along dependents,\n                        # so calculate dependents only for the nodes in play.\n                        inplay = set(priorities)\n                        dependents = reverse_dict(\n                            {k: inplay.intersection(dependencies[k]) for k in inplay}\n                        )\n                        # Begin with the node that was seen twice and the node `prev` from\n                        # which we detected the cycle.\n                        cycle = [nodes.pop()]\n                        cycle.append(prev)\n                        while prev != cycle[0]:\n                            # Greedily take a step that takes us closest to completing the cycle.\n                            # This may not give us the shortest cycle, but we get *a* short cycle.\n                            deps = dependents[cycle[-1]]\n                            prev = min(deps, key=priorities.__getitem__)\n                            cycle.append(prev)\n                        cycle.reverse()\n\n                        if returncycle:\n                            return cycle\n                        else:\n                            cycle = \"->\".join(str(x) for x in cycle)\n                            raise RuntimeError(\"Cycle detected in Dask: %s\" % cycle)\n                    next_nodes.append(nxt)\n\n            if next_nodes:\n                nodes.extend(next_nodes)\n            else:\n                # cur has no more descendants to explore, so we're done with it\n                if not returncycle:\n                    ordered.append(cur)\n                completed.add(cur)\n                seen.remove(cur)\n                nodes.pop()\n    if returncycle:\n        return []\n    return ordered\n\n\ndef toposort(dsk, dependencies=None):\n    \"\"\"Return a list of keys of dask sorted in topological order.\"\"\"\n    return _toposort(dsk, dependencies=dependencies)\n\n\ndef getcycle(d, keys):\n    \"\"\"Return a list of nodes that form a cycle if Dask is not a DAG.\n\n    Returns an empty list if no cycle is found.\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': (inc, 'z'), 'y': (inc, 'x'), 'z': (inc, 'y')}\n    >>> getcycle(d, 'x')\n    ['x', 'z', 'y', 'x']\n\n    See Also\n    --------\n    isdag\n    \"\"\"\n    return _toposort(d, keys=keys, returncycle=True)\n\n\ndef isdag(d, keys):\n    \"\"\"Does Dask form a directed acyclic graph when calculating keys?\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> inc = lambda x: x + 1\n    >>> isdag({'x': 0, 'y': (inc, 'x')}, 'y')\n    True\n    >>> isdag({'x': (inc, 'y'), 'y': (inc, 'x')}, 'y')\n    False\n\n    See Also\n    --------\n    getcycle\n    \"\"\"\n    return not getcycle(d, keys)\n\n\nclass literal:\n    \"\"\"A small serializable object to wrap literal values without copying\"\"\"\n\n    __slots__ = (\"data\",)\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return \"literal<type=%s>\" % type(self.data).__name__\n\n    def __reduce__(self):\n        return (literal, (self.data,))\n\n    def __call__(self):\n        return self.data\n\n\ndef quote(x):\n    \"\"\"Ensure that this value remains this value in a dask graph\n\n    Some values in dask graph take on special meaning. Sometimes we want to\n    ensure that our data is not interpreted but remains literal.\n\n    >>> add = lambda x, y: x + y\n    >>> quote((add, 1, 2))\n    (literal<type=tuple>,)\n    \"\"\"\n    if istask(x) or type(x) is list or type(x) is dict:\n        return (literal(x),)\n    return x\n\n\ndef reshapelist(shape, seq):\n    \"\"\"Reshape iterator to nested shape\n\n    >>> reshapelist((2, 3), range(6))\n    [[0, 1, 2], [3, 4, 5]]\n    \"\"\"\n    if len(shape) == 1:\n        return list(seq)\n    else:\n        n = int(len(seq) / shape[0])\n        return [reshapelist(shape[1:], part) for part in toolz.partition(n, seq)]\n\n\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom collections.abc import Collection, Iterable, Mapping, MutableMapping\nfrom typing import Any, Literal, TypeVar, cast, overload\n\nimport toolz\n\nfrom dask._task_spec import DependenciesMapping, convert_legacy_graph, execute_graph\nfrom dask.typing import Graph, Key, NoDefault, no_default\n\n\ndef ishashable(x):\n    \"\"\"Is x hashable?\n\n    Examples\n    --------\n\n    >>> ishashable(1)\n    True\n    >>> ishashable([1])\n    False\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    try:\n        hash(x)\n        return True\n    except TypeError:\n        return False\n\n\ndef istask(x):\n    \"\"\"Is x a runnable task?\n\n    A task is a tuple with a callable first argument\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> istask((inc, 1))\n    True\n    >>> istask(1)\n    False\n    \"\"\"\n    from dask._task_spec import DataNode, GraphNode\n\n    if isinstance(x, GraphNode):\n        if isinstance(x, DataNode):\n            return False\n        return True\n    return type(x) is tuple and x and callable(x[0])\n\n\ndef preorder_traversal(task):\n    \"\"\"A generator to preorder-traverse a task.\"\"\"\n\n    for item in task:\n        if istask(item):\n            yield from preorder_traversal(item)\n        elif isinstance(item, list):\n            yield list\n            yield from preorder_traversal(item)\n        else:\n            yield item\n\n\ndef lists_to_tuples(res, keys):\n    if isinstance(keys, list):\n        return tuple(lists_to_tuples(r, k) for r, k in zip(res, keys))\n    return res\n\n\ndef _pack_result(result: Mapping, keys: list | Key) -> Any:\n    if isinstance(keys, list):\n        return tuple(_pack_result(result, k) for k in keys)\n    return result[keys]\n\n\ndef get(dsk: Mapping, out: list | Key, cache: MutableMapping | None = None) -> Any:\n    \"\"\"Get value from Dask\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': 1, 'y': (inc, 'x')}\n\n    >>> get(d, 'x')\n    1\n    >>> get(d, 'y')\n    2\n    \"\"\"\n    for k in flatten(out):\n        if k not in dsk:\n            raise KeyError(f\"{k} is not a key in the graph\")\n    if cache is None:\n        cache = {}\n\n    dsk2 = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    result = execute_graph(dsk2, cache, keys=set(flatten([out])))\n    return _pack_result(result, out)\n\n\ndef keys_in_tasks(keys: Collection[Key], tasks: Iterable[Any], as_list: bool = False):\n    \"\"\"Returns the keys in `keys` that are also in `tasks`\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> keys_in_tasks(dsk, ['x', 'y', 'j'])  # doctest: +SKIP\n    {'x', 'y'}\n    \"\"\"\n    from dask._task_spec import GraphNode\n\n    ret: list[Key] = []\n    while tasks:\n        work = []\n        for w in tasks:\n            typ = type(w)\n            if typ is tuple and w and callable(w[0]):  # istask(w)\n                work.extend(w[1:])\n            elif typ is list:\n                work.extend(w)\n            elif typ is dict:\n                work.extend(w.values())\n            elif isinstance(w, GraphNode):\n                work.extend(w.dependencies)\n            else:\n                try:\n                    if w in keys:\n                        ret.append(w)\n                except TypeError:  # not hashable\n                    pass\n        tasks = work\n    return ret if as_list else set(ret)\n\n\ndef iskey(key: object) -> bool:\n    \"\"\"Return True if the given object is a potential dask key; False otherwise.\n\n    The definition of a key in a Dask graph is any str, bytes, int, float, or tuple\n    thereof.\n\n    See Also\n    --------\n    ishashable\n    validate_key\n    dask.typing.Key\n    \"\"\"\n    typ = type(key)\n    if typ is tuple:\n        return all(iskey(i) for i in cast(tuple, key))\n    return typ in {bytes, int, float, str}\n\n\ndef validate_key(key: object) -> None:\n    \"\"\"Validate the format of a dask key.\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    if iskey(key):\n        return\n    typ = type(key)\n\n    if typ is tuple:\n        index = None\n        try:\n            for index, part in enumerate(cast(tuple, key)):  # noqa: B007\n                validate_key(part)\n        except TypeError as e:\n            raise TypeError(\n                f\"Composite key contains unexpected key type at {index=} ({key=!r})\"\n            ) from e\n    raise TypeError(f\"Unexpected key type {typ} ({key=!r})\")\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = ...,\n    task: Key | NoDefault = ...,\n    as_list: Literal[False] = ...,\n) -> set[Key]: ...\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None,\n    task: Key | NoDefault,\n    as_list: Literal[True],\n) -> list[Key]: ...\n\n\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = None,\n    task: Key | NoDefault = no_default,\n    as_list: bool = False,\n) -> set[Key] | list[Key]:\n    \"\"\"Get the immediate tasks on which this task depends\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> get_dependencies(dsk, 'x')\n    set()\n\n    >>> get_dependencies(dsk, 'y')\n    {'x'}\n\n    >>> get_dependencies(dsk, 'z')  # doctest: +SKIP\n    {'x', 'y'}\n\n    >>> get_dependencies(dsk, 'w')  # Only direct dependencies\n    {'z'}\n\n    >>> get_dependencies(dsk, 'a')  # Ignore non-keys\n    {'x'}\n\n    >>> get_dependencies(dsk, task=(inc, 'x'))  # provide tasks directly\n    {'x'}\n    \"\"\"\n    if key is not None:\n        arg = dsk[key]\n    elif task is not no_default:\n        arg = task\n    else:\n        raise ValueError(\"Provide either key or task\")\n\n    return keys_in_tasks(dsk, [arg], as_list=as_list)\n\n\ndef get_deps(dsk: Graph) -> tuple[dict[Key, set[Key]], dict[Key, set[Key]]]:\n    \"\"\"Get dependencies and dependents from dask dask graph\n\n    >>> inc = lambda x: x + 1\n    >>> dsk = {'a': 1, 'b': (inc, 'a'), 'c': (inc, 'b')}\n    >>> dependencies, dependents = get_deps(dsk)\n    >>> dependencies\n    {'a': set(), 'b': {'a'}, 'c': {'b'}}\n    >>> dependents  # doctest: +SKIP\n    {'a': {'b'}, 'b': {'c'}, 'c': set()}\n    \"\"\"\n    dependencies = {k: get_dependencies(dsk, task=v) for k, v in dsk.items()}\n    dependents = reverse_dict(dependencies)\n    return dependencies, dependents\n\n\ndef flatten(seq, container=list):\n    \"\"\"\n\n    >>> list(flatten([1]))\n    [1]\n\n    >>> list(flatten([[1, 2], [1, 2]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten([[[1], [2]], [[1], [2]]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten(((1, 2), (1, 2)))) # Don't flatten tuples\n    [(1, 2), (1, 2)]\n\n    >>> list(flatten((1, 2, [3, 4]))) # support heterogeneous\n    [1, 2, 3, 4]\n    \"\"\"\n    if isinstance(seq, str):\n        yield seq\n    else:\n        for item in seq:\n            if isinstance(item, container):\n                yield from flatten(item, container=container)\n            else:\n                yield item\n\n\nT_ = TypeVar(\"T_\")\n\n\ndef reverse_dict(d: Mapping[T_, Iterable[T_]]) -> dict[T_, set[T_]]:\n    \"\"\"\n\n    >>> a, b, c = 'abc'\n    >>> d = {a: [b, c], b: [c]}\n    >>> reverse_dict(d)  # doctest: +SKIP\n    {'a': set([]), 'b': set(['a']}, 'c': set(['a', 'b'])}\n    \"\"\"\n    result: defaultdict[T_, set[T_]] = defaultdict(set)\n    _add = set.add\n    for k, vals in d.items():\n        result[k]\n        for val in vals:\n            _add(result[val], k)\n    return dict(result)\n\n\ndef subs(task, key, val):\n    \"\"\"Perform a substitution on a task\n\n    Examples\n    --------\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> subs((inc, 'x'), 'x', 1)  # doctest: +ELLIPSIS\n    (<function inc at ...>, 1)\n    \"\"\"\n    type_task = type(task)\n    if not (type_task is tuple and task and callable(task[0])):  # istask(task):\n        try:\n            if type_task is type(key) and task == key:\n                return val\n        except Exception:\n            pass\n        if type_task is list:\n            return [subs(x, key, val) for x in task]\n        return task\n    newargs = []\n    hash_key = {key}\n    for arg in task[1:]:\n        type_arg = type(arg)\n        if type_arg is tuple and arg and callable(arg[0]):  # istask(task):\n            arg = subs(arg, key, val)\n        elif type_arg is list:\n            arg = [subs(x, key, val) for x in arg]\n        else:\n            try:\n                if arg in hash_key:  # Hash and equality match\n                    arg = val\n            except TypeError:  # not hashable\n                pass\n        newargs.append(arg)\n    return task[:1] + tuple(newargs)\n\n\ndef _toposort(dsk, keys=None, returncycle=False, dependencies=None):\n\n    # Stack-based depth-first search traversal.  This is based on Tarjan's\n    # method for topological sorting (see wikipedia for pseudocode)\n    if keys is None:\n        keys = dsk\n    elif not isinstance(keys, list):\n        keys = [keys]\n    if not returncycle:\n        ordered = []\n\n    # Nodes whose descendents have been completely explored.\n    # These nodes are guaranteed to not be part of a cycle.\n    completed = set()\n\n    # All nodes that have been visited in the current traversal.  Because\n    # we are doing depth-first search, going \"deeper\" should never result\n    # in visiting a node that has already been seen.  The `seen` and\n    # `completed` sets are mutually exclusive; it is okay to visit a node\n    # that has already been added to `completed`.\n    seen = set()\n\n    if dependencies is None:\n\n        dependencies = DependenciesMapping(dsk)\n\n    for key in keys:\n        if key in completed:\n            continue\n        nodes = [key]\n        while nodes:\n            # Keep current node on the stack until all descendants are visited\n            cur = nodes[-1]\n            if cur in completed:\n                # Already fully traversed descendants of cur\n                nodes.pop()\n                continue\n            seen.add(cur)\n\n            # Add direct descendants of cur to nodes stack\n            next_nodes = []\n            for nxt in dependencies[cur]:\n                if nxt not in completed:\n                    if nxt in seen:\n                        # Cycle detected!\n                        # Let's report only the nodes that directly participate in the cycle.\n                        # We use `priorities` below to greedily construct a short cycle.\n                        # Shorter cycles may exist.\n                        priorities = {}\n                        prev = nodes[-1]\n                        # Give priority to nodes that were seen earlier.\n                        while nodes[-1] != nxt:\n                            priorities[nodes.pop()] = -len(priorities)\n                        priorities[nxt] = -len(priorities)\n                        # We're going to get the cycle by walking backwards along dependents,\n                        # so calculate dependents only for the nodes in play.\n                        inplay = set(priorities)\n                        dependents = reverse_dict(\n                            {k: inplay.intersection(dependencies[k]) for k in inplay}\n                        )\n                        # Begin with the node that was seen twice and the node `prev` from\n                        # which we detected the cycle.\n                        cycle = [nodes.pop()]\n                        cycle.append(prev)\n                        while prev != cycle[0]:\n                            # Greedily take a step that takes us closest to completing the cycle.\n                            # This may not give us the shortest cycle, but we get *a* short cycle.\n                            deps = dependents[cycle[-1]]\n                            prev = min(deps, key=priorities.__getitem__)\n                            cycle.append(prev)\n                        cycle.reverse()\n\n                        if returncycle:\n                            return cycle\n                        else:\n                            cycle = \"->\".join(str(x) for x in cycle)\n                            raise RuntimeError(\"Cycle detected in Dask: %s\" % cycle)\n                    next_nodes.append(nxt)\n\n            if next_nodes:\n                nodes.extend(next_nodes)\n            else:\n                # cur has no more descendants to explore, so we're done with it\n                if not returncycle:\n                    ordered.append(cur)\n                completed.add(cur)\n                seen.remove(cur)\n                nodes.pop()\n    if returncycle:\n        return []\n    return ordered\n\n\ndef toposort(dsk, dependencies=None):\n    \"\"\"Return a list of keys of dask sorted in topological order.\"\"\"\n    return _toposort(dsk, dependencies=dependencies)\n\n\ndef getcycle(d, keys):\n    \"\"\"Return a list of nodes that form a cycle if Dask is not a DAG.\n\n    Returns an empty list if no cycle is found.\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': (inc, 'z'), 'y': (inc, 'x'), 'z': (inc, 'y')}\n    >>> getcycle(d, 'x')\n    ['x', 'z', 'y', 'x']\n\n    See Also\n    --------\n    isdag\n    \"\"\"\n    return _toposort(d, keys=keys, returncycle=True)\n\n\ndef isdag(d, keys):\n    \"\"\"Does Dask form a directed acyclic graph when calculating keys?\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> inc = lambda x: x + 1\n    >>> isdag({'x': 0, 'y': (inc, 'x')}, 'y')\n    True\n    >>> isdag({'x': (inc, 'y'), 'y': (inc, 'x')}, 'y')\n    False\n\n    See Also\n    --------\n    getcycle\n    \"\"\"\n    return not getcycle(d, keys)\n\n\nclass literal:\n    \"\"\"A small serializable object to wrap literal values without copying\"\"\"\n\n    __slots__ = (\"data\",)\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return \"literal<type=%s>\" % type(self.data).__name__\n\n    def __reduce__(self):\n        return (literal, (self.data,))\n\n    def __call__(self):\n        return self.data\n\n\ndef quote(x):\n    \"\"\"Ensure that this value remains this value in a dask graph\n\n    Some values in dask graph take on special meaning. Sometimes we want to\n    ensure that our data is not interpreted but remains literal.\n\n    >>> add = lambda x, y: x + y\n    >>> quote((add, 1, 2))\n    (literal<type=tuple>,)\n    \"\"\"\n    if istask(x) or type(x) is list or type(x) is dict:\n        return (literal(x),)\n    return x\n\n\ndef reshapelist(shape, seq):\n    \"\"\"Reshape iterator to nested shape\n\n    >>> reshapelist((2, 3), range(6))\n    [[0, 1, 2], [3, 4, 5]]\n    \"\"\"\n    if len(shape) == 1:\n        return list(seq)\n    else:\n        n = int(len(seq) / shape[0])\n        return [reshapelist(shape[1:], part) for part in toolz.partition(n, seq)]\n\n\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom collections.abc import Collection, Iterable, Mapping, MutableMapping\nfrom typing import Any, Literal, TypeVar, cast, overload\n\nimport toolz\n\nfrom dask._task_spec import DependenciesMapping, convert_legacy_graph, execute_graph\nfrom dask.typing import Graph, Key, NoDefault, no_default\n\n\ndef ishashable(x):\n    \"\"\"Is x hashable?\n\n    Examples\n    --------\n\n    >>> ishashable(1)\n    True\n    >>> ishashable([1])\n    False\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    try:\n        hash(x)\n        return True\n    except TypeError:\n        return False\n\n\ndef istask(x):\n    \"\"\"Is x a runnable task?\n\n    A task is a tuple with a callable first argument\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> istask((inc, 1))\n    True\n    >>> istask(1)\n    False\n    \"\"\"\n    from dask._task_spec import DataNode, GraphNode\n\n    if isinstance(x, GraphNode):\n        if isinstance(x, DataNode):\n            return False\n        return True\n    return type(x) is tuple and x and callable(x[0])\n\n\ndef preorder_traversal(task):\n    \"\"\"A generator to preorder-traverse a task.\"\"\"\n\n    for item in task:\n        if istask(item):\n            yield from preorder_traversal(item)\n        elif isinstance(item, list):\n            yield list\n            yield from preorder_traversal(item)\n        else:\n            yield item\n\n\ndef lists_to_tuples(res, keys):\n    if isinstance(keys, list):\n        return tuple(lists_to_tuples(r, k) for r, k in zip(res, keys))\n    return res\n\n\ndef _pack_result(result: Mapping, keys: list | Key) -> Any:\n    if isinstance(keys, list):\n        return tuple(_pack_result(result, k) for k in keys)\n    return result[keys]\n\n\ndef get(dsk: Mapping, out: list | Key, cache: MutableMapping | None = None) -> Any:\n    \"\"\"Get value from Dask\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': 1, 'y': (inc, 'x')}\n\n    >>> get(d, 'x')\n    1\n    >>> get(d, 'y')\n    2\n    \"\"\"\n    for k in flatten(out):\n        if k not in dsk:\n            raise KeyError(f\"{k} is not a key in the graph\")\n    if cache is None:\n        cache = {}\n\n    dsk2 = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    result = execute_graph(dsk2, cache, keys=set(flatten([out])))\n    return _pack_result(result, out)\n\n\ndef keys_in_tasks(keys: Collection[Key], tasks: Iterable[Any], as_list: bool = False):\n    \"\"\"Returns the keys in `keys` that are also in `tasks`\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> keys_in_tasks(dsk, ['x', 'y', 'j'])  # doctest: +SKIP\n    {'x', 'y'}\n    \"\"\"\n    from dask._task_spec import GraphNode\n\n    ret: list[Key] = []\n    while tasks:\n        work = []\n        for w in tasks:\n            typ = type(w)\n            if typ is tuple and w and callable(w[0]):  # istask(w)\n                work.extend(w[1:])\n            elif typ is list:\n                work.extend(w)\n            elif typ is dict:\n                work.extend(w.values())\n            elif isinstance(w, GraphNode):\n                work.extend(w.dependencies)\n            else:\n                try:\n                    if w in keys:\n                        ret.append(w)\n                except TypeError:  # not hashable\n                    pass\n        tasks = work\n    return ret if as_list else set(ret)\n\n\ndef iskey(key: object) -> bool:\n    \"\"\"Return True if the given object is a potential dask key; False otherwise.\n\n    The definition of a key in a Dask graph is any str, bytes, int, float, or tuple\n    thereof.\n\n    See Also\n    --------\n    ishashable\n    validate_key\n    dask.typing.Key\n    \"\"\"\n    typ = type(key)\n    if typ is tuple:\n        return all(iskey(i) for i in cast(tuple, key))\n    return typ in {bytes, int, float, str}\n\n\ndef validate_key(key: object) -> None:\n    \"\"\"Validate the format of a dask key.\n\n    See Also\n    --------\n    iskey\n    \"\"\"\n    if iskey(key):\n        return\n    typ = type(key)\n\n    if typ is tuple:\n        index = None\n        try:\n            for index, part in enumerate(cast(tuple, key)):  # noqa: B007\n                validate_key(part)\n        except TypeError as e:\n            raise TypeError(\n                f\"Composite key contains unexpected key type at {index=} ({key=!r})\"\n            ) from e\n    raise TypeError(f\"Unexpected key type {typ} ({key=!r})\")\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = ...,\n    task: Key | NoDefault = ...,\n    as_list: Literal[False] = ...,\n) -> set[Key]: ...\n\n\n@overload\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None,\n    task: Key | NoDefault,\n    as_list: Literal[True],\n) -> list[Key]: ...\n\n\ndef get_dependencies(\n    dsk: Graph,\n    key: Key | None = None,\n    task: Key | NoDefault = no_default,\n    as_list: bool = False,\n) -> set[Key] | list[Key]:\n    \"\"\"Get the immediate tasks on which this task depends\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1,\n    ...        'y': (inc, 'x'),\n    ...        'z': (add, 'x', 'y'),\n    ...        'w': (inc, 'z'),\n    ...        'a': (add, (inc, 'x'), 1)}\n\n    >>> get_dependencies(dsk, 'x')\n    set()\n\n    >>> get_dependencies(dsk, 'y')\n    {'x'}\n\n    >>> get_dependencies(dsk, 'z')  # doctest: +SKIP\n    {'x', 'y'}\n\n    >>> get_dependencies(dsk, 'w')  # Only direct dependencies\n    {'z'}\n\n    >>> get_dependencies(dsk, 'a')  # Ignore non-keys\n    {'x'}\n\n    >>> get_dependencies(dsk, task=(inc, 'x'))  # provide tasks directly\n    {'x'}\n    \"\"\"\n    if key is not None:\n        arg = dsk[key]\n    elif task is not no_default:\n        arg = task\n    else:\n        raise ValueError(\"Provide either key or task\")\n\n    return keys_in_tasks(dsk, [arg], as_list=as_list)\n\n\ndef get_deps(dsk: Graph) -> tuple[dict[Key, set[Key]], dict[Key, set[Key]]]:\n    \"\"\"Get dependencies and dependents from dask dask graph\n\n    >>> inc = lambda x: x + 1\n    >>> dsk = {'a': 1, 'b': (inc, 'a'), 'c': (inc, 'b')}\n    >>> dependencies, dependents = get_deps(dsk)\n    >>> dependencies\n    {'a': set(), 'b': {'a'}, 'c': {'b'}}\n    >>> dependents  # doctest: +SKIP\n    {'a': {'b'}, 'b': {'c'}, 'c': set()}\n    \"\"\"\n    dependencies = {k: get_dependencies(dsk, task=v) for k, v in dsk.items()}\n    dependents = reverse_dict(dependencies)\n    return dependencies, dependents\n\n\ndef flatten(seq, container=list):\n    \"\"\"\n\n    >>> list(flatten([1]))\n    [1]\n\n    >>> list(flatten([[1, 2], [1, 2]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten([[[1], [2]], [[1], [2]]]))\n    [1, 2, 1, 2]\n\n    >>> list(flatten(((1, 2), (1, 2)))) # Don't flatten tuples\n    [(1, 2), (1, 2)]\n\n    >>> list(flatten((1, 2, [3, 4]))) # support heterogeneous\n    [1, 2, 3, 4]\n    \"\"\"\n    if isinstance(seq, str):\n        yield seq\n    else:\n        for item in seq:\n            if isinstance(item, container):\n                yield from flatten(item, container=container)\n            else:\n                yield item\n\n\nT_ = TypeVar(\"T_\")\n\n\ndef reverse_dict(d: Mapping[T_, Iterable[T_]]) -> dict[T_, set[T_]]:\n    \"\"\"\n\n    >>> a, b, c = 'abc'\n    >>> d = {a: [b, c], b: [c]}\n    >>> reverse_dict(d)  # doctest: +SKIP\n    {'a': set([]), 'b': set(['a']}, 'c': set(['a', 'b'])}\n    \"\"\"\n    result: defaultdict[T_, set[T_]] = defaultdict(set)\n    _add = set.add\n    for k, vals in d.items():\n        result[k]\n        for val in vals:\n            _add(result[val], k)\n    return dict(result)\n\n\ndef subs(task, key, val):\n    \"\"\"Perform a substitution on a task\n\n    Examples\n    --------\n    >>> def inc(x):\n    ...     return x + 1\n\n    >>> subs((inc, 'x'), 'x', 1)  # doctest: +ELLIPSIS\n    (<function inc at ...>, 1)\n    \"\"\"\n    type_task = type(task)\n    if not (type_task is tuple and task and callable(task[0])):  # istask(task):\n        try:\n            if type_task is type(key) and task == key:\n                return val\n        except Exception:\n            pass\n        if type_task is list:\n            return [subs(x, key, val) for x in task]\n        return task\n    newargs = []\n    hash_key = {key}\n    for arg in task[1:]:\n        type_arg = type(arg)\n        if type_arg is tuple and arg and callable(arg[0]):  # istask(task):\n            arg = subs(arg, key, val)\n        elif type_arg is list:\n            arg = [subs(x, key, val) for x in arg]\n        else:\n            try:\n                if arg in hash_key:  # Hash and equality match\n                    arg = val\n            except TypeError:  # not hashable\n                pass\n        newargs.append(arg)\n    return task[:1] + tuple(newargs)\n\n\ndef _toposort(dsk, keys=None, returncycle=False, dependencies=None):\n\n    # Stack-based depth-first search traversal.  This is based on Tarjan's\n    # method for topological sorting (see wikipedia for pseudocode)\n    if keys is None:\n        keys = dsk\n    elif not isinstance(keys, list):\n        keys = [keys]\n    if not returncycle:\n        ordered = []\n\n    # Nodes whose descendents have been completely explored.\n    # These nodes are guaranteed to not be part of a cycle.\n    completed = set()\n\n    # All nodes that have been visited in the current traversal.  Because\n    # we are doing depth-first search, going \"deeper\" should never result\n    # in visiting a node that has already been seen.  The `seen` and\n    # `completed` sets are mutually exclusive; it is okay to visit a node\n    # that has already been added to `completed`.\n    seen = set()\n\n    if dependencies is None:\n\n        dependencies = DependenciesMapping(dsk)\n\n    for key in keys:\n        if key in completed:\n            continue\n        nodes = [key]\n        while nodes:\n            # Keep current node on the stack until all descendants are visited\n            cur = nodes[-1]\n            if cur in completed:\n                # Already fully traversed descendants of cur\n                nodes.pop()\n                continue\n            seen.add(cur)\n\n            # Add direct descendants of cur to nodes stack\n            next_nodes = []\n            for nxt in dependencies[cur]:\n                if nxt not in completed:\n                    if nxt in seen:\n                        # Cycle detected!\n                        # Let's report only the nodes that directly participate in the cycle.\n                        # We use `priorities` below to greedily construct a short cycle.\n                        # Shorter cycles may exist.\n                        priorities = {}\n                        prev = nodes[-1]\n                        # Give priority to nodes that were seen earlier.\n                        while nodes[-1] != nxt:\n                            priorities[nodes.pop()] = -len(priorities)\n                        priorities[nxt] = -len(priorities)\n                        # We're going to get the cycle by walking backwards along dependents,\n                        # so calculate dependents only for the nodes in play.\n                        inplay = set(priorities)\n                        dependents = reverse_dict(\n                            {k: inplay.intersection(dependencies[k]) for k in inplay}\n                        )\n                        # Begin with the node that was seen twice and the node `prev` from\n                        # which we detected the cycle.\n                        cycle = [nodes.pop()]\n                        cycle.append(prev)\n                        while prev != cycle[0]:\n                            # Greedily take a step that takes us closest to completing the cycle.\n                            # This may not give us the shortest cycle, but we get *a* short cycle.\n                            deps = dependents[cycle[-1]]\n                            prev = min(deps, key=priorities.__getitem__)\n                            cycle.append(prev)\n                        cycle.reverse()\n\n                        if returncycle:\n                            return cycle\n                        else:\n                            cycle = \"->\".join(str(x) for x in cycle)\n                            raise RuntimeError(\"Cycle detected in Dask: %s\" % cycle)\n                    next_nodes.append(nxt)\n\n            if next_nodes:\n                nodes.extend(next_nodes)\n            else:\n                # cur has no more descendants to explore, so we're done with it\n                if not returncycle:\n                    ordered.append(cur)\n                completed.add(cur)\n                seen.remove(cur)\n                nodes.pop()\n    if returncycle:\n        return []\n    return ordered\n\n\ndef toposort(dsk, dependencies=None):\n    \"\"\"Return a list of keys of dask sorted in topological order.\"\"\"\n    return _toposort(dsk, dependencies=dependencies)\n\n\ndef getcycle(d, keys):\n    \"\"\"Return a list of nodes that form a cycle if Dask is not a DAG.\n\n    Returns an empty list if no cycle is found.\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> d = {'x': (inc, 'z'), 'y': (inc, 'x'), 'z': (inc, 'y')}\n    >>> getcycle(d, 'x')\n    ['x', 'z', 'y', 'x']\n\n    See Also\n    --------\n    isdag\n    \"\"\"\n    return _toposort(d, keys=keys, returncycle=True)\n\n\ndef isdag(d, keys):\n    \"\"\"Does Dask form a directed acyclic graph when calculating keys?\n\n    ``keys`` may be a single key or list of keys.\n\n    Examples\n    --------\n\n    >>> inc = lambda x: x + 1\n    >>> inc = lambda x: x + 1\n    >>> isdag({'x': 0, 'y': (inc, 'x')}, 'y')\n    True\n    >>> isdag({'x': (inc, 'y'), 'y': (inc, 'x')}, 'y')\n    False\n\n    See Also\n    --------\n    getcycle\n    \"\"\"\n    return not getcycle(d, keys)\n\n\nclass literal:\n    \"\"\"A small serializable object to wrap literal values without copying\"\"\"\n\n    __slots__ = (\"data\",)\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        return \"literal<type=%s>\" % type(self.data).__name__\n\n    def __reduce__(self):\n        return (literal, (self.data,))\n\n    def __call__(self):\n        return self.data\n\n\ndef quote(x):\n    \"\"\"Ensure that this value remains this value in a dask graph\n\n    Some values in dask graph take on special meaning. Sometimes we want to\n    ensure that our data is not interpreted but remains literal.\n\n    >>> add = lambda x, y: x + y\n    >>> quote((add, 1, 2))\n    (literal<type=tuple>,)\n    \"\"\"\n    if istask(x) or type(x) is list or type(x) is dict:\n        return (literal(x),)\n    return x\n\n\ndef reshapelist(shape, seq):\n    \"\"\"Reshape iterator to nested shape\n\n    >>> reshapelist((2, 3), range(6))\n    [[0, 1, 2], [3, 4, 5]]\n    \"\"\"\n    if len(shape) == 1:\n        return list(seq)\n    else:\n        n = int(len(seq) / shape[0])\n        return [reshapelist(shape[1:], part) for part in toolz.partition(n, seq)]\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:31:08"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport pickle\nfrom collections import namedtuple\n\nimport pytest\n\nfrom dask.core import (\n    flatten,\n    get,\n    get_dependencies,\n    get_deps,\n    getcycle,\n    ishashable,\n    iskey,\n    istask,\n    literal,\n    preorder_traversal,\n    quote,\n    subs,\n    validate_key,\n)\nfrom dask.utils_test import GetFunctionTestMixin, add, inc\n\n\ndef contains(a, b):\n    \"\"\"\n\n    >>> contains({'x': 1, 'y': 2}, {'x': 1})\n    True\n    >>> contains({'x': 1, 'y': 2}, {'z': 3})\n    False\n    \"\"\"\n    return all(a.get(k) == v for k, v in b.items())\n\n\ndef test_ishashable():\n    class C:\n        pass\n\n    assert ishashable(\"x\")\n    assert ishashable(1)\n    assert ishashable(C())\n    assert ishashable((1, 2))\n    assert not ishashable([1, 2])\n    assert not ishashable({1: 2})\n\n\ndef test_iskey():\n    class C:\n        pass\n\n    assert iskey(\"x\")\n    assert iskey(1)\n    assert not iskey(C())  # Custom hashables can't be dask keys\n    assert not iskey((C(),))\n    assert iskey((1, 2))\n    assert iskey(())\n    assert iskey((\"x\",))\n    assert not iskey([1, 2])\n    assert not iskey({1, 2})\n    assert not iskey({1: 2})\n\n\ndef test_iskey_numpy_types():\n    np = pytest.importorskip(\"numpy\")\n    one = np.int64(1)\n    assert not iskey(one)\n    assert not iskey((\"foo\", one))\n\n\ndef test_validate_key():\n    validate_key(1)\n    validate_key((\"x\", 1))\n    with pytest.raises(TypeError, match=\"Unexpected key type.*list\"):\n        validate_key([\"x\", 1])\n\n    with pytest.raises(TypeError, match=\"unexpected key type at index=1\"):\n        validate_key((2, int))\n\n\ndef test_istask():\n    assert istask((inc, 1))\n    assert not istask(1)\n    assert not istask((1, 2))\n    f = namedtuple(\"f\", [\"x\", \"y\"])\n    assert not istask(f(sum, 2))\n\n\ndef test_preorder_traversal():\n    t = (add, 1, 2)\n    assert list(preorder_traversal(t)) == [add, 1, 2]\n    t = (add, (add, 1, 2), (add, 3, 4))\n    assert list(preorder_traversal(t)) == [add, add, 1, 2, add, 3, 4]\n    t = (add, (sum, [1, 2]), 3)\n    assert list(preorder_traversal(t)) == [add, sum, list, 1, 2, 3]\n\n\nclass TestGet(GetFunctionTestMixin):\n    get = staticmethod(get)\n\n\ndef test_GetFunctionTestMixin_class():\n    class TestCustomGetFail(GetFunctionTestMixin):\n        get = staticmethod(lambda x, y: 1)\n\n    custom_testget = TestCustomGetFail()\n    pytest.raises(AssertionError, custom_testget.test_get)\n\n    class TestCustomGetPass(GetFunctionTestMixin):\n        get = staticmethod(get)\n\n    custom_testget = TestCustomGetPass()\n    custom_testget.test_get()\n\n\ndef test_get_dependencies_nested():\n    dsk = {\"x\": 1, \"y\": 2, \"z\": (add, (inc, [[\"x\"]]), \"y\")}\n\n    assert get_dependencies(dsk, \"z\") == {\"x\", \"y\"}\n    assert sorted(get_dependencies(dsk, \"z\", as_list=True)) == [\"x\", \"y\"]\n\n\ndef test_get_dependencies_empty():\n    dsk = {\"x\": (inc,)}\n    assert get_dependencies(dsk, \"x\") == set()\n    assert get_dependencies(dsk, \"x\", as_list=True) == []\n\n\ndef test_get_dependencies_list():\n    dsk = {\"x\": 1, \"y\": 2, \"z\": [\"x\", [(inc, \"y\")]]}\n    assert get_dependencies(dsk, \"z\") == {\"x\", \"y\"}\n    assert sorted(get_dependencies(dsk, \"z\", as_list=True)) == [\"x\", \"y\"]\n\n\ndef test_get_dependencies_task():\n    dsk = {\"x\": 1, \"y\": 2, \"z\": [\"x\", [(inc, \"y\")]]}\n    assert get_dependencies(dsk, task=(inc, \"x\")) == {\"x\"}\n    assert get_dependencies(dsk, task=(inc, \"x\"), as_list=True) == [\"x\"]\n\n\ndef test_get_dependencies_nothing():\n    with pytest.raises(ValueError):\n        get_dependencies({})\n\n\ndef test_get_dependencies_many():\n    dsk = {\n        \"a\": [1, 2, 3],\n        \"b\": \"a\",\n        \"c\": [1, (inc, 1)],\n        \"d\": [(sum, \"c\")],\n        \"e\": [\"a\", \"b\", \"zzz\"],\n        \"f\": [[\"a\", \"b\"], 2, 3],\n    }\n\n    tasks = [dsk[k] for k in (\"d\", \"f\")]\n    s = get_dependencies(dsk, task=tasks)\n    assert s == {\"a\", \"b\", \"c\"}\n    s = get_dependencies(dsk, task=tasks, as_list=True)\n    assert sorted(s) == [\"a\", \"b\", \"c\"]\n\n    s = get_dependencies(dsk, task=[])\n    assert s == set()\n    s = get_dependencies(dsk, task=[], as_list=True)\n    assert s == []\n\n\ndef test_get_dependencies_task_none():\n    # Regression test for https://github.com/dask/distributed/issues/2756\n    dsk = {\"foo\": None}\n    assert get_dependencies(dsk, task=dsk[\"foo\"]) == set()\n\n\ndef test_get_deps():\n    dsk = {\n        \"a\": [1, 2, 3],\n        \"b\": \"a\",\n        \"c\": [1, (inc, 1)],\n        \"d\": [(sum, \"c\")],\n        \"e\": [\"b\", \"zzz\", \"b\"],\n        \"f\": [[\"a\", \"b\"], 2, 3],\n    }\n    dependencies, dependents = get_deps(dsk)\n    assert dependencies == {\n        \"a\": set(),\n        \"b\": {\"a\"},\n        \"c\": set(),\n        \"d\": {\"c\"},\n        \"e\": {\"b\"},\n        \"f\": {\"a\", \"b\"},\n    }\n    assert dependents == {\n        \"a\": {\"b\", \"f\"},\n        \"b\": {\"e\", \"f\"},\n        \"c\": {\"d\"},\n        \"d\": set(),\n        \"e\": set(),\n        \"f\": set(),\n    }\n\n\ndef test_flatten():\n    assert list(flatten(())) == []\n    assert list(flatten(\"foo\")) == [\"foo\"]\n\n\ndef test_subs():\n    assert subs((sum, [1, \"x\"]), \"x\", 2) == (sum, [1, 2])\n    assert subs((sum, [1, [\"x\"]]), \"x\", 2) == (sum, [1, [2]])\n\n\nclass MutateOnEq:\n    hit_eq = 0\n\n    def __eq__(self, other):\n        self.hit_eq += 1\n        return False\n\n\ndef test_subs_no_key_data_eq():\n    # Numpy throws a deprecation warning on bool(array == scalar), which\n    # pollutes the terminal. This test checks that `subs` never tries to\n    # compare keys (scalars) with values (which could be arrays)`subs` never\n    # tries to compare keys (scalars) with values (which could be arrays).\n    a = MutateOnEq()\n    subs(a, \"x\", 1)\n    assert a.hit_eq == 0\n    subs((add, a, \"x\"), \"x\", 1)\n    assert a.hit_eq == 0\n\n\ndef test_subs_with_unfriendly_eq():\n    try:\n        import numpy as np\n    except ImportError:\n        return\n    else:\n        task = (np.sum, np.array([1, 2]))\n        assert (subs(task, (4, 5), 1) == task) is True\n\n    class MyException(Exception):\n        pass\n\n    class F:\n        def __eq__(self, other):\n            raise MyException()\n\n    task = F()\n    assert subs(task, 1, 2) is task\n\n\ndef test_subs_with_surprisingly_friendly_eq():\n    try:\n        import pandas as pd\n    except ImportError:\n        return\n    else:\n        df = pd.DataFrame()\n        assert subs(df, \"x\", 1) is df\n\n\ndef test_subs_arbitrary_key():\n    key = (1.2, \"foo\", (3,))\n    assert subs((id, key), key, 1) == (id, 1)\n\n\ndef test_quote():\n    literals = [[1, 2, 3], (add, 1, 2), [1, [2, 3]], (add, 1, (add, 2, 3)), {\"x\": \"x\"}]\n\n    for l in literals:\n        assert get({\"x\": quote(l)}, \"x\") == l\n\n\ndef test_literal_serializable():\n    l = literal((add, 1, 2))\n    assert pickle.loads(pickle.dumps(l)).data == (add, 1, 2)\n\n\ndef test_getcycle():\n    dsk = {\n        0: [7, 13, 7, 9, 13, 3, 9, 18, 18, 17],\n        1: [14, 14, 12, 1, 9, 16, 4, 5, 9, 8],\n        2: [3, 1, 7, 7, 2, 0, 0, 6, 3, 2],\n        3: [4, 8, 3, 14, 15, 19, 14, 1, 9, 1],\n        4: [9, 13, 19, 4, 16, 8, 11, 1, 16, 1],\n        5: [9, 8, 12, 13, 10, 13, 19, 3, 18, 18],\n        6: [10, 2, 13, 16, 3, 12, 7, 16, 5, 17],\n        7: [16, 8, 6, 4, 4, 10, 3, 1, 13, 10],\n        8: [11, 4, 12, 10, 14, 6, 18, 15, 16, 12],\n        9: [17, 18, 5, 16, 19, 16, 3, 6, 16, 18],\n    }\n    assert len(getcycle(dsk, list(dsk))) <= 3  # 7->6->7\n    dsk = {\n        0: [1, 27, 6, 25, 30, 16, 1, 1, 35, 17],\n        1: [35, 22, 21, 31, 2, 28, 36, 0, 29, 29],\n        2: [14, 27, 10, 1, 38, 18, 28, 28, 6, 0],\n        3: [0, 37, 7, 16, 38, 28, 34, 13, 30, 10],\n        4: [22, 22, 27, 13, 29, 36, 22, 9, 39, 19],\n        5: [38, 7, 18, 17, 33, 5, 29, 11, 23, 30],\n        6: [3, 30, 28, 38, 4, 13, 14, 27, 29, 38],\n        7: [22, 27, 12, 2, 22, 16, 34, 15, 18, 16],\n        8: [36, 21, 24, 22, 28, 38, 21, 2, 4, 24],\n        9: [38, 32, 38, 7, 31, 34, 39, 20, 30, 18],\n    }\n    assert len(getcycle(dsk, list(dsk))) <= 4  # 0->1->2->0\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_dot\\test_dot.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport os\nimport re\nfrom functools import partial\n\nfrom dask.core import get_dependencies, ishashable, istask\nfrom dask.utils import apply, funcname, import_required, key_split\n\n\ndef task_label(task):\n    \"\"\"Label for a task on a dot graph.\n\n    Examples\n    --------\n    >>> from operator import add\n    >>> task_label((add, 1, 2))\n    'add'\n    >>> task_label((add, (add, 1, 2), 3))\n    'add(...)'\n    \"\"\"\n    func = task[0]\n    if func is apply:\n        func = task[1]\n    if hasattr(func, \"funcs\"):\n        if len(func.funcs) > 1:\n            return f\"{funcname(func.funcs[0])}(...)\"\n        else:\n            head = funcname(func.funcs[0])\n    else:\n        head = funcname(func)\n    if any(has_sub_tasks(i) for i in task[1:]):\n        return f\"{head}(...)\"\n    else:\n        return head\n\n\ndef has_sub_tasks(task):\n    \"\"\"Returns True if the task has sub tasks\"\"\"\n    if istask(task):\n        return True\n    elif isinstance(task, list):\n        return any(has_sub_tasks(i) for i in task)\n    else:\n        return False\n\n\ndef name(x):\n    try:\n        return str(hash(x))\n    except TypeError:\n        return str(hash(str(x)))\n\n\n_HASHPAT = re.compile(\"([0-9a-z]{32})\")\n_UUIDPAT = re.compile(\"([0-9a-z]{8}-[0-9a-z]{4}-[0-9a-z]{4}-[0-9a-z]{4}-[0-9a-z]{12})\")\n\n\ndef label(x, cache=None):\n    \"\"\"\n\n    >>> label('x')\n    'x'\n\n    >>> label(('x', 1))\n    \"('x', 1)\"\n\n    >>> from hashlib import md5\n    >>> x = 'x-%s-hello' % md5(b'1234').hexdigest()\n    >>> x\n    'x-81dc9bdb52d04dc20036dbd8313ed055-hello'\n\n    >>> label(x)\n    'x-#-hello'\n\n    >>> from uuid import uuid1\n    >>> x = 'x-%s-hello' % uuid1()\n    >>> x  # doctest: +SKIP\n    'x-4c1a3d7e-0b45-11e6-8334-54ee75105593-hello'\n\n    >>> label(x)\n    'x-#-hello'\n    \"\"\"\n    s = str(x)\n    for pattern in (_HASHPAT, _UUIDPAT):\n        m = re.search(pattern, s)\n        if m is not None:\n            for h in m.groups():\n                if cache is not None:\n                    n = cache.get(h, len(cache))\n                    label = f\"#{n}\"\n                    # cache will be overwritten destructively\n                    cache[h] = n\n                else:\n                    label = \"#\"\n                s = s.replace(h, label)\n    return s\n\n\ndef box_label(key, verbose=False):\n    \"\"\"Label boxes in graph by chunk index\n\n    >>> box_label(('x', 1, 2, 3))\n    '(1, 2, 3)'\n    >>> box_label(('x', 123))\n    '123'\n    >>> box_label('x')\n    ''\n    \"\"\"\n    if isinstance(key, tuple):\n        key = key[1:]\n        if len(key) == 1:\n            [key] = key\n        return str(key)\n    elif verbose:\n        return str(key)\n    else:\n        return \"\"\n\n\ndef to_graphviz(\n    dsk,\n    data_attributes=None,\n    function_attributes=None,\n    rankdir=\"BT\",\n    graph_attr=None,\n    node_attr=None,\n    edge_attr=None,\n    collapse_outputs=False,\n    verbose=False,\n    **kwargs,\n):\n    graphviz = import_required(\n        \"graphviz\",\n        \"Drawing dask graphs with the graphviz engine requires the `graphviz` \"\n        \"python library and the `graphviz` system library.\\n\\n\"\n        \"Please either conda or pip install as follows:\\n\\n\"\n        \"  conda install python-graphviz     # either conda install\\n\"\n        \"  python -m pip install graphviz    # or pip install and follow installation instructions\",\n    )\n\n    data_attributes = data_attributes or {}\n    function_attributes = function_attributes or {}\n    graph_attr = graph_attr or {}\n    node_attr = node_attr or {}\n    edge_attr = edge_attr or {}\n\n    graph_attr[\"rankdir\"] = rankdir\n    node_attr[\"fontname\"] = \"helvetica\"\n\n    graph_attr.update(kwargs)\n    g = graphviz.Digraph(\n        graph_attr=graph_attr, node_attr=node_attr, edge_attr=edge_attr\n    )\n\n    seen = set()\n    connected = set()\n\n    for k, v in dsk.items():\n        k_name = name(k)\n        if istask(v):\n            func_name = name((k, \"function\")) if not collapse_outputs else k_name\n            if collapse_outputs or func_name not in seen:\n                seen.add(func_name)\n                attrs = function_attributes.get(k, {}).copy()\n                attrs.setdefault(\"label\", key_split(k))\n                attrs.setdefault(\"shape\", \"circle\")\n                g.node(func_name, **attrs)\n            if not collapse_outputs:\n                g.edge(func_name, k_name)\n                connected.add(func_name)\n                connected.add(k_name)\n\n            for dep in get_dependencies(dsk, k):\n                dep_name = name(dep)\n                if dep_name not in seen:\n                    seen.add(dep_name)\n                    attrs = data_attributes.get(dep, {}).copy()\n                    attrs.setdefault(\"label\", box_label(dep, verbose))\n                    attrs.setdefault(\"shape\", \"box\")\n                    g.node(dep_name, **attrs)\n                g.edge(dep_name, func_name)\n                connected.add(dep_name)\n                connected.add(func_name)\n\n        elif ishashable(v) and v in dsk:\n            v_name = name(v)\n            g.edge(v_name, k_name)\n            connected.add(v_name)\n            connected.add(k_name)\n\n        if (not collapse_outputs or k_name in connected) and k_name not in seen:\n            seen.add(k_name)\n            attrs = data_attributes.get(k, {}).copy()\n            attrs.setdefault(\"label\", box_label(k, verbose))\n            attrs.setdefault(\"shape\", \"box\")\n            g.node(k_name, **attrs)\n    return g\n\n\nIPYTHON_IMAGE_FORMATS = frozenset([\"jpeg\", \"png\"])\nIPYTHON_NO_DISPLAY_FORMATS = frozenset([\"dot\", \"pdf\"])\n\n\ndef _get_display_cls(format):\n    \"\"\"\n    Get the appropriate IPython display class for `format`.\n\n    Returns `IPython.display.SVG` if format=='svg', otherwise\n    `IPython.display.Image`.\n\n    If IPython is not importable, return dummy function that swallows its\n    arguments and returns None.\n    \"\"\"\n    dummy = lambda *args, **kwargs: None\n    try:\n        import IPython.display as display\n    except ImportError:\n        # Can't return a display object if no IPython.\n        return dummy\n\n    if format in IPYTHON_NO_DISPLAY_FORMATS:\n        # IPython can't display this format natively, so just return None.\n        return dummy\n    elif format in IPYTHON_IMAGE_FORMATS:\n        # Partially apply `format` so that `Image` and `SVG` supply a uniform\n        # interface to the caller.\n        return partial(display.Image, format=format)\n    elif format == \"svg\":\n        return display.SVG\n    else:\n        raise ValueError(\"Unknown format '%s' passed to `dot_graph`\" % format)\n\n\ndef dot_graph(dsk, filename=\"mydask\", format=None, **kwargs):\n    \"\"\"\n    Render a task graph using dot.\n\n    If `filename` is not None, write a file to disk with the specified name and extension.\n    If no extension is specified, '.png' will be used by default.\n\n    Parameters\n    ----------\n    dsk : dict\n        The graph to display.\n    filename : str or None, optional\n        The name of the file to write to disk. If the provided `filename`\n        doesn't include an extension, '.png' will be used by default.\n        If `filename` is None, no file will be written, and we communicate\n        with dot using only pipes.  Default is 'mydask'.\n    format : {'png', 'pdf', 'dot', 'svg', 'jpeg', 'jpg'}, optional\n        Format in which to write output file.  Default is 'png'.\n    **kwargs\n        Additional keyword arguments to forward to `to_graphviz`.\n\n    Returns\n    -------\n    result : None or IPython.display.Image or IPython.display.SVG  (See below.)\n\n    Notes\n    -----\n    If IPython is installed, we return an IPython.display object in the\n    requested format.  If IPython is not installed, we just return None.\n\n    We always return None if format is 'pdf' or 'dot', because IPython can't\n    display these formats natively. Passing these formats with filename=None\n    will not produce any useful output.\n\n    See Also\n    --------\n    dask.dot.to_graphviz\n    \"\"\"\n    g = to_graphviz(dsk, **kwargs)\n    return graphviz_to_file(g, filename, format)\n\n\ndef graphviz_to_file(g, filename, format):\n    fmts = [\".png\", \".pdf\", \".dot\", \".svg\", \".jpeg\", \".jpg\"]\n\n    if (\n        format is None\n        and filename is not None\n        and any(filename.lower().endswith(fmt) for fmt in fmts)\n    ):\n        filename, format = os.path.splitext(filename)\n        format = format[1:].lower()\n\n    if format is None:\n        format = \"png\"\n\n    data = g.pipe(format=format)\n    if not data:\n        raise RuntimeError(\n            \"Graphviz failed to properly produce an image. \"\n            \"This probably means your installation of graphviz \"\n            \"is missing png support. See: \"\n            \"https://github.com/ContinuumIO/anaconda-issues/\"\n            \"issues/485 for more information.\"\n        )\n\n    display_cls = _get_display_cls(format)\n\n    if filename is None:\n        return display_cls(data=data)\n\n    full_filename = \".\".join([filename, format])\n    with open(full_filename, \"wb\") as f:\n        f.write(data)\n\n    return display_cls(filename=full_filename)\n\n\ndef _to_cytoscape_json(\n    dsk,\n    data_attributes=None,\n    function_attributes=None,\n    collapse_outputs=False,\n    verbose=False,\n    **kwargs,\n):\n    \"\"\"\n    Convert a dask graph to Cytoscape JSON:\n    https://js.cytoscape.org/#notation/elements-json\n    \"\"\"\n    nodes = []\n    edges = []\n    data = {\"nodes\": nodes, \"edges\": edges}\n\n    data_attributes = data_attributes or {}\n    function_attributes = function_attributes or {}\n\n    seen = set()\n    connected = set()\n\n    for k, v in dsk.items():\n        k_name = name(k)\n        if istask(v):\n            func_name = name((k, \"function\")) if not collapse_outputs else k_name\n            if collapse_outputs or func_name not in seen:\n                seen.add(func_name)\n                attrs = function_attributes.get(k, {}).copy()\n                nodes.append(\n                    {\n                        \"data\": {\n                            \"id\": func_name,\n                            \"label\": key_split(k),\n                            \"shape\": \"ellipse\",\n                            \"color\": \"gray\",\n                            **attrs,\n                        }\n                    }\n                )\n            if not collapse_outputs:\n                edges.append({\"data\": {\"source\": func_name, \"target\": k_name}})\n\n                connected.add(func_name)\n                connected.add(k_name)\n\n            for dep in get_dependencies(dsk, k):\n                dep_name = name(dep)\n                if dep_name not in seen:\n                    seen.add(dep_name)\n                    attrs = data_attributes.get(dep, {}).copy()\n                    nodes.append(\n                        {\n                            \"data\": {\n                                \"id\": dep_name,\n                                \"label\": box_label(dep, verbose),\n                                \"shape\": \"rectangle\",\n                                \"color\": \"gray\",\n                                **attrs,\n                            }\n                        }\n                    )\n                edges.append(\n                    {\n                        \"data\": {\n                            \"source\": dep_name,\n                            \"target\": func_name,\n                        }\n                    }\n                )\n                connected.add(dep_name)\n                connected.add(func_name)\n\n        elif ishashable(v) and v in dsk:\n            v_name = name(v)\n            edges.append(\n                {\n                    \"data\": {\n                        \"source\": v_name,\n                        \"target\": k_name,\n                    }\n                }\n            )\n            connected.add(v_name)\n            connected.add(k_name)\n\n        if (not collapse_outputs or k_name in connected) and k_name not in seen:\n            seen.add(k_name)\n            attrs = data_attributes.get(k, {}).copy()\n            nodes.append(\n                {\n                    \"data\": {\n                        \"id\": k_name,\n                        \"label\": box_label(k, verbose),\n                        \"shape\": \"rectangle\",\n                        \"color\": \"gray\",\n                        **attrs,\n                    }\n                }\n            )\n    return data\n\n\ndef cytoscape_graph(\n    dsk,\n    filename: str | None = \"mydask\",\n    format: str | None = None,\n    *,\n    rankdir: str = \"BT\",\n    node_sep: float = 10,\n    edge_sep: float = 10,\n    spacing_factor: float = 1,\n    node_style: dict[str, str] | None = None,\n    edge_style: dict[str, str] | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Create an ipycytoscape widget for a dask graph.\n\n    If `filename` is not None, write an HTML file to disk with the specified name.\n\n    This uses the Cytoscape dagre layout algorithm. Options for that are documented here:\n    https://github.com/cytoscape/cytoscape.js-dagre#api\n\n    Parameters\n    ----------\n    dsk : dict\n        The graph to display.\n    filename : str or None, optional\n        The name of the HTML file to write to disk.\n    format : str, optional\n        Not used in this engine.\n    rankdir: str\n        The direction in which to orient the visualization.\n    node_sep: float\n        The separation (in px) between nodes in the DAG layout.\n    edge_sep: float\n        The separation (in px) between edges in the DAG layout.\n    spacing_factor: float\n        An overall scaling factor to increase (>1) or decrease (<1) the spacing\n        of the layout.\n    node_style: dict[str, str], optional\n        A dictionary of style attributes for nodes (refer to Cytoscape JSON docs\n        for available options: https://js.cytoscape.org/#notation/elements-json)\n    edge_style: dict[str, str], optional\n        A dictionary of style attributes for edges (refer to Cytoscape JSON docs\n        for available options: https://js.cytoscape.org/#notation/elements-json)\n    **kwargs\n        Additional keyword arguments to forward to `_to_cytoscape_json`.\n\n    Returns\n    -------\n    result : ipycytoscape.CytoscapeWidget\n    \"\"\"\n    ipycytoscape = import_required(\n        \"ipycytoscape\",\n        \"Drawing dask graphs with the cytoscape engine requires the `ipycytoscape` \"\n        \"python library.\\n\\n\"\n        \"Please either conda or pip install as follows:\\n\\n\"\n        \"  conda install ipycytoscape            # either conda install\\n\"\n        \"  python -m pip install ipycytoscape    # or pip install\",\n    )\n\n    node_style = node_style or {}\n    edge_style = edge_style or {}\n\n    data = _to_cytoscape_json(dsk, **kwargs)\n    # TODO: it's not easy to programmatically increase the height of the widget.\n    # Ideally we would make it a bit bigger, but that will probably require upstreaming\n    # some fixes.\n    g = ipycytoscape.CytoscapeWidget(\n        layout={\"height\": \"400px\"},\n    )\n    g.set_layout(\n        name=\"dagre\",\n        rankDir=rankdir,\n        nodeSep=node_sep,\n        edgeSep=edge_sep,\n        spacingFactor=spacing_factor,\n        nodeDimensionsIncludeLabels=True,\n    )\n    g.graph.add_graph_from_json(\n        data,\n        directed=True,\n    )\n    g.set_style(\n        [\n            {\n                \"selector\": \"node\",\n                \"style\": {\n                    \"font-family\": \"helvetica\",\n                    \"font-size\": \"24px\",\n                    \"font-weight\": \"bold\",\n                    \"color\": \"black\",\n                    \"background-color\": \"#eee\",\n                    \"border-color\": \"data(color)\",\n                    \"border-width\": 4,\n                    \"opacity\": \"1.0\",\n                    \"text-valign\": \"center\",\n                    \"text-halign\": \"center\",\n                    \"label\": \"data(label)\",\n                    \"shape\": \"data(shape)\",\n                    \"width\": 64,\n                    \"height\": 64,\n                    **node_style,\n                },\n            },\n            {\n                \"selector\": \"edge\",\n                \"style\": {\n                    \"width\": 8,\n                    \"line-color\": \"gray\",\n                    \"target-arrow-shape\": \"triangle\",\n                    \"target-arrow-color\": \"gray\",\n                    \"curve-style\": \"bezier\",\n                    **edge_style,\n                },\n            },\n        ],\n    )\n    # Tweak the zoom sensitivity\n    z = g.zoom\n    g.max_zoom = z * 2.0\n    g.min_zoom = z / 10.0\n    g.wheel_sensitivity = 0.1\n\n    if filename is not None:\n        from ipywidgets.embed import embed_minimal_html\n\n        filename = filename if filename.endswith(\".html\") else filename + \".html\"\n        embed_minimal_html(filename, views=[g], title=\"Dask task graph\")\n    return g\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:31:08"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport copy\nimport os\nimport re\nimport sys\nfrom functools import partial\nfrom operator import add, neg\n\nimport pytest\n\nipycytoscape = pytest.importorskip(\"ipycytoscape\")\nfrom dask.dot import _to_cytoscape_json, cytoscape_graph\n\nif sys.flags.optimize != 2:\n    pytest.importorskip(\"graphviz\")\n    from dask.dot import dot_graph, label, task_label, to_graphviz\nelse:\n    pytestmark = pytest.mark.skipif(\n        True, reason=\"graphviz exception with Python -OO flag\"\n    )\n\nfrom dask import delayed\nfrom dask.utils import ensure_not_exists\n\ntry:\n    from IPython.display import SVG, Image\nexcept ImportError:\n    ipython_not_installed = True\n    Image = None\n    SVG = None\nelse:\n    ipython_not_installed = False\nipython_not_installed_mark = pytest.mark.skipif(\n    ipython_not_installed, reason=\"IPython not installed\"\n)\n\n\n# Since graphviz doesn't store a graph, we need to parse the output\nlabel_re = re.compile(r\".*\\[label=(.*?) shape=(.*?)\\]\")\n\n\ndef get_label(line):\n    m = label_re.match(line)\n    if m:\n        return m.group(1)\n\n\ndef get_shape(line):\n    m = label_re.match(line)\n    if m:\n        return m.group(2)\n\n\ndsk = {\n    \"a\": 1,\n    \"b\": 2,\n    \"c\": (neg, \"a\"),\n    \"d\": (neg, \"b\"),\n    \"e\": (add, \"c\", \"d\"),\n    \"f\": (sum, [\"a\", \"e\"]),\n}\n\n\ndef test_task_label():\n    assert task_label((partial(add, 1), 1)) == \"add\"\n    assert task_label((add, 1)) == \"add\"\n    assert task_label((add, (add, 1, 2))) == \"add(...)\"\n\n\ndef test_label():\n    assert label(\"x\") == \"x\"\n    assert label(\"elemwise-ffcd9aa2231d466b5aa91e8bfa9e9487\") == \"elemwise-#\"\n\n    cache = {}\n    result = label(\"elemwise-ffcd9aa2231d466b5aa91e8bfa9e9487\", cache=cache)\n    assert result == \"elemwise-#0\"\n    # cached\n    result = label(\"elemwise-ffcd9aa2231d466b5aa91e8bfa9e9487\", cache=cache)\n    assert result == \"elemwise-#0\"\n    assert len(cache) == 1\n\n    result = label(\"elemwise-e890b510984f344edea9a5e5fe05c0db\", cache=cache)\n    assert result == \"elemwise-#1\"\n    assert len(cache) == 2\n\n    result = label(\"elemwise-ffcd9aa2231d466b5aa91e8bfa9e9487\", cache=cache)\n    assert result == \"elemwise-#0\"\n    assert len(cache) == 2\n\n    assert label(\"x\", cache=cache) == \"x\"\n    assert len(cache) == 2\n\n\ndef test_to_graphviz():\n    g = to_graphviz(dsk)\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 10  # 10 nodes total\n    assert set(labels) == {\"c\", \"d\", \"e\", \"f\", '\"\"'}\n    shapes = list(filter(None, map(get_shape, g.body)))\n    assert set(shapes) == {\"box\", \"circle\"}\n\n\ndef test_to_graphviz_custom():\n    g = to_graphviz(\n        dsk,\n        data_attributes={\"a\": {\"shape\": \"square\"}},\n        function_attributes={\"c\": {\"label\": \"neg_c\", \"shape\": \"ellipse\"}},\n    )\n    labels = set(filter(None, map(get_label, g.body)))\n    assert labels == {\"neg_c\", \"d\", \"e\", \"f\", '\"\"'}\n    shapes = list(filter(None, map(get_shape, g.body)))\n    assert set(shapes) == {\"box\", \"circle\", \"square\", \"ellipse\"}\n\n\ndef test_cytoscape_graph_custom(tmp_path):\n    # Check that custom styling and layout propagates through\n    g = cytoscape_graph(\n        dsk,\n        filename=os.fsdecode(tmp_path / \"mydask.html\"),\n        rankdir=\"LR\",\n        node_sep=20,\n        edge_sep=30,\n        spacing_factor=2,\n        edge_style={\"line-color\": \"red\"},\n        node_style={\"background-color\": \"green\"},\n    )\n    sty = g.cytoscape_style\n    layout = g.cytoscape_layout\n    node_sty = next(s for s in sty if s[\"selector\"] == \"node\")[\"style\"]\n    edge_sty = next(s for s in sty if s[\"selector\"] == \"edge\")[\"style\"]\n\n    assert edge_sty[\"line-color\"] == \"red\"\n    assert node_sty[\"background-color\"] == \"green\"\n    assert layout[\"rankDir\"] == \"LR\"\n    assert layout[\"nodeSep\"] == 20\n    assert layout[\"edgeSep\"] == 30\n    assert layout[\"spacingFactor\"] == 2\n\n\ndef test_cytoscape_graph_color():\n    pytest.importorskip(\"matplotlib.pyplot\")\n    from dask.delayed import Delayed\n\n    g = Delayed(\"f\", dsk).visualize(engine=\"cytoscape\")\n    init_color = g.graph.nodes[0].data[\"color\"]\n    # Check that we changed the semantic color\n    g = Delayed(\"f\", dsk).visualize(engine=\"cytoscape\", color=\"order\")\n    assert any(n.data[\"color\"] != init_color for n in g.graph.nodes)\n\n\ndef test_to_graphviz_attributes():\n    assert to_graphviz(dsk).graph_attr[\"rankdir\"] == \"BT\"\n    assert to_graphviz(dsk, rankdir=\"LR\").graph_attr[\"rankdir\"] == \"LR\"\n    assert to_graphviz(dsk, node_attr={\"color\": \"white\"}).node_attr[\"color\"] == \"white\"\n    assert to_graphviz(dsk, edge_attr={\"color\": \"white\"}).edge_attr[\"color\"] == \"white\"\n\n\ndef test_aliases():\n    g = to_graphviz({\"x\": 1, \"y\": \"x\"})\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 2\n    assert len(g.body) - len(labels) == 1  # Single edge\n\n\ndef test_to_graphviz_verbose():\n    g = to_graphviz(dsk, verbose=True)\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 10  # 10 nodes total\n    assert set(labels) == {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n    shapes = list(filter(None, map(get_shape, g.body)))\n    assert set(shapes) == {\"box\", \"circle\"}\n\n\ndef test__to_cytoscape_json_verbose():\n    data = _to_cytoscape_json(dsk, verbose=True)\n    labels = list(map(lambda x: x[\"data\"][\"label\"], data[\"nodes\"]))\n    assert len(labels) == 10\n    assert set(labels) == {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n    shapes = list(map(lambda x: x[\"data\"][\"shape\"], data[\"nodes\"]))\n    assert set(shapes) == {\"ellipse\", \"rectangle\"}\n\n\ndef test_to_graphviz_collapse_outputs():\n    g = to_graphviz(dsk, collapse_outputs=True)\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 6  # 6 nodes total\n    assert set(labels) == {\"c\", \"d\", \"e\", \"f\", '\"\"'}\n    shapes = list(filter(None, map(get_shape, g.body)))\n    assert set(shapes) == {\"box\", \"circle\"}\n\n\ndef test__to_cytoscape_json_collapse_outputs():\n    data = _to_cytoscape_json(dsk, collapse_outputs=True)\n    labels = list(map(lambda x: x[\"data\"][\"label\"], data[\"nodes\"]))\n    assert len(labels) == 6  # 6 nodes total\n    assert set(labels) == {\"c\", \"d\", \"e\", \"f\", \"\"}\n    shapes = list(map(lambda x: x[\"data\"][\"shape\"], data[\"nodes\"]))\n    assert set(shapes) == {\"ellipse\", \"rectangle\"}\n\n\ndef test_to_graphviz_collapse_outputs_and_verbose():\n    g = to_graphviz(dsk, collapse_outputs=True, verbose=True)\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 6  # 6 nodes total\n    assert set(labels) == {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n    shapes = list(filter(None, map(get_shape, g.body)))\n    assert set(shapes) == {\"box\", \"circle\"}\n\n\ndef test__to_cytoscape_json_collapse_outputs_and_verbose():\n    data = _to_cytoscape_json(dsk, collapse_outputs=True, verbose=True)\n    labels = list(map(lambda x: x[\"data\"][\"label\"], data[\"nodes\"]))\n    assert len(labels) == 6  # 6 nodes total\n    assert set(labels) == {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n    shapes = list(map(lambda x: x[\"data\"][\"shape\"], data[\"nodes\"]))\n    assert set(shapes) == {\"ellipse\", \"rectangle\"}\n\n\ndef test_to_graphviz_with_unconnected_node():\n    dsk2 = dsk.copy()\n    dsk2[\"g\"] = 3\n    g = to_graphviz(dsk2, verbose=True)\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 11  # 11 nodes total\n    assert set(labels) == {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"}\n\n    g = to_graphviz(dsk, verbose=True, collapse_outputs=True)\n    labels = list(filter(None, map(get_label, g.body)))\n    assert len(labels) == 6  # 6 nodes total\n    assert set(labels) == {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n\n\n@pytest.mark.parametrize(\n    \"format,typ\",\n    [\n        pytest.param(\"png\", Image, marks=ipython_not_installed_mark),\n        pytest.param(\n            \"jpeg\",\n            Image,\n            marks=pytest.mark.xfail(\n                reason=\"jpeg not always supported in dot\", strict=False\n            ),\n        ),\n        (\"dot\", type(None)),\n        (\"pdf\", type(None)),\n        pytest.param(\"svg\", SVG, marks=ipython_not_installed_mark),\n    ],\n)\n@pytest.mark.xfail(\n    sys.platform == \"win32\",\n    reason=\"graphviz/pango on conda-forge currently broken for windows\",\n    strict=False,\n)\ndef test_dot_graph(tmpdir, format, typ):\n    # Use a name that the shell would interpret specially to ensure that we're\n    # not vulnerable to shell injection when interacting with `dot`.\n    filename = str(tmpdir.join(\"$(touch should_not_get_created.txt)\"))\n\n    target = \".\".join([filename, format])\n    ensure_not_exists(target)\n    try:\n        result = dot_graph(dsk, filename=filename, format=format)\n\n        assert not os.path.exists(\"should_not_get_created.txt\")\n        assert os.path.isfile(target)\n        assert isinstance(result, typ)\n    finally:\n        ensure_not_exists(target)\n\n\n@pytest.mark.parametrize(\n    \"format,typ\",\n    [\n        pytest.param(\"png\", Image, marks=ipython_not_installed_mark),\n        pytest.param(\n            \"jpeg\",\n            Image,\n            marks=pytest.mark.xfail(\n                reason=\"jpeg not always supported in dot\", strict=False\n            ),\n        ),\n        (\"dot\", type(None)),\n        (\"pdf\", type(None)),\n        pytest.param(\"svg\", SVG, marks=ipython_not_installed_mark),\n    ],\n)\n@pytest.mark.xfail(\n    sys.platform == \"win32\",\n    reason=\"graphviz/pango on conda-forge currently broken for windows\",\n    strict=False,\n)\ndef test_dot_graph_no_filename(tmpdir, format, typ):\n    before = tmpdir.listdir()\n    result = dot_graph(dsk, filename=None, format=format)\n    # We shouldn't write any files if filename is None.\n    after = tmpdir.listdir()\n    assert before == after\n    assert isinstance(result, typ)\n\n\n@ipython_not_installed_mark\n@pytest.mark.xfail(\n    sys.platform == \"win32\",\n    reason=\"graphviz/pango on conda-forge currently broken for windows\",\n    strict=False,\n)\ndef test_dot_graph_defaults():\n    # Test with default args.\n    default_name = \"mydask\"\n    default_format = \"png\"\n    target = \".\".join([default_name, default_format])\n\n    ensure_not_exists(target)\n    try:\n        result = dot_graph(dsk)\n        assert os.path.isfile(target)\n        assert isinstance(result, Image)\n    finally:\n        ensure_not_exists(target)\n\n\ndef test_cytoscape_graph(tmpdir):\n    target = str(tmpdir.join(\"mydask.html\"))\n    ensure_not_exists(target)\n    try:\n        result = cytoscape_graph(dsk, target)\n        assert os.path.isfile(target)\n        assert isinstance(result, ipycytoscape.CytoscapeWidget)\n    finally:\n        ensure_not_exists(target)\n\n\n@pytest.mark.parametrize(\n    \"filename,format,target,expected_result_type\",\n    [\n        pytest.param(\n            \"mydaskpdf\", \"svg\", \"mydaskpdf.svg\", SVG, marks=ipython_not_installed_mark\n        ),\n        (\"mydask.pdf\", None, \"mydask.pdf\", type(None)),\n        pytest.param(\n            \"mydask.pdf\", \"svg\", \"mydask.pdf.svg\", SVG, marks=ipython_not_installed_mark\n        ),\n        pytest.param(\n            \"mydaskpdf\", None, \"mydaskpdf.png\", Image, marks=ipython_not_installed_mark\n        ),\n        pytest.param(\n            \"mydask.pdf.svg\",\n            None,\n            \"mydask.pdf.svg\",\n            SVG,\n            marks=ipython_not_installed_mark,\n        ),\n    ],\n)\n@pytest.mark.xfail(\n    sys.platform == \"win32\",\n    reason=\"graphviz/pango on conda-forge currently broken for windows\",\n    strict=False,\n)\ndef test_filenames_and_formats(tmpdir, filename, format, target, expected_result_type):\n    result = dot_graph(dsk, filename=str(tmpdir.join(filename)), format=format)\n    assert tmpdir.join(target).exists()\n    assert isinstance(result, expected_result_type)\n\n\ndef test_delayed_kwargs_apply():\n    def f(x, y=True):\n        return x + y\n\n    x = delayed(f)(1, y=2)\n    label = task_label(x.dask[x.key])\n    assert \"f\" in label\n    assert \"apply\" not in label\n\n\n@pytest.mark.parametrize(\"viz_func\", [to_graphviz, _to_cytoscape_json])\ndef test_immutable_attributes(viz_func):\n    def inc(x):\n        return x + 1\n\n    dsk = {\"a\": (inc, 1), \"b\": (inc, 2), \"c\": (add, \"a\", \"b\")}\n    attrs_func = {\"a\": {}}\n    attrs_data = {\"b\": {}}\n    attrs_func_test = copy.deepcopy(attrs_func)\n    attrs_data_test = copy.deepcopy(attrs_data)\n\n    viz_func(\n        dsk,\n        function_attributes=attrs_func,\n        data_attributes=attrs_data,\n    )\n\n    assert attrs_func_test == attrs_func\n    assert attrs_data_test == attrs_data\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_local\\test_local.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"\nAsynchronous Shared-Memory Scheduler for Dask Graphs.\n\nThis scheduler coordinates several workers to execute tasks in a dask graph in\nparallel.  It depends on a ``concurrent.futures.Executor``\nand a corresponding Queue for worker-to-scheduler communication.\n\nIt tries to execute tasks in an order which maintains a small memory footprint\nthroughout execution.  It does this by running tasks that allow us to release\ndata resources.\n\n\nTask Selection Policy\n=====================\n\nWhen we complete a task we add more data in to our set of available data; this\nnew data makes new tasks available.  We preferentially choose tasks that were\njust made available in a last-in-first-out fashion.  We implement this as a\nsimple stack.  This results in more depth-first rather than breadth first\nbehavior which encourages us to process batches of data to completion before\nstarting in on new data when possible.\n\nWhen the addition of new data readies multiple tasks simultaneously we add\ntasks to the stack in sorted order so that tasks with greater keynames are run\nfirst.  This can be handy to break ties in a predictable fashion.\n\n\nState\n=====\n\nMany functions pass around a ``state`` variable that holds the current state of\nthe computation.  This variable consists of several other dictionaries and\nsets, explained below.\n\nConstant state\n--------------\n\n1.  dependencies: {x: [a, b ,c]} a,b,c, must be run before x\n2.  dependents: {a: [x, y]} a must run before x or y\n\nChanging state\n--------------\n\n### Data\n\n1.  cache: available concrete data.  {key: actual-data}\n2.  released: data that we've seen, used, and released because it is no longer\n    needed\n\n### Jobs\n\n1.  ready: A fifo stack of ready-to-run tasks\n2.  running: A set of tasks currently in execution\n3.  finished: A set of finished tasks\n4.  waiting: which tasks are still waiting on others :: {key: {keys}}\n    Real-time equivalent of dependencies\n5.  waiting_data: available data to yet-to-be-run-tasks :: {key: {keys}}\n    Real-time equivalent of dependents\n\n\nExamples\n--------\n\n>>> import pprint  # doctest: +SKIP\n>>> inc = lambda x: x + 1\n>>> add = lambda x, y: x + y\n>>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n>>> pprint.pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n{'cache': {'x': 1, 'y': 2},\n 'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n 'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n 'finished': set(),\n 'ready': ['z'],\n 'released': set(),\n 'running': set(),\n 'waiting': {'w': {'z'}},\n 'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n\nOptimizations\n=============\n\nWe build this scheduler with out-of-core array operations in mind.  To this end\nwe have encoded some particular optimizations.\n\nCompute to release data\n-----------------------\n\nWhen we choose a new task to execute we often have many options.  Policies at\nthis stage are cheap and can significantly impact performance.  One could\nimagine policies that expose parallelism, drive towards a particular output,\netc..\n\nOur current policy is to run tasks that were most recently made available.\n\n\nInlining computations\n---------------------\n\nWe hold on to intermediate computations either in memory or on disk.\n\nFor very cheap computations that may emit new copies of the data, like\n``np.transpose`` or possibly even ``x + 1`` we choose not to store these as\nseparate pieces of data / tasks.  Instead we combine them with the computations\nthat require them.  This may result in repeated computation but saves\nsignificantly on space and computation complexity.\n\nSee the function ``inline_functions`` for more information.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections.abc import Mapping, Sequence\nfrom concurrent.futures import Executor, Future\nfrom functools import partial\nfrom queue import Empty, Queue\n\nfrom dask import config\nfrom dask._task_spec import DataNode, DependenciesMapping, convert_legacy_graph\nfrom dask.callbacks import local_callbacks, unpack_callbacks\nfrom dask.core import flatten, get_dependencies, reverse_dict\nfrom dask.order import order\nfrom dask.typing import Key\n\nif os.name == \"nt\":\n    # Python 3 windows Queue.get doesn't handle interrupts properly. To\n    # workaround this we poll at a sufficiently large interval that it\n    # shouldn't affect performance, but small enough that users trying to kill\n    # an application shouldn't care.\n    def queue_get(q):\n        while True:\n            try:\n                return q.get(block=True, timeout=0.1)\n            except Empty:\n                pass\n\nelse:\n\n    def queue_get(q):\n        return q.get()\n\n\ndef start_state_from_dask(dsk, cache=None, sortkey=None):\n    \"\"\"Start state from a dask\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n    >>> from pprint import pprint  # doctest: +SKIP\n    >>> pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n    {'cache': {'x': 1, 'y': 2},\n     'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n     'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n     'finished': set(),\n     'ready': ['z'],\n     'released': set(),\n     'running': set(),\n     'waiting': {'w': {'z'}},\n     'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n    \"\"\"\n    if sortkey is None:\n        sortkey = order(dsk).get\n    if cache is None:\n        cache = config.get(\"cache\", None)\n    if cache is None:\n        cache = dict()\n\n    dsk = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    data_keys = set()\n    for k, v in dsk.items():\n        if isinstance(v, DataNode):\n            cache[k] = v()\n            data_keys.add(k)\n\n    dsk2 = dsk.copy()\n    dsk2.update(cache)\n\n    dependencies = DependenciesMapping(dsk)\n    waiting = {k: set(v) for k, v in dependencies.items() if k not in data_keys}\n\n    dependents = reverse_dict(dependencies)\n    for a in cache:\n        for b in dependents.get(a, ()):\n            waiting[b].remove(a)\n    waiting_data = {k: v.copy() for k, v in dependents.items() if v}\n\n    ready_set = {k for k, v in waiting.items() if not v}\n    ready = sorted(ready_set, key=sortkey, reverse=True)\n    waiting = {k: v for k, v in waiting.items() if v}\n\n    state = {\n        \"dependencies\": dependencies,\n        \"dependents\": dependents,\n        \"waiting\": waiting,\n        \"waiting_data\": waiting_data,\n        \"cache\": cache,\n        \"ready\": ready,\n        \"running\": set(),\n        \"finished\": set(),\n        \"released\": set(),\n    }\n\n    return state\n\n\n\"\"\"\nRunning tasks\n-------------\n\nWhen we execute tasks we both\n\n1.  Perform the actual work of collecting the appropriate data and calling the function\n2.  Manage administrative state to coordinate with the scheduler\n\"\"\"\n\n\ndef execute_task(key, task_info, dumps, loads, get_id, pack_exception):\n    \"\"\"\n    Compute task and handle all administration\n\n    See Also\n    --------\n    _execute_task : actually execute task\n    \"\"\"\n    try:\n        task, data = loads(task_info)\n        result = task(data)\n        id = get_id()\n        result = dumps((result, id))\n        failed = False\n    except BaseException as e:  # noqa: B036\n        result = pack_exception(e, dumps)\n        failed = True\n    return key, result, failed\n\n\ndef batch_execute_tasks(it):\n    \"\"\"\n    Batch computing of multiple tasks with `execute_task`\n    \"\"\"\n    return [execute_task(*a) for a in it]\n\n\ndef release_data(key, state, delete=True):\n    \"\"\"Remove data from temporary storage\n\n    See Also\n    --------\n    finish_task\n    \"\"\"\n    if key in state[\"waiting_data\"]:\n        assert not state[\"waiting_data\"][key]\n        del state[\"waiting_data\"][key]\n\n    state[\"released\"].add(key)\n\n    if delete:\n        del state[\"cache\"][key]\n\n\ndef finish_task(\n    dsk, key, state, results, sortkey, delete=True, release_data=release_data\n):\n    \"\"\"\n    Update execution state after a task finishes\n\n    Mutates.  This should run atomically (with a lock).\n    \"\"\"\n    for dep in sorted(state[\"dependents\"][key], key=sortkey, reverse=True):\n        s = state[\"waiting\"][dep]\n        s.remove(key)\n        if not s:\n            del state[\"waiting\"][dep]\n            state[\"ready\"].append(dep)\n\n    for dep in state[\"dependencies\"][key]:\n        if dep in state[\"waiting_data\"]:\n            s = state[\"waiting_data\"][dep]\n            s.remove(key)\n            if not s and dep not in results:\n                release_data(dep, state, delete=delete)\n        elif delete and dep not in results:\n            release_data(dep, state, delete=delete)\n\n    state[\"finished\"].add(key)\n    state[\"running\"].remove(key)\n\n    return state\n\n\ndef nested_get(ind, coll):\n    \"\"\"Get nested index from collection\n\n    Examples\n    --------\n\n    >>> nested_get(1, 'abc')\n    'b'\n    >>> nested_get([1, 0], 'abc')\n    ('b', 'a')\n    >>> nested_get([[1, 0], [0, 1]], 'abc')\n    (('b', 'a'), ('a', 'b'))\n    \"\"\"\n    if isinstance(ind, list):\n        return tuple(nested_get(i, coll) for i in ind)\n    else:\n        return coll[ind]\n\n\ndef default_get_id():\n    \"\"\"Default get_id\"\"\"\n    return None\n\n\ndef default_pack_exception(e, dumps):\n    raise\n\n\ndef reraise(exc, tb=None):\n    if exc.__traceback__ is not tb:\n        raise exc.with_traceback(tb)\n    raise exc\n\n\ndef identity(x):\n    \"\"\"Identity function. Returns x.\n\n    >>> identity(3)\n    3\n    \"\"\"\n    return x\n\n\n\"\"\"\nTask Selection\n--------------\n\nWe often have a choice among many tasks to run next.  This choice is both\ncheap and can significantly impact performance.\n\nWe currently select tasks that have recently been made ready.  We hope that\nthis first-in-first-out policy reduces memory footprint\n\"\"\"\n\n\"\"\"\n`get`\n-----\n\nThe main function of the scheduler.  Get is the main entry point.\n\"\"\"\n\n\ndef get_async(\n    submit,\n    num_workers,\n    dsk,\n    result,\n    cache=None,\n    get_id=default_get_id,\n    rerun_exceptions_locally=None,\n    pack_exception=default_pack_exception,\n    raise_exception=reraise,\n    callbacks=None,\n    dumps=identity,\n    loads=identity,\n    chunksize=None,\n    **kwargs,\n):\n    \"\"\"Asynchronous get function\n\n    This is a general version of various asynchronous schedulers for dask.  It\n    takes a ``concurrent.futures.Executor.submit`` function to form a more\n    specific ``get`` method that walks through the dask array with parallel\n    workers, avoiding repeat computation and minimizing memory use.\n\n    Parameters\n    ----------\n    submit : function\n        A ``concurrent.futures.Executor.submit`` function\n    num_workers : int\n        The number of workers that task submissions can be spread over\n    dsk : dict\n        A dask dictionary specifying a workflow\n    result : key or list of keys\n        Keys corresponding to desired data\n    cache : dict-like, optional\n        Temporary storage of results\n    get_id : callable, optional\n        Function to return the worker id, takes no arguments. Examples are\n        `threading.current_thread` and `multiprocessing.current_process`.\n    rerun_exceptions_locally : bool, optional\n        Whether to rerun failing tasks in local process to enable debugging\n        (False by default)\n    pack_exception : callable, optional\n        Function to take an exception and ``dumps`` method, and return a\n        serialized tuple of ``(exception, traceback)`` to send back to the\n        scheduler. Default is to just raise the exception.\n    raise_exception : callable, optional\n        Function that takes an exception and a traceback, and raises an error.\n    callbacks : tuple or list of tuples, optional\n        Callbacks are passed in as tuples of length 5. Multiple sets of\n        callbacks may be passed in as a list of tuples. For more information,\n        see the dask.diagnostics documentation.\n    dumps: callable, optional\n        Function to serialize task data and results to communicate between\n        worker and parent.  Defaults to identity.\n    loads: callable, optional\n        Inverse function of `dumps`.  Defaults to identity.\n    chunksize: int, optional\n        Size of chunks to use when dispatching work. Defaults to 1.\n        If -1, will be computed to evenly divide ready work across workers.\n\n    See Also\n    --------\n    threaded.get\n    \"\"\"\n    chunksize = chunksize or config.get(\"chunksize\", 1)\n\n    queue = Queue()\n\n    if isinstance(result, list):\n        result_flat = set(flatten(result))\n    else:\n        result_flat = {result}\n    results = set(result_flat)\n\n    dsk = dict(convert_legacy_graph(dsk))\n    with local_callbacks(callbacks) as callbacks:\n        _, _, pretask_cbs, posttask_cbs, _ = unpack_callbacks(callbacks)\n        started_cbs = []\n        succeeded = False\n        # if start_state_from_dask fails, we will have something\n        # to pass to the final block.\n        state = {}\n        try:\n            for cb in callbacks:\n                if cb[0]:\n                    cb[0](dsk)\n                started_cbs.append(cb)\n\n            keyorder = order(dsk)\n\n            state = start_state_from_dask(dsk, cache=cache, sortkey=keyorder.get)\n\n            for _, start_state, _, _, _ in callbacks:\n                if start_state:\n                    start_state(dsk, state)\n\n            if rerun_exceptions_locally is None:\n                rerun_exceptions_locally = config.get(\"rerun_exceptions_locally\", False)\n\n            if state[\"waiting\"] and not state[\"ready\"]:\n                raise ValueError(\"Found no accessible jobs in dask\")\n\n            def fire_tasks(chunksize):\n                \"\"\"Fire off a task to the thread pool\"\"\"\n                # Determine chunksize and/or number of tasks to submit\n                nready = len(state[\"ready\"])\n                if chunksize == -1:\n                    ntasks = nready\n                    chunksize = -(ntasks // -num_workers)\n                else:\n                    used_workers = -(len(state[\"running\"]) // -chunksize)\n                    avail_workers = max(num_workers - used_workers, 0)\n                    ntasks = min(nready, chunksize * avail_workers)\n\n                # Prep all ready tasks for submission\n                args = []\n                for _ in range(ntasks):\n                    # Get the next task to compute (most recently added)\n                    key = state[\"ready\"].pop()\n                    # Notify task is running\n                    state[\"running\"].add(key)\n                    for f in pretask_cbs:\n                        f(key, dsk, state)\n\n                    # Prep args to send\n                    data = {\n                        dep: state[\"cache\"][dep] for dep in get_dependencies(dsk, key)\n                    }\n                    args.append(\n                        (\n                            key,\n                            dumps((dsk[key], data)),\n                            dumps,\n                            loads,\n                            get_id,\n                            pack_exception,\n                        )\n                    )\n\n                # Batch submit\n                for i in range(-(len(args) // -chunksize)):\n                    each_args = args[i * chunksize : (i + 1) * chunksize]\n                    if not each_args:\n                        break\n                    fut = submit(batch_execute_tasks, each_args)\n                    fut.add_done_callback(queue.put)\n\n            # Main loop, wait on tasks to finish, insert new ones\n            while state[\"waiting\"] or state[\"ready\"] or state[\"running\"]:\n                fire_tasks(chunksize)\n                for key, res_info, failed in queue_get(queue).result():\n                    if failed:\n                        exc, tb = loads(res_info)\n                        if rerun_exceptions_locally:\n                            data = {\n                                dep: state[\"cache\"][dep]\n                                for dep in get_dependencies(dsk, key)\n                            }\n                            task = dsk[key]\n                            task(data)  # Re-execute locally\n                        else:\n                            raise_exception(exc, tb)\n                    res, worker_id = loads(res_info)\n                    state[\"cache\"][key] = res\n                    finish_task(dsk, key, state, results, keyorder.get)\n                    for f in posttask_cbs:\n                        f(key, res, dsk, state, worker_id)\n\n            succeeded = True\n\n        finally:\n            for _, _, _, _, finish in started_cbs:\n                if finish:\n                    finish(dsk, state, not succeeded)\n\n    return nested_get(result, state[\"cache\"])\n\n\n\"\"\" Synchronous concrete version of get_async\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a\nsequential one.  This is useful for debugging and for code dominated by the\nGIL\n\"\"\"\n\n\nclass SynchronousExecutor(Executor):\n    _max_workers = 1\n\n    def submit(self, fn, *args, **kwargs):\n        fut = Future()\n        try:\n            fut.set_result(fn(*args, **kwargs))\n        except BaseException as e:  # noqa: B036\n            fut.set_exception(e)\n        return fut\n\n\nsynchronous_executor = SynchronousExecutor()\n\n\ndef get_sync(dsk: Mapping, keys: Sequence[Key] | Key, **kwargs):\n    \"\"\"A naive synchronous version of get_async\n\n    Can be useful for debugging.\n    \"\"\"\n    kwargs.pop(\"num_workers\", None)  # if num_workers present, remove it\n    return get_async(\n        synchronous_executor.submit,\n        synchronous_executor._max_workers,\n        dsk,\n        keys,\n        **kwargs,\n    )\n\n\n\"\"\" Adaptor for ``multiprocessing.Pool`` instances\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a wrapper\nclass for ``multiprocessing.Pool`` instances so we can treat them like\n``concurrent.futures.Executor`` instances instead.\n\nThis is mainly useful for legacy use cases or users that prefer\n``multiprocessing.Pool``.\n\"\"\"\n\n\nclass MultiprocessingPoolExecutor(Executor):\n    def __init__(self, pool):\n        self.pool = pool\n        self._max_workers = len(pool._pool)\n\n    def submit(self, fn, *args, **kwargs):\n        return submit_apply_async(self.pool.apply_async, fn, *args, **kwargs)\n\n\ndef submit_apply_async(apply_async, fn, *args, **kwargs):\n    fut = Future()\n    apply_async(fn, args, kwargs, fut.set_result, fut.set_exception)\n    return fut\n\n\ndef get_apply_async(apply_async, num_workers, *args, **kwargs):\n    return get_async(\n        partial(submit_apply_async, apply_async), num_workers, *args, **kwargs\n    )\n\n\ndef sortkey(item):\n    \"\"\"Sorting key function that is robust to different types\n\n    Both strings and tuples are common key types in dask graphs.\n    However In Python 3 one can not compare strings with tuples directly.\n    This function maps many types to a form where they can be compared\n\n    Examples\n    --------\n    >>> sortkey('Hello')\n    ('str', 'Hello')\n\n    >>> sortkey(('x', 1))\n    ('tuple', ('x', 1))\n    \"\"\"\n    return (type(item).__name__, item)\n\n\n### Source File Content:\n\"\"\"\nAsynchronous Shared-Memory Scheduler for Dask Graphs.\n\nThis scheduler coordinates several workers to execute tasks in a dask graph in\nparallel.  It depends on a ``concurrent.futures.Executor``\nand a corresponding Queue for worker-to-scheduler communication.\n\nIt tries to execute tasks in an order which maintains a small memory footprint\nthroughout execution.  It does this by running tasks that allow us to release\ndata resources.\n\n\nTask Selection Policy\n=====================\n\nWhen we complete a task we add more data in to our set of available data; this\nnew data makes new tasks available.  We preferentially choose tasks that were\njust made available in a last-in-first-out fashion.  We implement this as a\nsimple stack.  This results in more depth-first rather than breadth first\nbehavior which encourages us to process batches of data to completion before\nstarting in on new data when possible.\n\nWhen the addition of new data readies multiple tasks simultaneously we add\ntasks to the stack in sorted order so that tasks with greater keynames are run\nfirst.  This can be handy to break ties in a predictable fashion.\n\n\nState\n=====\n\nMany functions pass around a ``state`` variable that holds the current state of\nthe computation.  This variable consists of several other dictionaries and\nsets, explained below.\n\nConstant state\n--------------\n\n1.  dependencies: {x: [a, b ,c]} a,b,c, must be run before x\n2.  dependents: {a: [x, y]} a must run before x or y\n\nChanging state\n--------------\n\n### Data\n\n1.  cache: available concrete data.  {key: actual-data}\n2.  released: data that we've seen, used, and released because it is no longer\n    needed\n\n### Jobs\n\n1.  ready: A fifo stack of ready-to-run tasks\n2.  running: A set of tasks currently in execution\n3.  finished: A set of finished tasks\n4.  waiting: which tasks are still waiting on others :: {key: {keys}}\n    Real-time equivalent of dependencies\n5.  waiting_data: available data to yet-to-be-run-tasks :: {key: {keys}}\n    Real-time equivalent of dependents\n\n\nExamples\n--------\n\n>>> import pprint  # doctest: +SKIP\n>>> inc = lambda x: x + 1\n>>> add = lambda x, y: x + y\n>>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n>>> pprint.pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n{'cache': {'x': 1, 'y': 2},\n 'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n 'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n 'finished': set(),\n 'ready': ['z'],\n 'released': set(),\n 'running': set(),\n 'waiting': {'w': {'z'}},\n 'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n\nOptimizations\n=============\n\nWe build this scheduler with out-of-core array operations in mind.  To this end\nwe have encoded some particular optimizations.\n\nCompute to release data\n-----------------------\n\nWhen we choose a new task to execute we often have many options.  Policies at\nthis stage are cheap and can significantly impact performance.  One could\nimagine policies that expose parallelism, drive towards a particular output,\netc..\n\nOur current policy is to run tasks that were most recently made available.\n\n\nInlining computations\n---------------------\n\nWe hold on to intermediate computations either in memory or on disk.\n\nFor very cheap computations that may emit new copies of the data, like\n``np.transpose`` or possibly even ``x + 1`` we choose not to store these as\nseparate pieces of data / tasks.  Instead we combine them with the computations\nthat require them.  This may result in repeated computation but saves\nsignificantly on space and computation complexity.\n\nSee the function ``inline_functions`` for more information.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections.abc import Mapping, Sequence\nfrom concurrent.futures import Executor, Future\nfrom functools import partial\nfrom queue import Empty, Queue\n\nfrom dask import config\nfrom dask._task_spec import DataNode, DependenciesMapping, convert_legacy_graph\nfrom dask.callbacks import local_callbacks, unpack_callbacks\nfrom dask.core import flatten, get_dependencies, reverse_dict\nfrom dask.order import order\nfrom dask.typing import Key\n\nif os.name == \"nt\":\n    # Python 3 windows Queue.get doesn't handle interrupts properly. To\n    # workaround this we poll at a sufficiently large interval that it\n    # shouldn't affect performance, but small enough that users trying to kill\n    # an application shouldn't care.\n    def queue_get(q):\n        while True:\n            try:\n                return q.get(block=True, timeout=0.1)\n            except Empty:\n                pass\n\nelse:\n\n    def queue_get(q):\n        return q.get()\n\n\ndef start_state_from_dask(dsk, cache=None, sortkey=None):\n    \"\"\"Start state from a dask\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n    >>> from pprint import pprint  # doctest: +SKIP\n    >>> pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n    {'cache': {'x': 1, 'y': 2},\n     'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n     'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n     'finished': set(),\n     'ready': ['z'],\n     'released': set(),\n     'running': set(),\n     'waiting': {'w': {'z'}},\n     'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n    \"\"\"\n    if sortkey is None:\n        sortkey = order(dsk).get\n    if cache is None:\n        cache = config.get(\"cache\", None)\n    if cache is None:\n        cache = dict()\n\n    dsk = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    data_keys = set()\n    for k, v in dsk.items():\n        if isinstance(v, DataNode):\n            cache[k] = v()\n            data_keys.add(k)\n\n    dsk2 = dsk.copy()\n    dsk2.update(cache)\n\n    dependencies = DependenciesMapping(dsk)\n    waiting = {k: set(v) for k, v in dependencies.items() if k not in data_keys}\n\n    dependents = reverse_dict(dependencies)\n    for a in cache:\n        for b in dependents.get(a, ()):\n            waiting[b].remove(a)\n    waiting_data = {k: v.copy() for k, v in dependents.items() if v}\n\n    ready_set = {k for k, v in waiting.items() if not v}\n    ready = sorted(ready_set, key=sortkey, reverse=True)\n    waiting = {k: v for k, v in waiting.items() if v}\n\n    state = {\n        \"dependencies\": dependencies,\n        \"dependents\": dependents,\n        \"waiting\": waiting,\n        \"waiting_data\": waiting_data,\n        \"cache\": cache,\n        \"ready\": ready,\n        \"running\": set(),\n        \"finished\": set(),\n        \"released\": set(),\n    }\n\n    return state\n\n\n\"\"\"\nRunning tasks\n-------------\n\nWhen we execute tasks we both\n\n1.  Perform the actual work of collecting the appropriate data and calling the function\n2.  Manage administrative state to coordinate with the scheduler\n\"\"\"\n\n\ndef execute_task(key, task_info, dumps, loads, get_id, pack_exception):\n    \"\"\"\n    Compute task and handle all administration\n\n    See Also\n    --------\n    _execute_task : actually execute task\n    \"\"\"\n    try:\n        task, data = loads(task_info)\n        result = task(data)\n        id = get_id()\n        result = dumps((result, id))\n        failed = False\n    except BaseException as e:  # noqa: B036\n        result = pack_exception(e, dumps)\n        failed = True\n    return key, result, failed\n\n\ndef batch_execute_tasks(it):\n    \"\"\"\n    Batch computing of multiple tasks with `execute_task`\n    \"\"\"\n    return [execute_task(*a) for a in it]\n\n\ndef release_data(key, state, delete=True):\n    \"\"\"Remove data from temporary storage\n\n    See Also\n    --------\n    finish_task\n    \"\"\"\n    if key in state[\"waiting_data\"]:\n        assert not state[\"waiting_data\"][key]\n        del state[\"waiting_data\"][key]\n\n    state[\"released\"].add(key)\n\n    if delete:\n        del state[\"cache\"][key]\n\n\ndef finish_task(\n    dsk, key, state, results, sortkey, delete=True, release_data=release_data\n):\n    \"\"\"\n    Update execution state after a task finishes\n\n    Mutates.  This should run atomically (with a lock).\n    \"\"\"\n    for dep in sorted(state[\"dependents\"][key], key=sortkey, reverse=True):\n        s = state[\"waiting\"][dep]\n        s.remove(key)\n        if not s:\n            del state[\"waiting\"][dep]\n            state[\"ready\"].append(dep)\n\n    for dep in state[\"dependencies\"][key]:\n        if dep in state[\"waiting_data\"]:\n            s = state[\"waiting_data\"][dep]\n            s.remove(key)\n            if not s and dep not in results:\n                release_data(dep, state, delete=delete)\n        elif delete and dep not in results:\n            release_data(dep, state, delete=delete)\n\n    state[\"finished\"].add(key)\n    state[\"running\"].remove(key)\n\n    return state\n\n\ndef nested_get(ind, coll):\n    \"\"\"Get nested index from collection\n\n    Examples\n    --------\n\n    >>> nested_get(1, 'abc')\n    'b'\n    >>> nested_get([1, 0], 'abc')\n    ('b', 'a')\n    >>> nested_get([[1, 0], [0, 1]], 'abc')\n    (('b', 'a'), ('a', 'b'))\n    \"\"\"\n    if isinstance(ind, list):\n        return tuple(nested_get(i, coll) for i in ind)\n    else:\n        return coll[ind]\n\n\ndef default_get_id():\n    \"\"\"Default get_id\"\"\"\n    return None\n\n\ndef default_pack_exception(e, dumps):\n    raise\n\n\ndef reraise(exc, tb=None):\n    if exc.__traceback__ is not tb:\n        raise exc.with_traceback(tb)\n    raise exc\n\n\ndef identity(x):\n    \"\"\"Identity function. Returns x.\n\n    >>> identity(3)\n    3\n    \"\"\"\n    return x\n\n\n\"\"\"\nTask Selection\n--------------\n\nWe often have a choice among many tasks to run next.  This choice is both\ncheap and can significantly impact performance.\n\nWe currently select tasks that have recently been made ready.  We hope that\nthis first-in-first-out policy reduces memory footprint\n\"\"\"\n\n\"\"\"\n`get`\n-----\n\nThe main function of the scheduler.  Get is the main entry point.\n\"\"\"\n\n\ndef get_async(\n    submit,\n    num_workers,\n    dsk,\n    result,\n    cache=None,\n    get_id=default_get_id,\n    rerun_exceptions_locally=None,\n    pack_exception=default_pack_exception,\n    raise_exception=reraise,\n    callbacks=None,\n    dumps=identity,\n    loads=identity,\n    chunksize=None,\n    **kwargs,\n):\n    \"\"\"Asynchronous get function\n\n    This is a general version of various asynchronous schedulers for dask.  It\n    takes a ``concurrent.futures.Executor.submit`` function to form a more\n    specific ``get`` method that walks through the dask array with parallel\n    workers, avoiding repeat computation and minimizing memory use.\n\n    Parameters\n    ----------\n    submit : function\n        A ``concurrent.futures.Executor.submit`` function\n    num_workers : int\n        The number of workers that task submissions can be spread over\n    dsk : dict\n        A dask dictionary specifying a workflow\n    result : key or list of keys\n        Keys corresponding to desired data\n    cache : dict-like, optional\n        Temporary storage of results\n    get_id : callable, optional\n        Function to return the worker id, takes no arguments. Examples are\n        `threading.current_thread` and `multiprocessing.current_process`.\n    rerun_exceptions_locally : bool, optional\n        Whether to rerun failing tasks in local process to enable debugging\n        (False by default)\n    pack_exception : callable, optional\n        Function to take an exception and ``dumps`` method, and return a\n        serialized tuple of ``(exception, traceback)`` to send back to the\n        scheduler. Default is to just raise the exception.\n    raise_exception : callable, optional\n        Function that takes an exception and a traceback, and raises an error.\n    callbacks : tuple or list of tuples, optional\n        Callbacks are passed in as tuples of length 5. Multiple sets of\n        callbacks may be passed in as a list of tuples. For more information,\n        see the dask.diagnostics documentation.\n    dumps: callable, optional\n        Function to serialize task data and results to communicate between\n        worker and parent.  Defaults to identity.\n    loads: callable, optional\n        Inverse function of `dumps`.  Defaults to identity.\n    chunksize: int, optional\n        Size of chunks to use when dispatching work. Defaults to 1.\n        If -1, will be computed to evenly divide ready work across workers.\n\n    See Also\n    --------\n    threaded.get\n    \"\"\"\n    chunksize = chunksize or config.get(\"chunksize\", 1)\n\n    queue = Queue()\n\n    if isinstance(result, list):\n        result_flat = set(flatten(result))\n    else:\n        result_flat = {result}\n    results = set(result_flat)\n\n    dsk = dict(convert_legacy_graph(dsk))\n    with local_callbacks(callbacks) as callbacks:\n        _, _, pretask_cbs, posttask_cbs, _ = unpack_callbacks(callbacks)\n        started_cbs = []\n        succeeded = False\n        # if start_state_from_dask fails, we will have something\n        # to pass to the final block.\n        state = {}\n        try:\n            for cb in callbacks:\n                if cb[0]:\n                    cb[0](dsk)\n                started_cbs.append(cb)\n\n            keyorder = order(dsk)\n\n            state = start_state_from_dask(dsk, cache=cache, sortkey=keyorder.get)\n\n            for _, start_state, _, _, _ in callbacks:\n                if start_state:\n                    start_state(dsk, state)\n\n            if rerun_exceptions_locally is None:\n                rerun_exceptions_locally = config.get(\"rerun_exceptions_locally\", False)\n\n            if state[\"waiting\"] and not state[\"ready\"]:\n                raise ValueError(\"Found no accessible jobs in dask\")\n\n            def fire_tasks(chunksize):\n                \"\"\"Fire off a task to the thread pool\"\"\"\n                # Determine chunksize and/or number of tasks to submit\n                nready = len(state[\"ready\"])\n                if chunksize == -1:\n                    ntasks = nready\n                    chunksize = -(ntasks // -num_workers)\n                else:\n                    used_workers = -(len(state[\"running\"]) // -chunksize)\n                    avail_workers = max(num_workers - used_workers, 0)\n                    ntasks = min(nready, chunksize * avail_workers)\n\n                # Prep all ready tasks for submission\n                args = []\n                for _ in range(ntasks):\n                    # Get the next task to compute (most recently added)\n                    key = state[\"ready\"].pop()\n                    # Notify task is running\n                    state[\"running\"].add(key)\n                    for f in pretask_cbs:\n                        f(key, dsk, state)\n\n                    # Prep args to send\n                    data = {\n                        dep: state[\"cache\"][dep] for dep in get_dependencies(dsk, key)\n                    }\n                    args.append(\n                        (\n                            key,\n                            dumps((dsk[key], data)),\n                            dumps,\n                            loads,\n                            get_id,\n                            pack_exception,\n                        )\n                    )\n\n                # Batch submit\n                for i in range(-(len(args) // -chunksize)):\n                    each_args = args[i * chunksize : (i + 1) * chunksize]\n                    if not each_args:\n                        break\n                    fut = submit(batch_execute_tasks, each_args)\n                    fut.add_done_callback(queue.put)\n\n            # Main loop, wait on tasks to finish, insert new ones\n            while state[\"waiting\"] or state[\"ready\"] or state[\"running\"]:\n                fire_tasks(chunksize)\n                for key, res_info, failed in queue_get(queue).result():\n                    if failed:\n                        exc, tb = loads(res_info)\n                        if rerun_exceptions_locally:\n                            data = {\n                                dep: state[\"cache\"][dep]\n                                for dep in get_dependencies(dsk, key)\n                            }\n                            task = dsk[key]\n                            task(data)  # Re-execute locally\n                        else:\n                            raise_exception(exc, tb)\n                    res, worker_id = loads(res_info)\n                    state[\"cache\"][key] = res\n                    finish_task(dsk, key, state, results, keyorder.get)\n                    for f in posttask_cbs:\n                        f(key, res, dsk, state, worker_id)\n\n            succeeded = True\n\n        finally:\n            for _, _, _, _, finish in started_cbs:\n                if finish:\n                    finish(dsk, state, not succeeded)\n\n    return nested_get(result, state[\"cache\"])\n\n\n\"\"\" Synchronous concrete version of get_async\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a\nsequential one.  This is useful for debugging and for code dominated by the\nGIL\n\"\"\"\n\n\nclass SynchronousExecutor(Executor):\n    _max_workers = 1\n\n    def submit(self, fn, *args, **kwargs):\n        fut = Future()\n        try:\n            fut.set_result(fn(*args, **kwargs))\n        except BaseException as e:  # noqa: B036\n            fut.set_exception(e)\n        return fut\n\n\nsynchronous_executor = SynchronousExecutor()\n\n\ndef get_sync(dsk: Mapping, keys: Sequence[Key] | Key, **kwargs):\n    \"\"\"A naive synchronous version of get_async\n\n    Can be useful for debugging.\n    \"\"\"\n    kwargs.pop(\"num_workers\", None)  # if num_workers present, remove it\n    return get_async(\n        synchronous_executor.submit,\n        synchronous_executor._max_workers,\n        dsk,\n        keys,\n        **kwargs,\n    )\n\n\n\"\"\" Adaptor for ``multiprocessing.Pool`` instances\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a wrapper\nclass for ``multiprocessing.Pool`` instances so we can treat them like\n``concurrent.futures.Executor`` instances instead.\n\nThis is mainly useful for legacy use cases or users that prefer\n``multiprocessing.Pool``.\n\"\"\"\n\n\nclass MultiprocessingPoolExecutor(Executor):\n    def __init__(self, pool):\n        self.pool = pool\n        self._max_workers = len(pool._pool)\n\n    def submit(self, fn, *args, **kwargs):\n        return submit_apply_async(self.pool.apply_async, fn, *args, **kwargs)\n\n\ndef submit_apply_async(apply_async, fn, *args, **kwargs):\n    fut = Future()\n    apply_async(fn, args, kwargs, fut.set_result, fut.set_exception)\n    return fut\n\n\ndef get_apply_async(apply_async, num_workers, *args, **kwargs):\n    return get_async(\n        partial(submit_apply_async, apply_async), num_workers, *args, **kwargs\n    )\n\n\ndef sortkey(item):\n    \"\"\"Sorting key function that is robust to different types\n\n    Both strings and tuples are common key types in dask graphs.\n    However In Python 3 one can not compare strings with tuples directly.\n    This function maps many types to a form where they can be compared\n\n    Examples\n    --------\n    >>> sortkey('Hello')\n    ('str', 'Hello')\n\n    >>> sortkey(('x', 1))\n    ('tuple', ('x', 1))\n    \"\"\"\n    return (type(item).__name__, item)\n\n\n### Source File Content:\n\"\"\"\nAsynchronous Shared-Memory Scheduler for Dask Graphs.\n\nThis scheduler coordinates several workers to execute tasks in a dask graph in\nparallel.  It depends on a ``concurrent.futures.Executor``\nand a corresponding Queue for worker-to-scheduler communication.\n\nIt tries to execute tasks in an order which maintains a small memory footprint\nthroughout execution.  It does this by running tasks that allow us to release\ndata resources.\n\n\nTask Selection Policy\n=====================\n\nWhen we complete a task we add more data in to our set of available data; this\nnew data makes new tasks available.  We preferentially choose tasks that were\njust made available in a last-in-first-out fashion.  We implement this as a\nsimple stack.  This results in more depth-first rather than breadth first\nbehavior which encourages us to process batches of data to completion before\nstarting in on new data when possible.\n\nWhen the addition of new data readies multiple tasks simultaneously we add\ntasks to the stack in sorted order so that tasks with greater keynames are run\nfirst.  This can be handy to break ties in a predictable fashion.\n\n\nState\n=====\n\nMany functions pass around a ``state`` variable that holds the current state of\nthe computation.  This variable consists of several other dictionaries and\nsets, explained below.\n\nConstant state\n--------------\n\n1.  dependencies: {x: [a, b ,c]} a,b,c, must be run before x\n2.  dependents: {a: [x, y]} a must run before x or y\n\nChanging state\n--------------\n\n### Data\n\n1.  cache: available concrete data.  {key: actual-data}\n2.  released: data that we've seen, used, and released because it is no longer\n    needed\n\n### Jobs\n\n1.  ready: A fifo stack of ready-to-run tasks\n2.  running: A set of tasks currently in execution\n3.  finished: A set of finished tasks\n4.  waiting: which tasks are still waiting on others :: {key: {keys}}\n    Real-time equivalent of dependencies\n5.  waiting_data: available data to yet-to-be-run-tasks :: {key: {keys}}\n    Real-time equivalent of dependents\n\n\nExamples\n--------\n\n>>> import pprint  # doctest: +SKIP\n>>> inc = lambda x: x + 1\n>>> add = lambda x, y: x + y\n>>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n>>> pprint.pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n{'cache': {'x': 1, 'y': 2},\n 'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n 'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n 'finished': set(),\n 'ready': ['z'],\n 'released': set(),\n 'running': set(),\n 'waiting': {'w': {'z'}},\n 'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n\nOptimizations\n=============\n\nWe build this scheduler with out-of-core array operations in mind.  To this end\nwe have encoded some particular optimizations.\n\nCompute to release data\n-----------------------\n\nWhen we choose a new task to execute we often have many options.  Policies at\nthis stage are cheap and can significantly impact performance.  One could\nimagine policies that expose parallelism, drive towards a particular output,\netc..\n\nOur current policy is to run tasks that were most recently made available.\n\n\nInlining computations\n---------------------\n\nWe hold on to intermediate computations either in memory or on disk.\n\nFor very cheap computations that may emit new copies of the data, like\n``np.transpose`` or possibly even ``x + 1`` we choose not to store these as\nseparate pieces of data / tasks.  Instead we combine them with the computations\nthat require them.  This may result in repeated computation but saves\nsignificantly on space and computation complexity.\n\nSee the function ``inline_functions`` for more information.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections.abc import Mapping, Sequence\nfrom concurrent.futures import Executor, Future\nfrom functools import partial\nfrom queue import Empty, Queue\n\nfrom dask import config\nfrom dask._task_spec import DataNode, DependenciesMapping, convert_legacy_graph\nfrom dask.callbacks import local_callbacks, unpack_callbacks\nfrom dask.core import flatten, get_dependencies, reverse_dict\nfrom dask.order import order\nfrom dask.typing import Key\n\nif os.name == \"nt\":\n    # Python 3 windows Queue.get doesn't handle interrupts properly. To\n    # workaround this we poll at a sufficiently large interval that it\n    # shouldn't affect performance, but small enough that users trying to kill\n    # an application shouldn't care.\n    def queue_get(q):\n        while True:\n            try:\n                return q.get(block=True, timeout=0.1)\n            except Empty:\n                pass\n\nelse:\n\n    def queue_get(q):\n        return q.get()\n\n\ndef start_state_from_dask(dsk, cache=None, sortkey=None):\n    \"\"\"Start state from a dask\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n    >>> from pprint import pprint  # doctest: +SKIP\n    >>> pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n    {'cache': {'x': 1, 'y': 2},\n     'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n     'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n     'finished': set(),\n     'ready': ['z'],\n     'released': set(),\n     'running': set(),\n     'waiting': {'w': {'z'}},\n     'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n    \"\"\"\n    if sortkey is None:\n        sortkey = order(dsk).get\n    if cache is None:\n        cache = config.get(\"cache\", None)\n    if cache is None:\n        cache = dict()\n\n    dsk = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    data_keys = set()\n    for k, v in dsk.items():\n        if isinstance(v, DataNode):\n            cache[k] = v()\n            data_keys.add(k)\n\n    dsk2 = dsk.copy()\n    dsk2.update(cache)\n\n    dependencies = DependenciesMapping(dsk)\n    waiting = {k: set(v) for k, v in dependencies.items() if k not in data_keys}\n\n    dependents = reverse_dict(dependencies)\n    for a in cache:\n        for b in dependents.get(a, ()):\n            waiting[b].remove(a)\n    waiting_data = {k: v.copy() for k, v in dependents.items() if v}\n\n    ready_set = {k for k, v in waiting.items() if not v}\n    ready = sorted(ready_set, key=sortkey, reverse=True)\n    waiting = {k: v for k, v in waiting.items() if v}\n\n    state = {\n        \"dependencies\": dependencies,\n        \"dependents\": dependents,\n        \"waiting\": waiting,\n        \"waiting_data\": waiting_data,\n        \"cache\": cache,\n        \"ready\": ready,\n        \"running\": set(),\n        \"finished\": set(),\n        \"released\": set(),\n    }\n\n    return state\n\n\n\"\"\"\nRunning tasks\n-------------\n\nWhen we execute tasks we both\n\n1.  Perform the actual work of collecting the appropriate data and calling the function\n2.  Manage administrative state to coordinate with the scheduler\n\"\"\"\n\n\ndef execute_task(key, task_info, dumps, loads, get_id, pack_exception):\n    \"\"\"\n    Compute task and handle all administration\n\n    See Also\n    --------\n    _execute_task : actually execute task\n    \"\"\"\n    try:\n        task, data = loads(task_info)\n        result = task(data)\n        id = get_id()\n        result = dumps((result, id))\n        failed = False\n    except BaseException as e:  # noqa: B036\n        result = pack_exception(e, dumps)\n        failed = True\n    return key, result, failed\n\n\ndef batch_execute_tasks(it):\n    \"\"\"\n    Batch computing of multiple tasks with `execute_task`\n    \"\"\"\n    return [execute_task(*a) for a in it]\n\n\ndef release_data(key, state, delete=True):\n    \"\"\"Remove data from temporary storage\n\n    See Also\n    --------\n    finish_task\n    \"\"\"\n    if key in state[\"waiting_data\"]:\n        assert not state[\"waiting_data\"][key]\n        del state[\"waiting_data\"][key]\n\n    state[\"released\"].add(key)\n\n    if delete:\n        del state[\"cache\"][key]\n\n\ndef finish_task(\n    dsk, key, state, results, sortkey, delete=True, release_data=release_data\n):\n    \"\"\"\n    Update execution state after a task finishes\n\n    Mutates.  This should run atomically (with a lock).\n    \"\"\"\n    for dep in sorted(state[\"dependents\"][key], key=sortkey, reverse=True):\n        s = state[\"waiting\"][dep]\n        s.remove(key)\n        if not s:\n            del state[\"waiting\"][dep]\n            state[\"ready\"].append(dep)\n\n    for dep in state[\"dependencies\"][key]:\n        if dep in state[\"waiting_data\"]:\n            s = state[\"waiting_data\"][dep]\n            s.remove(key)\n            if not s and dep not in results:\n                release_data(dep, state, delete=delete)\n        elif delete and dep not in results:\n            release_data(dep, state, delete=delete)\n\n    state[\"finished\"].add(key)\n    state[\"running\"].remove(key)\n\n    return state\n\n\ndef nested_get(ind, coll):\n    \"\"\"Get nested index from collection\n\n    Examples\n    --------\n\n    >>> nested_get(1, 'abc')\n    'b'\n    >>> nested_get([1, 0], 'abc')\n    ('b', 'a')\n    >>> nested_get([[1, 0], [0, 1]], 'abc')\n    (('b', 'a'), ('a', 'b'))\n    \"\"\"\n    if isinstance(ind, list):\n        return tuple(nested_get(i, coll) for i in ind)\n    else:\n        return coll[ind]\n\n\ndef default_get_id():\n    \"\"\"Default get_id\"\"\"\n    return None\n\n\ndef default_pack_exception(e, dumps):\n    raise\n\n\ndef reraise(exc, tb=None):\n    if exc.__traceback__ is not tb:\n        raise exc.with_traceback(tb)\n    raise exc\n\n\ndef identity(x):\n    \"\"\"Identity function. Returns x.\n\n    >>> identity(3)\n    3\n    \"\"\"\n    return x\n\n\n\"\"\"\nTask Selection\n--------------\n\nWe often have a choice among many tasks to run next.  This choice is both\ncheap and can significantly impact performance.\n\nWe currently select tasks that have recently been made ready.  We hope that\nthis first-in-first-out policy reduces memory footprint\n\"\"\"\n\n\"\"\"\n`get`\n-----\n\nThe main function of the scheduler.  Get is the main entry point.\n\"\"\"\n\n\ndef get_async(\n    submit,\n    num_workers,\n    dsk,\n    result,\n    cache=None,\n    get_id=default_get_id,\n    rerun_exceptions_locally=None,\n    pack_exception=default_pack_exception,\n    raise_exception=reraise,\n    callbacks=None,\n    dumps=identity,\n    loads=identity,\n    chunksize=None,\n    **kwargs,\n):\n    \"\"\"Asynchronous get function\n\n    This is a general version of various asynchronous schedulers for dask.  It\n    takes a ``concurrent.futures.Executor.submit`` function to form a more\n    specific ``get`` method that walks through the dask array with parallel\n    workers, avoiding repeat computation and minimizing memory use.\n\n    Parameters\n    ----------\n    submit : function\n        A ``concurrent.futures.Executor.submit`` function\n    num_workers : int\n        The number of workers that task submissions can be spread over\n    dsk : dict\n        A dask dictionary specifying a workflow\n    result : key or list of keys\n        Keys corresponding to desired data\n    cache : dict-like, optional\n        Temporary storage of results\n    get_id : callable, optional\n        Function to return the worker id, takes no arguments. Examples are\n        `threading.current_thread` and `multiprocessing.current_process`.\n    rerun_exceptions_locally : bool, optional\n        Whether to rerun failing tasks in local process to enable debugging\n        (False by default)\n    pack_exception : callable, optional\n        Function to take an exception and ``dumps`` method, and return a\n        serialized tuple of ``(exception, traceback)`` to send back to the\n        scheduler. Default is to just raise the exception.\n    raise_exception : callable, optional\n        Function that takes an exception and a traceback, and raises an error.\n    callbacks : tuple or list of tuples, optional\n        Callbacks are passed in as tuples of length 5. Multiple sets of\n        callbacks may be passed in as a list of tuples. For more information,\n        see the dask.diagnostics documentation.\n    dumps: callable, optional\n        Function to serialize task data and results to communicate between\n        worker and parent.  Defaults to identity.\n    loads: callable, optional\n        Inverse function of `dumps`.  Defaults to identity.\n    chunksize: int, optional\n        Size of chunks to use when dispatching work. Defaults to 1.\n        If -1, will be computed to evenly divide ready work across workers.\n\n    See Also\n    --------\n    threaded.get\n    \"\"\"\n    chunksize = chunksize or config.get(\"chunksize\", 1)\n\n    queue = Queue()\n\n    if isinstance(result, list):\n        result_flat = set(flatten(result))\n    else:\n        result_flat = {result}\n    results = set(result_flat)\n\n    dsk = dict(convert_legacy_graph(dsk))\n    with local_callbacks(callbacks) as callbacks:\n        _, _, pretask_cbs, posttask_cbs, _ = unpack_callbacks(callbacks)\n        started_cbs = []\n        succeeded = False\n        # if start_state_from_dask fails, we will have something\n        # to pass to the final block.\n        state = {}\n        try:\n            for cb in callbacks:\n                if cb[0]:\n                    cb[0](dsk)\n                started_cbs.append(cb)\n\n            keyorder = order(dsk)\n\n            state = start_state_from_dask(dsk, cache=cache, sortkey=keyorder.get)\n\n            for _, start_state, _, _, _ in callbacks:\n                if start_state:\n                    start_state(dsk, state)\n\n            if rerun_exceptions_locally is None:\n                rerun_exceptions_locally = config.get(\"rerun_exceptions_locally\", False)\n\n            if state[\"waiting\"] and not state[\"ready\"]:\n                raise ValueError(\"Found no accessible jobs in dask\")\n\n            def fire_tasks(chunksize):\n                \"\"\"Fire off a task to the thread pool\"\"\"\n                # Determine chunksize and/or number of tasks to submit\n                nready = len(state[\"ready\"])\n                if chunksize == -1:\n                    ntasks = nready\n                    chunksize = -(ntasks // -num_workers)\n                else:\n                    used_workers = -(len(state[\"running\"]) // -chunksize)\n                    avail_workers = max(num_workers - used_workers, 0)\n                    ntasks = min(nready, chunksize * avail_workers)\n\n                # Prep all ready tasks for submission\n                args = []\n                for _ in range(ntasks):\n                    # Get the next task to compute (most recently added)\n                    key = state[\"ready\"].pop()\n                    # Notify task is running\n                    state[\"running\"].add(key)\n                    for f in pretask_cbs:\n                        f(key, dsk, state)\n\n                    # Prep args to send\n                    data = {\n                        dep: state[\"cache\"][dep] for dep in get_dependencies(dsk, key)\n                    }\n                    args.append(\n                        (\n                            key,\n                            dumps((dsk[key], data)),\n                            dumps,\n                            loads,\n                            get_id,\n                            pack_exception,\n                        )\n                    )\n\n                # Batch submit\n                for i in range(-(len(args) // -chunksize)):\n                    each_args = args[i * chunksize : (i + 1) * chunksize]\n                    if not each_args:\n                        break\n                    fut = submit(batch_execute_tasks, each_args)\n                    fut.add_done_callback(queue.put)\n\n            # Main loop, wait on tasks to finish, insert new ones\n            while state[\"waiting\"] or state[\"ready\"] or state[\"running\"]:\n                fire_tasks(chunksize)\n                for key, res_info, failed in queue_get(queue).result():\n                    if failed:\n                        exc, tb = loads(res_info)\n                        if rerun_exceptions_locally:\n                            data = {\n                                dep: state[\"cache\"][dep]\n                                for dep in get_dependencies(dsk, key)\n                            }\n                            task = dsk[key]\n                            task(data)  # Re-execute locally\n                        else:\n                            raise_exception(exc, tb)\n                    res, worker_id = loads(res_info)\n                    state[\"cache\"][key] = res\n                    finish_task(dsk, key, state, results, keyorder.get)\n                    for f in posttask_cbs:\n                        f(key, res, dsk, state, worker_id)\n\n            succeeded = True\n\n        finally:\n            for _, _, _, _, finish in started_cbs:\n                if finish:\n                    finish(dsk, state, not succeeded)\n\n    return nested_get(result, state[\"cache\"])\n\n\n\"\"\" Synchronous concrete version of get_async\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a\nsequential one.  This is useful for debugging and for code dominated by the\nGIL\n\"\"\"\n\n\nclass SynchronousExecutor(Executor):\n    _max_workers = 1\n\n    def submit(self, fn, *args, **kwargs):\n        fut = Future()\n        try:\n            fut.set_result(fn(*args, **kwargs))\n        except BaseException as e:  # noqa: B036\n            fut.set_exception(e)\n        return fut\n\n\nsynchronous_executor = SynchronousExecutor()\n\n\ndef get_sync(dsk: Mapping, keys: Sequence[Key] | Key, **kwargs):\n    \"\"\"A naive synchronous version of get_async\n\n    Can be useful for debugging.\n    \"\"\"\n    kwargs.pop(\"num_workers\", None)  # if num_workers present, remove it\n    return get_async(\n        synchronous_executor.submit,\n        synchronous_executor._max_workers,\n        dsk,\n        keys,\n        **kwargs,\n    )\n\n\n\"\"\" Adaptor for ``multiprocessing.Pool`` instances\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a wrapper\nclass for ``multiprocessing.Pool`` instances so we can treat them like\n``concurrent.futures.Executor`` instances instead.\n\nThis is mainly useful for legacy use cases or users that prefer\n``multiprocessing.Pool``.\n\"\"\"\n\n\nclass MultiprocessingPoolExecutor(Executor):\n    def __init__(self, pool):\n        self.pool = pool\n        self._max_workers = len(pool._pool)\n\n    def submit(self, fn, *args, **kwargs):\n        return submit_apply_async(self.pool.apply_async, fn, *args, **kwargs)\n\n\ndef submit_apply_async(apply_async, fn, *args, **kwargs):\n    fut = Future()\n    apply_async(fn, args, kwargs, fut.set_result, fut.set_exception)\n    return fut\n\n\ndef get_apply_async(apply_async, num_workers, *args, **kwargs):\n    return get_async(\n        partial(submit_apply_async, apply_async), num_workers, *args, **kwargs\n    )\n\n\ndef sortkey(item):\n    \"\"\"Sorting key function that is robust to different types\n\n    Both strings and tuples are common key types in dask graphs.\n    However In Python 3 one can not compare strings with tuples directly.\n    This function maps many types to a form where they can be compared\n\n    Examples\n    --------\n    >>> sortkey('Hello')\n    ('str', 'Hello')\n\n    >>> sortkey(('x', 1))\n    ('tuple', ('x', 1))\n    \"\"\"\n    return (type(item).__name__, item)\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:31:08"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport pytest\n\nimport dask\nfrom dask.local import finish_task, get_sync, sortkey, start_state_from_dask\nfrom dask.order import order\nfrom dask.utils_test import GetFunctionTestMixin, add, inc\n\nfib_dask = {\"f0\": 0, \"f1\": 1, \"f2\": 1, \"f3\": 2, \"f4\": 3, \"f5\": 5, \"f6\": 8}\n\n\ndef test_start_state():\n    dsk = {\"x\": 1, \"y\": 2, \"z\": (inc, \"x\"), \"w\": (add, \"z\", \"y\")}\n    result = start_state_from_dask(dsk)\n\n    expected = {\n        \"cache\": {\"x\": 1, \"y\": 2},\n        \"dependencies\": {\n            \"w\": {\"y\", \"z\"},\n            \"x\": set(),\n            \"y\": set(),\n            \"z\": {\"x\"},\n        },\n        \"dependents\": {\"w\": set(), \"x\": {\"z\"}, \"y\": {\"w\"}, \"z\": {\"w\"}},\n        \"finished\": set(),\n        \"released\": set(),\n        \"running\": set(),\n        \"ready\": [\"z\"],\n        \"waiting\": {\"w\": {\"z\"}},\n        \"waiting_data\": {\"x\": {\"z\"}, \"y\": {\"w\"}, \"z\": {\"w\"}},\n    }\n    assert result == expected\n\n\ndef test_start_state_looks_at_cache():\n    dsk = {\"b\": (inc, \"a\")}\n    cache = {\"a\": 1}\n    result = start_state_from_dask(dsk, cache)\n    assert result[\"dependencies\"][\"b\"] == {\"a\"}\n    assert result[\"ready\"] == [\"b\"]\n\n\ndef test_start_state_with_redirects():\n    dsk = {\"x\": 1, \"y\": \"x\", \"z\": (inc, \"y\")}\n    result = start_state_from_dask(dsk)\n    assert result[\"cache\"] == {\"x\": 1}\n\n\ndef test_start_state_with_independent_but_runnable_tasks():\n    assert start_state_from_dask({\"x\": (inc, 1)})[\"ready\"] == [\"x\"]\n\n\ndef test_start_state_with_tasks_no_deps():\n    dsk = {\"a\": [1, (inc, 2)], \"b\": [1, 2, 3, 4], \"c\": (inc, 3)}\n    state = start_state_from_dask(dsk)\n    assert list(state[\"cache\"].keys()) == [\"b\"]\n    assert \"a\" in state[\"ready\"] and \"c\" in state[\"ready\"]\n    deps = {k: set() for k in \"abc\"}\n    assert state[\"dependencies\"] == deps\n    assert state[\"dependents\"] == deps\n\n\ndef test_finish_task():\n    dsk = {\"x\": 1, \"y\": 2, \"z\": (inc, \"x\"), \"w\": (add, \"z\", \"y\")}\n    sortkey = order(dsk).get\n    state = start_state_from_dask(dsk)\n    state[\"ready\"].remove(\"z\")\n    state[\"running\"] = {\"z\", \"other-task\"}\n    task = \"z\"\n    result = 2\n\n    state[\"cache\"][\"z\"] = result\n    finish_task(dsk, task, state, set(), sortkey)\n\n    assert state == {\n        \"cache\": {\"y\": 2, \"z\": 2},\n        \"dependencies\": {\n            \"w\": {\"y\", \"z\"},\n            \"x\": set(),\n            \"y\": set(),\n            \"z\": {\"x\"},\n        },\n        \"finished\": {\"z\"},\n        \"released\": {\"x\"},\n        \"running\": {\"other-task\"},\n        \"dependents\": {\"w\": set(), \"x\": {\"z\"}, \"y\": {\"w\"}, \"z\": {\"w\"}},\n        \"ready\": [\"w\"],\n        \"waiting\": {},\n        \"waiting_data\": {\"y\": {\"w\"}, \"z\": {\"w\"}},\n    }\n\n\nclass TestGetAsync(GetFunctionTestMixin):\n    get = staticmethod(get_sync)\n\n    def test_get_sync_num_workers(self):\n        self.get({\"x\": (inc, \"y\"), \"y\": 1}, \"x\", num_workers=2)\n\n\ndef test_cache_options():\n    cache = {}\n\n    def inc2(x):\n        assert \"y\" in cache\n        return x + 1\n\n    with dask.config.set(cache=cache):\n        get_sync({\"x\": (inc2, \"y\"), \"y\": 1}, \"x\")\n\n\ndef test_sort_key():\n    L = [\"x\", (\"x\", 1), (\"z\", 0), (\"x\", 0)]\n    assert sorted(L, key=sortkey) == [\"x\", (\"x\", 0), (\"x\", 1), (\"z\", 0)]\n\n\ndef test_callback():\n    f = lambda x: x + 1\n    dsk = {\"a\": (f, 1)}\n    from dask.threaded import get\n\n    def start_callback(key, d, state):\n        assert key == \"a\" or key is None\n        assert d == dsk\n        assert isinstance(state, dict)\n\n    def end_callback(key, value, d, state, worker_id):\n        assert key == \"a\" or key is None\n        assert value == 2 or value is None\n        assert d == dsk\n        assert isinstance(state, dict)\n\n    get(dsk, \"a\", start_callback=start_callback, end_callback=end_callback)\n\n\ndef test_exceptions_propagate():\n    class MyException(Exception):\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n\n        def __str__(self):\n            return \"My Exception!\"\n\n    def f():\n        raise MyException(1, 2)\n\n    from dask.threaded import get\n\n    try:\n        get({\"x\": (f,)}, \"x\")\n        assert False\n    except MyException as e:\n        assert \"My Exception!\" in str(e)\n        assert \"a\" in dir(e)\n        assert e.a == 1\n        assert e.b == 2\n\n\ndef test_ordering():\n    L = []\n\n    def append(i):\n        L.append(i)\n\n    dsk = {(\"x\", i): (append, i) for i in range(10)}\n    x_keys = sorted(dsk)\n    dsk[\"y\"] = (lambda *args: None, list(x_keys))\n\n    get_sync(dsk, \"y\")\n\n    assert L == sorted(L, reverse=True)\n\n\ndef test_complex_ordering():\n    pytest.importorskip(\"numpy\")\n    da = pytest.importorskip(\"dask.array\")\n    from dask.diagnostics import Callback\n\n    actual_order = []\n\n    def track_order(key, dask, state):\n        actual_order.append(key)\n\n    x = da.random.normal(size=(20, 20), chunks=(-1, -1))\n    res = (x.dot(x.T) - x.mean(axis=0)).std()\n    dsk = dict(res.__dask_graph__())\n    exp_order_dict = order(dsk)\n    exp_order = sorted(exp_order_dict.keys(), key=exp_order_dict.get)\n    with Callback(pretask=track_order):\n        get_sync(dsk, exp_order[-1])\n    assert actual_order == exp_order\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_multiprocessing\\test_multiprocessing.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport copyreg\nimport multiprocessing\nimport multiprocessing.pool\nimport os\nimport pickle\nimport sys\nimport traceback\nfrom collections.abc import Mapping, Sequence\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nfrom warnings import warn\n\nimport cloudpickle\n\nfrom dask import config\nfrom dask.local import MultiprocessingPoolExecutor, get_async, reraise\nfrom dask.optimization import cull, fuse\nfrom dask.system import CPU_COUNT\nfrom dask.typing import Key\nfrom dask.utils import ensure_dict\n\n\ndef _reduce_method_descriptor(m):\n    return getattr, (m.__objclass__, m.__name__)\n\n\n# type(set.union) is used as a proxy to <class 'method_descriptor'>\ncopyreg.pickle(type(set.union), _reduce_method_descriptor)\n\n_dumps = partial(cloudpickle.dumps, protocol=pickle.HIGHEST_PROTOCOL)\n_loads = cloudpickle.loads\n\n\ndef _process_get_id():\n    return multiprocessing.current_process().ident\n\n\n# -- Remote Exception Handling --\n# By default, tracebacks can't be serialized using pickle. However, the\n# `tblib` library can enable support for this. Since we don't mandate\n# that tblib is installed, we do the following:\n#\n# - If tblib is installed, use it to serialize the traceback and reraise\n#   in the scheduler process\n# - Otherwise, use a ``RemoteException`` class to contain a serialized\n#   version of the formatted traceback, which will then print in the\n#   scheduler process.\n#\n# To enable testing of the ``RemoteException`` class even when tblib is\n# installed, we don't wrap the class in the try block below\nclass RemoteException(Exception):\n    \"\"\"Remote Exception\n\n    Contains the exception and traceback from a remotely run task\n    \"\"\"\n\n    def __init__(self, exception, traceback):\n        self.exception = exception\n        self.traceback = traceback\n\n    def __str__(self):\n        return str(self.exception) + \"\\n\\nTraceback\\n---------\\n\" + self.traceback\n\n    def __dir__(self):\n        return sorted(set(dir(type(self)) + list(self.__dict__) + dir(self.exception)))\n\n    def __getattr__(self, key):\n        try:\n            return object.__getattribute__(self, key)\n        except AttributeError:\n            return getattr(self.exception, key)\n\n\nexceptions: dict[type[Exception], type[Exception]] = {}\n\n\ndef remote_exception(exc: Exception, tb) -> Exception:\n    \"\"\"Metaclass that wraps exception type in RemoteException\"\"\"\n    if type(exc) in exceptions:\n        typ = exceptions[type(exc)]\n        return typ(exc, tb)\n    else:\n        try:\n            typ = type(\n                exc.__class__.__name__,\n                (RemoteException, type(exc)),\n                {\"exception_type\": type(exc)},\n            )\n            exceptions[type(exc)] = typ\n            return typ(exc, tb)\n        except TypeError:\n            return exc\n\n\ntry:\n    import tblib.pickling_support\n\n    tblib.pickling_support.install()\n\n    def _pack_traceback(tb):\n        return tb\n\nexcept ImportError:\n\n    def _pack_traceback(tb):\n        return \"\".join(traceback.format_tb(tb))\n\n    def reraise(exc, tb=None):\n        exc = remote_exception(exc, tb)\n        raise exc\n\n\ndef pack_exception(e, dumps):\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    tb = _pack_traceback(exc_traceback)\n    try:\n        result = dumps((e, tb))\n    except Exception as e:\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        tb = _pack_traceback(exc_traceback)\n        result = dumps((e, tb))\n    return result\n\n\n_CONTEXT_UNSUPPORTED = \"\"\"\\\nThe 'multiprocessing.context' configuration option will be ignored on Python 2\nand on Windows, because they each only support a single context.\n\"\"\"\n\n\ndef get_context():\n    \"\"\"Return the current multiprocessing context.\"\"\"\n    # fork context does fork()-without-exec(), which can lead to deadlocks,\n    # so default to \"spawn\".\n    context_name = config.get(\"multiprocessing.context\", \"spawn\")\n    if sys.platform == \"win32\":\n        if context_name != \"spawn\":\n            # Only spawn is supported on Win32, can't change it:\n            warn(_CONTEXT_UNSUPPORTED, UserWarning)\n        return multiprocessing\n    else:\n        return multiprocessing.get_context(context_name)\n\n\ndef get(\n    dsk: Mapping,\n    keys: Sequence[Key] | Key,\n    num_workers=None,\n    func_loads=None,\n    func_dumps=None,\n    optimize_graph=True,\n    pool=None,\n    initializer=None,\n    chunksize=None,\n    **kwargs,\n):\n    \"\"\"Multiprocessed get function appropriate for Bags\n\n    Parameters\n    ----------\n    dsk : dict\n        dask graph\n    keys : object or list\n        Desired results from graph\n    num_workers : int\n        Number of worker processes (defaults to number of cores)\n    func_dumps : function\n        Function to use for function serialization (defaults to cloudpickle.dumps)\n    func_loads : function\n        Function to use for function deserialization (defaults to cloudpickle.loads)\n    optimize_graph : bool\n        If True [default], `fuse` is applied to the graph before computation.\n    pool : Executor or Pool\n        Some sort of `Executor` or `Pool` to use\n    initializer: function\n        Ignored if ``pool`` has been set.\n        Function to initialize a worker process before running any tasks in it.\n    chunksize: int, optional\n        Size of chunks to use when dispatching work.\n        Defaults to 6 as some batching is helpful.\n        If -1, will be computed to evenly divide ready work across workers.\n    \"\"\"\n    chunksize = chunksize or config.get(\"chunksize\", 6)\n    pool = pool or config.get(\"pool\", None)\n    initializer = initializer or config.get(\"multiprocessing.initializer\", None)\n    num_workers = num_workers or config.get(\"num_workers\", None) or CPU_COUNT\n    if pool is None:\n        # In order to get consistent hashing in subprocesses, we need to set a\n        # consistent seed for the Python hash algorithm. Unfortunately, there\n        # is no way to specify environment variables only for the Pool\n        # processes, so we have to rely on environment variables being\n        # inherited.\n        if os.environ.get(\"PYTHONHASHSEED\") in (None, \"0\"):\n            # This number is arbitrary; it was chosen to commemorate\n            # https://github.com/dask/dask/issues/6640.\n            os.environ[\"PYTHONHASHSEED\"] = \"6640\"\n        context = get_context()\n        initializer = partial(initialize_worker_process, user_initializer=initializer)\n        pool = ProcessPoolExecutor(\n            num_workers, mp_context=context, initializer=initializer\n        )\n        cleanup = True\n    else:\n        if initializer is not None:\n            warn(\n                \"The ``initializer`` argument is ignored when ``pool`` is provided. \"\n                \"The user should configure ``pool`` with the needed ``initializer`` \"\n                \"on creation.\"\n            )\n        if isinstance(pool, multiprocessing.pool.Pool):\n            pool = MultiprocessingPoolExecutor(pool)\n        cleanup = False\n\n    # Optimize Dask\n    dsk = ensure_dict(dsk)\n    dsk2, dependencies = cull(dsk, keys)\n    if optimize_graph:\n        dsk3, dependencies = fuse(dsk2, keys, dependencies)\n    else:\n        dsk3 = dsk2\n\n    # We specify marshalling functions in order to catch serialization\n    # errors and report them to the user.\n    loads = func_loads or config.get(\"func_loads\", None) or _loads\n    dumps = func_dumps or config.get(\"func_dumps\", None) or _dumps\n\n    # Note former versions used a multiprocessing Manager to share\n    # a Queue between parent and workers, but this is fragile on Windows\n    # (issue #1652).\n    try:\n        # Run\n        result = get_async(\n            pool.submit,\n            pool._max_workers,\n            dsk3,\n            keys,\n            get_id=_process_get_id,\n            dumps=dumps,\n            loads=loads,\n            pack_exception=pack_exception,\n            raise_exception=reraise,\n            chunksize=chunksize,\n            **kwargs,\n        )\n    finally:\n        if cleanup:\n            pool.shutdown()\n    return result\n\n\ndef default_initializer():\n    # If Numpy is already imported, presumably its random state was\n    # inherited from the parent => re-seed it.\n    np = sys.modules.get(\"numpy\")\n    if np is not None:\n        np.random.seed()\n\n\ndef initialize_worker_process(user_initializer=None):\n    \"\"\"\n    Initialize a worker process before running any tasks in it.\n    \"\"\"\n    default_initializer()\n\n    if user_initializer is not None:\n        user_initializer()\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:31:08"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport multiprocessing\nimport pickle\nimport sys\nfrom concurrent.futures import ProcessPoolExecutor\nfrom operator import add\n\nimport pytest\n\nimport dask\nfrom dask import compute, delayed\nfrom dask.multiprocessing import _dumps, _loads, get, get_context, remote_exception\nfrom dask.system import CPU_COUNT\nfrom dask.utils_test import inc\n\n\ndef unrelated_function_global(a):\n    np = pytest.importorskip(\"numpy\")\n    return np.array([a])\n\n\ndef my_small_function_global(a, b):\n    return a + b\n\n\ndef test_pickle_globals():\n    \"\"\"Unrelated globals should not be included in serialized bytes\"\"\"\n    b = _dumps(my_small_function_global)\n    assert b\"my_small_function_global\" in b\n    assert b\"unrelated_function_global\" not in b\n    assert b\"numpy\" not in b\n\n\ndef test_pickle_locals():\n    \"\"\"Unrelated locals should not be included in serialized bytes\"\"\"\n    np = pytest.importorskip(\"numpy\")\n\n    def unrelated_function_local(a):\n        return np.array([a])\n\n    def my_small_function_local(a, b):\n        return a + b\n\n    b = _dumps(my_small_function_local)\n    assert b\"my_small_function_global\" not in b\n    assert b\"my_small_function_local\" in b\n    assert b\"unrelated_function_local\" not in b\n\n\n@pytest.mark.skipif(pickle.HIGHEST_PROTOCOL < 5, reason=\"requires pickle protocol 5\")\ndef test_out_of_band_pickling():\n    \"\"\"Test that out-of-band pickling works\"\"\"\n    np = pytest.importorskip(\"numpy\")\n    pytest.importorskip(\"cloudpickle\", minversion=\"1.3.0\")\n\n    a = np.arange(5)\n\n    l = []\n    b = _dumps(a, buffer_callback=l.append)\n    assert len(l) == 1\n    assert isinstance(l[0], pickle.PickleBuffer)\n    assert memoryview(l[0]) == memoryview(a)\n\n    a2 = _loads(b, buffers=l)\n    assert np.all(a == a2)\n\n\ndef bad():\n    raise ValueError(\"12345\")\n\n\ndef test_errors_propagate():\n    dsk = {\"x\": (bad,)}\n\n    with pytest.raises(ValueError) as e:\n        get(dsk, \"x\")\n    assert \"12345\" in str(e.value)\n\n\ndef test_remote_exception():\n    e = TypeError(\"hello\")\n    a = remote_exception(e, \"traceback-body\")\n    b = remote_exception(e, \"traceback-body\")\n\n    assert type(a) == type(b)\n    assert isinstance(a, TypeError)\n    assert \"hello\" in str(a)\n    assert \"Traceback\" in str(a)\n    assert \"traceback-body\" in str(a)\n\n\ndef test_lambda_with_cloudpickle():\n    dsk = {\"x\": 2, \"y\": (lambda x: x + 1, \"x\")}\n    assert get(dsk, \"y\") == 3\n\n\ndef lambda_result():\n    return lambda x: x + 1\n\n\ndef test_lambda_results_with_cloudpickle():\n    dsk = {\"x\": (lambda_result,)}\n    f = get(dsk, \"x\")\n    assert f(2) == 3\n\n\nclass NotUnpickleable:\n    def __getstate__(self):\n        return ()\n\n    def __setstate__(self, state):\n        raise ValueError(\"Can't unpickle me\")\n\n\ndef test_unpicklable_args_generate_errors():\n    a = NotUnpickleable()\n\n    dsk = {\"x\": (bool, a)}\n\n    with pytest.raises(ValueError):\n        get(dsk, \"x\")\n\n    dsk = {\"x\": (bool, \"a\"), \"a\": a}\n\n    with pytest.raises(ValueError):\n        get(dsk, \"x\")\n\n\n@pytest.mark.parametrize(\"pool_typ\", [multiprocessing.Pool, ProcessPoolExecutor])\ndef test_reuse_pool(pool_typ):\n    with pool_typ(CPU_COUNT) as pool:\n        with dask.config.set(pool=pool):\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n\n\ndef test_dumps_loads():\n    with dask.config.set(func_dumps=pickle.dumps, func_loads=pickle.loads):\n        assert get({\"x\": 1, \"y\": (add, \"x\", 2)}, \"y\") == 3\n\n\ndef test_fuse_doesnt_clobber_intermediates():\n    d = {\"x\": 1, \"y\": (inc, \"x\"), \"z\": (add, 10, \"y\")}\n    assert get(d, [\"y\", \"z\"]) == (2, 12)\n\n\ndef test_optimize_graph_false():\n    from dask.callbacks import Callback\n\n    d = {\"x\": 1, \"y\": (inc, \"x\"), \"z\": (add, 10, \"y\")}\n    keys = []\n    with Callback(pretask=lambda key, *args: keys.append(key)):\n        get(d, \"z\", optimize_graph=False)\n    assert len(keys) == 2\n\n\ndef test_works_with_highlevel_graph():\n    \"\"\"Previously `dask.multiprocessing.get` would accidentally forward\n    `HighLevelGraph` graphs through the dask optimization/scheduling routines,\n    resulting in odd errors. One way to trigger this was to have a\n    non-indexable object in a task. This is just a smoketest to ensure that\n    things work properly even if `HighLevelGraph` objects get passed to\n    `dask.multiprocessing.get`. See https://github.com/dask/dask/issues/7190.\n    \"\"\"\n\n    class NoIndex:\n        def __init__(self, x):\n            self.x = x\n\n        def __getitem__(self, key):\n            raise Exception(\"Oh no!\")\n\n    x = delayed(lambda x: x)(NoIndex(1))\n    (res,) = get(x.dask, x.__dask_keys__())\n    assert isinstance(res, NoIndex)\n    assert res.x == 1\n\n\n@pytest.mark.parametrize(\"random\", [\"numpy\", \"random\"])\ndef test_random_seeds(random):\n    if random == \"numpy\":\n        np = pytest.importorskip(\"numpy\")\n        random = np.random\n    else:\n        import random\n\n    @delayed(pure=False)\n    def f():\n        return tuple(random.randint(0, 10000) for i in range(5))\n\n    N = 10\n    with dask.config.set(scheduler=\"processes\"):\n        (results,) = compute([f() for _ in range(N)])\n\n    assert len(set(results)) == N\n\n\nclass global_:\n    value = 0\n\n\ndef proc_init():\n    global_.value = 1\n\n\n@pytest.mark.parametrize(\n    \"scheduler, initializer, expected_results\",\n    [\n        (\"threading\", None, [1] * 10),\n        (\"processes\", None, [0] * 10),\n        (\"processes\", proc_init, [1] * 10),\n    ],\n)\ndef test_process_initializer(scheduler, initializer, expected_results):\n    @delayed(pure=False)\n    def f():\n        return global_.value\n\n    global_.value = 1\n\n    with dask.config.set(\n        {\"scheduler\": scheduler, \"multiprocessing.initializer\": initializer}\n    ):\n        (results,) = compute([f() for _ in range(10)])\n    assert results == expected_results\n\n    (results2,) = compute(\n        [f() for _ in range(10)],\n        scheduler=scheduler,\n        initializer=initializer,\n    )\n    assert results2 == expected_results\n\n\ndef check_for_pytest():\n    \"\"\"We check for spawn by ensuring subprocess doesn't have modules only\n    parent process should have:\n    \"\"\"\n    import sys\n\n    return \"FAKE_MODULE_FOR_TEST\" in sys.modules\n\n\n@pytest.mark.skipif(\n    sys.platform == \"win32\", reason=\"Windows doesn't support different contexts\"\n)\ndef test_custom_context_used_python3_posix():\n    \"\"\"The 'multiprocessing.context' config is used to create the pool.\n\n    We assume default is 'spawn', and therefore test for 'fork'.\n    \"\"\"\n    # We check for 'fork' by ensuring subprocess doesn't have modules only\n    # parent process should have:\n\n    def check_for_pytest():\n        import sys\n\n        return \"FAKE_MODULE_FOR_TEST\" in sys.modules\n\n    import sys\n\n    sys.modules[\"FAKE_MODULE_FOR_TEST\"] = 1\n    try:\n        with dask.config.set({\"multiprocessing.context\": \"fork\"}):\n            result = get({\"x\": (check_for_pytest,)}, \"x\")\n        assert result\n    finally:\n        del sys.modules[\"FAKE_MODULE_FOR_TEST\"]\n\n\n@pytest.mark.skipif(\n    sys.platform == \"win32\", reason=\"Windows doesn't support different contexts\"\n)\ndef test_get_context_using_python3_posix():\n    \"\"\"get_context() respects configuration.\n\n    If default context is changed this test will need to change too.\n    \"\"\"\n    assert get_context() is multiprocessing.get_context(\"spawn\")\n    with dask.config.set({\"multiprocessing.context\": \"forkserver\"}):\n        assert get_context() is multiprocessing.get_context(\"forkserver\")\n    with dask.config.set({\"multiprocessing.context\": \"fork\"}):\n        assert get_context() is multiprocessing.get_context(\"fork\")\n\n\n@pytest.mark.skipif(sys.platform != \"win32\", reason=\"POSIX supports different contexts\")\ndef test_custom_context_ignored_elsewhere():\n    \"\"\"On Windows, setting 'multiprocessing.context' doesn't explode.\n\n    Presumption is it's not used since it's unsupported, but mostly we care about\n    not breaking anything.\n    \"\"\"\n    assert get({\"x\": (inc, 1)}, \"x\") == 2\n    with pytest.warns(UserWarning):\n        with dask.config.set({\"multiprocessing.context\": \"forkserver\"}):\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n\n\n@pytest.mark.skipif(sys.platform != \"win32\", reason=\"POSIX supports different contexts\")\ndef test_get_context_always_default():\n    \"\"\"On Python 2/Windows, get_context() always returns same context.\"\"\"\n    assert get_context() is multiprocessing\n    with pytest.warns(UserWarning):\n        with dask.config.set({\"multiprocessing.context\": \"forkserver\"}):\n            assert get_context() is multiprocessing\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_rewrite\\test_rewrite.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nfrom collections import deque\n\nfrom dask.core import istask, subs\n\n\ndef head(task):\n    \"\"\"Return the top level node of a task\"\"\"\n\n    if istask(task):\n        return task[0]\n    elif isinstance(task, list):\n        return list\n    else:\n        return task\n\n\ndef args(task):\n    \"\"\"Get the arguments for the current task\"\"\"\n\n    if istask(task):\n        return task[1:]\n    elif isinstance(task, list):\n        return task\n    else:\n        return ()\n\n\nclass Traverser:\n    \"\"\"Traverser interface for tasks.\n\n    Class for storing the state while performing a preorder-traversal of a\n    task.\n\n    Parameters\n    ----------\n    term : task\n        The task to be traversed\n\n    Attributes\n    ----------\n    term\n        The current element in the traversal\n    current\n        The head of the current element in the traversal. This is simply `head`\n        applied to the attribute `term`.\n    \"\"\"\n\n    def __init__(self, term, stack=None):\n        self.term = term\n        if not stack:\n            self._stack = deque([END])\n        else:\n            self._stack = stack\n\n    def __iter__(self):\n        while self.current is not END:\n            yield self.current\n            self.next()\n\n    def copy(self):\n        \"\"\"Copy the traverser in its current state.\n\n        This allows the traversal to be pushed onto a stack, for easy\n        backtracking.\"\"\"\n\n        return Traverser(self.term, deque(self._stack))\n\n    def next(self):\n        \"\"\"Proceed to the next term in the preorder traversal.\"\"\"\n\n        subterms = args(self.term)\n        if not subterms:\n            # No subterms, pop off stack\n            self.term = self._stack.pop()\n        else:\n            self.term = subterms[0]\n            self._stack.extend(reversed(subterms[1:]))\n\n    @property\n    def current(self):\n        return head(self.term)\n\n    def skip(self):\n        \"\"\"Skip over all subterms of the current level in the traversal\"\"\"\n        self.term = self._stack.pop()\n\n\nclass Token:\n    \"\"\"A token object.\n\n    Used to express certain objects in the traversal of a task or pattern.\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n\n    def __repr__(self):\n        return self.name\n\n\n# A variable to represent *all* variables in a discrimination net\nVAR = Token(\"?\")\n# Represents the end of the traversal of an expression. We can't use `None`,\n# 'False', etc... here, as anything may be an argument to a function.\nEND = Token(\"end\")\n\n\nclass Node(tuple):\n    \"\"\"A Discrimination Net node.\"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, edges=None, patterns=None):\n        edges = edges if edges else {}\n        patterns = patterns if patterns else []\n        return tuple.__new__(cls, (edges, patterns))\n\n    @property\n    def edges(self):\n        \"\"\"A dictionary, where the keys are edges, and the values are nodes\"\"\"\n        return self[0]\n\n    @property\n    def patterns(self):\n        \"\"\"A list of all patterns that currently match at this node\"\"\"\n        return self[1]\n\n\nclass RewriteRule:\n    \"\"\"A rewrite rule.\n\n    Expresses `lhs` -> `rhs`, for variables `vars`.\n\n    Parameters\n    ----------\n    lhs : task\n        The left-hand-side of the rewrite rule.\n    rhs : task or function\n        The right-hand-side of the rewrite rule. If it's a task, variables in\n        `rhs` will be replaced by terms in the subject that match the variables\n        in `lhs`. If it's a function, the function will be called with a dict\n        of such matches.\n    vars: tuple, optional\n        Tuple of variables found in the lhs. Variables can be represented as\n        any hashable object; a good convention is to use strings. If there are\n        no variables, this can be omitted.\n\n    Examples\n    --------\n    Here's a `RewriteRule` to replace all nested calls to `list`, so that\n    `(list, (list, 'x'))` is replaced with `(list, 'x')`, where `'x'` is a\n    variable.\n\n    >>> import dask.rewrite as dr\n    >>> lhs = (list, (list, 'x'))\n    >>> rhs = (list, 'x')\n    >>> variables = ('x',)\n    >>> rule = dr.RewriteRule(lhs, rhs, variables)\n\n    Here's a more complicated rule that uses a callable right-hand-side. A\n    callable `rhs` takes in a dictionary mapping variables to their matching\n    values. This rule replaces all occurrences of `(list, 'x')` with `'x'` if\n    `'x'` is a list itself.\n\n    >>> lhs = (list, 'x')\n    >>> def repl_list(sd):\n    ...     x = sd['x']\n    ...     if isinstance(x, list):\n    ...         return x\n    ...     else:\n    ...         return (list, x)\n    >>> rule = dr.RewriteRule(lhs, repl_list, variables)\n    \"\"\"\n\n    def __init__(self, lhs, rhs, vars=()):\n        if not isinstance(vars, tuple):\n            raise TypeError(\"vars must be a tuple of variables\")\n        self.lhs = lhs\n        if callable(rhs):\n            self.subs = rhs\n        else:\n            self.subs = self._apply\n        self.rhs = rhs\n        self._varlist = [t for t in Traverser(lhs) if t in vars]\n        # Reduce vars down to just variables found in lhs\n        self.vars = tuple(sorted(set(self._varlist)))\n\n    def _apply(self, sub_dict):\n        term = self.rhs\n        for key, val in sub_dict.items():\n            term = subs(term, key, val)\n        return term\n\n    def __str__(self):\n        return f\"RewriteRule({self.lhs}, {self.rhs}, {self.vars})\"\n\n    def __repr__(self):\n        return str(self)\n\n\nclass RuleSet:\n    \"\"\"A set of rewrite rules.\n\n    Forms a structure for fast rewriting over a set of rewrite rules. This\n    allows for syntactic matching of terms to patterns for many patterns at\n    the same time.\n\n    Examples\n    --------\n\n    >>> import dask.rewrite as dr\n    >>> def f(*args): pass\n    >>> def g(*args): pass\n    >>> def h(*args): pass\n    >>> from operator import add\n\n    >>> rs = dr.RuleSet(\n    ...         dr.RewriteRule((add, 'x', 0), 'x', ('x',)),\n    ...         dr.RewriteRule((f, (g, 'x'), 'y'),\n    ...                        (h, 'x', 'y'),\n    ...                        ('x', 'y')))\n\n    >>> rs.rewrite((add, 2, 0))\n    2\n\n    >>> rs.rewrite((f, (g, 'a', 3)))    # doctest: +ELLIPSIS\n    (<function h at ...>, 'a', 3)\n\n    >>> dsk = {'a': (add, 2, 0),\n    ...        'b': (f, (g, 'a', 3))}\n\n    >>> from toolz import valmap\n    >>> valmap(rs.rewrite, dsk)         # doctest: +ELLIPSIS\n    {'a': 2, 'b': (<function h at ...>, 'a', 3)}\n\n    Attributes\n    ----------\n    rules : list\n        A list of `RewriteRule`s included in the `RuleSet`.\n    \"\"\"\n\n    def __init__(self, *rules):\n        \"\"\"Create a `RuleSet` for a number of rules\n\n        Parameters\n        ----------\n        rules\n            One or more instances of RewriteRule\n        \"\"\"\n        self._net = Node()\n        self.rules = []\n        for p in rules:\n            self.add(p)\n\n    def add(self, rule):\n        \"\"\"Add a rule to the RuleSet.\n\n        Parameters\n        ----------\n        rule : RewriteRule\n        \"\"\"\n\n        if not isinstance(rule, RewriteRule):\n            raise TypeError(\"rule must be instance of RewriteRule\")\n        vars = rule.vars\n        curr_node = self._net\n        ind = len(self.rules)\n        # List of variables, in order they appear in the POT of the term\n        for t in Traverser(rule.lhs):\n            prev_node = curr_node\n            if t in vars:\n                t = VAR\n            if t in curr_node.edges:\n                curr_node = curr_node.edges[t]\n            else:\n                curr_node.edges[t] = Node()\n                curr_node = curr_node.edges[t]\n        # We've reached a leaf node. Add the term index to this leaf.\n        prev_node.edges[t].patterns.append(ind)\n        self.rules.append(rule)\n\n    def iter_matches(self, term):\n        \"\"\"A generator that lazily finds matchings for term from the RuleSet.\n\n        Parameters\n        ----------\n        term : task\n\n        Yields\n        ------\n        Tuples of `(rule, subs)`, where `rule` is the rewrite rule being\n        matched, and `subs` is a dictionary mapping the variables in the lhs\n        of the rule to their matching values in the term.\"\"\"\n\n        S = Traverser(term)\n        for m, syms in _match(S, self._net):\n            for i in m:\n                rule = self.rules[i]\n                subs = _process_match(rule, syms)\n                if subs is not None:\n                    yield rule, subs\n\n    def _rewrite(self, term):\n        \"\"\"Apply the rewrite rules in RuleSet to top level of term\"\"\"\n\n        for rule, sd in self.iter_matches(term):\n            # We use for (...) because it's fast in all cases for getting the\n            # first element from the match iterator. As we only want that\n            # element, we break here\n            term = rule.subs(sd)\n            break\n        return term\n\n    def rewrite(self, task, strategy=\"bottom_up\"):\n        \"\"\"Apply the `RuleSet` to `task`.\n\n        This applies the most specific matching rule in the RuleSet to the\n        task, using the provided strategy.\n\n        Parameters\n        ----------\n        term: a task\n            The task to be rewritten\n        strategy: str, optional\n            The rewriting strategy to use. Options are \"bottom_up\" (default),\n            or \"top_level\".\n\n        Examples\n        --------\n        Suppose there was a function `add` that returned the sum of 2 numbers,\n        and another function `double` that returned twice its input:\n\n        >>> add = lambda x, y: x + y\n        >>> double = lambda x: 2*x\n\n        Now suppose `double` was *significantly* faster than `add`, so\n        you'd like to replace all expressions `(add, x, x)` with `(double,\n        x)`, where `x` is a variable. This can be expressed as a rewrite rule:\n\n        >>> rule = RewriteRule((add, 'x', 'x'), (double, 'x'), ('x',))\n        >>> rs = RuleSet(rule)\n\n        This can then be applied to terms to perform the rewriting:\n\n        >>> term = (add, (add, 2, 2), (add, 2, 2))\n        >>> rs.rewrite(term)  # doctest: +SKIP\n        (double, (double, 2))\n\n        If we only wanted to apply this to the top level of the term, the\n        `strategy` kwarg can be set to \"top_level\".\n\n        >>> rs.rewrite(term)  # doctest: +SKIP\n        (double, (add, 2, 2))\n        \"\"\"\n        return strategies[strategy](self, task)\n\n\ndef _top_level(net, term):\n    return net._rewrite(term)\n\n\ndef _bottom_up(net, term):\n    if istask(term):\n        term = (head(term),) + tuple(_bottom_up(net, t) for t in args(term))\n    elif isinstance(term, list):\n        term = [_bottom_up(net, t) for t in args(term)]\n    return net._rewrite(term)\n\n\nstrategies = {\"top_level\": _top_level, \"bottom_up\": _bottom_up}\n\n\ndef _match(S, N):\n    \"\"\"Structural matching of term S to discrimination net node N.\"\"\"\n\n    stack = deque()\n    restore_state_flag = False\n    # matches are stored in a tuple, because all mutations result in a copy,\n    # preventing operations from changing matches stored on the stack.\n    matches = ()\n    while True:\n        if S.current is END:\n            yield N.patterns, matches\n        try:\n            # This try-except block is to catch hashing errors from un-hashable\n            # types. This allows for variables to be matched with un-hashable\n            # objects.\n            n = N.edges.get(S.current, None)\n            if n and not restore_state_flag:\n                stack.append((S.copy(), N, matches))\n                N = n\n                S.next()\n                continue\n        except TypeError:\n            pass\n        n = N.edges.get(VAR, None)\n        if n:\n            restore_state_flag = False\n            matches = matches + (S.term,)\n            S.skip()\n            N = n\n            continue\n        try:\n            # Backtrack here\n            (S, N, matches) = stack.pop()\n            restore_state_flag = True\n        except Exception:\n            return\n\n\ndef _process_match(rule, syms):\n    \"\"\"Process a match to determine if it is correct, and to find the correct\n    substitution that will convert the term into the pattern.\n\n    Parameters\n    ----------\n    rule : RewriteRule\n    syms : iterable\n        Iterable of subterms that match a corresponding variable.\n\n    Returns\n    -------\n    A dictionary of {vars : subterms} describing the substitution to make the\n    pattern equivalent with the term. Returns `None` if the match is\n    invalid.\"\"\"\n\n    subs = {}\n    varlist = rule._varlist\n    if not len(varlist) == len(syms):\n        raise RuntimeError(\"length of varlist doesn't match length of syms.\")\n    for v, s in zip(varlist, syms):\n        if v in subs and subs[v] != s:\n            return None\n        else:\n            subs[v] = s\n    return subs\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:31:08"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nfrom dask.rewrite import VAR, RewriteRule, RuleSet, Traverser, args, head\nfrom dask.utils_test import add, inc\n\n\ndef double(x):\n    return x * 2\n\n\ndef test_head():\n    assert head((inc, 1)) == inc\n    assert head((add, 1, 2)) == add\n    assert head((add, (inc, 1), (inc, 1))) == add\n    assert head([1, 2, 3]) == list\n\n\ndef test_args():\n    assert args((inc, 1)) == (1,)\n    assert args((add, 1, 2)) == (1, 2)\n    assert args(1) == ()\n    assert args([1, 2, 3]) == [1, 2, 3]\n\n\ndef test_traverser():\n    term = (add, (inc, 1), (double, (inc, 1), 2))\n    t = Traverser(term)\n    t2 = t.copy()\n    assert t.current == add\n    t.next()\n    assert t.current == inc\n    # Ensure copies aren't advanced when the original advances\n    assert t2.current == add\n    t.skip()\n    assert t.current == double\n    t.next()\n    assert t.current == inc\n    assert list(t2) == [add, inc, 1, double, inc, 1, 2]\n\n\nvars = (\"a\", \"b\", \"c\")\n# add(a, 1) -> inc(a)\nrule1 = RewriteRule((add, \"a\", 1), (inc, \"a\"), vars)\n# add(a, a) -> double(a)\nrule2 = RewriteRule((add, \"a\", \"a\"), (double, \"a\"), vars)\n# add(inc(a), inc(a)) -> add(double(a), 2)\nrule3 = RewriteRule((add, (inc, \"a\"), (inc, \"a\")), (add, (double, \"a\"), 2), vars)\n# add(inc(b), inc(a)) -> add(add(a, b), 2)\nrule4 = RewriteRule((add, (inc, \"b\"), (inc, \"a\")), (add, (add, \"a\", \"b\"), 2), vars)\n# sum([c, b, a]) -> add(add(a, b), c)\nrule5 = RewriteRule((sum, [\"c\", \"b\", \"a\"]), (add, (add, \"a\", \"b\"), \"c\"), vars)\n# list(x) -> x if x is a list\n\n\ndef repl_list(sd):\n    x = sd[\"x\"]\n    if isinstance(x, list):\n        return x\n    else:\n        return (list, x)\n\n\nrule6 = RewriteRule((list, \"x\"), repl_list, (\"x\",))\n\n\ndef test_RewriteRule():\n    # Test extraneous vars are removed, varlist is correct\n    assert rule1.vars == (\"a\",)\n    assert rule1._varlist == [\"a\"]\n    assert rule2.vars == (\"a\",)\n    assert rule2._varlist == [\"a\", \"a\"]\n    assert rule3.vars == (\"a\",)\n    assert rule3._varlist == [\"a\", \"a\"]\n    assert rule4.vars == (\"a\", \"b\")\n    assert rule4._varlist == [\"b\", \"a\"]\n    assert rule5.vars == (\"a\", \"b\", \"c\")\n    assert rule5._varlist == [\"c\", \"b\", \"a\"]\n\n\ndef test_RewriteRuleSubs():\n    # Test both rhs substitution and callable rhs\n    assert rule1.subs({\"a\": 1}) == (inc, 1)\n    assert rule6.subs({\"x\": [1, 2, 3]}) == [1, 2, 3]\n\n\nrules = [rule1, rule2, rule3, rule4, rule5, rule6]\nrs = RuleSet(*rules)\n\n\ndef test_RuleSet():\n    net = (\n        {\n            add: (\n                {\n                    VAR: ({VAR: ({}, [1]), 1: ({}, [0])}, []),\n                    inc: ({VAR: ({inc: ({VAR: ({}, [2, 3])}, [])}, [])}, []),\n                },\n                [],\n            ),\n            list: ({VAR: ({}, [5])}, []),\n            sum: ({list: ({VAR: ({VAR: ({VAR: ({}, [4])}, [])}, [])}, [])}, []),\n        },\n        [],\n    )\n    assert rs._net == net\n    assert rs.rules == rules\n\n\ndef test_matches():\n    term = (add, 2, 1)\n    matches = list(rs.iter_matches(term))\n    assert len(matches) == 1\n    assert matches[0] == (rule1, {\"a\": 2})\n    # Test matches specific before general\n    term = (add, 1, 1)\n    matches = list(rs.iter_matches(term))\n    assert len(matches) == 2\n    assert matches[0] == (rule1, {\"a\": 1})\n    assert matches[1] == (rule2, {\"a\": 1})\n    # Test matches unhashable. What it's getting rewritten to doesn't make\n    # sense, this is just to test that it works. :)\n    term = (add, [1], [1])\n    matches = list(rs.iter_matches(term))\n    assert len(matches) == 1\n    assert matches[0] == (rule2, {\"a\": [1]})\n    # Test match at depth\n    term = (add, (inc, 1), (inc, 1))\n    matches = list(rs.iter_matches(term))\n    assert len(matches) == 3\n    assert matches[0] == (rule3, {\"a\": 1})\n    assert matches[1] == (rule4, {\"a\": 1, \"b\": 1})\n    assert matches[2] == (rule2, {\"a\": (inc, 1)})\n    # Test non-linear pattern checking\n    term = (add, 2, 3)\n    matches = list(rs.iter_matches(term))\n    assert len(matches) == 0\n\n\ndef test_rewrite():\n    # Rewrite inside list\n    term = (sum, [(add, 1, 1), (add, 1, 1), (add, 1, 1)])\n    new_term = rs.rewrite(term)\n    assert new_term == (add, (add, (inc, 1), (inc, 1)), (inc, 1))\n    # Rules aren't applied to exhaustion, this can be further simplified\n    new_term = rs.rewrite(new_term)\n    assert new_term == (add, (add, (double, 1), 2), (inc, 1))\n    term = (\n        add,\n        (add, (add, (add, 1, 2), (add, 1, 2)), (add, (add, 1, 2), (add, 1, 2))),\n        1,\n    )\n    assert rs.rewrite(term) == (inc, (double, (double, (add, 1, 2))))\n    # Callable RewriteRule rhs\n    term = (list, [1, 2, 3])\n    assert rs.rewrite(term) == [1, 2, 3]\n    term = (list, (map, inc, [1, 2, 3]))\n    assert rs.rewrite(term) == term\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_sizeof\\test_sizeof.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport itertools\nimport logging\nimport random\nimport sys\nfrom array import array\n\nfrom packaging.version import Version\n\nfrom dask._compatibility import importlib_metadata\nfrom dask.utils import Dispatch\n\nsizeof = Dispatch(name=\"sizeof\")\n\nlogger = logging.getLogger(__name__)\n\n\n@sizeof.register(object)\ndef sizeof_default(o):\n    return sys.getsizeof(o)\n\n\n@sizeof.register(bytes)\n@sizeof.register(bytearray)\ndef sizeof_bytes(o):\n    return len(o)\n\n\n@sizeof.register(memoryview)\ndef sizeof_memoryview(o):\n    return o.nbytes\n\n\n@sizeof.register(array)\ndef sizeof_array(o):\n    return o.itemsize * len(o)\n\n\n@sizeof.register(list)\n@sizeof.register(tuple)\n@sizeof.register(set)\n@sizeof.register(frozenset)\ndef sizeof_python_collection(seq):\n    num_items = len(seq)\n    num_samples = 10\n    if num_items > num_samples:\n        if isinstance(seq, (set, frozenset)):\n            # As of Python v3.9, it is deprecated to call random.sample() on\n            # sets but since sets are unordered anyways we can simply pick\n            # the first `num_samples` items.\n            samples = itertools.islice(seq, num_samples)\n        else:\n            samples = random.sample(seq, num_samples)\n        return sys.getsizeof(seq) + int(\n            num_items / num_samples * sum(map(sizeof, samples))\n        )\n    else:\n        return sys.getsizeof(seq) + sum(map(sizeof, seq))\n\n\nclass SimpleSizeof:\n    \"\"\"Sentinel class to mark a class to be skipped by the dispatcher. This only\n    works if this sentinel mixin is first in the mro.\n\n    Examples\n    --------\n    >>> def _get_gc_overhead():\n    ...     class _CustomObject:\n    ...         def __sizeof__(self):\n    ...             return 0\n    ...\n    ...     return sys.getsizeof(_CustomObject())\n\n    >>> class TheAnswer(SimpleSizeof):\n    ...     def __sizeof__(self):\n    ...         # Sizeof always add overhead of an object for GC\n    ...         return 42 - _get_gc_overhead()\n\n    >>> sizeof(TheAnswer())\n    42\n\n    \"\"\"\n\n\n@sizeof.register(SimpleSizeof)\ndef sizeof_blocked(d):\n    return sys.getsizeof(d)\n\n\n@sizeof.register(dict)\ndef sizeof_python_dict(d):\n    return (\n        sys.getsizeof(d)\n        + sizeof(list(d.keys()))\n        + sizeof(list(d.values()))\n        - 2 * sizeof(list())\n    )\n\n\n@sizeof.register_lazy(\"cupy\")\ndef register_cupy():\n    import cupy\n\n    @sizeof.register(cupy.ndarray)\n    def sizeof_cupy_ndarray(x):\n        return int(x.nbytes)\n\n\n@sizeof.register_lazy(\"numba\")\ndef register_numba():\n    import numba.cuda\n\n    @sizeof.register(numba.cuda.cudadrv.devicearray.DeviceNDArray)\n    def sizeof_numba_devicendarray(x):\n        return int(x.nbytes)\n\n\n@sizeof.register_lazy(\"rmm\")\ndef register_rmm():\n    import rmm\n\n    # Only included in 0.11.0+\n    if hasattr(rmm, \"DeviceBuffer\"):\n\n        @sizeof.register(rmm.DeviceBuffer)\n        def sizeof_rmm_devicebuffer(x):\n            return int(x.nbytes)\n\n\n@sizeof.register_lazy(\"numpy\")\ndef register_numpy():\n    import numpy as np\n\n    @sizeof.register(np.ndarray)\n    def sizeof_numpy_ndarray(x):\n        if 0 in x.strides:\n            xs = x[tuple(slice(None) if s != 0 else slice(1) for s in x.strides)]\n            return xs.nbytes\n        return int(x.nbytes)\n\n\n@sizeof.register_lazy(\"pandas\")\ndef register_pandas():\n    import numpy as np\n    import pandas as pd\n\n    OBJECT_DTYPES = (object, pd.StringDtype(\"python\"))\n\n    def object_size(*xs):\n        if not xs:\n            return 0\n        ncells = sum(len(x) for x in xs)\n        if not ncells:\n            return 0\n\n        # Deduplicate Series of references to the same objects,\n        # e.g. as produced by read_parquet\n        unique_samples = {}\n        for x in xs:\n            sample = np.random.choice(x, size=100, replace=True)\n            for i in sample.tolist():\n                unique_samples[id(i)] = i\n\n        nsamples = 100 * len(xs)\n        sample_nbytes = sum(sizeof(i) for i in unique_samples.values())\n        if len(unique_samples) / nsamples > 0.5:\n            # Less than half of the references are duplicated.\n            # Assume that, if we were to analyze twice the amount of random references,\n            # we would get twice the amount of unique objects too.\n            return int(sample_nbytes * ncells / nsamples)\n        else:\n            # Assume we've already found all unique objects and that all references that\n            # we have not yet analyzed are going to point to the same data.\n            return sample_nbytes\n\n    @sizeof.register(pd.DataFrame)\n    def sizeof_pandas_dataframe(df):\n        p = sizeof(df.index) + sizeof(df.columns)\n        object_cols = []\n        prev_dtype = None\n\n        # Unlike df.items(), df._series will not duplicate multiple views of the same\n        # column e.g. df[[\"x\", \"x\", \"x\"]]\n        for col in df._series.values():\n            if prev_dtype is None or col.dtype != prev_dtype:\n                prev_dtype = col.dtype\n                # Contiguous columns of the same dtype share the same overhead\n                p += 1200\n            p += col.memory_usage(index=False, deep=False)\n            if col.dtype in OBJECT_DTYPES:\n                object_cols.append(col._values)\n\n        # Deduplicate references to the same objects appearing in different Series\n        p += object_size(*object_cols)\n\n        return max(1200, p)\n\n    @sizeof.register(pd.Series)\n    def sizeof_pandas_series(s):\n        # https://github.com/dask/dask/pull/9776#issuecomment-1359085962\n        p = 1200 + sizeof(s.index) + s.memory_usage(index=False, deep=False)\n        if s.dtype in OBJECT_DTYPES:\n            p += object_size(s._values)\n        return p\n\n    @sizeof.register(pd.Index)\n    def sizeof_pandas_index(i):\n        p = 400 + i.memory_usage(deep=False)\n        if i.dtype in OBJECT_DTYPES:\n            p += object_size(i)\n        return p\n\n    @sizeof.register(pd.MultiIndex)\n    def sizeof_pandas_multiindex(i):\n        return sum(sizeof(l) for l in i.levels) + sum(c.nbytes for c in i.codes)\n\n\n@sizeof.register_lazy(\"scipy\")\ndef register_spmatrix():\n    import scipy\n    from scipy import sparse\n\n    if Version(scipy.__version__) < Version(\"1.12.0.dev0\"):\n\n        @sizeof.register(sparse.dok_matrix)\n        def sizeof_spmatrix_dok(s):\n            return s.__sizeof__()\n\n    @sizeof.register(sparse.spmatrix)\n    def sizeof_spmatrix(s):\n        return sum(sizeof(v) for v in s.__dict__.values())\n\n\n@sizeof.register_lazy(\"pyarrow\")\ndef register_pyarrow():\n    import pyarrow as pa\n\n    def _get_col_size(data):\n        p = 0\n        if not isinstance(data, pa.ChunkedArray):\n            data = data.data  # pyarrow <0.15.0\n        for chunk in data.iterchunks():\n            for buffer in chunk.buffers():\n                if buffer:\n                    p += buffer.size\n        return p\n\n    @sizeof.register(pa.Table)\n    def sizeof_pyarrow_table(table):\n        p = sizeof(table.schema.metadata)\n        for col in table.itercolumns():\n            p += _get_col_size(col)\n        return int(p) + 1000\n\n    @sizeof.register(pa.ChunkedArray)\n    def sizeof_pyarrow_chunked_array(data):\n        return int(_get_col_size(data)) + 1000\n\n\n@sizeof.register_lazy(\"xarray\")\ndef register_xarray():\n    import sys\n\n    import xarray as xr\n\n    XARRAY_VERSION = Version(xr.__version__)\n    XARRAY_GE_2024_02 = XARRAY_VERSION >= Version(\"2024.02.0\")\n\n    @sizeof.register(xr.core.utils.Frozen)\n    def xarray_sizeof_frozen(obj):\n        return sys.getsizeof(obj) + sizeof(obj.mapping)\n\n    @sizeof.register(xr.DataArray)\n    @sizeof.register(xr.Variable)\n    def xarray_sizeof_da(obj):\n        return sys.getsizeof(obj) + sizeof(obj.data)\n\n    @sizeof.register(xr.Dataset)\n    def xarray_sizeof_ds(obj):\n        return sys.getsizeof(obj) + sizeof(obj.variables)\n\n    if XARRAY_GE_2024_02:\n        xarray_sizeof_da = sizeof.register(xr.NamedArray)(xarray_sizeof_da)\n\n    @sizeof.register(xr.core.indexes.Indexes)\n    def xarray_sizeof_indexes(obj):\n        return (\n            sys.getsizeof(obj)\n            + sizeof(obj._index_type)\n            + sizeof(obj._indexes)\n            + sizeof(obj._variables)\n            + sizeof(obj._dims)\n        )\n\n    @sizeof.register(xr.core.indexes.PandasIndex)\n    def xarray_sizeof_pd_index(obj):\n        return (\n            sys.getsizeof(obj)\n            + sizeof(obj.index)\n            + sizeof(obj.dim)\n            + sizeof(obj.coord_dtype)\n        )\n\n\ndef _register_entry_point_plugins():\n    \"\"\"Register sizeof implementations exposed by the entry_point mechanism.\"\"\"\n    for entry_point in importlib_metadata.entry_points(group=\"dask.sizeof\"):\n        registrar = entry_point.load()\n        try:\n            registrar(sizeof)\n        except Exception:\n            logger.exception(\n                f\"Failed to register sizeof entry point {entry_point.name}\"\n            )\n\n\n_register_entry_point_plugins()\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:31:08"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport os\nimport sys\nfrom array import array\n\nimport pytest\nfrom packaging.version import Version\n\nfrom dask.multiprocessing import get_context\nfrom dask.sizeof import sizeof\nfrom dask.utils import funcname, tmpdir\n\ntry:\n    import pandas as pd\nexcept ImportError:\n    pd = None\n\nrequires_pandas = pytest.mark.skipif(pd is None, reason=\"requires pandas\")\n\n\ndef test_base():\n    assert sizeof(1) == sys.getsizeof(1)\n\n\ndef test_name():\n    assert funcname(sizeof) == \"sizeof\"\n\n\ndef test_containers():\n    assert sizeof([1, 2, [3]]) > (sys.getsizeof(3) * 3 + sys.getsizeof([]))\n\n\ndef test_bytes_like():\n    assert 1000 <= sizeof(bytes(1000)) <= 2000\n    assert 1000 <= sizeof(bytearray(1000)) <= 2000\n    assert 1000 <= sizeof(memoryview(bytes(1000))) <= 2000\n    assert 8000 <= sizeof(array(\"d\", range(1000))) <= 9000\n\n\ndef test_numpy():\n    np = pytest.importorskip(\"numpy\")\n    assert 8000 <= sizeof(np.empty(1000, dtype=\"f8\")) <= 9000\n    dt = np.dtype(\"f8\")\n    assert sizeof(dt) == sys.getsizeof(dt)\n\n\ndef test_numpy_0_strided():\n    np = pytest.importorskip(\"numpy\")\n    x = np.broadcast_to(1, (100, 100, 100))\n    assert sizeof(x) <= 8\n\n\n@requires_pandas\ndef test_pandas():\n    df = pd.DataFrame(\n        {\"x\": [1, 2, 3], \"y\": [\"a\" * 100, \"b\" * 100, \"c\" * 100]}, index=[10, 20, 30]\n    )\n\n    assert sizeof(df) >= sizeof(df.x) + sizeof(df.y) - sizeof(df.index)\n    assert sizeof(df.x) >= sizeof(df.index)\n    assert sizeof(df.y) >= 100 * 3\n    assert sizeof(df.index) >= 20\n\n    assert isinstance(sizeof(df), int)\n    assert isinstance(sizeof(df.x), int)\n    assert isinstance(sizeof(df.index), int)\n\n\n@requires_pandas\ndef test_pandas_contiguous_dtypes():\n    \"\"\"2+ contiguous columns of the same dtype in the same DataFrame share the same\n    surface thus have lower overhead\n    \"\"\"\n    df1 = pd.DataFrame([[1, 2.2], [3, 4.4]])\n    df2 = pd.DataFrame([[1.1, 2.2], [3.3, 4.4]])\n    assert sizeof(df2) < sizeof(df1)\n\n\n@requires_pandas\ndef test_pandas_multiindex():\n    index = pd.MultiIndex.from_product([range(50), list(\"abcdefghilmnopqrstuvwxyz\")])\n    actual_size = sys.getsizeof(index)\n\n    assert 0.5 * actual_size < sizeof(index) < 3 * actual_size\n    assert isinstance(sizeof(index), int)\n\n\n@requires_pandas\ndef test_pandas_repeated_column():\n    df = pd.DataFrame({\"x\": list(range(10_000))})\n    df2 = df[[\"x\", \"x\", \"x\"]]\n    df3 = pd.DataFrame({\"x\": list(range(10_000)), \"y\": list(range(10_000))})\n    assert 80_000 < sizeof(df) < 85_000\n    assert 80_000 < sizeof(df2) < 85_000\n    assert 160_000 < sizeof(df3) < 165_000\n\n\ndef test_sparse_matrix():\n    sparse = pytest.importorskip(\"scipy.sparse\")\n    sp = sparse.eye(10)\n    # These are the 32-bit Python 2.7 values.\n    assert sizeof(sp.todia()) >= 152\n    assert sizeof(sp.tobsr()) >= 232\n    assert sizeof(sp.tocoo()) >= 240\n    assert sizeof(sp.tocsc()) >= 232\n    assert sizeof(sp.tocsr()) >= 232\n    assert sizeof(sp.todok()) >= 184\n    assert sizeof(sp.tolil()) >= 204\n\n\n@requires_pandas\n@pytest.mark.parametrize(\"cls_name\", [\"Series\", \"DataFrame\", \"Index\"])\n@pytest.mark.parametrize(\"dtype\", [object, \"string[python]\"])\ndef test_pandas_object_dtype(dtype, cls_name):\n    cls = getattr(pd, cls_name)\n    s1 = cls([f\"x{i:3d}\" for i in range(1000)], dtype=dtype)\n    assert sizeof(\"x000\") * 1000 < sizeof(s1) < 2 * sizeof(\"x000\") * 1000\n\n    x = \"x\" * 100_000\n    y = \"y\" * 100_000\n    z = \"z\" * 100_000\n    w = \"w\" * 100_000\n\n    # High duplication of references to the same object\n    s2 = cls([x, y, z, w] * 1000, dtype=dtype)\n    assert 400_000 < sizeof(s2) < 500_000\n\n    # Low duplication of references to the same object\n    s3 = cls([x, y, z, w], dtype=dtype)\n    s4 = cls([x, y, z, x], dtype=dtype)\n    s5 = cls([x, x, x, x], dtype=dtype)\n    assert sizeof(s5) < sizeof(s4) < sizeof(s3)\n\n\n@requires_pandas\n@pytest.mark.parametrize(\"dtype\", [object, \"string[python]\"])\ndef test_dataframe_object_dtype(dtype):\n    x = \"x\" * 100_000\n    y = \"y\" * 100_000\n    z = \"z\" * 100_000\n    w = \"w\" * 100_000\n\n    # High duplication of references to the same object, across different columns\n    objs = [x, y, z, w]\n    df1 = pd.DataFrame([objs * 3] * 1000, dtype=dtype)\n    assert 400_000 < sizeof(df1) < 550_000\n\n    # Low duplication of references to the same object, across different columns\n    df2 = pd.DataFrame([[x, y], [z, w]], dtype=dtype)\n    df3 = pd.DataFrame([[x, y], [z, x]], dtype=dtype)\n    df4 = pd.DataFrame([[x, x], [x, x]], dtype=dtype)\n    assert sizeof(df4) < sizeof(df3) < sizeof(df2)\n\n\n@pytest.mark.parametrize(\"cls_name\", [\"Series\", \"DataFrame\", \"Index\"])\ndef test_pandas_string_arrow_dtype(cls_name):\n    pytest.importorskip(\"pyarrow\")\n    cls = getattr(pd, cls_name)\n\n    s = cls([\"x\" * 100_000, \"y\" * 50_000], dtype=\"string[pyarrow]\")\n    assert 150_000 < sizeof(s) < 155_000\n\n\n@requires_pandas\ndef test_pandas_empty():\n    df = pd.DataFrame(\n        {\"x\": [1, 2, 3], \"y\": [\"a\" * 100, \"b\" * 100, \"c\" * 100]}, index=[10, 20, 30]\n    )\n    empty = df.head(0)\n\n    assert sizeof(empty) > 0\n    assert sizeof(empty.x) > 0\n    assert sizeof(empty.y) > 0\n    assert sizeof(empty.index) > 0\n\n\n@requires_pandas\ndef test_pyarrow_table():\n    pa = pytest.importorskip(\"pyarrow\")\n    df = pd.DataFrame(\n        {\"x\": [1, 2, 3], \"y\": [\"a\" * 100, \"b\" * 100, \"c\" * 100]}, index=[10, 20, 30]\n    )\n    table = pa.Table.from_pandas(df)\n\n    assert sizeof(table) > sizeof(table.schema.metadata)\n    assert isinstance(sizeof(table), int)\n    assert isinstance(sizeof(table.columns[0]), int)\n    assert isinstance(sizeof(table.columns[1]), int)\n    assert isinstance(sizeof(table.columns[2]), int)\n\n    empty = pa.Table.from_pandas(df.head(0))\n\n    assert sizeof(empty) > sizeof(empty.schema.metadata)\n    assert sizeof(empty.columns[0]) > 0\n    assert sizeof(empty.columns[1]) > 0\n    assert sizeof(empty.columns[2]) > 0\n\n\ndef test_dict():\n    np = pytest.importorskip(\"numpy\")\n    x = np.ones(10000)\n    assert sizeof({\"x\": x}) > x.nbytes\n    assert sizeof({\"x\": [x]}) > x.nbytes\n    assert sizeof({\"x\": [{\"y\": x}]}) > x.nbytes\n\n    d = {i: x for i in range(100)}\n    assert sizeof(d) > x.nbytes * 100\n    assert isinstance(sizeof(d), int)\n\n\ndef _get_sizeof_on_path(path, size):\n    sys.path.append(os.fsdecode(path))\n\n    # Dask will have already called _register_entry_point_plugins\n    # before we can modify sys.path, so we re-register here.\n    import dask.sizeof\n\n    dask.sizeof._register_entry_point_plugins()\n\n    import class_impl\n\n    cls = class_impl.Impl(size)\n    return sizeof(cls)\n\n\ndef test_register_backend_entrypoint(tmp_path):\n    # Create special sizeof implementation for a dummy class\n    (tmp_path / \"impl_sizeof.py\").write_bytes(\n        b\"def sizeof_plugin(sizeof):\\n\"\n        b'    print(\"REG\")\\n'\n        b'    @sizeof.register_lazy(\"class_impl\")\\n'\n        b\"    def register_impl():\\n\"\n        b\"        import class_impl\\n\"\n        b\"        @sizeof.register(class_impl.Impl)\\n\"\n        b\"        def sizeof_impl(obj):\\n\"\n        b\"            return obj.size \\n\"\n    )\n    # Define dummy class that possesses a size attribute\n    (tmp_path / \"class_impl.py\").write_bytes(\n        b\"class Impl:\\n    def __init__(self, size):\\n        self.size = size\"\n    )\n    dist_info = tmp_path / \"impl_sizeof-0.0.0.dist-info\"\n    dist_info.mkdir()\n    (dist_info / \"entry_points.txt\").write_bytes(\n        b\"[dask.sizeof]\\nimpl = impl_sizeof:sizeof_plugin\\n\"\n    )\n\n    with get_context().Pool(1) as pool:\n        assert (\n            pool.apply(_get_sizeof_on_path, args=(tmp_path, 3_14159265)) == 3_14159265\n        )\n    pool.join()\n\n\ndef test_xarray():\n    xr = pytest.importorskip(\"xarray\")\n    np = pytest.importorskip(\"numpy\")\n\n    ind = np.arange(-66, 67, 1).astype(float)\n    arr = np.random.random((len(ind),))\n\n    dataset = (\n        xr.DataArray(\n            arr,\n            dims=[\"coord\"],\n            coords={\"coord\": ind},\n        )\n        .rename(\"foo\")\n        .to_dataset()\n    )\n    assert sizeof(dataset) > sizeof(arr)\n    assert sizeof(dataset.foo) >= sizeof(arr)\n    assert sizeof(dataset[\"coord\"]) >= sizeof(ind)\n    assert sizeof(dataset.indexes) >= sizeof(ind)\n\n\ndef test_xarray_not_in_memory():\n    xr = pytest.importorskip(\"xarray\")\n    zarr = pytest.importorskip(\"zarr\")\n    np = pytest.importorskip(\"numpy\")\n    pytest.importorskip(\"zarr\")\n\n    ind = np.arange(-66, 67, 1).astype(float)\n    arr = np.random.random((len(ind),))\n\n    # TODO: remove this conditional after consolidated metadata lands in v3\n    if Version(zarr.__version__) > Version(\"3.0.0.a0\") and Version(\n        zarr.__version__\n    ) < Version(\"3.0.0\"):\n        pytest.xfail(\"consolidated metadata and xarray support is not complete\")\n\n    with tmpdir() as path:\n        xr.DataArray(\n            arr,\n            dims=[\"coord\"],\n            coords={\"coord\": ind},\n        ).rename(\n            \"foo\"\n        ).to_dataset().to_zarr(path)\n        dataset = xr.open_zarr(path, chunks={\"foo\": 10})\n        assert not dataset.foo._in_memory\n        assert sizeof(ind) < sizeof(dataset) < sizeof(arr) + sizeof(ind)\n        assert sizeof(dataset.foo) < sizeof(arr)\n        assert sizeof(dataset[\"coord\"]) >= sizeof(ind)\n        assert sizeof(dataset.indexes) >= sizeof(ind)\n        assert not dataset.foo._in_memory\n\n        dataset.load()\n        assert dataset.foo._in_memory\n        assert sizeof(dataset) >= sizeof(arr) + sizeof(ind)\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_system\\test_system.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\nfrom __future__ import annotations\n\nimport math\nimport os\nimport sys\n\ntry:\n    import psutil\nexcept ImportError:\n    psutil = None  # type: ignore\n\n__all__ = (\"cpu_count\", \"CPU_COUNT\")\n\n\ndef _try_extract_cgroup_cpu_quota():\n    # cgroup v1\n    # The directory name isn't standardized across linux distros, check both\n    for dirname in [\"cpuacct,cpu\", \"cpu,cpuacct\"]:\n        try:\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_quota_us\" % dirname) as f:\n                quota = int(f.read())\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_period_us\" % dirname) as f:\n                period = int(f.read())\n            return quota, period\n        except Exception:\n            pass\n\n    # cgroup v2\n    try:\n        with open(\"/proc/self/cgroup\") as f:\n            group_path = f.read().strip().split(\":\")[-1]\n        if not group_path.endswith(\"/\"):\n            group_path = f\"{group_path}/\"\n        with open(\"/sys/fs/cgroup%scpu.max\" % group_path) as f:\n            quota, period = map(int, f.read().split(\" \"))\n            return quota, period\n    except Exception:\n        pass\n\n    # No cgroup CPU quota found\n    return None, None\n\n\ndef cpu_count():\n    \"\"\"Get the available CPU count for this system.\n\n    Takes the minimum value from the following locations:\n\n    - Total system cpus available on the host.\n    - CPU Affinity (if set)\n    - Cgroups limit (if set)\n    \"\"\"\n    count = os.cpu_count()\n\n    # Check CPU affinity if available\n    if psutil is not None:\n        try:\n            affinity_count = len(psutil.Process().cpu_affinity())\n            if affinity_count > 0:\n                count = min(count, affinity_count)\n        except Exception:\n            pass\n\n    # Check cgroups if available\n    if sys.platform == \"linux\":\n        quota, period = _try_extract_cgroup_cpu_quota()\n        if quota is not None and period is not None:\n            # We round up on fractional CPUs\n            cgroups_count = math.ceil(quota / period)\n            if cgroups_count > 0:\n                count = min(count, cgroups_count)\n\n    return count\n\n\nCPU_COUNT = cpu_count()\n\n\n### Source File Content:\nfrom __future__ import annotations\n\nimport math\nimport os\nimport sys\n\ntry:\n    import psutil\nexcept ImportError:\n    psutil = None  # type: ignore\n\n__all__ = (\"cpu_count\", \"CPU_COUNT\")\n\n\ndef _try_extract_cgroup_cpu_quota():\n    # cgroup v1\n    # The directory name isn't standardized across linux distros, check both\n    for dirname in [\"cpuacct,cpu\", \"cpu,cpuacct\"]:\n        try:\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_quota_us\" % dirname) as f:\n                quota = int(f.read())\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_period_us\" % dirname) as f:\n                period = int(f.read())\n            return quota, period\n        except Exception:\n            pass\n\n    # cgroup v2\n    try:\n        with open(\"/proc/self/cgroup\") as f:\n            group_path = f.read().strip().split(\":\")[-1]\n        if not group_path.endswith(\"/\"):\n            group_path = f\"{group_path}/\"\n        with open(\"/sys/fs/cgroup%scpu.max\" % group_path) as f:\n            quota, period = map(int, f.read().split(\" \"))\n            return quota, period\n    except Exception:\n        pass\n\n    # No cgroup CPU quota found\n    return None, None\n\n\ndef cpu_count():\n    \"\"\"Get the available CPU count for this system.\n\n    Takes the minimum value from the following locations:\n\n    - Total system cpus available on the host.\n    - CPU Affinity (if set)\n    - Cgroups limit (if set)\n    \"\"\"\n    count = os.cpu_count()\n\n    # Check CPU affinity if available\n    if psutil is not None:\n        try:\n            affinity_count = len(psutil.Process().cpu_affinity())\n            if affinity_count > 0:\n                count = min(count, affinity_count)\n        except Exception:\n            pass\n\n    # Check cgroups if available\n    if sys.platform == \"linux\":\n        quota, period = _try_extract_cgroup_cpu_quota()\n        if quota is not None and period is not None:\n            # We round up on fractional CPUs\n            cgroups_count = math.ceil(quota / period)\n            if cgroups_count > 0:\n                count = min(count, cgroups_count)\n\n    return count\n\n\nCPU_COUNT = cpu_count()\n\n\n### Source File Content:\nfrom __future__ import annotations\n\nimport math\nimport os\nimport sys\n\ntry:\n    import psutil\nexcept ImportError:\n    psutil = None  # type: ignore\n\n__all__ = (\"cpu_count\", \"CPU_COUNT\")\n\n\ndef _try_extract_cgroup_cpu_quota():\n    # cgroup v1\n    # The directory name isn't standardized across linux distros, check both\n    for dirname in [\"cpuacct,cpu\", \"cpu,cpuacct\"]:\n        try:\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_quota_us\" % dirname) as f:\n                quota = int(f.read())\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_period_us\" % dirname) as f:\n                period = int(f.read())\n            return quota, period\n        except Exception:\n            pass\n\n    # cgroup v2\n    try:\n        with open(\"/proc/self/cgroup\") as f:\n            group_path = f.read().strip().split(\":\")[-1]\n        if not group_path.endswith(\"/\"):\n            group_path = f\"{group_path}/\"\n        with open(\"/sys/fs/cgroup%scpu.max\" % group_path) as f:\n            quota, period = map(int, f.read().split(\" \"))\n            return quota, period\n    except Exception:\n        pass\n\n    # No cgroup CPU quota found\n    return None, None\n\n\ndef cpu_count():\n    \"\"\"Get the available CPU count for this system.\n\n    Takes the minimum value from the following locations:\n\n    - Total system cpus available on the host.\n    - CPU Affinity (if set)\n    - Cgroups limit (if set)\n    \"\"\"\n    count = os.cpu_count()\n\n    # Check CPU affinity if available\n    if psutil is not None:\n        try:\n            affinity_count = len(psutil.Process().cpu_affinity())\n            if affinity_count > 0:\n                count = min(count, affinity_count)\n        except Exception:\n            pass\n\n    # Check cgroups if available\n    if sys.platform == \"linux\":\n        quota, period = _try_extract_cgroup_cpu_quota()\n        if quota is not None and period is not None:\n            # We round up on fractional CPUs\n            cgroups_count = math.ceil(quota / period)\n            if cgroups_count > 0:\n                count = min(count, cgroups_count)\n\n    return count\n\n\nCPU_COUNT = cpu_count()\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:31:08"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport builtins\nimport io\nimport os\nimport sys\n\nimport pytest\n\nfrom dask.system import cpu_count\n\npsutil = pytest.importorskip(\"psutil\")\n\n\ndef test_cpu_count():\n    count = cpu_count()\n    assert isinstance(count, int)\n    assert count <= os.cpu_count()\n    assert count >= 1\n\n\n@pytest.mark.parametrize(\"dirname\", [\"cpuacct,cpu\", \"cpu,cpuacct\", None])\ndef test_cpu_count_cgroups(dirname, monkeypatch):\n    def mycpu_count():\n        # Absurdly high, unlikely to match real value\n        return 250\n\n    monkeypatch.setattr(os, \"cpu_count\", mycpu_count)\n\n    class MyProcess:\n        def cpu_affinity(self):\n            # No affinity set\n            return []\n\n    monkeypatch.setattr(psutil, \"Process\", MyProcess)\n\n    if dirname:\n        paths = {\n            \"/sys/fs/cgroup/%s/cpu.cfs_quota_us\" % dirname: io.StringIO(\"2005\"),\n            \"/sys/fs/cgroup/%s/cpu.cfs_period_us\" % dirname: io.StringIO(\"10\"),\n        }\n        builtin_open = builtins.open\n\n        def myopen(path, *args, **kwargs):\n            if path in paths:\n                return paths.get(path)\n            return builtin_open(path, *args, **kwargs)\n\n        monkeypatch.setattr(builtins, \"open\", myopen)\n        monkeypatch.setattr(sys, \"platform\", \"linux\")\n\n    count = cpu_count()\n    if dirname:\n        # Rounds up\n        assert count == 201\n    else:\n        assert count == 250\n\n\n@pytest.mark.parametrize(\"group_name\", [\"/\", \"/user.slice\", \"/user.slice/more.slice\"])\n@pytest.mark.parametrize(\"quota\", [\"max\", \"2005\"])\ndef test_cpu_count_cgroups_v2(quota, group_name, monkeypatch):\n    def mycpu_count():\n        # Absurdly high, unlikely to match real value\n        return 250\n\n    monkeypatch.setattr(os, \"cpu_count\", mycpu_count)\n\n    class MyProcess:\n        def cpu_affinity(self):\n            # No affinity set\n            return []\n\n    monkeypatch.setattr(psutil, \"Process\", MyProcess)\n\n    if not group_name.endswith(\"/\"):\n        group_name = f\"{group_name}/\"\n\n    paths = {\n        \"/proc/self/cgroup\": io.StringIO(\"0::%s\" % group_name),\n        \"/sys/fs/cgroup%scpu.max\" % group_name: io.StringIO(\"%s 10\" % quota),\n    }\n    builtin_open = builtins.open\n\n    def myopen(path, *args, **kwargs):\n        if path in paths:\n            return paths.get(path)\n        return builtin_open(path, *args, **kwargs)\n\n    monkeypatch.setattr(builtins, \"open\", myopen)\n    monkeypatch.setattr(sys, \"platform\", \"linux\")\n\n    count = cpu_count()\n    if quota == \"max\":\n        assert count == 250\n    else:\n        # Rounds up\n        assert count == 201\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: Dask\nTest File Path: Dask\\test_threaded\\test_threaded.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File Content:\n\"\"\"\nA threaded shared-memory scheduler\n\nSee local.py\n\"\"\"\n\nfrom __future__ import annotations\n\nimport atexit\nimport multiprocessing.pool\nimport sys\nimport threading\nfrom collections import defaultdict\nfrom collections.abc import Mapping, Sequence\nfrom concurrent.futures import Executor, ThreadPoolExecutor\nfrom threading import Lock, current_thread\n\nfrom dask import config\nfrom dask.local import MultiprocessingPoolExecutor, get_async\nfrom dask.system import CPU_COUNT\nfrom dask.typing import Key\n\n\ndef _thread_get_id():\n    return current_thread().ident\n\n\nmain_thread = current_thread()\ndefault_pool: Executor | None = None\npools: defaultdict[threading.Thread, dict[int, Executor]] = defaultdict(dict)\npools_lock = Lock()\n\n\ndef pack_exception(e, dumps):\n    return e, sys.exc_info()[2]\n\n\ndef get(\n    dsk: Mapping,\n    keys: Sequence[Key] | Key,\n    cache=None,\n    num_workers=None,\n    pool=None,\n    **kwargs,\n):\n    \"\"\"Threaded cached implementation of dask.get\n\n    Parameters\n    ----------\n\n    dsk: dict\n        A dask dictionary specifying a workflow\n    keys: key or list of keys\n        Keys corresponding to desired data\n    num_workers: integer of thread count\n        The number of threads to use in the ThreadPool that will actually execute tasks\n    cache: dict-like (optional)\n        Temporary storage of results\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}\n    >>> get(dsk, 'w')\n    4\n    >>> get(dsk, ['w', 'y'])\n    (4, 2)\n    \"\"\"\n    global default_pool\n    pool = pool or config.get(\"pool\", None)\n    num_workers = num_workers or config.get(\"num_workers\", None)\n    thread = current_thread()\n\n    with pools_lock:\n        if pool is None:\n            if num_workers is None and thread is main_thread:\n                if default_pool is None:\n                    default_pool = ThreadPoolExecutor(CPU_COUNT)\n                    atexit.register(default_pool.shutdown)\n                pool = default_pool\n            elif thread in pools and num_workers in pools[thread]:\n                pool = pools[thread][num_workers]\n            else:\n                pool = ThreadPoolExecutor(num_workers)\n                atexit.register(pool.shutdown)\n                pools[thread][num_workers] = pool\n        elif isinstance(pool, multiprocessing.pool.Pool):\n            pool = MultiprocessingPoolExecutor(pool)\n\n    results = get_async(\n        pool.submit,\n        pool._max_workers,\n        dsk,\n        keys,\n        cache=cache,\n        get_id=_thread_get_id,\n        pack_exception=pack_exception,\n        **kwargs,\n    )\n\n    # Cleanup pools associated to dead threads\n    with pools_lock:\n        active_threads = set(threading.enumerate())\n        if thread is not main_thread:\n            for t in list(pools):\n                if t not in active_threads:\n                    for p in pools.pop(t).values():\n                        p.shutdown()\n\n    return results\n\n### Source File Dependency Files Content\n### Dependency File: empty.txt\n\nOutput the complete test file, code only, no explanations.\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:31:08"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport signal\nimport sys\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\nfrom multiprocessing.pool import ThreadPool\nfrom time import sleep, time\n\nimport pytest\n\nimport dask\nfrom dask.system import CPU_COUNT\nfrom dask.threaded import get\nfrom dask.utils_test import add, inc\n\n\ndef test_get():\n    dsk = {\"x\": 1, \"y\": 2, \"z\": (inc, \"x\"), \"w\": (add, \"z\", \"y\")}\n    assert get(dsk, \"w\") == 4\n    assert get(dsk, [\"w\", \"z\"]) == (4, 2)\n\n\ndef test_nested_get():\n    dsk = {\"x\": 1, \"y\": 2, \"a\": (add, \"x\", \"y\"), \"b\": (sum, [\"x\", \"y\"])}\n    assert get(dsk, [\"a\", \"b\"]) == (3, 3)\n\n\ndef test_get_without_computation():\n    dsk = {\"x\": 1}\n    assert get(dsk, \"x\") == 1\n\n\ndef test_broken_callback():\n    from dask.callbacks import Callback\n\n    def _f_ok(*args, **kwargs):\n        pass\n\n    def _f_broken(*args, **kwargs):\n        raise ValueError(\"my_exception\")\n\n    dsk = {\"x\": 1}\n\n    with Callback(start=_f_broken, finish=_f_ok):\n        with Callback(start=_f_ok, finish=_f_ok):\n            with pytest.raises(ValueError, match=\"my_exception\"):\n                get(dsk, \"x\")\n\n\ndef bad(x):\n    raise ValueError()\n\n\ndef test_exceptions_rise_to_top():\n    dsk = {\"x\": 1, \"y\": (bad, \"x\")}\n    pytest.raises(ValueError, lambda: get(dsk, \"y\"))\n\n\n@pytest.mark.parametrize(\"pool_typ\", [ThreadPool, ThreadPoolExecutor])\ndef test_reuse_pool(pool_typ):\n    with pool_typ(CPU_COUNT) as pool:\n        with dask.config.set(pool=pool):\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n\n\n@pytest.mark.parametrize(\"pool_typ\", [ThreadPool, ThreadPoolExecutor])\ndef test_pool_kwarg(pool_typ):\n    def f():\n        sleep(0.01)\n        return threading.get_ident()\n\n    dsk = {(\"x\", i): (f,) for i in range(30)}\n    dsk[\"x\"] = (len, (set, [(\"x\", i) for i in range(len(dsk))]))\n\n    with pool_typ(3) as pool:\n        assert get(dsk, \"x\", pool=pool) == 3\n\n\ndef test_threaded_within_thread():\n    L = []\n\n    def f(i):\n        result = get({\"x\": (lambda: i,)}, \"x\", num_workers=2)\n        L.append(result)\n\n    before = threading.active_count()\n\n    for _ in range(20):\n        t = threading.Thread(target=f, args=(1,))\n        t.daemon = True\n        t.start()\n        t.join()\n        assert L == [1]\n        del L[:]\n\n    start = time()  # wait for most threads to join\n    while threading.active_count() > before + 10:\n        sleep(0.01)\n        assert time() < start + 5\n\n\ndef test_dont_spawn_too_many_threads():\n    before = threading.active_count()\n\n    dsk = {(\"x\", i): (lambda i=i: i,) for i in range(10)}\n    dsk[\"x\"] = (sum, list(dsk))\n    for _ in range(20):\n        get(dsk, \"x\", num_workers=4)\n\n    after = threading.active_count()\n\n    assert after <= before + 8\n\n\ndef test_dont_spawn_too_many_threads_CPU_COUNT():\n    before = threading.active_count()\n\n    dsk = {(\"x\", i): (lambda i=i: i,) for i in range(10)}\n    dsk[\"x\"] = (sum, list(dsk))\n    for _ in range(20):\n        get(dsk, \"x\")\n\n    after = threading.active_count()\n\n    assert after <= before + CPU_COUNT * 2\n\n\ndef test_thread_safety():\n    def f(x):\n        return 1\n\n    dsk = {\"x\": (sleep, 0.05), \"y\": (f, \"x\")}\n\n    L = []\n\n    def test_f():\n        L.append(get(dsk, \"y\"))\n\n    threads = []\n    for _ in range(20):\n        t = threading.Thread(target=test_f)\n        t.daemon = True\n        t.start()\n        threads.append(t)\n\n    for thread in threads:\n        thread.join()\n\n    assert L == [1] * 20\n\n\ndef test_interrupt():\n    # Windows implements `queue.get` using polling,\n    # which means we can set an exception to interrupt the call to `get`.\n    # Python 3 on other platforms requires sending SIGINT to the main thread.\n    if sys.platform == \"win32\":\n        from _thread import interrupt_main\n    else:\n        main_thread = threading.get_ident()\n\n        def interrupt_main() -> None:\n            signal.pthread_kill(main_thread, signal.SIGINT)\n\n    in_clog_event = threading.Event()\n    clog_event = threading.Event()\n\n    def clog(in_clog_event: threading.Event, clog_event: threading.Event) -> None:\n        in_clog_event.set()\n        clog_event.wait()\n\n    def interrupt(in_clog_event: threading.Event) -> None:\n        in_clog_event.wait()\n        interrupt_main()\n\n    dsk = {(\"x\", i): (clog, in_clog_event, clog_event) for i in range(20)}\n    dsk[\"x\"] = (len, list(dsk.keys()))\n\n    interrupter = threading.Thread(target=interrupt, args=(in_clog_event,))\n    interrupter.start()\n\n    # Use explicitly created ThreadPoolExecutor to avoid leaking threads after\n    # the KeyboardInterrupt\n    with ThreadPoolExecutor(CPU_COUNT) as pool:\n        with pytest.raises(KeyboardInterrupt):\n            get(dsk, \"x\", pool=pool)\n        clog_event.set()\n    interrupter.join()\n\n```"}]}
