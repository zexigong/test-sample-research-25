{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code file of the repo. "}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nMake sure the tests can be executed without lint, or compile errors.\nRepository: Dask\nSource File Path: ['dask/system.py']\nTest File Path: dask/tests/test_system.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File: dask/system.py\nfrom __future__ import annotations\n\nimport math\nimport os\nimport sys\n\ntry:\n    import psutil\nexcept ImportError:\n    psutil = None  # type: ignore\n\n__all__ = (\"cpu_count\", \"CPU_COUNT\")\n\n\ndef _try_extract_cgroup_cpu_quota():\n    # cgroup v1\n    # The directory name isn't standardized across linux distros, check both\n    for dirname in [\"cpuacct,cpu\", \"cpu,cpuacct\"]:\n        try:\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_quota_us\" % dirname) as f:\n                quota = int(f.read())\n            with open(\"/sys/fs/cgroup/%s/cpu.cfs_period_us\" % dirname) as f:\n                period = int(f.read())\n            return quota, period\n        except Exception:\n            pass\n\n    # cgroup v2\n    try:\n        with open(\"/proc/self/cgroup\") as f:\n            group_path = f.read().strip().split(\":\")[-1]\n        if not group_path.endswith(\"/\"):\n            group_path = f\"{group_path}/\"\n        with open(\"/sys/fs/cgroup%scpu.max\" % group_path) as f:\n            quota, period = map(int, f.read().split(\" \"))\n            return quota, period\n    except Exception:\n        pass\n\n    # No cgroup CPU quota found\n    return None, None\n\n\ndef cpu_count():\n    \"\"\"Get the available CPU count for this system.\n\n    Takes the minimum value from the following locations:\n\n    - Total system cpus available on the host.\n    - CPU Affinity (if set)\n    - Cgroups limit (if set)\n    \"\"\"\n    count = os.cpu_count()\n\n    # Check CPU affinity if available\n    if psutil is not None:\n        try:\n            affinity_count = len(psutil.Process().cpu_affinity())\n            if affinity_count > 0:\n                count = min(count, affinity_count)\n        except Exception:\n            pass\n\n    # Check cgroups if available\n    if sys.platform == \"linux\":\n        quota, period = _try_extract_cgroup_cpu_quota()\n        if quota is not None and period is not None:\n            # We round up on fractional CPUs\n            cgroups_count = math.ceil(quota / period)\n            if cgroups_count > 0:\n                count = min(count, cgroups_count)\n\n    return count\n\n\nCPU_COUNT = cpu_count()\n\n### Source File Dependency Files Content\n### Dependency File: dask/system.py\n\nOutput the complete test file, code only, no explanations. For example:\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 20:59:23"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport builtins\nimport io\nimport os\nimport sys\n\nimport pytest\n\nfrom dask.system import cpu_count\n\npsutil = pytest.importorskip(\"psutil\")\n\n\ndef test_cpu_count():\n    count = cpu_count()\n    assert isinstance(count, int)\n    assert count <= os.cpu_count()\n    assert count >= 1\n\n\n@pytest.mark.parametrize(\"dirname\", [\"cpuacct,cpu\", \"cpu,cpuacct\", None])\ndef test_cpu_count_cgroups(dirname, monkeypatch):\n    def mycpu_count():\n        # Absurdly high, unlikely to match real value\n        return 250\n\n    monkeypatch.setattr(os, \"cpu_count\", mycpu_count)\n\n    class MyProcess:\n        def cpu_affinity(self):\n            # No affinity set\n            return []\n\n    monkeypatch.setattr(psutil, \"Process\", MyProcess)\n\n    if dirname:\n        paths = {\n            \"/sys/fs/cgroup/%s/cpu.cfs_quota_us\" % dirname: io.StringIO(\"2005\"),\n            \"/sys/fs/cgroup/%s/cpu.cfs_period_us\" % dirname: io.StringIO(\"10\"),\n        }\n        builtin_open = builtins.open\n\n        def myopen(path, *args, **kwargs):\n            if path in paths:\n                return paths.get(path)\n            return builtin_open(path, *args, **kwargs)\n\n        monkeypatch.setattr(builtins, \"open\", myopen)\n        monkeypatch.setattr(sys, \"platform\", \"linux\")\n\n    count = cpu_count()\n    if dirname:\n        # Rounds up\n        assert count == 201\n    else:\n        assert count == 250\n\n\n@pytest.mark.parametrize(\"group_name\", [\"/\", \"/user.slice\", \"/user.slice/more.slice\"])\n@pytest.mark.parametrize(\"quota\", [\"max\", \"2005\"])\ndef test_cpu_count_cgroups_v2(quota, group_name, monkeypatch):\n    def mycpu_count():\n        # Absurdly high, unlikely to match real value\n        return 250\n\n    monkeypatch.setattr(os, \"cpu_count\", mycpu_count)\n\n    class MyProcess:\n        def cpu_affinity(self):\n            # No affinity set\n            return []\n\n    monkeypatch.setattr(psutil, \"Process\", MyProcess)\n\n    if not group_name.endswith(\"/\"):\n        group_name = f\"{group_name}/\"\n\n    paths = {\n        \"/proc/self/cgroup\": io.StringIO(\"0::%s\" % group_name),\n        \"/sys/fs/cgroup%scpu.max\" % group_name: io.StringIO(\"%s 10\" % quota),\n    }\n    builtin_open = builtins.open\n\n    def myopen(path, *args, **kwargs):\n        if path in paths:\n            return paths.get(path)\n        return builtin_open(path, *args, **kwargs)\n\n    monkeypatch.setattr(builtins, \"open\", myopen)\n    monkeypatch.setattr(sys, \"platform\", \"linux\")\n\n    count = cpu_count()\n    if quota == \"max\":\n        assert count == 250\n    else:\n        # Rounds up\n        assert count == 201\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code file of the repo. "}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nMake sure the tests can be executed without lint, or compile errors.\nRepository: Dask\nSource File Path: ['dask/multiprocessing.py']\nTest File Path: dask/tests/test_multiprocessing.py\nProject Programming Language: Python\nTesting Framework: pytest\n### Source File Content\n### Source File: dask/multiprocessing.py\nfrom __future__ import annotations\n\nimport copyreg\nimport multiprocessing\nimport multiprocessing.pool\nimport os\nimport pickle\nimport sys\nimport traceback\nfrom collections.abc import Mapping, Sequence\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nfrom warnings import warn\n\nimport cloudpickle\n\nfrom dask import config\nfrom dask.local import MultiprocessingPoolExecutor, get_async, reraise\nfrom dask.optimization import cull, fuse\nfrom dask.system import CPU_COUNT\nfrom dask.typing import Key\nfrom dask.utils import ensure_dict\n\n\ndef _reduce_method_descriptor(m):\n    return getattr, (m.__objclass__, m.__name__)\n\n\n# type(set.union) is used as a proxy to <class 'method_descriptor'>\ncopyreg.pickle(type(set.union), _reduce_method_descriptor)\n\n_dumps = partial(cloudpickle.dumps, protocol=pickle.HIGHEST_PROTOCOL)\n_loads = cloudpickle.loads\n\n\ndef _process_get_id():\n    return multiprocessing.current_process().ident\n\n\n# -- Remote Exception Handling --\n# By default, tracebacks can't be serialized using pickle. However, the\n# `tblib` library can enable support for this. Since we don't mandate\n# that tblib is installed, we do the following:\n#\n# - If tblib is installed, use it to serialize the traceback and reraise\n#   in the scheduler process\n# - Otherwise, use a ``RemoteException`` class to contain a serialized\n#   version of the formatted traceback, which will then print in the\n#   scheduler process.\n#\n# To enable testing of the ``RemoteException`` class even when tblib is\n# installed, we don't wrap the class in the try block below\nclass RemoteException(Exception):\n    \"\"\"Remote Exception\n\n    Contains the exception and traceback from a remotely run task\n    \"\"\"\n\n    def __init__(self, exception, traceback):\n        self.exception = exception\n        self.traceback = traceback\n\n    def __str__(self):\n        return str(self.exception) + \"\\n\\nTraceback\\n---------\\n\" + self.traceback\n\n    def __dir__(self):\n        return sorted(set(dir(type(self)) + list(self.__dict__) + dir(self.exception)))\n\n    def __getattr__(self, key):\n        try:\n            return object.__getattribute__(self, key)\n        except AttributeError:\n            return getattr(self.exception, key)\n\n\nexceptions: dict[type[Exception], type[Exception]] = {}\n\n\ndef remote_exception(exc: Exception, tb) -> Exception:\n    \"\"\"Metaclass that wraps exception type in RemoteException\"\"\"\n    if type(exc) in exceptions:\n        typ = exceptions[type(exc)]\n        return typ(exc, tb)\n    else:\n        try:\n            typ = type(\n                exc.__class__.__name__,\n                (RemoteException, type(exc)),\n                {\"exception_type\": type(exc)},\n            )\n            exceptions[type(exc)] = typ\n            return typ(exc, tb)\n        except TypeError:\n            return exc\n\n\ntry:\n    import tblib.pickling_support\n\n    tblib.pickling_support.install()\n\n    def _pack_traceback(tb):\n        return tb\n\nexcept ImportError:\n\n    def _pack_traceback(tb):\n        return \"\".join(traceback.format_tb(tb))\n\n    def reraise(exc, tb=None):\n        exc = remote_exception(exc, tb)\n        raise exc\n\n\ndef pack_exception(e, dumps):\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    tb = _pack_traceback(exc_traceback)\n    try:\n        result = dumps((e, tb))\n    except Exception as e:\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        tb = _pack_traceback(exc_traceback)\n        result = dumps((e, tb))\n    return result\n\n\n_CONTEXT_UNSUPPORTED = \"\"\"\\\nThe 'multiprocessing.context' configuration option will be ignored on Python 2\nand on Windows, because they each only support a single context.\n\"\"\"\n\n\ndef get_context():\n    \"\"\"Return the current multiprocessing context.\"\"\"\n    # fork context does fork()-without-exec(), which can lead to deadlocks,\n    # so default to \"spawn\".\n    context_name = config.get(\"multiprocessing.context\", \"spawn\")\n    if sys.platform == \"win32\":\n        if context_name != \"spawn\":\n            # Only spawn is supported on Win32, can't change it:\n            warn(_CONTEXT_UNSUPPORTED, UserWarning)\n        return multiprocessing\n    else:\n        return multiprocessing.get_context(context_name)\n\n\ndef get(\n    dsk: Mapping,\n    keys: Sequence[Key] | Key,\n    num_workers=None,\n    func_loads=None,\n    func_dumps=None,\n    optimize_graph=True,\n    pool=None,\n    initializer=None,\n    chunksize=None,\n    **kwargs,\n):\n    \"\"\"Multiprocessed get function appropriate for Bags\n\n    Parameters\n    ----------\n    dsk : dict\n        dask graph\n    keys : object or list\n        Desired results from graph\n    num_workers : int\n        Number of worker processes (defaults to number of cores)\n    func_dumps : function\n        Function to use for function serialization (defaults to cloudpickle.dumps)\n    func_loads : function\n        Function to use for function deserialization (defaults to cloudpickle.loads)\n    optimize_graph : bool\n        If True [default], `fuse` is applied to the graph before computation.\n    pool : Executor or Pool\n        Some sort of `Executor` or `Pool` to use\n    initializer: function\n        Ignored if ``pool`` has been set.\n        Function to initialize a worker process before running any tasks in it.\n    chunksize: int, optional\n        Size of chunks to use when dispatching work.\n        Defaults to 6 as some batching is helpful.\n        If -1, will be computed to evenly divide ready work across workers.\n    \"\"\"\n    chunksize = chunksize or config.get(\"chunksize\", 6)\n    pool = pool or config.get(\"pool\", None)\n    initializer = initializer or config.get(\"multiprocessing.initializer\", None)\n    num_workers = num_workers or config.get(\"num_workers\", None) or CPU_COUNT\n    if pool is None:\n        # In order to get consistent hashing in subprocesses, we need to set a\n        # consistent seed for the Python hash algorithm. Unfortunately, there\n        # is no way to specify environment variables only for the Pool\n        # processes, so we have to rely on environment variables being\n        # inherited.\n        if os.environ.get(\"PYTHONHASHSEED\") in (None, \"0\"):\n            # This number is arbitrary; it was chosen to commemorate\n            # https://github.com/dask/dask/issues/6640.\n            os.environ[\"PYTHONHASHSEED\"] = \"6640\"\n        context = get_context()\n        initializer = partial(initialize_worker_process, user_initializer=initializer)\n        pool = ProcessPoolExecutor(\n            num_workers, mp_context=context, initializer=initializer\n        )\n        cleanup = True\n    else:\n        if initializer is not None:\n            warn(\n                \"The ``initializer`` argument is ignored when ``pool`` is provided. \"\n                \"The user should configure ``pool`` with the needed ``initializer`` \"\n                \"on creation.\"\n            )\n        if isinstance(pool, multiprocessing.pool.Pool):\n            pool = MultiprocessingPoolExecutor(pool)\n        cleanup = False\n\n    # Optimize Dask\n    dsk = ensure_dict(dsk)\n    dsk2, dependencies = cull(dsk, keys)\n    if optimize_graph:\n        dsk3, dependencies = fuse(dsk2, keys, dependencies)\n    else:\n        dsk3 = dsk2\n\n    # We specify marshalling functions in order to catch serialization\n    # errors and report them to the user.\n    loads = func_loads or config.get(\"func_loads\", None) or _loads\n    dumps = func_dumps or config.get(\"func_dumps\", None) or _dumps\n\n    # Note former versions used a multiprocessing Manager to share\n    # a Queue between parent and workers, but this is fragile on Windows\n    # (issue #1652).\n    try:\n        # Run\n        result = get_async(\n            pool.submit,\n            pool._max_workers,\n            dsk3,\n            keys,\n            get_id=_process_get_id,\n            dumps=dumps,\n            loads=loads,\n            pack_exception=pack_exception,\n            raise_exception=reraise,\n            chunksize=chunksize,\n            **kwargs,\n        )\n    finally:\n        if cleanup:\n            pool.shutdown()\n    return result\n\n\ndef default_initializer():\n    # If Numpy is already imported, presumably its random state was\n    # inherited from the parent => re-seed it.\n    np = sys.modules.get(\"numpy\")\n    if np is not None:\n        np.random.seed()\n\n\ndef initialize_worker_process(user_initializer=None):\n    \"\"\"\n    Initialize a worker process before running any tasks in it.\n    \"\"\"\n    default_initializer()\n\n    if user_initializer is not None:\n        user_initializer()\n\n### Source File Dependency Files Content\n### Dependency File: dask/multiprocessing.py\n\"\"\"\nAsynchronous Shared-Memory Scheduler for Dask Graphs.\n\nThis scheduler coordinates several workers to execute tasks in a dask graph in\nparallel.  It depends on a ``concurrent.futures.Executor``\nand a corresponding Queue for worker-to-scheduler communication.\n\nIt tries to execute tasks in an order which maintains a small memory footprint\nthroughout execution.  It does this by running tasks that allow us to release\ndata resources.\n\n\nTask Selection Policy\n=====================\n\nWhen we complete a task we add more data in to our set of available data; this\nnew data makes new tasks available.  We preferentially choose tasks that were\njust made available in a last-in-first-out fashion.  We implement this as a\nsimple stack.  This results in more depth-first rather than breadth first\nbehavior which encourages us to process batches of data to completion before\nstarting in on new data when possible.\n\nWhen the addition of new data readies multiple tasks simultaneously we add\ntasks to the stack in sorted order so that tasks with greater keynames are run\nfirst.  This can be handy to break ties in a predictable fashion.\n\n\nState\n=====\n\nMany functions pass around a ``state`` variable that holds the current state of\nthe computation.  This variable consists of several other dictionaries and\nsets, explained below.\n\nConstant state\n--------------\n\n1.  dependencies: {x: [a, b ,c]} a,b,c, must be run before x\n2.  dependents: {a: [x, y]} a must run before x or y\n\nChanging state\n--------------\n\n### Data\n\n1.  cache: available concrete data.  {key: actual-data}\n2.  released: data that we've seen, used, and released because it is no longer\n    needed\n\n### Jobs\n\n1.  ready: A fifo stack of ready-to-run tasks\n2.  running: A set of tasks currently in execution\n3.  finished: A set of finished tasks\n4.  waiting: which tasks are still waiting on others :: {key: {keys}}\n    Real-time equivalent of dependencies\n5.  waiting_data: available data to yet-to-be-run-tasks :: {key: {keys}}\n    Real-time equivalent of dependents\n\n\nExamples\n--------\n\n>>> import pprint  # doctest: +SKIP\n>>> inc = lambda x: x + 1\n>>> add = lambda x, y: x + y\n>>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n>>> pprint.pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n{'cache': {'x': 1, 'y': 2},\n 'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n 'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n 'finished': set(),\n 'ready': ['z'],\n 'released': set(),\n 'running': set(),\n 'waiting': {'w': {'z'}},\n 'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n\nOptimizations\n=============\n\nWe build this scheduler with out-of-core array operations in mind.  To this end\nwe have encoded some particular optimizations.\n\nCompute to release data\n-----------------------\n\nWhen we choose a new task to execute we often have many options.  Policies at\nthis stage are cheap and can significantly impact performance.  One could\nimagine policies that expose parallelism, drive towards a particular output,\netc..\n\nOur current policy is to run tasks that were most recently made available.\n\n\nInlining computations\n---------------------\n\nWe hold on to intermediate computations either in memory or on disk.\n\nFor very cheap computations that may emit new copies of the data, like\n``np.transpose`` or possibly even ``x + 1`` we choose not to store these as\nseparate pieces of data / tasks.  Instead we combine them with the computations\nthat require them.  This may result in repeated computation but saves\nsignificantly on space and computation complexity.\n\nSee the function ``inline_functions`` for more information.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections.abc import Mapping, Sequence\nfrom concurrent.futures import Executor, Future\nfrom functools import partial\nfrom queue import Empty, Queue\n\nfrom dask import config\nfrom dask._task_spec import DataNode, DependenciesMapping, convert_legacy_graph\nfrom dask.callbacks import local_callbacks, unpack_callbacks\nfrom dask.core import flatten, get_dependencies, reverse_dict\nfrom dask.order import order\nfrom dask.typing import Key\n\nif os.name == \"nt\":\n    # Python 3 windows Queue.get doesn't handle interrupts properly. To\n    # workaround this we poll at a sufficiently large interval that it\n    # shouldn't affect performance, but small enough that users trying to kill\n    # an application shouldn't care.\n    def queue_get(q):\n        while True:\n            try:\n                return q.get(block=True, timeout=0.1)\n            except Empty:\n                pass\n\nelse:\n\n    def queue_get(q):\n        return q.get()\n\n\ndef start_state_from_dask(dsk, cache=None, sortkey=None):\n    \"\"\"Start state from a dask\n\n    Examples\n    --------\n    >>> inc = lambda x: x + 1\n    >>> add = lambda x, y: x + y\n    >>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}  # doctest: +SKIP\n    >>> from pprint import pprint  # doctest: +SKIP\n    >>> pprint(start_state_from_dask(dsk))  # doctest: +SKIP\n    {'cache': {'x': 1, 'y': 2},\n     'dependencies': {'w': {'z', 'y'}, 'x': set(), 'y': set(), 'z': {'x'}},\n     'dependents': defaultdict(None, {'w': set(), 'x': {'z'}, 'y': {'w'}, 'z': {'w'}}),\n     'finished': set(),\n     'ready': ['z'],\n     'released': set(),\n     'running': set(),\n     'waiting': {'w': {'z'}},\n     'waiting_data': {'x': {'z'}, 'y': {'w'}, 'z': {'w'}}}\n    \"\"\"\n    if sortkey is None:\n        sortkey = order(dsk).get\n    if cache is None:\n        cache = config.get(\"cache\", None)\n    if cache is None:\n        cache = dict()\n\n    dsk = convert_legacy_graph(dsk, all_keys=set(dsk) | set(cache))\n    data_keys = set()\n    for k, v in dsk.items():\n        if isinstance(v, DataNode):\n            cache[k] = v()\n            data_keys.add(k)\n\n    dsk2 = dsk.copy()\n    dsk2.update(cache)\n\n    dependencies = DependenciesMapping(dsk)\n    waiting = {k: set(v) for k, v in dependencies.items() if k not in data_keys}\n\n    dependents = reverse_dict(dependencies)\n    for a in cache:\n        for b in dependents.get(a, ()):\n            waiting[b].remove(a)\n    waiting_data = {k: v.copy() for k, v in dependents.items() if v}\n\n    ready_set = {k for k, v in waiting.items() if not v}\n    ready = sorted(ready_set, key=sortkey, reverse=True)\n    waiting = {k: v for k, v in waiting.items() if v}\n\n    state = {\n        \"dependencies\": dependencies,\n        \"dependents\": dependents,\n        \"waiting\": waiting,\n        \"waiting_data\": waiting_data,\n        \"cache\": cache,\n        \"ready\": ready,\n        \"running\": set(),\n        \"finished\": set(),\n        \"released\": set(),\n    }\n\n    return state\n\n\n\"\"\"\nRunning tasks\n-------------\n\nWhen we execute tasks we both\n\n1.  Perform the actual work of collecting the appropriate data and calling the function\n2.  Manage administrative state to coordinate with the scheduler\n\"\"\"\n\n\ndef execute_task(key, task_info, dumps, loads, get_id, pack_exception):\n    \"\"\"\n    Compute task and handle all administration\n\n    See Also\n    --------\n    _execute_task : actually execute task\n    \"\"\"\n    try:\n        task, data = loads(task_info)\n        result = task(data)\n        id = get_id()\n        result = dumps((result, id))\n        failed = False\n    except BaseException as e:  # noqa: B036\n        result = pack_exception(e, dumps)\n        failed = True\n    return key, result, failed\n\n\ndef batch_execute_tasks(it):\n    \"\"\"\n    Batch computing of multiple tasks with `execute_task`\n    \"\"\"\n    return [execute_task(*a) for a in it]\n\n\ndef release_data(key, state, delete=True):\n    \"\"\"Remove data from temporary storage\n\n    See Also\n    --------\n    finish_task\n    \"\"\"\n    if key in state[\"waiting_data\"]:\n        assert not state[\"waiting_data\"][key]\n        del state[\"waiting_data\"][key]\n\n    state[\"released\"].add(key)\n\n    if delete:\n        del state[\"cache\"][key]\n\n\ndef finish_task(\n    dsk, key, state, results, sortkey, delete=True, release_data=release_data\n):\n    \"\"\"\n    Update execution state after a task finishes\n\n    Mutates.  This should run atomically (with a lock).\n    \"\"\"\n    for dep in sorted(state[\"dependents\"][key], key=sortkey, reverse=True):\n        s = state[\"waiting\"][dep]\n        s.remove(key)\n        if not s:\n            del state[\"waiting\"][dep]\n            state[\"ready\"].append(dep)\n\n    for dep in state[\"dependencies\"][key]:\n        if dep in state[\"waiting_data\"]:\n            s = state[\"waiting_data\"][dep]\n            s.remove(key)\n            if not s and dep not in results:\n                release_data(dep, state, delete=delete)\n        elif delete and dep not in results:\n            release_data(dep, state, delete=delete)\n\n    state[\"finished\"].add(key)\n    state[\"running\"].remove(key)\n\n    return state\n\n\ndef nested_get(ind, coll):\n    \"\"\"Get nested index from collection\n\n    Examples\n    --------\n\n    >>> nested_get(1, 'abc')\n    'b'\n    >>> nested_get([1, 0], 'abc')\n    ('b', 'a')\n    >>> nested_get([[1, 0], [0, 1]], 'abc')\n    (('b', 'a'), ('a', 'b'))\n    \"\"\"\n    if isinstance(ind, list):\n        return tuple(nested_get(i, coll) for i in ind)\n    else:\n        return coll[ind]\n\n\ndef default_get_id():\n    \"\"\"Default get_id\"\"\"\n    return None\n\n\ndef default_pack_exception(e, dumps):\n    raise\n\n\ndef reraise(exc, tb=None):\n    if exc.__traceback__ is not tb:\n        raise exc.with_traceback(tb)\n    raise exc\n\n\ndef identity(x):\n    \"\"\"Identity function. Returns x.\n\n    >>> identity(3)\n    3\n    \"\"\"\n    return x\n\n\n\"\"\"\nTask Selection\n--------------\n\nWe often have a choice among many tasks to run next.  This choice is both\ncheap and can significantly impact performance.\n\nWe currently select tasks that have recently been made ready.  We hope that\nthis first-in-first-out policy reduces memory footprint\n\"\"\"\n\n\"\"\"\n`get`\n-----\n\nThe main function of the scheduler.  Get is the main entry point.\n\"\"\"\n\n\ndef get_async(\n    submit,\n    num_workers,\n    dsk,\n    result,\n    cache=None,\n    get_id=default_get_id,\n    rerun_exceptions_locally=None,\n    pack_exception=default_pack_exception,\n    raise_exception=reraise,\n    callbacks=None,\n    dumps=identity,\n    loads=identity,\n    chunksize=None,\n    **kwargs,\n):\n    \"\"\"Asynchronous get function\n\n    This is a general version of various asynchronous schedulers for dask.  It\n    takes a ``concurrent.futures.Executor.submit`` function to form a more\n    specific ``get`` method that walks through the dask array with parallel\n    workers, avoiding repeat computation and minimizing memory use.\n\n    Parameters\n    ----------\n    submit : function\n        A ``concurrent.futures.Executor.submit`` function\n    num_workers : int\n        The number of workers that task submissions can be spread over\n    dsk : dict\n        A dask dictionary specifying a workflow\n    result : key or list of keys\n        Keys corresponding to desired data\n    cache : dict-like, optional\n        Temporary storage of results\n    get_id : callable, optional\n        Function to return the worker id, takes no arguments. Examples are\n        `threading.current_thread` and `multiprocessing.current_process`.\n    rerun_exceptions_locally : bool, optional\n        Whether to rerun failing tasks in local process to enable debugging\n        (False by default)\n    pack_exception : callable, optional\n        Function to take an exception and ``dumps`` method, and return a\n        serialized tuple of ``(exception, traceback)`` to send back to the\n        scheduler. Default is to just raise the exception.\n    raise_exception : callable, optional\n        Function that takes an exception and a traceback, and raises an error.\n    callbacks : tuple or list of tuples, optional\n        Callbacks are passed in as tuples of length 5. Multiple sets of\n        callbacks may be passed in as a list of tuples. For more information,\n        see the dask.diagnostics documentation.\n    dumps: callable, optional\n        Function to serialize task data and results to communicate between\n        worker and parent.  Defaults to identity.\n    loads: callable, optional\n        Inverse function of `dumps`.  Defaults to identity.\n    chunksize: int, optional\n        Size of chunks to use when dispatching work. Defaults to 1.\n        If -1, will be computed to evenly divide ready work across workers.\n\n    See Also\n    --------\n    threaded.get\n    \"\"\"\n    chunksize = chunksize or config.get(\"chunksize\", 1)\n\n    queue = Queue()\n\n    if isinstance(result, list):\n        result_flat = set(flatten(result))\n    else:\n        result_flat = {result}\n    results = set(result_flat)\n\n    dsk = dict(convert_legacy_graph(dsk))\n    with local_callbacks(callbacks) as callbacks:\n        _, _, pretask_cbs, posttask_cbs, _ = unpack_callbacks(callbacks)\n        started_cbs = []\n        succeeded = False\n        # if start_state_from_dask fails, we will have something\n        # to pass to the final block.\n        state = {}\n        try:\n            for cb in callbacks:\n                if cb[0]:\n                    cb[0](dsk)\n                started_cbs.append(cb)\n\n            keyorder = order(dsk)\n\n            state = start_state_from_dask(dsk, cache=cache, sortkey=keyorder.get)\n\n            for _, start_state, _, _, _ in callbacks:\n                if start_state:\n                    start_state(dsk, state)\n\n            if rerun_exceptions_locally is None:\n                rerun_exceptions_locally = config.get(\"rerun_exceptions_locally\", False)\n\n            if state[\"waiting\"] and not state[\"ready\"]:\n                raise ValueError(\"Found no accessible jobs in dask\")\n\n            def fire_tasks(chunksize):\n                \"\"\"Fire off a task to the thread pool\"\"\"\n                # Determine chunksize and/or number of tasks to submit\n                nready = len(state[\"ready\"])\n                if chunksize == -1:\n                    ntasks = nready\n                    chunksize = -(ntasks // -num_workers)\n                else:\n                    used_workers = -(len(state[\"running\"]) // -chunksize)\n                    avail_workers = max(num_workers - used_workers, 0)\n                    ntasks = min(nready, chunksize * avail_workers)\n\n                # Prep all ready tasks for submission\n                args = []\n                for _ in range(ntasks):\n                    # Get the next task to compute (most recently added)\n                    key = state[\"ready\"].pop()\n                    # Notify task is running\n                    state[\"running\"].add(key)\n                    for f in pretask_cbs:\n                        f(key, dsk, state)\n\n                    # Prep args to send\n                    data = {\n                        dep: state[\"cache\"][dep] for dep in get_dependencies(dsk, key)\n                    }\n                    args.append(\n                        (\n                            key,\n                            dumps((dsk[key], data)),\n                            dumps,\n                            loads,\n                            get_id,\n                            pack_exception,\n                        )\n                    )\n\n                # Batch submit\n                for i in range(-(len(args) // -chunksize)):\n                    each_args = args[i * chunksize : (i + 1) * chunksize]\n                    if not each_args:\n                        break\n                    fut = submit(batch_execute_tasks, each_args)\n                    fut.add_done_callback(queue.put)\n\n            # Main loop, wait on tasks to finish, insert new ones\n            while state[\"waiting\"] or state[\"ready\"] or state[\"running\"]:\n                fire_tasks(chunksize)\n                for key, res_info, failed in queue_get(queue).result():\n                    if failed:\n                        exc, tb = loads(res_info)\n                        if rerun_exceptions_locally:\n                            data = {\n                                dep: state[\"cache\"][dep]\n                                for dep in get_dependencies(dsk, key)\n                            }\n                            task = dsk[key]\n                            task(data)  # Re-execute locally\n                        else:\n                            raise_exception(exc, tb)\n                    res, worker_id = loads(res_info)\n                    state[\"cache\"][key] = res\n                    finish_task(dsk, key, state, results, keyorder.get)\n                    for f in posttask_cbs:\n                        f(key, res, dsk, state, worker_id)\n\n            succeeded = True\n\n        finally:\n            for _, _, _, _, finish in started_cbs:\n                if finish:\n                    finish(dsk, state, not succeeded)\n\n    return nested_get(result, state[\"cache\"])\n\n\n\"\"\" Synchronous concrete version of get_async\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a\nsequential one.  This is useful for debugging and for code dominated by the\nGIL\n\"\"\"\n\n\nclass SynchronousExecutor(Executor):\n    _max_workers = 1\n\n    def submit(self, fn, *args, **kwargs):\n        fut = Future()\n        try:\n            fut.set_result(fn(*args, **kwargs))\n        except BaseException as e:  # noqa: B036\n            fut.set_exception(e)\n        return fut\n\n\nsynchronous_executor = SynchronousExecutor()\n\n\ndef get_sync(dsk: Mapping, keys: Sequence[Key] | Key, **kwargs):\n    \"\"\"A naive synchronous version of get_async\n\n    Can be useful for debugging.\n    \"\"\"\n    kwargs.pop(\"num_workers\", None)  # if num_workers present, remove it\n    return get_async(\n        synchronous_executor.submit,\n        synchronous_executor._max_workers,\n        dsk,\n        keys,\n        **kwargs,\n    )\n\n\n\"\"\" Adaptor for ``multiprocessing.Pool`` instances\n\nUsually we supply a ``concurrent.futures.Executor``.  Here we provide a wrapper\nclass for ``multiprocessing.Pool`` instances so we can treat them like\n``concurrent.futures.Executor`` instances instead.\n\nThis is mainly useful for legacy use cases or users that prefer\n``multiprocessing.Pool``.\n\"\"\"\n\n\nclass MultiprocessingPoolExecutor(Executor):\n    def __init__(self, pool):\n        self.pool = pool\n        self._max_workers = len(pool._pool)\n\n    def submit(self, fn, *args, **kwargs):\n        return submit_apply_async(self.pool.apply_async, fn, *args, **kwargs)\n\n\ndef submit_apply_async(apply_async, fn, *args, **kwargs):\n    fut = Future()\n    apply_async(fn, args, kwargs, fut.set_result, fut.set_exception)\n    return fut\n\n\ndef get_apply_async(apply_async, num_workers, *args, **kwargs):\n    return get_async(\n        partial(submit_apply_async, apply_async), num_workers, *args, **kwargs\n    )\n\n\ndef sortkey(item):\n    \"\"\"Sorting key function that is robust to different types\n\n    Both strings and tuples are common key types in dask graphs.\n    However In Python 3 one can not compare strings with tuples directly.\n    This function maps many types to a form where they can be compared\n\n    Examples\n    --------\n    >>> sortkey('Hello')\n    ('str', 'Hello')\n\n    >>> sortkey(('x', 1))\n    ('tuple', ('x', 1))\n    \"\"\"\n    return (type(item).__name__, item)\n\nOutput the complete test file, code only, no explanations. For example:\n```Python\n<complete test code>\n```\n### Time\nCurrent time: 2025-03-02 21:07:10"}, {"role": "assistant", "content": "```python\nfrom __future__ import annotations\n\nimport multiprocessing\nimport pickle\nimport sys\nfrom concurrent.futures import ProcessPoolExecutor\nfrom operator import add\n\nimport pytest\n\nimport dask\nfrom dask import compute, delayed\nfrom dask.multiprocessing import _dumps, _loads, get, get_context, remote_exception\nfrom dask.system import CPU_COUNT\nfrom dask.utils_test import inc\n\n\ndef unrelated_function_global(a):\n    np = pytest.importorskip(\"numpy\")\n    return np.array([a])\n\n\ndef my_small_function_global(a, b):\n    return a + b\n\n\ndef test_pickle_globals():\n    \"\"\"Unrelated globals should not be included in serialized bytes\"\"\"\n    b = _dumps(my_small_function_global)\n    assert b\"my_small_function_global\" in b\n    assert b\"unrelated_function_global\" not in b\n    assert b\"numpy\" not in b\n\n\ndef test_pickle_locals():\n    \"\"\"Unrelated locals should not be included in serialized bytes\"\"\"\n    np = pytest.importorskip(\"numpy\")\n\n    def unrelated_function_local(a):\n        return np.array([a])\n\n    def my_small_function_local(a, b):\n        return a + b\n\n    b = _dumps(my_small_function_local)\n    assert b\"my_small_function_global\" not in b\n    assert b\"my_small_function_local\" in b\n    assert b\"unrelated_function_local\" not in b\n\n\n@pytest.mark.skipif(pickle.HIGHEST_PROTOCOL < 5, reason=\"requires pickle protocol 5\")\ndef test_out_of_band_pickling():\n    \"\"\"Test that out-of-band pickling works\"\"\"\n    np = pytest.importorskip(\"numpy\")\n    pytest.importorskip(\"cloudpickle\", minversion=\"1.3.0\")\n\n    a = np.arange(5)\n\n    l = []\n    b = _dumps(a, buffer_callback=l.append)\n    assert len(l) == 1\n    assert isinstance(l[0], pickle.PickleBuffer)\n    assert memoryview(l[0]) == memoryview(a)\n\n    a2 = _loads(b, buffers=l)\n    assert np.all(a == a2)\n\n\ndef bad():\n    raise ValueError(\"12345\")\n\n\ndef test_errors_propagate():\n    dsk = {\"x\": (bad,)}\n\n    with pytest.raises(ValueError) as e:\n        get(dsk, \"x\")\n    assert \"12345\" in str(e.value)\n\n\ndef test_remote_exception():\n    e = TypeError(\"hello\")\n    a = remote_exception(e, \"traceback-body\")\n    b = remote_exception(e, \"traceback-body\")\n\n    assert type(a) == type(b)\n    assert isinstance(a, TypeError)\n    assert \"hello\" in str(a)\n    assert \"Traceback\" in str(a)\n    assert \"traceback-body\" in str(a)\n\n\ndef test_lambda_with_cloudpickle():\n    dsk = {\"x\": 2, \"y\": (lambda x: x + 1, \"x\")}\n    assert get(dsk, \"y\") == 3\n\n\ndef lambda_result():\n    return lambda x: x + 1\n\n\ndef test_lambda_results_with_cloudpickle():\n    dsk = {\"x\": (lambda_result,)}\n    f = get(dsk, \"x\")\n    assert f(2) == 3\n\n\nclass NotUnpickleable:\n    def __getstate__(self):\n        return ()\n\n    def __setstate__(self, state):\n        raise ValueError(\"Can't unpickle me\")\n\n\ndef test_unpicklable_args_generate_errors():\n    a = NotUnpickleable()\n\n    dsk = {\"x\": (bool, a)}\n\n    with pytest.raises(ValueError):\n        get(dsk, \"x\")\n\n    dsk = {\"x\": (bool, \"a\"), \"a\": a}\n\n    with pytest.raises(ValueError):\n        get(dsk, \"x\")\n\n\n@pytest.mark.parametrize(\"pool_typ\", [multiprocessing.Pool, ProcessPoolExecutor])\ndef test_reuse_pool(pool_typ):\n    with pool_typ(CPU_COUNT) as pool:\n        with dask.config.set(pool=pool):\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n\n\ndef test_dumps_loads():\n    with dask.config.set(func_dumps=pickle.dumps, func_loads=pickle.loads):\n        assert get({\"x\": 1, \"y\": (add, \"x\", 2)}, \"y\") == 3\n\n\ndef test_fuse_doesnt_clobber_intermediates():\n    d = {\"x\": 1, \"y\": (inc, \"x\"), \"z\": (add, 10, \"y\")}\n    assert get(d, [\"y\", \"z\"]) == (2, 12)\n\n\ndef test_optimize_graph_false():\n    from dask.callbacks import Callback\n\n    d = {\"x\": 1, \"y\": (inc, \"x\"), \"z\": (add, 10, \"y\")}\n    keys = []\n    with Callback(pretask=lambda key, *args: keys.append(key)):\n        get(d, \"z\", optimize_graph=False)\n    assert len(keys) == 2\n\n\ndef test_works_with_highlevel_graph():\n    \"\"\"Previously `dask.multiprocessing.get` would accidentally forward\n    `HighLevelGraph` graphs through the dask optimization/scheduling routines,\n    resulting in odd errors. One way to trigger this was to have a\n    non-indexable object in a task. This is just a smoketest to ensure that\n    things work properly even if `HighLevelGraph` objects get passed to\n    `dask.multiprocessing.get`. See https://github.com/dask/dask/issues/7190.\n    \"\"\"\n\n    class NoIndex:\n        def __init__(self, x):\n            self.x = x\n\n        def __getitem__(self, key):\n            raise Exception(\"Oh no!\")\n\n    x = delayed(lambda x: x)(NoIndex(1))\n    (res,) = get(x.dask, x.__dask_keys__())\n    assert isinstance(res, NoIndex)\n    assert res.x == 1\n\n\n@pytest.mark.parametrize(\"random\", [\"numpy\", \"random\"])\ndef test_random_seeds(random):\n    if random == \"numpy\":\n        np = pytest.importorskip(\"numpy\")\n        random = np.random\n    else:\n        import random\n\n    @delayed(pure=False)\n    def f():\n        return tuple(random.randint(0, 10000) for i in range(5))\n\n    N = 10\n    with dask.config.set(scheduler=\"processes\"):\n        (results,) = compute([f() for _ in range(N)])\n\n    assert len(set(results)) == N\n\n\nclass global_:\n    value = 0\n\n\ndef proc_init():\n    global_.value = 1\n\n\n@pytest.mark.parametrize(\n    \"scheduler, initializer, expected_results\",\n    [\n        (\"threading\", None, [1] * 10),\n        (\"processes\", None, [0] * 10),\n        (\"processes\", proc_init, [1] * 10),\n    ],\n)\ndef test_process_initializer(scheduler, initializer, expected_results):\n    @delayed(pure=False)\n    def f():\n        return global_.value\n\n    global_.value = 1\n\n    with dask.config.set(\n        {\"scheduler\": scheduler, \"multiprocessing.initializer\": initializer}\n    ):\n        (results,) = compute([f() for _ in range(10)])\n    assert results == expected_results\n\n    (results2,) = compute(\n        [f() for _ in range(10)],\n        scheduler=scheduler,\n        initializer=initializer,\n    )\n    assert results2 == expected_results\n\n\ndef check_for_pytest():\n    \"\"\"We check for spawn by ensuring subprocess doesn't have modules only\n    parent process should have:\n    \"\"\"\n    import sys\n\n    return \"FAKE_MODULE_FOR_TEST\" in sys.modules\n\n\n@pytest.mark.skipif(\n    sys.platform == \"win32\", reason=\"Windows doesn't support different contexts\"\n)\ndef test_custom_context_used_python3_posix():\n    \"\"\"The 'multiprocessing.context' config is used to create the pool.\n\n    We assume default is 'spawn', and therefore test for 'fork'.\n    \"\"\"\n    # We check for 'fork' by ensuring subprocess doesn't have modules only\n    # parent process should have:\n\n    def check_for_pytest():\n        import sys\n\n        return \"FAKE_MODULE_FOR_TEST\" in sys.modules\n\n    import sys\n\n    sys.modules[\"FAKE_MODULE_FOR_TEST\"] = 1\n    try:\n        with dask.config.set({\"multiprocessing.context\": \"fork\"}):\n            result = get({\"x\": (check_for_pytest,)}, \"x\")\n        assert result\n    finally:\n        del sys.modules[\"FAKE_MODULE_FOR_TEST\"]\n\n\n@pytest.mark.skipif(\n    sys.platform == \"win32\", reason=\"Windows doesn't support different contexts\"\n)\ndef test_get_context_using_python3_posix():\n    \"\"\"get_context() respects configuration.\n\n    If default context is changed this test will need to change too.\n    \"\"\"\n    assert get_context() is multiprocessing.get_context(\"spawn\")\n    with dask.config.set({\"multiprocessing.context\": \"forkserver\"}):\n        assert get_context() is multiprocessing.get_context(\"forkserver\")\n    with dask.config.set({\"multiprocessing.context\": \"fork\"}):\n        assert get_context() is multiprocessing.get_context(\"fork\")\n\n\n@pytest.mark.skipif(sys.platform != \"win32\", reason=\"POSIX supports different contexts\")\ndef test_custom_context_ignored_elsewhere():\n    \"\"\"On Windows, setting 'multiprocessing.context' doesn't explode.\n\n    Presumption is it's not used since it's unsupported, but mostly we care about\n    not breaking anything.\n    \"\"\"\n    assert get({\"x\": (inc, 1)}, \"x\") == 2\n    with pytest.warns(UserWarning):\n        with dask.config.set({\"multiprocessing.context\": \"forkserver\"}):\n            assert get({\"x\": (inc, 1)}, \"x\") == 2\n\n\n@pytest.mark.skipif(sys.platform != \"win32\", reason=\"POSIX supports different contexts\")\ndef test_get_context_always_default():\n    \"\"\"On Python 2/Windows, get_context() always returns same context.\"\"\"\n    assert get_context() is multiprocessing\n    with pytest.warns(UserWarning):\n        with dask.config.set({\"multiprocessing.context\": \"forkserver\"}):\n            assert get_context() is multiprocessing\n\n```"}]}
